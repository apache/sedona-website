{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#01172024-sedona-151-released-it-adds-spark-35-support-snowflake-support-and-many-more-raster-and-vector-functions","title":"01/17/2024: Sedona 1.5.1 released. It adds Spark 3.5 support, Snowflake support, and many more raster and vector functions!","text":""},{"location":"#10122023-sedona-150-released-it-adds-comprehensive-raster-data-etl-and-analytics-native-support-of-uber-h3-functions-and-sedonakepler-sedonapydeck-for-interactive-map-visualization","title":"10/12/2023: Sedona 1.5.0 released. It adds comprehensive raster data ETL and analytics, native support of Uber H3 functions, and SedonaKepler / SedonaPyDeck for interactive map visualization","text":""},{"location":"#06252023-sedona-141-released-it-adds-geodesic-geography-functions-more-raster-functions-and-support-spark-34","title":"06/25/2023: Sedona 1.4.1 released. It adds geodesic / geography functions, more raster functions and support Spark 3.4.","text":""},{"location":"#03192023-sedona-140-released-it-provides-geoparquet-filter-pushdown-10x-less-memory-footprint-faster-serialization-3x-speed-s2-based-fast-approximate-join-and-enhanced-r-language-support","title":"03/19/2023: Sedona 1.4.0 released. It provides GeoParquet filter pushdown (10X less memory footprint), faster serialization (3X speed), S2-based fast approximate join and enhanced R language support","text":""},{"location":"download/","title":"Download","text":""},{"location":"download/#github-repository","title":"GitHub repository","text":"<p>Latest source code: GitHub repository</p> <p>Old GeoSpark releases: GitHub releases</p> <p>Automatically generated binary JARs (per each Master branch commit): GitHub Action</p>"},{"location":"download/#verify-the-integrity","title":"Verify the integrity","text":"<p>Public keys</p> <p>Instructions</p>"},{"location":"download/#versions","title":"Versions","text":""},{"location":"download/#151","title":"1.5.1","text":"Download from ASF Checksum Signature Source code src sha512 asc Binary bin sha512 asc"},{"location":"download/#150","title":"1.5.0","text":"Download from ASF Checksum Signature Source code src sha512 asc Binary bin sha512 asc"},{"location":"download/#141","title":"1.4.1","text":"Download from ASF Checksum Signature Source code src sha512 asc Binary bin sha512 asc"},{"location":"download/#past-releases","title":"Past releases","text":"<p>Past Sedona releases are archived and can be found here: Apache archive (on and after 1.2.1-incubating).</p>"},{"location":"download/#security","title":"Security","text":"<p>For security issues, please refer to https://www.apache.org/security/</p>"},{"location":"api/java-api/","title":"Scala/Java doc","text":"<p>Please read Javadoc</p> <p>Note: Scala can call Java APIs seamlessly. That means Scala users use the same APIs with Java users</p>"},{"location":"api/python-api/","title":"Python api","text":"<p>Will be available soon.</p>"},{"location":"api/flink/Aggregator/","title":"Aggregator","text":""},{"location":"api/flink/Aggregator/#st_envelope_aggr","title":"ST_Envelope_Aggr","text":"<p>Introduction: Return the entire envelope boundary of all geometries in A</p> <p>Format: <code>ST_Envelope_Aggr (A: geometryColumn)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Envelope_Aggr(ST_GeomFromText('MULTIPOINT(1.1 101.1,2.1 102.1,3.1 103.1,4.1 104.1,5.1 105.1,6.1 106.1,7.1 107.1,8.1 108.1,9.1 109.1,10.1 110.1)'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((1.1 101.1, 1.1 120.1, 20.1 120.1, 20.1 101.1, 1.1 101.1))\n</code></pre>"},{"location":"api/flink/Aggregator/#st_intersection_aggr","title":"ST_Intersection_Aggr","text":"<p>Introduction: Return the polygon intersection of all polygons in A</p> <p>Format: <code>ST_Intersection_Aggr (A: geometryColumn)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Intersection_Aggr(ST_GeomFromText('MULTIPOINT(1.1 101.1,2.1 102.1,3.1 103.1,4.1 104.1,5.1 105.1,6.1 106.1,7.1 107.1,8.1 108.1,9.1 109.1,10.1 110.1)'))\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT ((1.1 101.1), (2.1 102.1), (3.1 103.1), (4.1 104.1), (5.1 105.1), (6.1 106.1), (7.1 107.1), (8.1 108.1), (9.1 109.1), (10.1 110.1))\n</code></pre>"},{"location":"api/flink/Aggregator/#st_union_aggr","title":"ST_Union_Aggr","text":"<p>Introduction: Return the polygon union of all polygons in A. All inputs must be polygons.</p> <p>Format: <code>ST_Union_Aggr (A: geometryColumn)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Union_Aggr(ST_GeomFromText('MULTIPOINT(1.1 101.1,2.1 102.1,3.1 103.1,4.1 104.1,5.1 105.1,6.1 106.1,7.1 107.1,8.1 108.1,9.1 109.1,10.1 110.1)'))\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT ((1.1 101.1), (2.1 102.1), (3.1 103.1), (4.1 104.1), (5.1 105.1), (6.1 106.1), (7.1 107.1), (8.1 108.1), (9.1 109.1), (10.1 110.1))\n</code></pre>"},{"location":"api/flink/Constructor/","title":"Constructor","text":""},{"location":"api/flink/Constructor/#st_geomfromgeohash","title":"ST_GeomFromGeoHash","text":"<p>Introduction: Create Geometry from geohash string and optional precision</p> <p>Format: <code>ST_GeomFromGeoHash(geohash: String, precision: Integer)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_GeomFromGeoHash('s00twy01mt', 4) AS geom\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0.703125 0.87890625, 0.703125 1.0546875, 1.0546875 1.0546875, 1.0546875 0.87890625, 0.703125 0.87890625))\n</code></pre>"},{"location":"api/flink/Constructor/#st_geomfromgeojson","title":"ST_GeomFromGeoJSON","text":"<p>Introduction: Construct a Geometry from GeoJson</p> <p>Format: <code>ST_GeomFromGeoJSON (GeoJson: String)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_GeomFromGeoJSON('{\n   \"type\":\"Feature\",\n   \"properties\":{\n      \"STATEFP\":\"01\",\n      \"COUNTYFP\":\"077\",\n      \"TRACTCE\":\"011501\",\n      \"BLKGRPCE\":\"5\",\n      \"AFFGEOID\":\"1500000US010770115015\",\n      \"GEOID\":\"010770115015\",\n      \"NAME\":\"5\",\n      \"LSAD\":\"BG\",\n      \"ALAND\":6844991,\n      \"AWATER\":32636\n   },\n   \"geometry\":{\n      \"type\":\"Polygon\",\n      \"coordinates\":[\n         [\n            [-87.621765, 34.873444],\n            [-87.617535, 34.873369],\n            [-87.62119, 34.85053],\n            [-87.62144, 34.865379],\n            [-87.621765, 34.873444]\n         ]\n      ]\n   }\n}')\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((-87.621765 34.873444, -87.617535 34.873369, -87.62119 34.85053, -87.62144 34.865379, -87.621765 34.873444))\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_GeomFromGeoJSON('{\n   \"type\":\"Polygon\",\n   \"coordinates\":[\n      [\n         [-87.621765, 34.873444],\n         [-87.617535, 34.873369],\n         [-87.62119, 34.85053],\n         [-87.62144, 34.865379],\n         [-87.621765, 34.873444]\n      ]\n   ]\n}')\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((-87.621765 34.873444, -87.617535 34.873369, -87.62119 34.85053, -87.62144 34.865379, -87.621765 34.873444))\n</code></pre>"},{"location":"api/flink/Constructor/#st_geomfromgml","title":"ST_GeomFromGML","text":"<p>Introduction: Construct a Geometry from GML.</p> <p>Format: <code>ST_GeomFromGML (gml: String)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_GeomFromGML('\n    &lt;gml:LineString srsName=\"EPSG:4269\"&gt;\n        &lt;gml:coordinates&gt;\n            -71.16028,42.258729\n            -71.160837,42.259112\n            -71.161143,42.25932\n        &lt;/gml:coordinates&gt;\n    &lt;/gml:LineString&gt;\n')\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-71.16028 42.258729, -71.160837 42.259112, -71.161143 42.25932)\n</code></pre>"},{"location":"api/flink/Constructor/#st_geomfromkml","title":"ST_GeomFromKML","text":"<p>Introduction: Construct a Geometry from KML.</p> <p>Format: <code>ST_GeomFromKML (kml: String)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_GeomFromKML('\n    &lt;LineString&gt;\n        &lt;coordinates&gt;\n            -71.1663,42.2614\n            -71.1667,42.2616\n        &lt;/coordinates&gt;\n    &lt;/LineString&gt;\n')\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-71.1663 42.2614, -71.1667 42.2616)\n</code></pre>"},{"location":"api/flink/Constructor/#st_geomfromtext","title":"ST_GeomFromText","text":"<p>Introduction: Construct a Geometry from WKT. Alias of  ST_GeomFromWKT</p> <p>Format: <code>ST_GeomFromText (Wkt: String)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_GeomFromText('POINT(40.7128 -74.0060)')\n</code></pre> <p>Output:</p> <pre><code>POINT(40.7128 -74.006)\n</code></pre>"},{"location":"api/flink/Constructor/#st_geomfromwkb","title":"ST_GeomFromWKB","text":"<p>Introduction: Construct a Geometry from WKB string or Binary. This function also supports EWKB format.</p> <p>Format:</p> <p><code>ST_GeomFromWKB (Wkb: String)</code></p> <p><code>ST_GeomFromWKB (Wkb: Binary)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_GeomFromWKB([01 02 00 00 00 02 00 00 00 00 00 00 00 84 D6 00 C0 00 00 00 00 80 B5 D6 BF 00 00 00 60 E1 EF F7 BF 00 00 00 80 07 5D E5 BF])\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-2.1047439575195312 -0.354827880859375, -1.49606454372406 -0.6676061153411865)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_asEWKT(ST_GeomFromWKB('01010000a0e6100000000000000000f03f000000000000f03f000000000000f03f'))\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POINT Z(1 1 1)\n</code></pre> <p>Format: <code>ST_GeomFromWKB (Wkb: Bytes)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example: <pre><code>SELECT ST_GeomFromWKB(polygontable._c0) AS polygonshape\nFROM polygontable\n</code></pre></p>"},{"location":"api/flink/Constructor/#st_geomfromwkt","title":"ST_GeomFromWKT","text":"<p>Introduction: Construct a Geometry from WKT</p> <p>Format: <code>ST_GeomFromWKT (Wkt: String)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example: <pre><code>SELECT ST_GeomFromWKT('POINT(40.7128 -74.0060)')\n</code></pre></p> <p>Output:</p> <pre><code>POINT(40.7128 -74.006)\n</code></pre>"},{"location":"api/flink/Constructor/#st_geomfromewkt","title":"ST_GeomFromEWKT","text":"<p>Introduction: Construct a Geometry from OGC Extended WKT</p> <p>Format: <code>ST_GeomFromEWKT (EWkt: String)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example: <pre><code>SELECT ST_AsText(ST_GeomFromEWKT('SRID=4269;POINT(40.7128 -74.0060)'))\n</code></pre></p> <p>Output:</p> <pre><code>POINT(40.7128 -74.006)\n</code></pre>"},{"location":"api/flink/Constructor/#st_linefromtext","title":"ST_LineFromText","text":"<p>Introduction: Construct a LineString from Text</p> <p>Format: <code>ST_LineFromText (Text: String)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example: <pre><code>SELECT ST_LineFromText('Linestring(1 2, 3 4)')\n</code></pre></p> <p>Output:</p> <pre><code>LINESTRING (1 2, 3 4)\n</code></pre>"},{"location":"api/flink/Constructor/#st_linestringfromtext","title":"ST_LineStringFromText","text":"<p>Introduction: Construct a LineString from Text, delimited by Delimiter (Optional). Alias of  ST_LineFromText</p> <p>Format: <code>ST_LineStringFromText (Text: String, Delimiter: Char)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_LineStringFromText('-74.0428197,40.6867969,-74.0421975,40.6921336,-74.0508020,40.6912794', ',')\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-74.0428197 40.6867969, -74.0421975 40.6921336, -74.050802 40.6912794)\n</code></pre>"},{"location":"api/flink/Constructor/#st_makepoint","title":"ST_MakePoint","text":"<p>Introduction: Creates a 2D, 3D Z or 4D ZM Point geometry. Use ST_MakePointM to make points with XYM coordinates. Z and M values are optional.</p> <p>Format: <code>ST_MakePoint (X: Double, Y: Double, Z: Double, M: Double)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_MakePoint(1.2345, 2.3456));\n</code></pre> <p>Output:</p> <pre><code>POINT (1.2345 2.3456)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_MakePoint(1.2345, 2.3456, 3.4567));\n</code></pre> <p>Output:</p> <pre><code>POINT Z (1.2345 2.3456 3.4567)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_MakePoint(1.2345, 2.3456, 3.4567, 4));\n</code></pre> <p>Output:</p> <pre><code>POINT ZM (1.2345 2.3456 3.4567 4)\n</code></pre>"},{"location":"api/flink/Constructor/#st_mlinefromtext","title":"ST_MLineFromText","text":"<p>Introduction: Construct a MultiLineString from Text and Optional SRID</p> <p>Format: <code>ST_MLineFromText (Text: String, Srid: Integer)</code></p> <p>Since: <code>1.3.1</code></p> <p>Example:</p> <pre><code>SELECT ST_MLineFromText('MULTILINESTRING((1 2, 3 4), (4 5, 6 7))')\n</code></pre> <p>Output:</p> <pre><code>MULTILINESTRING ((1 2, 3 4), (4 5, 6 7))\n</code></pre>"},{"location":"api/flink/Constructor/#st_mpolyfromtext","title":"ST_MPolyFromText","text":"<p>Introduction: Construct a MultiPolygon from Text and Optional SRID</p> <p>Format: <code>ST_MPolyFromText (Text: String, Srid: Integer)</code></p> <p>Since: <code>1.3.1</code></p> <p>Example:</p> <pre><code>SELECT ST_MPolyFromText('MULTIPOLYGON(((0 0 1,20 0 1,20 20 1,0 20 1,0 0 1),(5 5 3,5 7 3,7 7 3,7 5 3,5 5 3)))')\n</code></pre> <p>Output:</p> <pre><code>MULTIPOLYGON (((0 0, 20 0, 20 20, 0 20, 0 0), (5 5, 5 7, 7 7, 7 5, 5 5)))\n</code></pre>"},{"location":"api/flink/Constructor/#st_point","title":"ST_Point","text":"<p>Introduction: Construct a Point from X and Y</p> <p>Format: <code>ST_Point (X: Double, Y: Double)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_Point(double(1.2345), 2.3456)\n</code></pre> <p>Output:</p> <pre><code>POINT (1.2345 2.3456)\n</code></pre>"},{"location":"api/flink/Constructor/#st_pointz","title":"ST_PointZ","text":"<p>Introduction: Construct a Point from X, Y and Z and an optional srid. If srid is not set, it defaults to 0 (unknown). Must use ST_AsEWKT function to print the Z coordinate.</p> <p>Format:</p> <p><code>ST_PointZ (X: Double, Y: Double, Z: Double)</code></p> <p><code>ST_PointZ (X: Double, Y: Double, Z: Double, srid: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsEWKT(ST_PointZ(1.2345, 2.3456, 3.4567))\n</code></pre> <p>Output:</p> <pre><code>POINT Z(1.2345 2.3456 3.4567)\n</code></pre>"},{"location":"api/flink/Constructor/#st_pointfromtext","title":"ST_PointFromText","text":"<p>Introduction: Construct a Point from Text, delimited by Delimiter</p> <p>Format: <code>ST_PointFromText (Text: String, Delimiter: Char)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_PointFromText('40.7128,-74.0060', ',')\n</code></pre> <p>Output:</p> <pre><code>POINT (40.7128 -74.006)\n</code></pre>"},{"location":"api/flink/Constructor/#st_polygonfromenvelope","title":"ST_PolygonFromEnvelope","text":"<p>Introduction: Construct a Polygon from MinX, MinY, MaxX, MaxY.</p> <p>Format:</p> <p><code>ST_PolygonFromEnvelope (MinX: Double, MinY: Double, MaxX: Double, MaxY: Double)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_PolygonFromEnvelope(double(1.234),double(2.234),double(3.345),double(3.345))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((1.234 2.234, 1.234 3.345, 3.345 3.345, 3.345 2.234, 1.234 2.234))\n</code></pre>"},{"location":"api/flink/Constructor/#st_polygonfromtext","title":"ST_PolygonFromText","text":"<p>Introduction: Construct a Polygon from Text, delimited by Delimiter. Path must be closed</p> <p>Format: <code>ST_PolygonFromText (Text: String, Delimiter: Char)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_PolygonFromText('-74.0428197,40.6867969,-74.0421975,40.6921336,-74.0508020,40.6912794,-74.0428197,40.6867969', ',')\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((-74.0428197 40.6867969, -74.0421975 40.6921336, -74.050802 40.6912794, -74.0428197 40.6867969))\n</code></pre>"},{"location":"api/flink/Function/","title":"Function","text":""},{"location":"api/flink/Function/#geometrytype","title":"GeometryType","text":"<p>Introduction: Returns the type of the geometry as a string. Eg: 'LINESTRING', 'POLYGON', 'MULTIPOINT', etc. This function also indicates if the geometry is measured, by returning a string of the form 'POINTM'.</p> <p>Format: <code>GeometryType (A: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT GeometryType(ST_GeomFromText('LINESTRING(77.29 29.07,77.42 29.26,77.27 29.31,77.29 29.07)'));\n</code></pre> <p>Result:</p> <pre><code> geometrytype\n--------------\n LINESTRING\n</code></pre> <pre><code>SELECT GeometryType(ST_GeomFromText('POINTM(0 0 1)'));\n</code></pre> <p>Result:</p> <pre><code> geometrytype\n--------------\n POINTM\n</code></pre>"},{"location":"api/flink/Function/#st_3ddistance","title":"ST_3DDistance","text":"<p>Introduction: Return the 3-dimensional minimum cartesian distance between A and B</p> <p>Format: <code>ST_3DDistance (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_3DDistance(ST_GeomFromText(\"POINT Z (0 0 -5)\"),\nST_GeomFromText(\"POINT Z(1  1 -6\"))\n</code></pre> <p>Output: <pre><code>1.7320508075688772\n</code></pre></p>"},{"location":"api/flink/Function/#st_addpoint","title":"ST_AddPoint","text":"<p>Introduction: Return Linestring with additional point at the given index, if position is not available the point will be added at the end of line.</p> <p>Format:</p> <p><code>ST_AddPoint(geom: Geometry, point: Geometry, position: Integer)</code></p> <p><code>ST_AddPoint(geom: Geometry, point: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example: <pre><code>SELECT ST_AddPoint(ST_GeomFromText(\"LINESTRING(0 0, 1 1, 1 0)\"), ST_GeomFromText(\"Point(21 52)\"), 1)\n\nSELECT ST_AddPoint(ST_GeomFromText(\"Linestring(0 0, 1 1, 1 0)\"), ST_GeomFromText(\"Point(21 52)\"))\n</code></pre></p> <p>Output: <pre><code>LINESTRING(0 0, 21 52, 1 1, 1 0)\nLINESTRING(0 0, 1 1, 1 0, 21 52)\n</code></pre></p>"},{"location":"api/flink/Function/#st_affine","title":"ST_Affine","text":"<p>Introduction: Apply an affine transformation to the given geometry.</p> <p>ST_Affine has 2 overloaded signatures:</p> <p><code>ST_Affine(geometry, a, b, c, d, e, f, g, h, i, xOff, yOff, zOff)</code></p> <p><code>ST_Affine(geometry, a, b, d, e, xOff, yOff)</code></p> <p>Based on the invoked function, the following transformation is applied:</p> <p><code>x = a * x + b * y + c * z + xOff OR x = a * x + b * y + xOff</code></p> <p><code>y = d * x + e * y + f * z + yOff OR y = d * x + e * y + yOff</code></p> <p><code>z = g * x + f * y + i * z + zOff OR z = g * x + f * y + zOff</code></p> <p>If the given geometry is empty, the result is also empty.</p> <p>Format:</p> <p><code>ST_Affine(geometry, a, b, c, d, e, f, g, h, i, xOff, yOff, zOff)</code></p> <p><code>ST_Affine(geometry, a, b, d, e, xOff, yOff)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Examples:</p> <pre><code>ST_Affine(geometry, 1, 2, 4, 1, 1, 2, 3, 2, 5, 4, 8, 3)\n</code></pre> <p>Input: <code>LINESTRING EMPTY</code></p> <p>Output: <code>LINESTRING EMPTY</code></p> <p>Input: <code>POLYGON ((1 0 1, 1 1 1, 2 2 2, 1 0 1))</code></p> <p>Output: <code>POLYGON Z((9 11 11, 11 12 13, 18 16 23, 9 11 11))</code></p> <p>Input: <code>POLYGON ((1 0, 1 1, 2 1, 2 0, 1 0), (1 0.5, 1 0.75, 1.5 0.75, 1.5 0.5, 1 0.5))</code></p> <p>Output: <code>POLYGON((5 9, 7 10, 8 11, 6 10, 5 9), (6 9.5, 6.5 9.75, 7 10.25, 6.5 10, 6 9.5))</code></p> <pre><code>ST_Affine(geometry, 1, 2, 1, 2, 1, 2)\n</code></pre> <p>Input: <code>POLYGON EMPTY</code></p> <p>Output: <code>POLYGON EMPTY</code></p> <p>Input: <code>GEOMETRYCOLLECTION (MULTIPOLYGON (((1 0, 1 1, 2 1, 2 0, 1 0), (1 0.5, 1 0.75, 1.5 0.75, 1.5 0.5, 1 0.5)), ((5 0, 5 5, 7 5, 7 0, 5 0))), POINT (10 10))</code></p> <p>Output: <code>GEOMETRYCOLLECTION (MULTIPOLYGON (((2 3, 4 5, 5 6, 3 4, 2 3), (3 4, 3.5 4.5, 4 5, 3.5 4.5, 3 4)), ((6 7, 16 17, 18 19, 8 9, 6 7))), POINT (31 32))</code></p> <p>Input: <code>POLYGON ((1 0 1, 1 1 1, 2 2 2, 1 0 1))</code></p> <p>Output: <code>POLYGON Z((2 3 1, 4 5 1, 7 8 2, 2 3 1))</code></p>"},{"location":"api/flink/Function/#st_angle","title":"ST_Angle","text":"<p>Introduction: Compute and return the angle between two vectors represented by the provided points or linestrings.</p> <p>There are three variants possible for ST_Angle:</p> <p><code>ST_Angle(point1: Geometry, point2: Geometry, point3: Geometry, point4: Geometry)</code></p> <p>Computes the angle formed by vectors represented by point1 - point2 and point3 - point4</p> <p><code>ST_Angle(point1: Geometry, point2: Geometry, point3: Geometry)</code></p> <p>Computes the angle formed by vectors represented by point2 - point1 and point2 - point3</p> <p><code>ST_Angle(line1: Geometry, line2: Geometry)</code></p> <p>Computes the angle formed by vectors S1 - E1 and S2 - E2, where S and E denote start and end points respectively</p> <p>Note</p> <p>If any other geometry type is provided, ST_Angle throws an IllegalArgumentException. Additionally, if any of the provided geometry is empty, ST_Angle throws an IllegalArgumentException.</p> <p>Note</p> <p>If a 3D geometry is provided, ST_Angle computes the angle ignoring the z ordinate, equivalent to calling ST_Angle for corresponding 2D geometries.</p> <p>Tip</p> <p>ST_Angle returns the angle in radian between 0 and 2\\Pi. To convert the angle to degrees, use ST_Degrees.</p> <p>Format: <code>ST_Angle(p1, p2, p3, p4) | ST_Angle(p1, p2, p3) | ST_Angle(line1, line2)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Angle(ST_GeomFromWKT('POINT(0 0)'), ST_GeomFromWKT('POINT (1 1)'), ST_GeomFromWKT('POINT(1 0)'), ST_GeomFromWKT('POINT(6 2)'))\n</code></pre> <p>Output:</p> <pre><code>0.4048917862850834\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_Angle(ST_GeomFromWKT('POINT (1 1)'), ST_GeomFromWKT('POINT (0 0)'), ST_GeomFromWKT('POINT(3 2)'))\n</code></pre> <p>Output:</p> <pre><code>0.19739555984988044\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_Angle(ST_GeomFromWKT('LINESTRING (0 0, 1 1)'), ST_GeomFromWKT('LINESTRING (0 0, 3 2)'))\n</code></pre> <p>Output:</p> <pre><code>0.19739555984988044\n</code></pre>"},{"location":"api/flink/Function/#st_area","title":"ST_Area","text":"<p>Introduction: Return the area of A</p> <p>Format: <code>ST_Area (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Area(ST_GeomFromText(\"POLYGON(0 0, 0 10, 10 10, 0 10, 0 0)\"))\n</code></pre> <p>Output: <pre><code>10\n</code></pre></p>"},{"location":"api/flink/Function/#st_areaspheroid","title":"ST_AreaSpheroid","text":"<p>Introduction: Return the geodesic area of A using WGS84 spheroid. Unit is meter. Works better for large geometries (country level) compared to <code>ST_Area</code> + <code>ST_Transform</code>. It is equivalent to PostGIS <code>ST_Area(geography, use_spheroid=true)</code> function and produces nearly identical results.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lon/lat order. You can use ST_FlipCoordinates to swap lat and lon.</p> <p>Note</p> <p>By default, this function uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order.</p> <p>Format: <code>ST_AreaSpheroid (A: Geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Example:</p> <pre><code>SELECT ST_AreaSpheroid(ST_GeomFromWKT('Polygon ((34 35, 28 30, 25 34, 34 35))'))\n</code></pre> <p>Output:</p> <pre><code>201824850811.76245\n</code></pre>"},{"location":"api/flink/Function/#st_asbinary","title":"ST_AsBinary","text":"<p>Introduction: Return the Well-Known Binary representation of a geometry</p> <p>Format: <code>ST_AsBinary (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsBinary(ST_GeomFromWKT('POINT (1 1)'))\n</code></pre> <p>Output:</p> <pre><code>0101000000000000000000f87f000000000000f87f\n</code></pre>"},{"location":"api/flink/Function/#st_asewkb","title":"ST_AsEWKB","text":"<p>Introduction: Return the Extended Well-Known Binary representation of a geometry. EWKB is an extended version of WKB which includes the SRID of the geometry. The format originated in PostGIS but is supported by many GIS tools. If the geometry is lacking SRID a WKB format is produced. It will ignore the M coordinate if present.</p> <p>Format: <code>ST_AsEWKB (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsEWKB(ST_SetSrid(ST_GeomFromWKT('POINT (1 1)'), 3021))\n</code></pre> <p>Output:</p> <pre><code>0101000020cd0b0000000000000000f03f000000000000f03f\n</code></pre>"},{"location":"api/flink/Function/#st_asewkt","title":"ST_AsEWKT","text":"<p>Introduction: Return the Extended Well-Known Text representation of a geometry. EWKT is an extended version of WKT which includes the SRID of the geometry. The format originated in PostGIS but is supported by many GIS tools. If the geometry is lacking SRID a WKT format is produced. See ST_SetSRID It will support M coordinate if present since v1.5.0.</p> <p>Format: <code>ST_AsEWKT (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example: <pre><code>SELECT ST_AsEWKT(ST_SetSrid(ST_GeomFromWKT('POLYGON((0 0,0 1,1 1,1 0,0 0))'), 4326))\n</code></pre></p> <p>Output:</p> <pre><code>SRID=4326;POLYGON ((0 0, 0 1, 1 1, 1 0, 0 0))\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsEWKT(ST_MakePointM(1.0, 1.0, 1.0))\n</code></pre> <p>Output:</p> <pre><code>POINT M(1 1 1)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsEWKT(ST_MakePoint(1.0, 1.0, 1.0, 1.0))\n</code></pre> <p>Output:</p> <pre><code>POINT ZM(1 1 1 1)\n</code></pre>"},{"location":"api/flink/Function/#st_asgeojson","title":"ST_AsGeoJSON","text":"<p>Introduction: Return the GeoJSON string representation of a geometry</p> <p>Format: <code>ST_AsGeoJSON (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example: <pre><code>SELECT ST_AsGeoJSON(ST_GeomFromWKT('POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))'))\n</code></pre></p> <p>Output:</p> <pre><code>{\n\"type\":\"Polygon\",\n\"coordinates\":[\n[[1.0,1.0],\n[8.0,1.0],\n[8.0,8.0],\n[1.0,8.0],\n[1.0,1.0]]\n]\n}\n</code></pre>"},{"location":"api/flink/Function/#st_asgml","title":"ST_AsGML","text":"<p>Introduction: Return the GML string representation of a geometry</p> <p>Format: <code>ST_AsGML (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsGML(ST_GeomFromWKT('POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>1.0,1.0 8.0,1.0 8.0,8.0 1.0,8.0 1.0,1.0\n</code></pre>"},{"location":"api/flink/Function/#st_askml","title":"ST_AsKML","text":"<p>Introduction: Return the KML string representation of a geometry</p> <p>Format: <code>ST_AsKML (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsKML(ST_GeomFromWKT('POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>1.0,1.0 8.0,1.0 8.0,8.0 1.0,8.0 1.0,1.0\n</code></pre>"},{"location":"api/flink/Function/#st_astext","title":"ST_AsText","text":"<p>Introduction: Return the Well-Known Text string representation of a geometry. It will support M coordinate if present since v1.5.0.</p> <p>Format: <code>ST_AsText (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_SetSRID(ST_Point(1.0,1.0), 3021))\n</code></pre> <p>Output:</p> <pre><code>POINT (1 1)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_MakePointM(1.0, 1.0, 1.0))\n</code></pre> <p>Output:</p> <pre><code>POINT M(1 1 1)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_MakePoint(1.0, 1.0, 1.0, 1.0))\n</code></pre> <p>Output:</p> <pre><code>POINT ZM(1 1 1 1)\n</code></pre>"},{"location":"api/flink/Function/#st_azimuth","title":"ST_Azimuth","text":"<p>Introduction: Returns Azimuth for two given points in radians null otherwise.</p> <p>Format: <code>ST_Azimuth(pointA: Point, pointB: Point)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Azimuth(ST_POINT(0.0, 25.0), ST_POINT(0.0, 0.0))\n</code></pre> <p>Output:</p> <pre><code>3.141592653589793\n</code></pre>"},{"location":"api/flink/Function/#st_boundary","title":"ST_Boundary","text":"<p>Introduction: Returns the closure of the combinatorial boundary of this Geometry.</p> <p>Format: <code>ST_Boundary(geom: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Boundary(ST_GeomFromText('POLYGON ((1 1, 0 0, -1 1, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>LINEARRING (1 1, 0 0, -1 1, 1 1)\n</code></pre>"},{"location":"api/flink/Function/#st_boundingdiagonal","title":"ST_BoundingDiagonal","text":"<p>Introduction: Returns a linestring spanning minimum and maximum values of each dimension of the given geometry's coordinates as its start and end point respectively. If an empty geometry is provided, the returned LineString is also empty. If a single vertex (POINT) is provided, the returned LineString has both the start and end points same as the points coordinates</p> <p>Format: <code>ST_BoundingDiagonal(geom: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example: <pre><code>SELECT ST_BoundingDiagonal(ST_GeomFromWKT(geom))\n</code></pre></p> <p>Input: <code>POLYGON ((1 1 1, 3 3 3, 0 1 4, 4 4 0, 1 1 1))</code></p> <p>Output: <code>LINESTRING Z(0 1 1, 4 4 4)</code></p> <p>Input: <code>POINT (10 10)</code></p> <p>Output: <code>LINESTRING (10 10, 10 10)</code></p> <p>Input: <code>GEOMETRYCOLLECTION(POLYGON ((5 5 5, -1 2 3, -1 -1 0, 5 5 5)), POINT (10 3 3))</code></p> <p>Output: <code>LINESTRING Z(-1 -1 0, 10 5 5)</code></p>"},{"location":"api/flink/Function/#st_buffer","title":"ST_Buffer","text":"<p>Introduction: Returns a geometry/geography that represents all points whose distance from this Geometry/geography is less than or equal to distance.</p> <p>The optional third parameter controls the buffer accuracy and style. Buffer accuracy is specified by the number of line segments approximating a quarter circle, with a default of 8 segments. Buffer style can be set by providing blank-separated key=value pairs in a list format.</p> <ul> <li><code>quad_segs=#</code> : Number of line segments utilized to approximate a quarter circle (default is 8).</li> <li><code>endcap=round|flat|square</code> : End cap style (default is <code>round</code>). <code>butt</code> is an accepted synonym for <code>flat</code>.</li> <li><code>join=round|mitre|bevel</code> : Join style (default is <code>round</code>). <code>miter</code> is an accepted synonym for <code>mitre</code>.</li> <li><code>mitre_limit=#.#</code> : mitre ratio limit and it only affects mitred join style. <code>miter_limit</code> is an accepted synonym for <code>mitre_limit</code>.</li> <li><code>side=both|left|right</code> : The option <code>left</code> or <code>right</code> enables a single-sided buffer operation on the geometry, with the buffered side aligned according to the direction of the line. This functionality is specific to LINESTRING geometry and has no impact on POINT or POLYGON geometries. By default, square end caps are applied.</li> </ul> <p>Note</p> <p><code>ST_Buffer</code> throws an <code>IllegalArgumentException</code> if the correct format, parameters, or options are not provided.</p> <p>Format:</p> <pre><code>ST_Buffer (A: Geometry, buffer: Double, bufferStyleParameters: String [Optional])\n</code></pre> <p>Since: <code>v1.5.1</code></p> <p>Example:</p> <pre><code>SELECT ST_Buffer(ST_GeomFromWKT('POINT(0 0)'), 10)\nSELECT ST_Buffer(ST_GeomFromWKT('POINT(0 0)'), 10, 'quad_segs=2')\n</code></pre> <p>Output:</p> <p> </p> <p>8 Segments \u2002 2 Segments</p> <p>Example:</p> <pre><code>SELECT ST_Buffer(ST_GeomFromWKT('LINESTRING(0 0, 50 70, 100 100)'), 10, 'side=left')\n</code></pre> <p>Output:</p> <p> </p> <p>Original Linestring \u2003 Left side buffed Linestring</p>"},{"location":"api/flink/Function/#st_buildarea","title":"ST_BuildArea","text":"<p>Introduction: Returns the areal geometry formed by the constituent linework of the input geometry.</p> <p>Format: <code>ST_BuildArea (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_BuildArea(ST_Collect(smallDf, bigDf)) AS geom\nFROM smallDf, bigDf\n</code></pre> <p>Input: <code>MULTILINESTRING((0 0, 10 0, 10 10, 0 10, 0 0),(10 10, 20 10, 20 20, 10 20, 10 10))</code></p> <p>Output: <code>MULTIPOLYGON(((0 0,0 10,10 10,10 0,0 0)),((10 10,10 20,20 20,20 10,10 10)))</code></p>"},{"location":"api/flink/Function/#st_centroid","title":"ST_Centroid","text":"<p>Introduction: Return the centroid point of A</p> <p>Format: <code>ST_Centroid (A: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example: <pre><code>SELECT ST_Centroid(ST_GeomFromWKT('MULTIPOINT(-1  0, -1 2, 7 8, 9 8, 10 6)'))\n</code></pre></p> <p>Output:</p> <pre><code>POINT (4.8 4.8)\n</code></pre>"},{"location":"api/flink/Function/#st_closestpoint","title":"ST_ClosestPoint","text":"<p>Introduction: Returns the 2-dimensional point on geom1 that is closest to geom2. This is the first point of the shortest line between the geometries. If using 3D geometries, the Z coordinates will be ignored. If you have a 3D Geometry, you may prefer to use ST_3DClosestPoint. It will throw an exception indicates illegal argument if one of the params is an empty geometry.</p> <p>Format: <code>ST_ClosestPoint(g1: Geometry, g2: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example: <pre><code>SELECT ST_AsText( ST_ClosestPoint(g1, g2)) As ptwkt;\n</code></pre></p> <p>Input: <code>g1: POINT (160 40), g2: LINESTRING (10 30, 50 50, 30 110, 70 90, 180 140, 130 190)</code></p> <p>Output: <code>POINT(160 40)</code></p> <p>Input: <code>g1: LINESTRING (10 30, 50 50, 30 110, 70 90, 180 140, 130 190), g2: POINT (160 40)</code></p> <p>Output: <code>POINT(125.75342465753425 115.34246575342466)</code></p> <p>Input: <code>g1: 'POLYGON ((190 150, 20 10, 160 70, 190 150))', g2: ST_Buffer('POINT(80 160)', 30)</code></p> <p>Output: <code>POINT(131.59149149528952 101.89887534906197)</code></p>"},{"location":"api/flink/Function/#st_collect","title":"ST_Collect","text":"<p>Introduction: Returns MultiGeometry object based on geometry column/s or array with geometries</p> <p>Format:</p> <p><code>ST_Collect(*geom: Geometry)</code></p> <p><code>ST_Collect(geom: ARRAY[Geometry])</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Collect(\nST_GeomFromText('POINT(21.427834 52.042576573)'),\nST_GeomFromText('POINT(45.342524 56.342354355)')\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+---------------------------------------------------------------+\n|geom                                                           |\n+---------------------------------------------------------------+\n|MULTIPOINT ((21.427834 52.042576573), (45.342524 56.342354355))|\n+---------------------------------------------------------------+\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_Collect(\nArray(\nST_GeomFromText('POINT(21.427834 52.042576573)'),\nST_GeomFromText('POINT(45.342524 56.342354355)')\n)\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+---------------------------------------------------------------+\n|geom                                                           |\n+---------------------------------------------------------------+\n|MULTIPOINT ((21.427834 52.042576573), (45.342524 56.342354355))|\n+---------------------------------------------------------------+\n</code></pre>"},{"location":"api/flink/Function/#st_collectionextract","title":"ST_CollectionExtract","text":"<p>Introduction: Returns a homogeneous multi-geometry from a given geometry collection.</p> <p>The type numbers are: 1. POINT 2. LINESTRING 3. POLYGON</p> <p>If the type parameter is omitted a multi-geometry of the highest dimension is returned.</p> <p>Format:</p> <p><code>ST_CollectionExtract (A: Geometry)</code></p> <p><code>ST_CollectionExtract (A: Geometry, type: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>WITH test_data as (\nST_GeomFromText(\n'GEOMETRYCOLLECTION(POINT(40 10), POLYGON((0 0, 0 5, 5 5, 5 0, 0 0)))'\n) as geom\n)\nSELECT ST_CollectionExtract(geom) as c1, ST_CollectionExtract(geom, 1) as c2\nFROM test_data\n</code></pre> <p>Result:</p> <pre><code>+----------------------------------------------------------------------------+\n|c1                                        |c2                               |\n+----------------------------------------------------------------------------+\n|MULTIPOLYGON(((0 0, 0 5, 5 5, 5 0, 0 0))) |MULTIPOINT(40 10)                |              |\n+----------------------------------------------------------------------------+\n</code></pre>"},{"location":"api/flink/Function/#st_concavehull","title":"ST_ConcaveHull","text":"<p>Introduction: Return the Concave Hull of polygon A, with alpha set to pctConvex[0, 1] in the Delaunay Triangulation method, the concave hull will not contain a hole unless allowHoles is set to true</p> <p>Format:</p> <p><code>ST_ConcaveHull (A: Geometry, pctConvex: Double)</code></p> <p><code>ST_ConcaveHull (A: Geometry, pctConvex: Double, allowHoles: Boolean)</code></p> <p>Since: <code>v1.4.0</code></p> <p>Example:</p> <pre><code>SELECT ST_ConcaveHull(ST_GeomFromWKT('POLYGON((175 150, 20 40, 50 60, 125 100, 175 150))'), 1)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((125 100, 20 40, 50 60, 175 150, 125 100))\n</code></pre>"},{"location":"api/flink/Function/#st_convexhull","title":"ST_ConvexHull","text":"<p>Introduction: Return the Convex Hull of polygon A</p> <p>Format: <code>ST_ConvexHull (A: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_ConvexHull(ST_GeomFromText('POLYGON((175 150, 20 40, 50 60, 125 100, 175 150))'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((20 40, 175 150, 125 100, 20 40))\n</code></pre>"},{"location":"api/flink/Function/#st_coorddim","title":"ST_CoordDim","text":"<p>Introduction: Returns the coordinate dimensions of the geometry. It is an alias of <code>ST_NDims</code>.</p> <p>Format: <code>ST_CoordDim(geom: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example with x, y, z coordinate:</p> <pre><code>SELECT ST_CoordDim(ST_GeomFromText('POINT(1 1 2'))\n</code></pre> <p>Output:</p> <pre><code>3\n</code></pre> <p>Example with x, y coordinate:</p> <pre><code>SELECT ST_CoordDim(ST_GeomFromWKT('POINT(3 7)'))\n</code></pre> <p>Output:</p> <pre><code>2\n</code></pre>"},{"location":"api/flink/Function/#st_dimension","title":"ST_Dimension","text":"<p>Introduction: Return the topological dimension of this Geometry object, which must be less than or equal to the coordinate dimension. OGC SPEC s2.1.1.1 - returns 0 for POINT, 1 for LINESTRING, 2 for POLYGON, and the largest dimension of the components of a GEOMETRYCOLLECTION. If the dimension is unknown (e.g. for an empty GEOMETRYCOLLECTION) 0 is returned.</p> <p>Format: <code>ST_Dimension (A: Geometry) | ST_Dimension (C: Geometrycollection)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Dimension('GEOMETRYCOLLECTION(LINESTRING(1 1,0 0),POINT(0 0))');\n</code></pre> <p>Result:</p> <pre><code>1\n</code></pre>"},{"location":"api/flink/Function/#st_distance","title":"ST_Distance","text":"<p>Introduction: Return the Euclidean distance between A and B</p> <p>Format: <code>ST_Distance (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Distance(ST_GeomFromText('POINT(72 42)'), ST_GeomFromText('LINESTRING(-72 -42, 82 92)'))\n</code></pre> <p>Output:</p> <pre><code>31.155515639003543\n</code></pre>"},{"location":"api/flink/Function/#st_distancesphere","title":"ST_DistanceSphere","text":"<p>Introduction: Return the haversine / great-circle distance of A using a given earth radius (default radius: 6371008.0). Unit is meter. Works better for large geometries (country level) compared to <code>ST_Distance</code> + <code>ST_Transform</code>. It is equivalent to PostGIS <code>ST_Distance(geography, use_spheroid=false)</code> and <code>ST_DistanceSphere</code> function and produces nearly identical results. It provides faster but less accurate result compared to <code>ST_DistanceSpheroid</code>.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lon/lat order. You can use ST_FlipCoordinates to swap lat and lon. For non-point data, we first take the centroids of both geometries and then compute the distance.</p> <p>Note</p> <p>By default, this function uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order.</p> <p>Format: <code>ST_DistanceSphere (A: Geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Example 1:</p> <pre><code>SELECT ST_DistanceSphere(ST_GeomFromWKT('POINT (-0.56 51.3168)'), ST_GeomFromWKT('POINT (-3.1883 55.9533)'))\n</code></pre> <p>Output:</p> <pre><code>543796.9506134904\n</code></pre> <p>Example 2:</p> <pre><code>SELECT ST_DistanceSphere(ST_GeomFromWKT('POINT (-0.56 51.3168)'), ST_GeomFromWKT('POINT (-3.1883 55.9533)'), 6378137.0)\n</code></pre> <p>Output:</p> <pre><code>544405.4459192449\n</code></pre>"},{"location":"api/flink/Function/#st_distancespheroid","title":"ST_DistanceSpheroid","text":"<p>Introduction: Return the geodesic distance of A using WGS84 spheroid. Unit is meter. Works better for large geometries (country level) compared to <code>ST_Distance</code> + <code>ST_Transform</code>. It is equivalent to PostGIS <code>ST_Distance(geography, use_spheroid=true)</code> and <code>ST_DistanceSpheroid</code> function and produces nearly identical results. It provides slower but more accurate result compared to <code>ST_DistanceSphere</code>.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lon/lat order. You can use ST_FlipCoordinates to swap lat and lon. For non-point data, we first take the centroids of both geometries and then compute the distance.</p> <p>Note</p> <p>By default, this function uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order.</p> <p>Format: <code>ST_DistanceSpheroid (A: Geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Example:</p> <pre><code>SELECT ST_DistanceSpheroid(ST_GeomFromWKT('POINT (-0.56 51.3168)'), ST_GeomFromWKT('POINT (-3.1883 55.9533)'))\n</code></pre> <p>Output:</p> <pre><code>544430.9411996207\n</code></pre>"},{"location":"api/flink/Function/#st_degrees","title":"ST_Degrees","text":"<p>Introduction: Convert an angle in radian to degrees.</p> <p>Format: <code>ST_Degrees(angleInRadian)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Degrees(0.19739555984988044)\n</code></pre> <p>Output:</p> <pre><code>11.309932474020195\n</code></pre>"},{"location":"api/flink/Function/#st_difference","title":"ST_Difference","text":"<p>Introduction: Return the difference between geometry A and B (return part of geometry A that does not intersect geometry B)</p> <p>Format: <code>ST_Difference (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Difference(ST_GeomFromWKT('POLYGON ((-3 -3, 3 -3, 3 3, -3 3, -3 -3))'), ST_GeomFromWKT('POLYGON ((0 -4, 4 -4, 4 4, 0 4, 0 -4))'))\n</code></pre> <p>Result:</p> <pre><code>POLYGON ((0 -3, -3 -3, -3 3, 0 3, 0 -3))\n</code></pre>"},{"location":"api/flink/Function/#st_dump","title":"ST_Dump","text":"<p>Introduction: It expands the geometries. If the geometry is simple (Point, Polygon Linestring etc.) it returns the geometry itself, if the geometry is collection or multi it returns record for each of collection components.</p> <p>Format: <code>ST_Dump(geom: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example: <pre><code>SELECT ST_Dump(ST_GeomFromText('MULTIPOINT ((10 40), (40 30), (20 20), (30 10))'))\n</code></pre></p> <p>Output:</p> <pre><code>[POINT (10 40), POINT (40 30), POINT (20 20), POINT (30 10)]\n</code></pre>"},{"location":"api/flink/Function/#st_dumppoints","title":"ST_DumpPoints","text":"<p>Introduction: Returns list of Points which geometry consists of.</p> <p>Format: <code>ST_DumpPoints(geom: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_DumpPoints(ST_GeomFromText('LINESTRING (0 0, 1 1, 1 0)'))\n</code></pre> <p>Output:</p> <pre><code>[POINT (0 0), POINT (0 1), POINT (1 1), POINT (1 0), POINT (0 0)]\n</code></pre>"},{"location":"api/flink/Function/#st_endpoint","title":"ST_EndPoint","text":"<p>Introduction: Returns last point of given linestring.</p> <p>Format: <code>ST_EndPoint(geom: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_EndPoint(ST_GeomFromText('LINESTRING(100 150,50 60, 70 80, 160 170)'))\n</code></pre> <p>Output:</p> <pre><code>POINT(160 170)\n</code></pre>"},{"location":"api/flink/Function/#st_envelope","title":"ST_Envelope","text":"<p>Introduction: Return the envelope boundary of A</p> <p>Format: <code>ST_Envelope (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Envelope(ST_GeomFromWKT('LINESTRING(0 0, 1 3)'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0 0, 0 3, 1 3, 1 0, 0 0))\n</code></pre>"},{"location":"api/flink/Function/#st_exteriorring","title":"ST_ExteriorRing","text":"<p>Introduction: Returns a LINESTRING representing the exterior ring (shell) of a POLYGON. Returns NULL if the geometry is not a polygon.</p> <p>Format: <code>ST_ExteriorRing(A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_ExteriorRing(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (0 0, 1 1, 1 2, 1 1, 0 0)\n</code></pre>"},{"location":"api/flink/Function/#st_flipcoordinates","title":"ST_FlipCoordinates","text":"<p>Introduction: Returns a version of the given geometry with X and Y axis flipped.</p> <p>Format: <code>ST_FlipCoordinates(A: Geometry)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_FlipCoordinates(ST_GeomFromWKT(\"POINT (1 2)\"))\n</code></pre> <p>Output:</p> <pre><code>POINT (2 1)\n</code></pre>"},{"location":"api/flink/Function/#st_force_2d","title":"ST_Force_2D","text":"<p>Introduction: Forces the geometries into a \"2-dimensional mode\" so that all output representations will only have the X and Y coordinates</p> <p>Format: <code>ST_Force_2D (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_Force_2D(ST_GeomFromText('POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON((0 0,0 5,5 0,0 0),(1 1,3 1,1 3,1 1))\n</code></pre>"},{"location":"api/flink/Function/#st_force3d","title":"ST_Force3D","text":"<p>Introduction: Forces the geometry into a 3-dimensional model so that all output representations will have X, Y and Z coordinates. An optionally given zValue is tacked onto the geometry if the geometry is 2-dimensional. Default value of zValue is 0.0 If the given geometry is 3-dimensional, no change is performed on it. If the given geometry is empty, no change is performed on it.</p> <p>Note</p> <p>Example output is after calling ST_AsText() on returned geometry, which adds Z for in the WKT for 3D geometries</p> <p>Format: <code>ST_Force3D(geometry: Geometry, zValue: Double)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_Force3D(ST_GeomFromText('POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))'), 2.3))\n</code></pre> <p>Output:</p> <pre><code>POLYGON Z((0 0 2, 0 5 2, 5 0 2, 0 0 2), (1 1 2, 3 1 2, 1 3 2, 1 1 2))\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_Force3D(ST_GeomFromText('LINESTRING(0 1,1 0,2 0)'), 2.3))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING Z(0 1 2.3, 1 0 2.3, 2 0 2.3)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_Force3D(ST_GeomFromText('LINESTRING EMPTY'), 3))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING EMPTY\n</code></pre>"},{"location":"api/flink/Function/#st_frechetdistance","title":"ST_FrechetDistance","text":"<p>Introduction: Computes and returns discrete Frechet Distance between the given two geometries, based on Computing Discrete Frechet Distance</p> <p>If any of the geometries is empty, returns 0.0</p> <p>Format: <code>ST_FrechetDistance(g1: Geometry, g2: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_FrechetDistance(ST_GeomFromWKT('POINT (0 1)'), ST_GeomFromWKT('LINESTRING (0 0, 1 0, 2 0, 3 0, 4 0, 5 0)'))\n</code></pre> <p>Output:</p> <pre><code>5.0990195135927845\n</code></pre>"},{"location":"api/flink/Function/#st_geohash","title":"ST_GeoHash","text":"<p>Introduction: Returns GeoHash of the geometry with given precision</p> <p>Format: <code>ST_GeoHash(geom: Geometry, precision: Integer)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_GeoHash(ST_GeomFromText('POINT(21.427834 52.042576573)'), 5) AS geohash\n</code></pre> <p>Output:</p> <pre><code>u3r0p\n</code></pre>"},{"location":"api/flink/Function/#st_geometricmedian","title":"ST_GeometricMedian","text":"<p>Introduction: Computes the approximate geometric median of a MultiPoint geometry using the Weiszfeld algorithm. The geometric median provides a centrality measure that is less sensitive to outlier points than the centroid.</p> <p>The algorithm will iterate until the distance change between successive iterations is less than the supplied <code>tolerance</code> parameter. If this condition has not been met after <code>maxIter</code> iterations, the function will produce an error and exit, unless <code>failIfNotConverged</code> is set to <code>false</code>.</p> <p>If a <code>tolerance</code> value is not provided, a default <code>tolerance</code> value is <code>1e-6</code>.</p> <p>Format:</p> <pre><code>ST_GeometricMedian(geom: Geometry, tolerance: Double, maxIter: Integer, failIfNotConverged: Boolean)\n</code></pre> <pre><code>ST_GeometricMedian(geom: Geometry, tolerance: Double, maxIter: Integer)\n</code></pre> <pre><code>ST_GeometricMedian(geom: Geometry, tolerance: Double)\n</code></pre> <pre><code>ST_GeometricMedian(geom: Geometry)\n</code></pre> <p>Default parameters: <code>tolerance: 1e-6, maxIter: 1000, failIfNotConverged: false</code></p> <p>Since: <code>v1.4.1</code></p> <p>Example: <pre><code>SELECT ST_GeometricMedian(ST_GeomFromWKT('MULTIPOINT((0 0), (1 1), (2 2), (200 200))'))\n</code></pre></p> <p>Output: <pre><code>POINT (1.9761550281255005 1.9761550281255005)\n</code></pre></p>"},{"location":"api/flink/Function/#st_geometryn","title":"ST_GeometryN","text":"<p>Introduction: Return the 0-based Nth geometry if the geometry is a GEOMETRYCOLLECTION, (MULTI)POINT, (MULTI)LINESTRING, MULTICURVE or (MULTI)POLYGON. Otherwise, return null</p> <p>Format: <code>ST_GeometryN(geom: Geometry, n: Integer)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_GeometryN(ST_GeomFromText('MULTIPOINT((1 2), (3 4), (5 6), (8 9))'), 1)\n</code></pre> <p>Output:</p> <pre><code>POINT (3 4)\n</code></pre>"},{"location":"api/flink/Function/#st_geometrytype","title":"ST_GeometryType","text":"<p>Introduction: Returns the type of the geometry as a string. EG: 'ST_Linestring', 'ST_Polygon' etc.</p> <p>Format: <code>ST_GeometryType (A: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_GeometryType(ST_GeomFromText('LINESTRING(77.29 29.07,77.42 29.26,77.27 29.31,77.29 29.07)'))\n</code></pre> <p>Output:</p> <pre><code>ST_LINESTRING\n</code></pre>"},{"location":"api/flink/Function/#st_h3celldistance","title":"ST_H3CellDistance","text":"<p>Introduction: return result of h3 function gridDistance(cel1, cell2). As described by H3 documentation</p> <p>Finding the distance can fail because the two indexes are not comparable (different resolutions), too far apart, or are separated by pentagonal distortion. This is the same set of limitations as the local IJ coordinate space functions.</p> <p>In this case, Sedona use in-house implementation of estimation the shortest path and return the size as distance.</p> <p>Format: <code>ST_H3CellDistance(cell1: Long, cell2: Long)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example: <pre><code>select ST_H3CellDistance(ST_H3CellIDs(ST_GeomFromWKT('POINT(1 2)'), 8, true)[1], ST_H3CellIDs(ST_GeomFromWKT('POINT(1.23 1.59)'), 8, true)[1])\n</code></pre></p> <p>Output: <pre><code>+----+----------------------+\n| op |               EXPR$0 |\n+----+----------------------+\n| +I |                   78 |\n+----+----------------------+\n</code></pre></p>"},{"location":"api/flink/Function/#st_h3cellids","title":"ST_H3CellIDs","text":"<p>Introduction: Cover the geometry by H3 cell IDs with the given resolution(level). To understand the cell statistics please refer to H3 Doc H3 native fill functions doesn't guarantee full coverage on the shapes.</p>"},{"location":"api/flink/Function/#cover-polygon","title":"Cover Polygon","text":"<p>When fullCover = false, for polygon sedona will use polygonToCells. This can't guarantee full coverage but will guarantee no false positive.</p> <p>When fullCover = true, sedona will add on extra traversal logic to guarantee full coverage on shapes. This will lead to redundancy but can guarantee full coverage.</p> <p>Choose the option according to your use case.</p>"},{"location":"api/flink/Function/#cover-linestring","title":"Cover LineString","text":"<p>For the lineString, sedona will call gridPathCells(https://h3geo.org/docs/api/traversal#gridpathcells) per segment. From H3's documentation</p> <p>This function may fail to find the line between two indexes, for example if they are very far apart. It may also fail when finding distances for indexes on opposite sides of a pentagon.</p> <p>When the <code>gridPathCells</code> function throw error, Sedona implemented in-house approximate implementation to generate the shortest path, which can cover the corner cases.</p> <p>Both functions can't guarantee full coverage. When the <code>fullCover = true</code>, we'll do extra cell traversal to guarantee full cover. In worst case, sedona will use MBR to guarantee the full coverage.</p> <p>If you seek to get the shortest path between cells, you can call this function with <code>fullCover = false</code></p> <p>Format: <code>ST_H3CellIDs(geom: geometry, level: Int, fullCover: true)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example: <pre><code>SELECT ST_H3CellIDs(ST_GeomFromText('LINESTRING(1 3 4, 5 6 7)'), 6, true)\n</code></pre></p> <p>Output: <pre><code>+----+--------------------------------+\n| op |                         EXPR$0 |\n+----+--------------------------------+\n| +I | [605547539457900543, 605547... |\n+----+--------------------------------+\n</code></pre></p>"},{"location":"api/flink/Function/#st_h3kring","title":"ST_H3KRing","text":"<p>Introduction: return the result of H3 function gridDisk(cell, k).</p> <p>K means <code>the distance of the origin index</code>, <code>gridDisk(cell, k)</code> return cells with distance <code>&lt;=k</code> from the original cell.</p> <p><code>exactRing : Boolean</code>, when set to <code>true</code>, sedona will remove the result of <code>gridDisk(cell, k-1)</code> from the original results, means only keep the cells with distance exactly <code>k</code> from the original cell</p> <p>Format: <code>ST_H3KRing(cell: Long, k: Int, exactRing: Boolean)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example: <pre><code>select ST_H3KRing(ST_H3CellIDs(ST_GeomFromWKT('POINT(1 2)'), 8, true)[1], 1, false), ST_H3KRing(ST_H3CellIDs(ST_GeomFromWKT('POINT(1 2)'), 8, true)[1], 1, true)\n</code></pre></p> <p>Output: <pre><code>+----+--------------------------------+--------------------------------+\n| op |                         EXPR$0 |                         EXPR$1 |\n+----+--------------------------------+--------------------------------+\n| +I | [614552609325318143, 614552... | [614552597293957119, 614552... |\n+----+--------------------------------+--------------------------------+\n</code></pre></p>"},{"location":"api/flink/Function/#st_h3togeom","title":"ST_H3ToGeom","text":"<p>Introduction: return the result of H3 function cellsToMultiPolygon(cells).</p> <p>Reverse the uber h3 cells to MultiPolygon object composed by the geometry hexagons.</p> <p>Format: <code>ST_H3ToGeom(cells: Array[Long])</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example: <pre><code>SELECT ST_H3ToGeom(ST_H3CellIDs(ST_GeomFromWKT('POINT(1 2)'), 8, true)[0], 1, true))\n</code></pre></p> <p>Output: <pre><code>|st_h3togeom(st_h3cellids(st_geomfromwkt(POINT(1 2), 0), 8, true))                                                                                                                                                                                                                              |\n+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|MULTIPOLYGON (((1.0057629565404935 1.9984665139177658, 1.0037116327309032 2.001832524914011, 0.9997277993570498 2.0011632704656668, 0.9977951427833285 1.99712822839324, 0.9998461908217768 1.9937621529331915, 1.0038301712104252 1.9944311839965554, 1.0057629565404935 1.9984665139177658)))|\n+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre></p>"},{"location":"api/flink/Function/#st_hausdorffdistance","title":"ST_HausdorffDistance","text":"<p>Introduction: Returns a discretized (and hence approximate) Hausdorff distance between the given 2 geometries. Optionally, a densityFraction parameter can be specified, which gives more accurate results by densifying segments before computing hausdorff distance between them. Each segment is broken down into equal-length subsegments whose ratio with segment length is closest to the given density fraction.</p> <p>Hence, the lower the densityFrac value, the more accurate is the computed hausdorff distance, and the more time it takes to compute it.</p> <p>If any of the geometry is empty, 0.0 is returned.</p> <p>Note</p> <p>Accepted range of densityFrac is (0.0, 1.0], if any other value is provided, ST_HausdorffDistance throws an IllegalArgumentException</p> <p>Note</p> <p>Even though the function accepts 3D geometry, the z ordinate is ignored and the computed hausdorff distance is equivalent to the geometries not having the z ordinate.</p> <p>Format: <code>ST_HausdorffDistance(g1: Geometry, g2: Geometry, densityFrac: Double)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_HausdorffDistance(ST_GeomFromWKT('POINT (0.0 1.0)'), ST_GeomFromWKT('LINESTRING (0 0, 1 0, 2 0, 3 0, 4 0, 5 0)'), 0.1)\n</code></pre> <p>Output:</p> <pre><code>5.0990195135927845\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_HausdorffDistance(ST_GeomFromText('POLYGON Z((1 0 1, 1 1 2, 2 1 5, 2 0 1, 1 0 1))'), ST_GeomFromText('POLYGON Z((4 0 4, 6 1 4, 6 4 9, 6 1 3, 4 0 4))'))\n</code></pre> <p>Output:</p> <pre><code>5.0\n</code></pre>"},{"location":"api/flink/Function/#st_interiorringn","title":"ST_InteriorRingN","text":"<p>Introduction: Returns the Nth interior linestring ring of the polygon geometry. Returns NULL if the geometry is not a polygon or the given N is out of range</p> <p>Format: <code>ST_InteriorRingN(geom: Geometry, n: Integer)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_InteriorRingN(ST_GeomFromText('POLYGON((0 0, 0 5, 5 5, 5 0, 0 0), (1 1, 2 1, 2 2, 1 2, 1 1), (1 3, 2 3, 2 4, 1 4, 1 3), (3 3, 4 3, 4 4, 3 4, 3 3))'), 0)\n</code></pre> <p>Output:</p> <pre><code>LINEARRING (1 1, 2 1, 2 2, 1 2, 1 1)\n</code></pre>"},{"location":"api/flink/Function/#st_intersection","title":"ST_Intersection","text":"<p>Introduction: Return the intersection geometry of A and B</p> <p>Format: <code>ST_Intersection (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Intersection(\nST_GeomFromWKT(\"POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))\"),\nST_GeomFromWKT(\"POLYGON((2 2, 9 2, 9 9, 2 9, 2 2))\")\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((2 8, 8 8, 8 2, 2 2, 2 8))\n</code></pre>"},{"location":"api/flink/Function/#st_isclosed","title":"ST_IsClosed","text":"<p>Introduction: RETURNS true if the LINESTRING start and end point are the same.</p> <p>Format: <code>ST_IsClosed(geom: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_IsClosed(ST_GeomFromText('LINESTRING(0 0, 1 1, 1 0)'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/flink/Function/#st_iscollection","title":"ST_IsCollection","text":"<p>Introduction: Returns <code>TRUE</code> if the geometry type of the input is a geometry collection type. Collection types are the following:</p> <ul> <li>GEOMETRYCOLLECTION</li> <li>MULTI{POINT, POLYGON, LINESTRING}</li> </ul> <p>Format: <code>ST_IsCollection(geom: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_IsCollection(ST_GeomFromText('MULTIPOINT(0 0), (6 6)'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_IsCollection(ST_GeomFromText('POINT(5 5)'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/flink/Function/#st_isempty","title":"ST_IsEmpty","text":"<p>Introduction: Test if a geometry is empty geometry</p> <p>Format: <code>ST_IsEmpty (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_IsEmpty(ST_GeomFromWKT('POLYGON((0 0,0 1,1 1,1 0,0 0))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/flink/Function/#st_isring","title":"ST_IsRing","text":"<p>Introduction: RETURN true if LINESTRING is ST_IsClosed and ST_IsSimple.</p> <p>Format: <code>ST_IsRing(geom: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_IsRing(ST_GeomFromText(\"LINESTRING(0 0, 0 1, 1 1, 1 0, 0 0)\"))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/flink/Function/#st_issimple","title":"ST_IsSimple","text":"<p>Introduction: Test if geometry's only self-intersections are at boundary points.</p> <p>Format: <code>ST_IsSimple (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_IsSimple(ST_GeomFromWKT('POLYGON((1 1, 3 1, 3 3, 1 3, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/flink/Function/#st_isvalid","title":"ST_IsValid","text":"<p>Introduction: Test if a geometry is well formed. The function can be invoked with just the geometry or with an additional flag (from <code>v1.5.1</code>). The flag alters the validity checking behavior. The flags parameter is a bitfield with the following options:</p> <ul> <li>0 (default): Use usual OGC SFS (Simple Features Specification) validity semantics.</li> <li>1: \"ESRI flag\", Accepts certain self-touching rings as valid, which are considered invalid under OGC standards.</li> </ul> <p>Formats: <pre><code>ST_IsValid (A: Geometry)\n</code></pre> <pre><code>ST_IsValid (A: Geometry, flag: Integer)\n</code></pre></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_IsValid(ST_GeomFromWKT('POLYGON((0 0, 10 0, 10 10, 0 10, 0 0), (15 15, 15 20, 20 20, 20 15, 15 15))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/flink/Function/#st_isvalidreason","title":"ST_IsValidReason","text":"<p>Introduction: Returns text stating if the geometry is valid. If not, it provides a reason why it is invalid. The function can be invoked with just the geometry or with an additional flag. The flag alters the validity checking behavior. The flags parameter is a bitfield with the following options:</p> <ul> <li>0 (default): Use usual OGC SFS (Simple Features Specification) validity semantics.</li> <li>1: \"ESRI flag\", Accepts certain self-touching rings as valid, which are considered invalid under OGC standards.</li> </ul> <p>Formats: <pre><code>ST_IsValidReason (A: Geometry)\n</code></pre> <pre><code>ST_IsValidReason (A: Geometry, flag: Integer)\n</code></pre></p> <p>Since: <code>v1.5.1</code></p> <p>SQL Example for valid geometry:</p> <pre><code>SELECT ST_IsValidReason(ST_GeomFromWKT('POLYGON ((100 100, 100 300, 300 300, 300 100, 100 100))')) as validity_info\n</code></pre> <p>Output:</p> <pre><code>Valid Geometry\n</code></pre> <p>SQL Example for invalid geometries:</p> <pre><code>SELECT gid, ST_IsValidReason(geom) as validity_info\nFROM Geometry_table\nWHERE ST_IsValid(geom) = false\nORDER BY gid\n</code></pre> <p>Output:</p> <pre><code>gid  |                  validity_info\n-----+----------------------------------------------------\n5330 | Self-intersection at or near point (32.0, 5.0, NaN)\n5340 | Self-intersection at or near point (42.0, 5.0, NaN)\n5350 | Self-intersection at or near point (52.0, 5.0, NaN)\n</code></pre>"},{"location":"api/flink/Function/#st_length","title":"ST_Length","text":"<p>Introduction: Return the perimeter of A</p> <p>Format: <code>ST_Length (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Length(ST_GeomFromWKT('LINESTRING(38 16,38 50,65 50,66 16,38 16)'))\n</code></pre> <p>Output:</p> <pre><code>123.0147027033899\n</code></pre>"},{"location":"api/flink/Function/#st_lengthspheroid","title":"ST_LengthSpheroid","text":"<p>Introduction: Return the geodesic perimeter of A using WGS84 spheroid. Unit is meter. Works better for large geometries (country level) compared to <code>ST_Length</code> + <code>ST_Transform</code>. It is equivalent to PostGIS <code>ST_Length(geography, use_spheroid=true)</code> and <code>ST_LengthSpheroid</code> function and produces nearly identical results.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lon/lat order. You can use ST_FlipCoordinates to swap lat and lon.</p> <p>Note</p> <p>By default, this function uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order.</p> <p>Format: <code>ST_LengthSpheroid (A: Geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Example:</p> <pre><code>SELECT ST_LengthSpheroid(ST_GeomFromWKT('Polygon ((0 0, 90 0, 0 0))'))\n</code></pre> <p>Output:</p> <pre><code>20037508.342789244\n</code></pre>"},{"location":"api/flink/Function/#st_linefrommultipoint","title":"ST_LineFromMultiPoint","text":"<p>Introduction: Creates a LineString from a MultiPoint geometry.</p> <p>Format: <code>ST_LineFromMultiPoint (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_LineFromMultiPoint(ST_GeomFromText('MULTIPOINT((10 40), (40 30), (20 20), (30 10))'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (10 40, 40 30, 20 20, 30 10)\n</code></pre>"},{"location":"api/flink/Function/#st_lineinterpolatepoint","title":"ST_LineInterpolatePoint","text":"<p>Introduction: Returns a point interpolated along a line. First argument must be a LINESTRING. Second argument is a Double between 0 and 1 representing fraction of total linestring length the point has to be located.</p> <p>Format: <code>ST_LineInterpolatePoint (geom: Geometry, fraction: Double)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_LineInterpolatePoint(ST_GeomFromWKT('LINESTRING(25 50, 100 125, 150 190)'), 0.2)\n</code></pre> <p>Output: <pre><code>POINT (51.5974135047432 76.5974135047432)\n</code></pre></p>"},{"location":"api/flink/Function/#st_linelocatepoint","title":"ST_LineLocatePoint","text":"<p>Introduction: Returns a double between 0 and 1, representing the location of the closest point on the LineString as a fraction of its total length. The first argument must be a LINESTRING, and the second argument is a POINT geometry.</p> <p>Format: <code>ST_LineLocatePoint(linestring: Geometry, point: Geometry)</code></p> <p>Since: <code>v1.5.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_LineLocatePoint(ST_GeomFromWKT('LINESTRING(0 0, 1 1, 2 2)'), ST_GeomFromWKT('POINT(0 2)'))\n</code></pre> <p>Output: <pre><code>0.5\n</code></pre></p>"},{"location":"api/flink/Function/#st_linemerge","title":"ST_LineMerge","text":"<p>Introduction: Returns a LineString formed by sewing together the constituent line work of a MULTILINESTRING.</p> <p>Note</p> <p>Only works for MULTILINESTRING. Using other geometry will return a GEOMETRYCOLLECTION EMPTY. If the MultiLineString can't be merged, the original MULTILINESTRING is returned.</p> <p>Format: <code>ST_LineMerge (A: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example: <pre><code>SELECT ST_LineMerge(ST_GeomFromWKT('MULTILINESTRING ((-29 -27, -30 -29.7, -45 -33), (-45 -33, -46 -32))'))\n</code></pre></p> <p>Output:</p> <pre><code>LINESTRING (-29 -27, -30 -29.7, -45 -33, -46 -32)\n</code></pre>"},{"location":"api/flink/Function/#st_linesubstring","title":"ST_LineSubstring","text":"<p>Introduction: Return a linestring being a substring of the input one starting and ending at the given fractions of total 2d length. Second and third arguments are Double values between 0 and 1. This only works with LINESTRINGs.</p> <p>Format: <code>ST_LineSubstring (geom: Geometry, startfraction: Double, endfraction: Double)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_LineSubstring(ST_GeomFromWKT('LINESTRING(25 50, 100 125, 150 190)'), 0.333, 0.666)\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (69.28469348539744 94.28469348539744, 100 125, 111.70035626068274 140.21046313888758)\n</code></pre>"},{"location":"api/flink/Function/#st_makeline","title":"ST_MakeLine","text":"<p>Introduction: Creates a LineString containing the points of Point, MultiPoint, or LineString geometries. Other geometry types cause an error.</p> <p>Format:</p> <p><code>ST_MakeLine(geom1: Geometry, geom2: Geometry)</code></p> <p><code>ST_MakeLine(geoms: ARRAY[Geometry])</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText( ST_MakeLine(ST_Point(1,2), ST_Point(3,4)) );\n</code></pre> <p>Output:</p> <pre><code>LINESTRING(1 2,3 4)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsText( ST_MakeLine( 'LINESTRING(0 0, 1 1)', 'LINESTRING(2 2, 3 3)' ) );\n</code></pre> <p>Output:</p> <pre><code> LINESTRING(0 0,1 1,2 2,3 3)\n</code></pre>"},{"location":"api/flink/Function/#st_makepolygon","title":"ST_MakePolygon","text":"<p>Introduction: Function to convert closed linestring to polygon including holes</p> <p>Format: <code>ST_MakePolygon(geom: Geometry, holes: ARRAY[Geometry])</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_MakePolygon(\nST_GeomFromText('LINESTRING(7 -1, 7 6, 9 6, 9 1, 7 -1)'),\nARRAY(ST_GeomFromText('LINESTRING(6 2, 8 2, 8 1, 6 1, 6 2)'))\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((7 -1, 7 6, 9 6, 9 1, 7 -1), (6 2, 8 2, 8 1, 6 1, 6 2))\n</code></pre>"},{"location":"api/flink/Function/#st_makevalid","title":"ST_MakeValid","text":"<p>Introduction: Given an invalid geometry, create a valid representation of the geometry.</p> <p>Collapsed geometries are either converted to empty (keepCollapsed=true) or a valid geometry of lower dimension (keepCollapsed=false). Default is keepCollapsed=false.</p> <p>Format:</p> <p><code>ST_MakeValid (A: Geometry)</code></p> <p><code>ST_MakeValid (A: Geometry, keepCollapsed: Boolean)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>WITH linestring AS (\nSELECT ST_GeomFromWKT('LINESTRING(1 1, 1 1)') AS geom\n) SELECT ST_MakeValid(geom), ST_MakeValid(geom, true) FROM linestring\n</code></pre> <p>Result: <pre><code>+------------------+------------------------+\n|st_makevalid(geom)|st_makevalid(geom, true)|\n+------------------+------------------------+\n|  LINESTRING EMPTY|             POINT (1 1)|\n+------------------+------------------------+\n</code></pre></p>"},{"location":"api/flink/Function/#st_minimumboundingcircle","title":"ST_MinimumBoundingCircle","text":"<p>Introduction: Returns the smallest circle polygon that contains a geometry. The optional quadrantSegments parameter determines how many segments to use per quadrant and the default number of segments is 48.</p> <p>Format:</p> <p><code>ST_MinimumBoundingCircle(geom: Geometry, [Optional] quadrantSegments: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_MinimumBoundingCircle(ST_GeomFromWKT('LINESTRING(0 0, 0 1)'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0.5 0.5, 0.4997322937381828 0.4836404585891119, 0.4989294616193017 0.4672984353849285, 0.4975923633360985 0.4509914298352197, 0.4957224306869052 0.4347369038899742, 0.4933216660424395 0.4185522633027057, 0.4903926402016152 0.4024548389919359, 0.4869384896386668 0.3864618684828134, 0.4829629131445342 0.3705904774487396, 0.4784701678661044 0.3548576613727689, 0.4734650647475528 0.3392802673484192, 0.4679529633786629 0.3238749760393833, 0.4619397662556434 0.3086582838174551, 0.4554319124605879 0.2936464850978027, 0.4484363707663442 0.2788556548904993, 0.4409606321741775 0.2643016315870012, 0.4330127018922194 0.25, 0.4246010907632894 0.2359660746748161, 0.4157348061512726 0.2222148834901989, 0.4064233422958076 0.2087611515660989, 0.3966766701456176 0.1956192854956397, 0.3865052266813685 0.1828033579181773, 0.3759199037394887 0.1703270924499656, 0.3649320363489179 0.1582038489885644, 0.3535533905932738 0.1464466094067263, 0.3417961510114357 0.1350679636510822, 0.3296729075500345 0.1240800962605114, 0.3171966420818228 0.1134947733186316, 0.3043807145043603 0.1033233298543824, 0.2912388484339011 0.0935766577041924, 0.2777851165098012 0.0842651938487274, 0.264033925325184 0.0753989092367106, 0.2500000000000001 0.0669872981077807, 0.2356983684129989 0.0590393678258225, 0.2211443451095007 0.0515636292336559, 0.2063535149021975 0.0445680875394122, 0.1913417161825449 0.0380602337443566, 0.1761250239606168 0.0320470366213372, 0.1607197326515808 0.0265349352524472, 0.1451423386272312 0.0215298321338956, 0.1294095225512605 0.0170370868554659, 0.1135381315171867 0.0130615103613332, 0.0975451610080642 0.0096073597983848, 0.0814477366972944 0.0066783339575605, 0.0652630961100259 0.0042775693130948, 0.0490085701647804 0.0024076366639016, 0.0327015646150716 0.0010705383806983, 0.0163595414108882 0.0002677062618172, 0 0, -0.016359541410888 0.0002677062618172, -0.0327015646150715 0.0010705383806983, -0.0490085701647802 0.0024076366639015, -0.0652630961100257 0.0042775693130948, -0.0814477366972942 0.0066783339575605, -0.097545161008064 0.0096073597983847, -0.1135381315171866 0.0130615103613332, -0.1294095225512603 0.0170370868554658, -0.1451423386272311 0.0215298321338955, -0.1607197326515807 0.0265349352524472, -0.1761250239606166 0.0320470366213371, -0.1913417161825448 0.0380602337443566, -0.2063535149021973 0.044568087539412, -0.2211443451095006 0.0515636292336558, -0.2356983684129987 0.0590393678258224, -0.2499999999999999 0.0669872981077806, -0.264033925325184 0.0753989092367106, -0.277785116509801 0.0842651938487273, -0.291238848433901 0.0935766577041924, -0.3043807145043602 0.1033233298543823, -0.3171966420818227 0.1134947733186314, -0.3296729075500343 0.1240800962605111, -0.3417961510114356 0.1350679636510821, -0.3535533905932737 0.1464466094067262, -0.3649320363489177 0.1582038489885642, -0.3759199037394886 0.1703270924499655, -0.3865052266813683 0.1828033579181771, -0.3966766701456175 0.1956192854956396, -0.4064233422958076 0.2087611515660989, -0.4157348061512725 0.2222148834901987, -0.4246010907632894 0.235966074674816, -0.4330127018922192 0.2499999999999998, -0.4409606321741775 0.264301631587001, -0.4484363707663441 0.2788556548904991, -0.4554319124605878 0.2936464850978025, -0.4619397662556434 0.3086582838174551, -0.4679529633786628 0.3238749760393831, -0.4734650647475528 0.3392802673484191, -0.4784701678661044 0.3548576613727686, -0.4829629131445341 0.3705904774487395, -0.4869384896386668 0.3864618684828132, -0.4903926402016152 0.4024548389919357, -0.4933216660424395 0.4185522633027056, -0.4957224306869052 0.434736903889974, -0.4975923633360984 0.4509914298352196, -0.4989294616193017 0.4672984353849282, -0.4997322937381828 0.4836404585891118, -0.5 0.4999999999999999, -0.4997322937381828 0.5163595414108879, -0.4989294616193017 0.5327015646150715, -0.4975923633360985 0.5490085701647801, -0.4957224306869052 0.5652630961100257, -0.4933216660424395 0.5814477366972941, -0.4903926402016153 0.597545161008064, -0.4869384896386668 0.6135381315171865, -0.4829629131445342 0.6294095225512601, -0.4784701678661045 0.645142338627231, -0.4734650647475529 0.6607197326515806, -0.4679529633786629 0.6761250239606166, -0.4619397662556435 0.6913417161825446, -0.455431912460588 0.7063535149021972, -0.4484363707663442 0.7211443451095005, -0.4409606321741776 0.7356983684129986, -0.4330127018922194 0.7499999999999999, -0.4246010907632896 0.7640339253251838, -0.4157348061512727 0.777785116509801, -0.4064233422958078 0.7912388484339008, -0.3966766701456177 0.8043807145043602, -0.3865052266813686 0.8171966420818226, -0.3759199037394889 0.8296729075500342, -0.3649320363489179 0.8417961510114356, -0.353553390593274 0.8535533905932735, -0.3417961510114358 0.8649320363489177, -0.3296729075500345 0.8759199037394887, -0.317196642081823 0.8865052266813683, -0.3043807145043604 0.8966766701456175, -0.2912388484339011 0.9064233422958076, -0.2777851165098015 0.9157348061512725, -0.2640339253251843 0.9246010907632893, -0.2500000000000002 0.9330127018922192, -0.235698368412999 0.9409606321741775, -0.2211443451095007 0.9484363707663441, -0.2063535149021977 0.9554319124605877, -0.1913417161825452 0.9619397662556433, -0.176125023960617 0.9679529633786628, -0.1607197326515809 0.9734650647475528, -0.1451423386272312 0.9784701678661044, -0.1294095225512608 0.9829629131445341, -0.1135381315171869 0.9869384896386668, -0.0975451610080643 0.9903926402016152, -0.0814477366972945 0.9933216660424395, -0.0652630961100262 0.9957224306869051, -0.0490085701647807 0.9975923633360984, -0.0327015646150718 0.9989294616193017, -0.0163595414108883 0.9997322937381828, -0.0000000000000001 1, 0.0163595414108876 0.9997322937381828, 0.0327015646150712 0.9989294616193019, 0.04900857016478 0.9975923633360985, 0.0652630961100256 0.9957224306869052, 0.0814477366972943 0.9933216660424395, 0.0975451610080637 0.9903926402016153, 0.1135381315171863 0.9869384896386669, 0.1294095225512601 0.9829629131445342, 0.145142338627231 0.9784701678661045, 0.1607197326515807 0.9734650647475529, 0.1761250239606164 0.967952963378663, 0.1913417161825446 0.9619397662556435, 0.2063535149021972 0.955431912460588, 0.2211443451095005 0.9484363707663442, 0.2356983684129984 0.9409606321741777, 0.2499999999999997 0.9330127018922195, 0.2640339253251837 0.9246010907632896, 0.2777851165098009 0.9157348061512727, 0.291238848433901 0.9064233422958077, 0.3043807145043599 0.8966766701456179, 0.3171966420818225 0.8865052266813687, 0.3296729075500342 0.8759199037394889, 0.3417961510114355 0.8649320363489179, 0.3535533905932737 0.8535533905932738, 0.3649320363489175 0.841796151011436, 0.3759199037394885 0.8296729075500346, 0.3865052266813683 0.817196642081823, 0.3966766701456175 0.8043807145043604, 0.4064233422958076 0.7912388484339011, 0.4157348061512723 0.7777851165098015, 0.4246010907632893 0.7640339253251842, 0.4330127018922192 0.7500000000000002, 0.4409606321741774 0.735698368412999, 0.4484363707663439 0.7211443451095011, 0.4554319124605877 0.7063535149021978, 0.4619397662556433 0.6913417161825453, 0.4679529633786628 0.676125023960617, 0.4734650647475528 0.6607197326515809, 0.4784701678661043 0.6451423386272317, 0.482962913144534 0.6294095225512608, 0.4869384896386668 0.613538131517187, 0.4903926402016152 0.5975451610080643, 0.4933216660424395 0.5814477366972945, 0.4957224306869051 0.5652630961100262, 0.4975923633360984 0.5490085701647807, 0.4989294616193017 0.5327015646150718, 0.4997322937381828 0.5163595414108882, 0.5 0.5))\n</code></pre>"},{"location":"api/flink/Function/#st_minimumboundingradius","title":"ST_MinimumBoundingRadius","text":"<p>Introduction: Returns a struct containing the center point and radius of the smallest circle that contains a geometry.</p> <p>Format: <code>ST_MinimumBoundingRadius(geom: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_MinimumBoundingRadius(ST_GeomFromText('POLYGON((1 1,0 0, -1 1, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>{POINT (0 1), 1.0}\n</code></pre>"},{"location":"api/flink/Function/#st_multi","title":"ST_Multi","text":"<p>Introduction: Returns a MultiGeometry object based on the geometry input. ST_Multi is basically an alias for ST_Collect with one geometry.</p> <p>Format: <code>ST_Multi(geom: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Multi(ST_GeomFromText('POINT(1 1)'))\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT (1 1)\n</code></pre>"},{"location":"api/flink/Function/#st_normalize","title":"ST_Normalize","text":"<p>Introduction: Returns the input geometry in its normalized form.</p> <p>Format: <code>ST_Normalize(geom: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsEWKT(ST_Normalize(ST_GeomFromWKT('POLYGON((0 1, 1 1, 1 0, 0 0, 0 1))')))\n</code></pre> <p>Result:</p> <pre><code>POLYGON ((0 0, 0 1, 1 1, 1 0, 0 0))\n</code></pre>"},{"location":"api/flink/Function/#st_npoints","title":"ST_NPoints","text":"<p>Introduction: Returns the number of points of the geometry</p> <p>Format: <code>ST_NPoints (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_NPoints(ST_GeomFromText('LINESTRING(77.29 29.07,77.42 29.26,77.27 29.31,77.29 29.07)'))\n</code></pre> <p>Output:</p> <pre><code>4\n</code></pre>"},{"location":"api/flink/Function/#st_ndims","title":"ST_NDims","text":"<p>Introduction: Returns the coordinate dimension of the geometry.</p> <p>Format: <code>ST_NDims(geom: Geometry)</code></p> <p>Since: <code>v1.3.1</code></p> <p>Example with z coordinate:</p> <pre><code>SELECT ST_NDims(ST_GeomFromEWKT('POINT(1 1 2)'))\n</code></pre> <p>Output:</p> <pre><code>3\n</code></pre> <p>Example with x,y coordinate:</p> <pre><code>SELECT ST_NDims(ST_GeomFromText('POINT(1 1)'))\n</code></pre> <p>Output:</p> <pre><code>2\n</code></pre>"},{"location":"api/flink/Function/#st_nrings","title":"ST_NRings","text":"<p>Introduction: Returns the number of rings in a Polygon or MultiPolygon. Contrary to ST_NumInteriorRings, this function also takes into account the number of  exterior rings.</p> <p>This function returns 0 for an empty Polygon or MultiPolygon. If the geometry is not a Polygon or MultiPolygon, an IllegalArgument Exception is thrown.</p> <p>Format: <code>ST_NRings(geom: Geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Examples:</p> <p>Input: <code>POLYGON ((1 0, 1 1, 2 1, 2 0, 1 0))</code></p> <p>Output: <code>1</code></p> <p>Input: <code>'MULTIPOLYGON (((1 0, 1 6, 6 6, 6 0, 1 0), (2 1, 2 2, 3 2, 3 1, 2 1)), ((10 0, 10 6, 16 6, 16 0, 10 0), (12 1, 12 2, 13 2, 13 1, 12 1)))'</code></p> <p>Output: <code>4</code></p> <p>Input: <code>'POLYGON EMPTY'</code></p> <p>Output: <code>0</code></p> <p>Input: <code>'LINESTRING (1 0, 1 1, 2 1)'</code></p> <p>Output: <code>Unsupported geometry type: LineString, only Polygon or MultiPolygon geometries are supported.</code></p>"},{"location":"api/flink/Function/#st_numgeometries","title":"ST_NumGeometries","text":"<p>Introduction: Returns the number of Geometries. If geometry is a GEOMETRYCOLLECTION (or MULTI*) return the number of geometries, for single geometries will return 1.</p> <p>Format: <code>ST_NumGeometries (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example</p> <pre><code>SELECT ST_NumGeometries(ST_GeomFromWKT('LINESTRING (-29 -27, -30 -29.7, -45 -33)'))\n</code></pre> <p>Output:</p> <pre><code>1\n</code></pre>"},{"location":"api/flink/Function/#st_numinteriorrings","title":"ST_NumInteriorRings","text":"<p>Introduction: Returns number of interior rings of polygon geometries.</p> <p>Format: <code>ST_NumInteriorRings(geom: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_NumInteriorRings(ST_GeomFromText('POLYGON ((0 0, 0 5, 5 5, 5 0, 0 0), (1 1, 2 1, 2 2, 1 2, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>1\n</code></pre>"},{"location":"api/flink/Function/#st_numpoints","title":"ST_NumPoints","text":"<p>Introduction: Returns number of points in a LineString.</p> <p>Note</p> <p>If any other geometry is provided as an argument, an IllegalArgumentException is thrown. Example: <code>SELECT ST_NumPoints(ST_GeomFromWKT('MULTIPOINT ((0 0), (1 1), (0 1), (2 2))'))</code></p> <p>Output: <code>IllegalArgumentException: Unsupported geometry type: MultiPoint, only LineString geometry is supported.</code></p> <p>Format: <code>ST_NumPoints(geom: Geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Example:</p> <pre><code>SELECT ST_NumPoints(ST_GeomFromText('LINESTRING(1 2, 1 3)'))\n</code></pre> <p>Output:</p> <pre><code>2\n</code></pre>"},{"location":"api/flink/Function/#st_pointn","title":"ST_PointN","text":"<p>Introduction: Return the Nth point in a single linestring or circular linestring in the geometry. Negative values are counted backwards from the end of the LineString, so that -1 is the last point. Returns NULL if there is no linestring in the geometry.</p> <p>Format: <code>ST_PointN(A: Geometry, B: Integer)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Examples:</p> <pre><code>SELECT ST_PointN(df.geometry, 2)\nFROM df\n</code></pre> <p>Input: <code>LINESTRING(0 0, 1 2, 2 4, 3 6), 2</code></p> <p>Output: <code>POINT (1 2)</code></p> <p>Input: <code>LINESTRING(0 0, 1 2, 2 4, 3 6), -2</code></p> <p>Output: <code>POINT (2 4)</code></p> <p>Input: <code>CIRCULARSTRING(1 1, 1 2, 2 4, 3 6, 1 2, 1 1), -1</code></p> <p>Output: <code>POINT (1 1)</code></p>"},{"location":"api/flink/Function/#st_pointonsurface","title":"ST_PointOnSurface","text":"<p>Introduction: Returns a POINT guaranteed to lie on the surface.</p> <p>Format: <code>ST_PointOnSurface(A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Examples:</p> <pre><code>SELECT ST_PointOnSurface(df.geometry)\nFROM df\n</code></pre> <ol> <li>Input: <code>POINT (0 5)</code></li> </ol> <p>Output: <code>POINT (0 5)</code></p> <ol> <li>Input: <code>LINESTRING(0 5, 0 10)</code></li> </ol> <p>Output: <code>POINT (0 5)</code></p> <ol> <li>Input: <code>POLYGON((0 0, 0 5, 5 5, 5 0, 0 0))</code></li> </ol> <p>Output: <code>POINT (2.5 2.5)</code></p> <ol> <li>Input: <code>LINESTRING(0 5 1, 0 0 1, 0 10 2)</code></li> </ol> <p>Output: <code>POINT Z(0 0 1)</code></p>"},{"location":"api/flink/Function/#st_polygon","title":"ST_Polygon","text":"<p>Introduction: Function to create a polygon built from the given LineString and sets the spatial reference system from the srid</p> <p>Format: <code>ST_Polygon(geom: Geometry, srid: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText( ST_Polygon(ST_GeomFromEWKT('LINESTRING(75 29 1, 77 29 2, 77 29 3, 75 29 1)'), 4326) );\n</code></pre> <p>Output:</p> <pre><code>POLYGON((75 29 1, 77 29 2, 77 29 3, 75 29 1))\n</code></pre>"},{"location":"api/flink/Function/#st_reduceprecision","title":"ST_ReducePrecision","text":"<p>Introduction: Reduce the decimals places in the coordinates of the geometry to the given number of decimal places. The last decimal place will be rounded.</p> <p>Format: <code>ST_ReducePrecision (A: Geometry, B: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <p><pre><code>SELECT ST_ReducePrecision(ST_GeomFromWKT('Point(0.1234567890123456789 0.1234567890123456789)')\n, 9)\n</code></pre> The new coordinates will only have 9 decimal places.</p> <p>Output:</p> <pre><code>POINT (0.123456789 0.123456789)\n</code></pre>"},{"location":"api/flink/Function/#st_reverse","title":"ST_Reverse","text":"<p>Introduction: Return the geometry with vertex order reversed</p> <p>Format: <code>ST_Reverse (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_Reverse(ST_GeomFromWKT('LINESTRING(0 0, 1 2, 2 4, 3 6)'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (3 6, 2 4, 1 2, 0 0)\n</code></pre>"},{"location":"api/flink/Function/#st_removepoint","title":"ST_RemovePoint","text":"<p>Introduction: Return Linestring with removed point at given index, position can be omitted and then last one will be removed.</p> <p>Format:</p> <p><code>ST_RemovePoint(geom: Geometry, position: Integer)</code></p> <p><code>ST_RemovePoint(geom: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_RemovePoint(ST_GeomFromText(\"LINESTRING(0 0, 1 1, 1 0)\"), 1)\n</code></pre> <p>Output:</p> <pre><code>LINESTRING(0 0, 1 0)\n</code></pre>"},{"location":"api/flink/Function/#st_s2cellids","title":"ST_S2CellIDs","text":"<p>Introduction: Cover the geometry with Google S2 Cells, return the corresponding cell IDs with the given level. The level indicates the size of cells. With a bigger level, the cells will be smaller, the coverage will be more accurate, but the result size will be exponentially increasing.</p> <p>Format: <code>ST_S2CellIDs(geom: Geometry, level: Integer)</code></p> <p>Since: <code>v1.4.0</code></p> <p>Example:</p> <pre><code>SELECT ST_S2CellIDs(ST_GeomFromText('LINESTRING(1 3 4, 5 6 7)'), 6)\n</code></pre> <p>Output:</p> <pre><code>[1159395429071192064, 1159958379024613376, 1160521328978034688, 1161084278931456000, 1170091478186196992, 1170654428139618304]\n</code></pre>"},{"location":"api/flink/Function/#st_setpoint","title":"ST_SetPoint","text":"<p>Introduction: Replace Nth point of linestring with given point. Index is 0-based. Negative index are counted backwards, e.g., -1 is last point.</p> <p>Format: <code>ST_SetPoint (linestring: Geometry, index: Integer, point: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_SetPoint(ST_GeomFromText('LINESTRING (0 0, 0 1, 1 1)'), 2, ST_GeomFromText('POINT (1 0)'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (0 0, 0 1, 1 0)\n</code></pre>"},{"location":"api/flink/Function/#st_setsrid","title":"ST_SetSRID","text":"<p>Introduction: Sets the spatial reference system identifier (SRID) of the geometry.</p> <p>Format: <code>ST_SetSRID (A: Geometry, srid: Integer)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsEWKT(ST_SetSRID(ST_GeomFromWKT('POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))'), 3021))\n</code></pre> <p>Output:</p> <pre><code>SRID=3021;POLYGON ((1 1, 8 1, 8 8, 1 8, 1 1))\n</code></pre>"},{"location":"api/flink/Function/#st_srid","title":"ST_SRID","text":"<p>Introduction: Return the spatial reference system identifier (SRID) of the geometry.</p> <p>Format: <code>ST_SRID (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_SRID(ST_SetSRID(ST_GeomFromWKT('POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))'), 3021))\n</code></pre> <p>Output:</p> <pre><code>3021\n</code></pre>"},{"location":"api/flink/Function/#st_simplifypreservetopology","title":"ST_SimplifyPreserveTopology","text":"<p>Introduction: Simplifies a geometry and ensures that the result is a valid geometry having the same dimension and number of components as the input, and with the components having the same topological relationship.</p> <p>Since: <code>v1.5.0</code></p> <p>Format: <code>ST_SimplifyPreserveTopology (A: Geometry, distanceTolerance: Double)</code></p> <p>Example:</p> <pre><code>SELECT ST_SimplifyPreserveTopology(ST_GeomFromText('POLYGON((8 25, 28 22, 28 20, 15 11, 33 3, 56 30, 46 33,46 34, 47 44, 35 36, 45 33, 43 19, 29 21, 29 22,35 26, 24 39, 8 25))'), 10)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((8 25, 28 22, 15 11, 33 3, 56 30, 47 44, 35 36, 43 19, 24 39, 8 25))\n</code></pre>"},{"location":"api/flink/Function/#st_startpoint","title":"ST_StartPoint","text":"<p>Introduction: Returns first point of given linestring.</p> <p>Format: <code>ST_StartPoint(geom: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_StartPoint(ST_GeomFromText('LINESTRING(100 150,50 60, 70 80, 160 170)'))\n</code></pre> <p>Output:</p> <pre><code>POINT(100 150)\n</code></pre>"},{"location":"api/flink/Function/#st_subdivide","title":"ST_SubDivide","text":"<p>Introduction: Returns list of geometries divided based of given maximum number of vertices.</p> <p>Format: <code>ST_SubDivide(geom: Geometry, maxVertices: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example: <pre><code>SELECT ST_SubDivide(ST_GeomFromText(\"POLYGON((35 10, 45 45, 15 40, 10 20, 35 10), (20 30, 35 35, 30 20, 20 30))\"), 5)\n</code></pre></p> <p>Output: <pre><code>[\n    POLYGON((37.857142857142854 20, 35 10, 10 20, 37.857142857142854 20)),\n    POLYGON((15 20, 10 20, 15 40, 15 20)),\n    POLYGON((20 20, 15 20, 15 30, 20 30, 20 20)),\n    POLYGON((26.428571428571427 20, 20 20, 20 30, 26.4285714 23.5714285, 26.4285714 20)),\n    POLYGON((15 30, 15 40, 20 40, 20 30, 15 30)),\n    POLYGON((20 40, 26.4285714 40, 26.4285714 32.1428571, 20 30, 20 40)),\n    POLYGON((37.8571428 20, 30 20, 34.0476190 32.1428571, 37.8571428 32.1428571, 37.8571428 20)),\n    POLYGON((34.0476190 34.6825396, 26.4285714 32.1428571, 26.4285714 40, 34.0476190 40, 34.0476190 34.6825396)),\n    POLYGON((34.0476190 32.1428571, 35 35, 37.8571428 35, 37.8571428 32.1428571, 34.0476190 32.1428571)),\n    POLYGON((35 35, 34.0476190 34.6825396, 34.0476190 35, 35 35)),\n    POLYGON((34.0476190 35, 34.0476190 40, 37.8571428 40, 37.8571428 35, 34.0476190 35)),\n    POLYGON((30 20, 26.4285714 20, 26.4285714 23.5714285, 30 20)),\n    POLYGON((15 40, 37.8571428 43.8095238, 37.8571428 40, 15 40)),\n    POLYGON((45 45, 37.8571428 20, 37.8571428 43.8095238, 45 45))\n]\n</code></pre></p> <p>Example:</p> <pre><code>SELECT ST_SubDivide(ST_GeomFromText(\"LINESTRING(0 0, 85 85, 100 100, 120 120, 21 21, 10 10, 5 5)\"), 5)\n</code></pre> <p>Output: <pre><code>[\n    LINESTRING(0 0, 5 5)\n    LINESTRING(5 5, 10 10)\n    LINESTRING(10 10, 21 21)\n    LINESTRING(21 21, 60 60)\n    LINESTRING(60 60, 85 85)\n    LINESTRING(85 85, 100 100)\n    LINESTRING(100 100, 120 120)\n]\n</code></pre></p>"},{"location":"api/flink/Function/#st_symdifference","title":"ST_SymDifference","text":"<p>Introduction: Return the symmetrical difference between geometry A and B (return parts of geometries which are in either of the sets, but not in their intersection)</p> <p>Format: <code>ST_SymDifference (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_SymDifference(ST_GeomFromWKT('POLYGON ((-3 -3, 3 -3, 3 3, -3 3, -3 -3))'), ST_GeomFromWKT('POLYGON ((-2 -3, 4 -3, 4 3, -2 3, -2 -3))'))\n</code></pre> <p>Output:</p> <pre><code>MULTIPOLYGON (((-2 -3, -3 -3, -3 3, -2 3, -2 -3)), ((3 -3, 3 3, 4 3, 4 -3, 3 -3)))\n</code></pre>"},{"location":"api/flink/Function/#st_transform","title":"ST_Transform","text":"<p>Introduction:</p> <p>Transform the Spatial Reference System / Coordinate Reference System of A, from SourceCRS to TargetCRS. For SourceCRS and TargetCRS, WKT format is also available since v1.3.1.</p> <p>Lon/Lat Order in the input geometry</p> <p>If the input geometry is in lat/lon order, it might throw an error such as <code>too close to pole</code>, <code>latitude or longitude exceeded limits</code>, or give unexpected results. You need to make sure that the input geometry is in lon/lat order. If the input geometry is in lat/lon order, you can use ST_FlipCoordinates to swap X and Y.</p> <p>Lon/Lat Order in the source and target CRS</p> <p>Sedona will force the source and target CRS to be in lon/lat order. If the source CRS or target CRS is in lat/lon order, it will be swapped to lon/lat order.</p> <p>CRS code</p> <p>The CRS code is the code of the CRS in the official EPSG database (https://epsg.org/) in the format of <code>EPSG:XXXX</code>. A community tool EPSG.io can help you quick identify a CRS code. For example, the code of WGS84 is <code>EPSG:4326</code>.</p> <p>WKT format</p> <p>You can also use OGC WKT v1 format to specify the source CRS and target CRS. An example OGC WKT v1 CRS of <code>EPGS:3857</code> is as follows:</p> <pre><code>PROJCS[\"WGS 84 / Pseudo-Mercator\",\n    GEOGCS[\"WGS 84\",\n        DATUM[\"WGS_1984\",\n            SPHEROID[\"WGS 84\",6378137,298.257223563,\n                AUTHORITY[\"EPSG\",\"7030\"]],\n            AUTHORITY[\"EPSG\",\"6326\"]],\n        PRIMEM[\"Greenwich\",0,\n            AUTHORITY[\"EPSG\",\"8901\"]],\n        UNIT[\"degree\",0.0174532925199433,\n            AUTHORITY[\"EPSG\",\"9122\"]],\n        AUTHORITY[\"EPSG\",\"4326\"]],\n    PROJECTION[\"Mercator_1SP\"],\n    PARAMETER[\"central_meridian\",0],\n    PARAMETER[\"scale_factor\",1],\n    PARAMETER[\"false_easting\",0],\n    PARAMETER[\"false_northing\",0],\n    UNIT[\"metre\",1,\n        AUTHORITY[\"EPSG\",\"9001\"]],\n    AXIS[\"Easting\",EAST],\n    AXIS[\"Northing\",NORTH],\n    EXTENSION[\"PROJ4\",\"+proj=merc +a=6378137 +b=6378137 +lat_ts=0 +lon_0=0 +x_0=0 +y_0=0 +k=1 +units=m +nadgrids=@null +wktext +no_defs\"],\n    AUTHORITY[\"EPSG\",\"3857\"]]\n</code></pre> <p>Note</p> <p>By default, this function uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order.</p> <p>Note</p> <p>By default, ST_Transform follows the <code>lenient</code> mode which tries to fix issues by itself. You can append a boolean value at the end to enable the <code>strict</code> mode. In <code>strict</code> mode, ST_Transform will throw an error if it finds any issue.</p> <p>Format:</p> <pre><code>ST_Transform (A: Geometry, SourceCRS: String, TargetCRS: String, [Optional] lenientMode: Boolean)\n</code></pre> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_Transform(ST_GeomFromText('POLYGON((170 50,170 72,-130 72,-130 50,170 50))'),'EPSG:4326', 'EPSG:32649'))\n</code></pre> <pre><code>SELECT ST_AsText(ST_Transform(ST_GeomFromText('POLYGON((170 50,170 72,-130 72,-130 50,170 50))'),'EPSG:4326', 'EPSG:32649', false))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((8766047.980342899 17809098.336766362, 5122546.516721856 18580261.912528664, 3240775.0740796793 -13688660.50985159, 4556241.924514083 -12463044.21488129, 8766047.980342899 17809098.336766362))\n</code></pre>"},{"location":"api/flink/Function/#st_translate","title":"ST_Translate","text":"<p>Introduction: Returns the input geometry with its X, Y and Z coordinates (if present in the geometry) translated by deltaX, deltaY and deltaZ (if specified)</p> <p>If the geometry is 2D, and a deltaZ parameter is specified, no change is done to the Z coordinate of the geometry and the resultant geometry is also 2D.</p> <p>If the geometry is empty, no change is done to it.</p> <p>If the given geometry contains sub-geometries (GEOMETRY COLLECTION, MULTI POLYGON/LINE/POINT), all underlying geometries are individually translated.</p> <p>Format:</p> <p><code>ST_Translate(geometry: Geometry, deltaX: Double, deltaY: Double, deltaZ: Double)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Example:</p> <pre><code>SELECT ST_Translate(ST_GeomFromText('GEOMETRYCOLLECTION(MULTIPOLYGON(((3 2,3 3,4 3,4 2,3 2)),((3 4,5 6,5 7,3 4))), POINT(1 1 1), LINESTRING EMPTY)'), 2, 2, 3)\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (MULTIPOLYGON (((5 4, 5 5, 6 5, 6 4, 5 4)), ((5 6, 7 8, 7 9, 5 6))), POINT (3 3), LINESTRING EMPTY)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_Translate(ST_GeomFromText('POINT(-71.01 42.37)'),1,2)\n</code></pre> <p>Output:</p> <pre><code>POINT (-70.01 44.37)\n</code></pre>"},{"location":"api/flink/Function/#st_voronoipolygons","title":"ST_VoronoiPolygons","text":"<p>Introduction: Returns a two-dimensional Voronoi diagram from the vertices of the supplied geometry. The result is a GeometryCollection of Polygons that covers an envelope larger than the extent of the input vertices. Returns null if input geometry is null. Returns an empty geometry collection if the input geometry contains only one vertex. Returns an empty geometry collection if the extend_to envelope has zero area.</p> <p>Format: <code>ST_VoronoiPolygons(g1: Geometry, tolerance: Double, extend_to: Geometry)</code></p> <p>Optional parameters:</p> <p><code>tolerance</code> : The distance within which vertices will be considered equivalent. Robustness of the algorithm can be improved by supplying a nonzero tolerance distance. (default = 0.0)</p> <p><code>extend_to</code> : If a geometry is supplied as the \"extend_to\" parameter, the diagram will be extended to cover the envelope of the \"extend_to\" geometry, unless that envelope is smaller than the default envelope (default = NULL. By default, we extend the bounding box of the diagram by the max between bounding box's height and bounding box's width).</p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT st_astext(ST_VoronoiPolygons(ST_GeomFromText('MULTIPOINT ((0 0), (1 1))')));\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION(POLYGON((-1 2,2 -1,-1 -1,-1 2)),POLYGON((-1 2,2 2,2 -1,-1 2)))\n</code></pre>"},{"location":"api/flink/Function/#st_x","title":"ST_X","text":"<p>Introduction: Returns X Coordinate of given Point, null otherwise.</p> <p>Format: <code>ST_X(pointA: Point)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_X(ST_POINT(0.0 25.0))\n</code></pre> <p>Output:</p> <pre><code>0.0\n</code></pre>"},{"location":"api/flink/Function/#st_xmax","title":"ST_XMax","text":"<p>Introduction: Returns the maximum X coordinate of a geometry</p> <p>Format: <code>ST_XMax (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_XMax(ST_GeomFromText('POLYGON ((-1 -11, 0 10, 1 11, 2 12, -1 -11))'))\n</code></pre> <p>Output:</p> <pre><code>2\n</code></pre>"},{"location":"api/flink/Function/#st_xmin","title":"ST_XMin","text":"<p>Introduction: Returns the minimum X coordinate of a geometry</p> <p>Format: <code>ST_XMin (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_XMin(ST_GeomFromText('POLYGON ((-1 -11, 0 10, 1 11, 2 12, -1 -11))'))\n</code></pre> <p>Output:</p> <pre><code>-1\n</code></pre>"},{"location":"api/flink/Function/#st_y","title":"ST_Y","text":"<p>Introduction: Returns Y Coordinate of given Point, null otherwise.</p> <p>Format: <code>ST_Y(pointA: Point)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Y(ST_POINT(0.0 25.0))\n</code></pre> <p>Output:</p> <pre><code>25.0\n</code></pre>"},{"location":"api/flink/Function/#st_ymax","title":"ST_YMax","text":"<p>Introduction: Return the minimum Y coordinate of A</p> <p>Format: <code>ST_YMax (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_YMax(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre> <p>Output :</p> <pre><code>2\n</code></pre>"},{"location":"api/flink/Function/#st_ymin","title":"ST_YMin","text":"<p>Introduction: Return the minimum Y coordinate of A</p> <p>Format: <code>ST_Y_Min (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_YMin(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre> <p>Output:</p> <pre><code>0\n</code></pre>"},{"location":"api/flink/Function/#st_z","title":"ST_Z","text":"<p>Introduction: Returns Z Coordinate of given Point, null otherwise.</p> <p>Format: <code>ST_Z(pointA: Point)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Z(ST_POINT(0.0 25.0 11.0))\n</code></pre> <p>Output:</p> <pre><code>11.0\n</code></pre>"},{"location":"api/flink/Function/#st_zmax","title":"ST_ZMax","text":"<p>Introduction: Returns Z maxima of the given geometry or null if there is no Z coordinate.</p> <p>Format: <code>ST_ZMax(geom: Geometry)</code></p> <p>Since: <code>v1.3.1</code></p> <p>Example: <pre><code>SELECT ST_ZMax(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre></p> <p>Output:</p> <pre><code>1.0\n</code></pre>"},{"location":"api/flink/Function/#st_zmin","title":"ST_ZMin","text":"<p>Introduction: Returns Z minima of the given geometry or null if there is no Z coordinate.</p> <p>Format: <code>ST_ZMin(geom: Geometry)</code></p> <p>Since: <code>v1.3.1</code></p> <p>Example:</p> <pre><code>SELECT ST_ZMin(ST_GeomFromText('LINESTRING(1 3 4, 5 6 7)'))\n</code></pre> <p>Output:</p> <pre><code>4.0\n</code></pre>"},{"location":"api/flink/Overview/","title":"Introduction","text":"<p>SedonaSQL supports SQL/MM Part3 Spatial SQL Standard. Please read the programming guide: Sedona with Flink SQL app.</p> <p>Sedona includes SQL operators as follows.</p> <ul> <li>Constructor: Construct a Geometry given an input string or coordinates<ul> <li>Example: ST_GeomFromWKT (string). Create a Geometry from a WKT String.</li> </ul> </li> <li>Function: Execute a function on the given column or columns<ul> <li>Example: ST_Distance (A, B). Given two Geometry A and B, return the Euclidean distance of A and B.</li> </ul> </li> <li>Aggregator: Return a single aggregated value on the given column<ul> <li>Example: ST_Envelope_Aggr (Geometry column). Given a Geometry column, calculate the entire envelope boundary of this column.</li> </ul> </li> <li>Predicate: Execute a logic judgement on the given columns and return true or false<ul> <li>Example: ST_Contains (A, B). Check if A fully contains B. Return \"True\" if yes, else return \"False\".</li> </ul> </li> </ul>"},{"location":"api/flink/Predicate/","title":"Predicate","text":""},{"location":"api/flink/Predicate/#st_contains","title":"ST_Contains","text":"<p>Introduction: Return true if A fully contains B</p> <p>Format: <code>ST_Contains (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Contains(ST_GeomFromWKT('POLYGON((175 150,20 40,50 60,125 100,175 150))'), ST_GeomFromWKT('POINT(174 149)'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/flink/Predicate/#st_crosses","title":"ST_Crosses","text":"<p>Introduction: Return true if A crosses B</p> <p>Format: <code>ST_Crosses (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Crosses(ST_GeomFromWKT('POLYGON((1 1, 4 1, 4 4, 1 4, 1 1))'),ST_GeomFromWKT('POLYGON((2 2, 5 2, 5 5, 2 5, 2 2))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/flink/Predicate/#st_disjoint","title":"ST_Disjoint","text":"<p>Introduction: Return true if A and B are disjoint</p> <p>Format: <code>ST_Disjoint (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_Disjoint(ST_GeomFromWKT('POLYGON((1 4, 4.5 4, 4.5 2, 1 2, 1 4))'),ST_GeomFromWKT('POLYGON((5 4, 6 4, 6 2, 5 2, 5 4))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/flink/Predicate/#st_dwithin","title":"ST_DWithin","text":"<p>Introduction: Returns true if 'leftGeometry' and 'rightGeometry' are within a specified 'distance'. This function essentially checks if the shortest distance between the envelope of the two geometries is &lt;= the provided distance.</p> <p>Format: <code>ST_DWithin (leftGeometry: Geometry, rightGeometry: Geometry, distance: Double)</code></p> <p>Since: <code>v1.5.1</code></p> <p>Example:</p> <pre><code>SELECT ST_DWithin(ST_GeomFromWKT('POINT (0 0)'), ST_GeomFromWKT('POINT (1 0)'), 2.5)\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/flink/Predicate/#st_equals","title":"ST_Equals","text":"<p>Introduction: Return true if A equals to B</p> <p>Format: <code>ST_Equals (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Equals(ST_GeomFromWKT('LINESTRING(0 0,10 10)'), ST_GeomFromWKT('LINESTRING(0 0,5 5,10 10)'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/flink/Predicate/#st_intersects","title":"ST_Intersects","text":"<p>Introduction: Return true if A intersects B</p> <p>Format: <code>ST_Intersects (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Intersects(ST_GeomFromWKT('LINESTRING(-43.23456 72.4567,-43.23456 72.4568)'), ST_GeomFromWKT('POINT(-43.23456 72.4567772)'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/flink/Predicate/#st_overlaps","title":"ST_Overlaps","text":"<p>Introduction: Return true if A overlaps B</p> <p>Format: <code>ST_Overlaps (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Overlaps(ST_GeomFromWKT('POLYGON((2.5 2.5, 2.5 4.5, 4.5 4.5, 4.5 2.5, 2.5 2.5))'), ST_GeomFromWKT('POLYGON((4 4, 4 6, 6 6, 6 4, 4 4))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/flink/Predicate/#st_touches","title":"ST_Touches","text":"<p>Introduction: Return true if A touches B</p> <p>Format: <code>ST_Touches (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Touches(ST_GeomFromWKT('LINESTRING(0 0,1 1,0 2)'), ST_GeomFromWKT('POINT(0 2)'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/flink/Predicate/#st_within","title":"ST_Within","text":"<p>Introduction: Return true if A is within B</p> <p>Format: <code>ST_Within (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Within(ST_GeomFromWKT('POLYGON((0 0,3 0,3 3,0 3,0 0))'), ST_GeomFromWKT('POLYGON((1 1,2 1,2 2,1 2,1 1))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/flink/Predicate/#st_orderingequals","title":"ST_OrderingEquals","text":"<p>Introduction: Returns true if the geometries are equal and the coordinates are in the same order</p> <p>Format: <code>ST_OrderingEquals(A: geometry, B: geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example 1:</p> <pre><code>SELECT ST_OrderingEquals(ST_GeomFromWKT('POLYGON((2 0, 0 2, -2 0, 2 0))'), ST_GeomFromWKT('POLYGON((2 0, 0 2, -2 0, 2 0))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre> <p>Example 2:</p> <pre><code>SELECT ST_OrderingEquals(ST_GeomFromWKT('POLYGON((2 0, 0 2, -2 0, 2 0))'), ST_GeomFromWKT('POLYGON((0 2, -2 0, 2 0, 0 2))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/flink/Predicate/#st_covers","title":"ST_Covers","text":"<p>Introduction: Return true if A covers B</p> <p>Format: <code>ST_Covers (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Covers(ST_GeomFromWKT('POLYGON((-2 0,0 2,2 0,-2 0))'), ST_GeomFromWKT('POLYGON((-1 0,0 1,1 0,-1 0))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/flink/Predicate/#st_coveredby","title":"ST_CoveredBy","text":"<p>Introduction: Return true if A is covered by B</p> <p>Format: <code>ST_CoveredBy (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_CoveredBy(ST_GeomFromWKT('POLYGON((0 0,3 0,3 3,0 3,0 0))'),  ST_GeomFromWKT('POLYGON((1 1,2 1,2 2,1 2,1 1))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/snowflake/vector-data/AggregateFunction/","title":"Aggregate Function","text":"<p>Note</p> <p>Please always keep the schema name <code>SEDONA</code> (e.g., <code>SEDONA.ST_GeomFromWKT</code>) when you use Sedona functions to avoid conflicting with Snowflake's built-in functions.</p>"},{"location":"api/snowflake/vector-data/AggregateFunction/#st_envelope_aggr","title":"ST_Envelope_Aggr","text":"<p>Introduction: Return the entire envelope boundary of all geometries in A</p> <p>Format: <code>ST_Envelope_Aggr (A:geometryColumn)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_Envelope_Aggr(ST_GeomFromText('MULTIPOINT(1.1 101.1,2.1 102.1,3.1 103.1,4.1 104.1,5.1 105.1,6.1 106.1,7.1 107.1,8.1 108.1,9.1 109.1,10.1 110.1)'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((1.1 101.1, 1.1 120.1, 20.1 120.1, 20.1 101.1, 1.1 101.1))\n</code></pre>"},{"location":"api/snowflake/vector-data/AggregateFunction/#st_intersection_aggr","title":"ST_Intersection_Aggr","text":"<p>Introduction: Return the polygon intersection of all polygons in A</p> <p>Format: <code>ST_Intersection_Aggr (A:geometryColumn)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_Intersection_Aggr(ST_GeomFromText('MULTIPOINT(1.1 101.1,2.1 102.1,3.1 103.1,4.1 104.1,5.1 105.1,6.1 106.1,7.1 107.1,8.1 108.1,9.1 109.1,10.1 110.1)'))\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT ((1.1 101.1), (2.1 102.1), (3.1 103.1), (4.1 104.1), (5.1 105.1), (6.1 106.1), (7.1 107.1), (8.1 108.1), (9.1 109.1), (10.1 110.1))\n</code></pre>"},{"location":"api/snowflake/vector-data/AggregateFunction/#st_union_aggr","title":"ST_Union_Aggr","text":"<p>Introduction: Return the polygon union of all polygons in A</p> <p>Format: <code>ST_Union_Aggr (A:geometryColumn)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_Union_Aggr(ST_GeomFromText('MULTIPOINT(1.1 101.1,2.1 102.1,3.1 103.1,4.1 104.1,5.1 105.1,6.1 106.1,7.1 107.1,8.1 108.1,9.1 109.1,10.1 110.1)'))\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT ((1.1 101.1), (2.1 102.1), (3.1 103.1), (4.1 104.1), (5.1 105.1), (6.1 106.1), (7.1 107.1), (8.1 108.1), (9.1 109.1), (10.1 110.1))\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/","title":"Constructor","text":"<p>Note</p> <p>Please always keep the schema name <code>SEDONA</code> (e.g., <code>SEDONA.ST_GeomFromWKT</code>) when you use Sedona functions to avoid conflicting with Snowflake's built-in functions.</p>"},{"location":"api/snowflake/vector-data/Constructor/#st_geomfromgeohash","title":"ST_GeomFromGeoHash","text":"<p>Introduction: Create Geometry from geohash string and optional precision</p> <p>Format: <code>ST_GeomFromGeoHash(geohash: string, precision: int)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_GeomFromGeoHash('s00twy01mt', 4)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0.703125 0.87890625, 0.703125 1.0546875, 1.0546875 1.0546875, 1.0546875 0.87890625, 0.703125 0.87890625))\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_geomfromgeojson","title":"ST_GeomFromGeoJSON","text":"<p>Introduction: Construct a Geometry from GeoJson</p> <p>Format: <code>ST_GeomFromGeoJSON (GeoJson:string)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_GeomFromGeoJSON('{\n   \"type\":\"Feature\",\n   \"properties\":{\n      \"STATEFP\":\"01\",\n      \"COUNTYFP\":\"077\",\n      \"TRACTCE\":\"011501\",\n      \"BLKGRPCE\":\"5\",\n      \"AFFGEOID\":\"1500000US010770115015\",\n      \"GEOID\":\"010770115015\",\n      \"NAME\":\"5\",\n      \"LSAD\":\"BG\",\n      \"ALAND\":6844991,\n      \"AWATER\":32636\n   },\n   \"geometry\":{\n      \"type\":\"Polygon\",\n      \"coordinates\":[\n         [\n            [-87.621765, 34.873444],\n            [-87.617535, 34.873369],\n            [-87.62119, 34.85053],\n            [-87.62144, 34.865379],\n            [-87.621765, 34.873444]\n         ]\n      ]\n   }\n}')\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((-87.621765 34.873444, -87.617535 34.873369, -87.62119 34.85053, -87.62144 34.865379, -87.621765 34.873444))\n</code></pre> <p>SQL example:</p> <pre><code>SELECT ST_GeomFromGeoJSON('{\n   \"type\":\"Polygon\",\n   \"coordinates\":[\n      [\n         [-87.621765, 34.873444],\n         [-87.617535, 34.873369],\n         [-87.62119, 34.85053],\n         [-87.62144, 34.865379],\n         [-87.621765, 34.873444]\n      ]\n   ]\n}')\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((-87.621765 34.873444, -87.617535 34.873369, -87.62119 34.85053, -87.62144 34.865379, -87.621765 34.873444))\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_geomfromgml","title":"ST_GeomFromGML","text":"<p>Introduction: Construct a Geometry from GML.</p> <p>Format: <code>ST_GeomFromGML (gml:string)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_GeomFromGML('\n    &lt;gml:LineString srsName=\"EPSG:4269\"&gt;\n        &lt;gml:coordinates&gt;\n            -71.16028,42.258729\n            -71.160837,42.259112\n            -71.161143,42.25932\n        &lt;/gml:coordinates&gt;\n    &lt;/gml:LineString&gt;\n')\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-71.16028 42.258729, -71.160837 42.259112, -71.161143 42.25932)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_geomfromkml","title":"ST_GeomFromKML","text":"<p>Introduction: Construct a Geometry from KML.</p> <p>Format: <code>ST_GeomFromKML (kml:string)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_GeomFromKML('\n    &lt;LineString&gt;\n        &lt;coordinates&gt;\n            -71.1663,42.2614\n            -71.1667,42.2616\n        &lt;/coordinates&gt;\n    &lt;/LineString&gt;\n')\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-71.1663 42.2614, -71.1667 42.2616)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_geomfromtext","title":"ST_GeomFromText","text":"<p>Introduction: Construct a Geometry from WKT. If SRID is not set, it defaults to 0 (unknown). Alias of ST_GeomFromWKT</p> <p>Format: <code>ST_GeomFromText (Wkt:string)</code> <code>ST_GeomFromText (Wkt:string, srid:integer)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_GeomFromText('POINT(40.7128 -74.0060)')\n</code></pre> <p>Output:</p> <pre><code>POINT(40.7128 -74.006)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_geomfromwkb","title":"ST_GeomFromWKB","text":"<p>Introduction: Construct a Geometry from WKB string or Binary. This function also supports EWKB format.</p> <p>Format: <code>ST_GeomFromWKB (Wkb:string)</code> <code>ST_GeomFromWKB (Wkb:binary)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_GeomFromWKB([01 02 00 00 00 02 00 00 00 00 00 00 00 84 D6 00 C0 00 00 00 00 80 B5 D6 BF 00 00 00 60 E1 EF F7 BF 00 00 00 80 07 5D E5 BF])\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-2.1047439575195312 -0.354827880859375, -1.49606454372406 -0.6676061153411865)\n</code></pre> <p>SQL example:</p> <pre><code>SELECT ST_asEWKT(ST_GeomFromWKB('01010000a0e6100000000000000000f03f000000000000f03f000000000000f03f'))\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POINT Z(1 1 1)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_geomfromwkt","title":"ST_GeomFromWKT","text":"<p>Introduction: Construct a Geometry from WKT. If SRID is not set, it defaults to 0 (unknown).</p> <p>Format: <code>ST_GeomFromWKT (Wkt:string)</code> <code>ST_GeomFromWKT (Wkt:string, srid:integer)</code></p> <p>The optional srid parameter was added in <code>v1.3.1</code></p> <p>SQL example:</p> <pre><code>SELECT ST_GeomFromWKT('POINT(40.7128 -74.0060)')\n</code></pre> <p>Output:</p> <pre><code>POINT(40.7128 -74.006)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_geomfromewkt","title":"ST_GeomFromEWKT","text":"<p>Introduction: Construct a Geometry from OGC Extended WKT</p> <p>Format: <code>ST_GeomFromEWKT (EWkt:string)</code></p> <p>SQL example: <pre><code>SELECT ST_AsText(ST_GeomFromEWKT('SRID=4269;POINT(40.7128 -74.0060)'))\n</code></pre></p> <p>Output:</p> <pre><code>POINT(40.7128 -74.006)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_linefromtext","title":"ST_LineFromText","text":"<p>Introduction: Construct a Line from Wkt text</p> <p>Format: <code>ST_LineFromText (Wkt:string)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_LineFromText('LINESTRING(1 2,3 4)')\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (1 2, 3 4)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_linestringfromtext","title":"ST_LineStringFromText","text":"<p>Introduction: Construct a LineString from Text, delimited by Delimiter</p> <p>Format: <code>ST_LineStringFromText (Text:string, Delimiter:char)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_LineStringFromText('-74.0428197,40.6867969,-74.0421975,40.6921336,-74.0508020,40.6912794', ',')\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-74.0428197 40.6867969, -74.0421975 40.6921336, -74.050802 40.6912794)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_makepoint","title":"ST_MakePoint","text":"<p>Introduction: Creates a 2D, 3D Z or 4D ZM Point geometry. Use ST_MakePointM to make points with XYM coordinates. Z and M values are optional.</p> <p>Format: <code>ST_MakePoint (X:decimal, Y:decimal, Z:decimal, M:decimal)</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_MakePoint(1.2345, 2.3456));\n</code></pre> <p>Output:</p> <pre><code>POINT (1.2345 2.3456)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_MakePoint(1.2345, 2.3456, 3.4567));\n</code></pre> <p>Output:</p> <pre><code>POINT Z (1.2345 2.3456 3.4567)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_MakePoint(1.2345, 2.3456, 3.4567, 4));\n</code></pre> <p>Output:</p> <pre><code>POINT ZM (1.2345 2.3456 3.4567 4)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_mlinefromtext","title":"ST_MLineFromText","text":"<p>Introduction: Construct a MultiLineString from Wkt. If srid is not set, it defaults to 0 (unknown).</p> <p>Format: <code>ST_MLineFromText (Wkt:string)</code> <code>ST_MLineFromText (Wkt:string, srid:integer)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_MLineFromText('MULTILINESTRING((1 2, 3 4), (4 5, 6 7))')\n</code></pre> <p>Output:</p> <pre><code>MULTILINESTRING ((1 2, 3 4), (4 5, 6 7))\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_mpolyfromtext","title":"ST_MPolyFromText","text":"<p>Introduction: Construct a MultiPolygon from Wkt. If srid is not set, it defaults to 0 (unknown).</p> <p>Format: <code>ST_MPolyFromText (Wkt:string)</code> <code>ST_MPolyFromText (Wkt:string, srid:integer)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_MPolyFromText('MULTIPOLYGON(((0 0 1,20 0 1,20 20 1,0 20 1,0 0 1),(5 5 3,5 7 3,7 7 3,7 5 3,5 5 3)))')\n</code></pre> <p>Output:</p> <pre><code>MULTIPOLYGON (((0 0, 20 0, 20 20, 0 20, 0 0), (5 5, 5 7, 7 7, 7 5, 5 5)))\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_point","title":"ST_Point","text":"<p>Introduction: Construct a Point from X and Y</p> <p>Format: <code>ST_Point (X:decimal, Y:decimal)</code></p> <p>In <code>v1.4.0</code> an optional Z parameter was removed to be more consistent with other spatial SQL implementations. If you are upgrading from an older version of Sedona - please use ST_PointZ to create 3D points.</p> <p>SQL example:</p> <pre><code>SELECT ST_Point(double(1.2345), 2.3456)\n</code></pre> <p>Output:</p> <pre><code>POINT (1.2345 2.3456)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_pointz","title":"ST_PointZ","text":"<p>Introduction: Construct a Point from X, Y and Z and an optional srid. If srid is not set, it defaults to 0 (unknown). Must use ST_AsEWKT function to print the Z coordinate.</p> <p>Format: <code>ST_PointZ (X:decimal, Y:decimal, Z:decimal)</code></p> <p>Format: <code>ST_PointZ (X:decimal, Y:decimal, Z:decimal, srid:integer)</code></p> <pre><code>SELECT ST_AsEWKT(ST_PointZ(1.2345, 2.3456, 3.4567))\n</code></pre> <p>Output:</p> <pre><code>POINT Z(1.2345 2.3456 3.4567)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_pointfromtext","title":"ST_PointFromText","text":"<p>Introduction: Construct a Point from Text, delimited by Delimiter</p> <p>Format: <code>ST_PointFromText (Text:string, Delimiter:char)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_PointFromText('40.7128,-74.0060', ',')\n</code></pre> <p>Output:</p> <pre><code>POINT (40.7128 -74.006)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_polygonfromenvelope","title":"ST_PolygonFromEnvelope","text":"<p>Introduction: Construct a Polygon from MinX, MinY, MaxX, MaxY.</p> <p>Format: <code>ST_PolygonFromEnvelope (MinX:decimal, MinY:decimal, MaxX:decimal, MaxY:decimal)</code></p> <pre><code>SELECT ST_PolygonFromEnvelope(double(1.234),double(2.234),double(3.345),double(3.345))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((1.234 2.234, 1.234 3.345, 3.345 3.345, 3.345 2.234, 1.234 2.234))\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_polygonfromtext","title":"ST_PolygonFromText","text":"<p>Introduction: Construct a Polygon from Text, delimited by Delimiter. Path must be closed</p> <p>Format: <code>ST_PolygonFromText (Text:string, Delimiter:char)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_PolygonFromText('-74.0428197,40.6867969,-74.0421975,40.6921336,-74.0508020,40.6912794,-74.0428197,40.6867969', ',')\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((-74.0428197 40.6867969, -74.0421975 40.6921336, -74.050802 40.6912794, -74.0428197 40.6867969))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/","title":"Function","text":"<p>Note</p> <p>Please always keep the schema name <code>SEDONA</code> (e.g., <code>SEDONA.ST_GeomFromWKT</code>) when you use Sedona functions to avoid conflicting with Snowflake's built-in functions.</p>"},{"location":"api/snowflake/vector-data/Function/#geometrytype","title":"GeometryType","text":"<p>Introduction: Returns the type of the geometry as a string. Eg: 'LINESTRING', 'POLYGON', 'MULTIPOINT', etc. This function also indicates if the geometry is measured, by returning a string of the form 'POINTM'.</p> <p>Format: <code>GeometryType (A: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT GeometryType(ST_GeomFromText('LINESTRING(77.29 29.07,77.42 29.26,77.27 29.31,77.29 29.07)'));\n</code></pre> <p>Output:</p> <pre><code> geometrytype\n--------------\n LINESTRING\n</code></pre> <pre><code>SELECT GeometryType(ST_GeomFromText('POINTM(0 0 1)'));\n</code></pre> <p>Output:</p> <pre><code> geometrytype\n--------------\n POINTM\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_3ddistance","title":"ST_3DDistance","text":"<p>Introduction: Return the 3-dimensional minimum cartesian distance between A and B</p> <p>Format: <code>ST_3DDistance (A:geometry, B:geometry)</code></p> <p>SQL example: <pre><code>SELECT ST_3DDistance(polygondf.countyshape, polygondf.countyshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/snowflake/vector-data/Function/#st_addpoint","title":"ST_AddPoint","text":"<p>Introduction: RETURN Linestring with additional point at the given index, if position is not available the point will be added at the end of line.</p> <p>Format: <code>ST_AddPoint(geom: geometry, point: geometry, position: integer)</code></p> <p>Format: <code>ST_AddPoint(geom: geometry, point: geometry)</code></p> <p>SQL example: <pre><code>SELECT ST_AddPoint(ST_GeomFromText('LINESTRING(0 0, 1 1, 1 0)'), ST_GeomFromText('Point(21 52)'), 1)\n\nSELECT ST_AddPoint(ST_GeomFromText('Linestring(0 0, 1 1, 1 0)'), ST_GeomFromText('Point(21 52)'))\n</code></pre></p> <p>Output: <pre><code>LINESTRING(0 0, 21 52, 1 1, 1 0)\nLINESTRING(0 0, 1 1, 1 0, 21 52)\n</code></pre></p>"},{"location":"api/snowflake/vector-data/Function/#st_affine","title":"ST_Affine","text":"<p>Introduction: Apply an affine transformation to the given geometry.</p> <p>ST_Affine has 2 overloaded signatures:</p> <p><code>ST_Affine(geometry, a, b, c, d, e, f, g, h, i, xOff, yOff, zOff)</code></p> <p><code>ST_Affine(geometry, a, b, d, e, xOff, yOff)</code></p> <p>Based on the invoked function, the following transformation is applied:</p> <p><code>x = a * x + b * y + c * z + xOff OR x = a * x + b * y + xOff</code></p> <p><code>y = d * x + e * y + f * z + yOff OR y = d * x + e * y + yOff</code></p> <p><code>z = g * x + f * y + i * z + zOff OR z = g * x + f * y + zOff</code></p> <p>If the given geometry is empty, the result is also empty.</p> <p>Format:</p> <p><code>ST_Affine(geometry, a, b, c, d, e, f, g, h, i, xOff, yOff, zOff)</code></p> <p><code>ST_Affine(geometry, a, b, d, e, xOff, yOff)</code></p> <pre><code>ST_Affine(geometry, 1, 2, 4, 1, 1, 2, 3, 2, 5, 4, 8, 3)\n</code></pre> <p>Input: <code>LINESTRING EMPTY</code></p> <p>Output: <code>LINESTRING EMPTY</code></p> <p>Input: <code>POLYGON ((1 0 1, 1 1 1, 2 2 2, 1 0 1))</code></p> <p>Output: <code>POLYGON Z((9 11 11, 11 12 13, 18 16 23, 9 11 11))</code></p> <p>Input: <code>POLYGON ((1 0, 1 1, 2 1, 2 0, 1 0), (1 0.5, 1 0.75, 1.5 0.75, 1.5 0.5, 1 0.5))</code></p> <p>Output: <code>POLYGON((5 9, 7 10, 8 11, 6 10, 5 9), (6 9.5, 6.5 9.75, 7 10.25, 6.5 10, 6 9.5))</code></p> <pre><code>ST_Affine(geometry, 1, 2, 1, 2, 1, 2)\n</code></pre> <p>Input: <code>POLYGON EMPTY</code></p> <p>Output: <code>POLYGON EMPTY</code></p> <p>Input: <code>GEOMETRYCOLLECTION (MULTIPOLYGON (((1 0, 1 1, 2 1, 2 0, 1 0), (1 0.5, 1 0.75, 1.5 0.75, 1.5 0.5, 1 0.5)), ((5 0, 5 5, 7 5, 7 0, 5 0))), POINT (10 10))</code></p> <p>Output: <code>GEOMETRYCOLLECTION (MULTIPOLYGON (((2 3, 4 5, 5 6, 3 4, 2 3), (3 4, 3.5 4.5, 4 5, 3.5 4.5, 3 4)), ((6 7, 16 17, 18 19, 8 9, 6 7))), POINT (31 32))</code></p> <p>Input: <code>POLYGON ((1 0 1, 1 1 1, 2 2 2, 1 0 1))</code></p> <p>Output: <code>POLYGON Z((2 3 1, 4 5 1, 7 8 2, 2 3 1))</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_angle","title":"ST_Angle","text":"<p>Introduction: Computes and returns the angle between two vectors represented by the provided points or linestrings.</p> <p>There are three variants possible for ST_Angle:</p> <p><code>ST_Angle(point1: Geometry, point2: Geometry, point3: Geometry, point4: Geometry)</code> Computes the angle formed by vectors represented by point1 - point2 and point3 - point4</p> <p><code>ST_Angle(point1: Geometry, point2: Geometry, point3: Geometry)</code> Computes the angle formed by vectors represented by point2 - point1 and point2 - point3</p> <p><code>ST_Angle(line1: Geometry, line2: Geometry)</code> Computes the angle formed by vectors S1 - E1 and S2 - E2, where S and E denote start and end points respectively</p> <p>Note</p> <p>If any other geometry type is provided, ST_Angle throws an IllegalArgumentException.</p> <p>Additionally, if any of the provided geometry is empty, ST_Angle throws an IllegalArgumentException.</p> <p>Note</p> <p>If a 3D geometry is provided, ST_Angle computes the angle ignoring the z ordinate, equivalent to calling ST_Angle for corresponding 2D geometries.</p> <p>Tip</p> <p>ST_Angle returns the angle in radian between 0 and 2\\Pi. To convert the angle to degrees, use ST_Degrees.</p> <p>Format: <code>ST_Angle(p1, p2, p3, p4) | ST_Angle(p1, p2, p3) | ST_Angle(line1, line2)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Angle(ST_GeomFromWKT('POINT(0 0)'), ST_GeomFromWKT('POINT (1 1)'), ST_GeomFromWKT('POINT(1 0)'), ST_GeomFromWKT('POINT(6 2)'))\n</code></pre> <p>Output:</p> <pre><code>0.4048917862850834\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_Angle(ST_GeomFromWKT('POINT (1 1)'), ST_GeomFromWKT('POINT (0 0)'), ST_GeomFromWKT('POINT(3 2)'))\n</code></pre> <p>Output:</p> <pre><code>0.19739555984988044\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_Angle(ST_GeomFromWKT('LINESTRING (0 0, 1 1)'), ST_GeomFromWKT('LINESTRING (0 0, 3 2)'))\n</code></pre> <p>Output:</p> <pre><code>0.19739555984988044\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_area","title":"ST_Area","text":"<p>Introduction: Return the area of A</p> <p>Format: <code>ST_Area (A:geometry)</code></p> <p>SQL example: <pre><code>SELECT ST_Area(polygondf.countyshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/snowflake/vector-data/Function/#st_areaspheroid","title":"ST_AreaSpheroid","text":"<p>Introduction: Return the geodesic area of A using WGS84 spheroid. Unit is square meter. Works better for large geometries (country level) compared to <code>ST_Area</code> + <code>ST_Transform</code>. It is equivalent to PostGIS <code>ST_Area(geography, use_spheroid=true)</code> function and produces nearly identical results.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lat/lon order. You can use ST_FlipCoordinates to swap lat and lon.</p> <p>Format: <code>ST_AreaSpheroid (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_AreaSpheroid(ST_GeomFromWKT('Polygon ((35 34, 30 28, 34 25, 35 34))'))\n</code></pre> <p>Output: <code>201824850811.76245</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_asbinary","title":"ST_AsBinary","text":"<p>Introduction: Return the Well-Known Binary representation of a geometry</p> <p>Format: <code>ST_AsBinary (A:geometry)</code></p> <p>SQL example: <pre><code>SELECT ST_AsBinary(polygondf.countyshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/snowflake/vector-data/Function/#st_asewkb","title":"ST_AsEWKB","text":"<p>Introduction: Return the Extended Well-Known Binary representation of a geometry. EWKB is an extended version of WKB which includes the SRID of the geometry. The format originated in PostGIS but is supported by many GIS tools. If the geometry is lacking SRID a WKB format is produced. See ST_SetSRID</p> <p>Format: <code>ST_AsEWKB (A:geometry)</code></p> <p>SQL example: <pre><code>SELECT ST_AsEWKB(polygondf.countyshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/snowflake/vector-data/Function/#st_asewkt","title":"ST_AsEWKT","text":"<p>Introduction: Return the Extended Well-Known Text representation of a geometry. EWKT is an extended version of WKT which includes the SRID of the geometry. The format originated in PostGIS but is supported by many GIS tools. If the geometry is lacking SRID a WKT format is produced. See ST_SetSRID</p> <p>Format: <code>ST_AsEWKT (A:geometry)</code></p> <p>SQL example: <pre><code>SELECT ST_AsEWKT(polygondf.countyshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/snowflake/vector-data/Function/#st_asgeojson","title":"ST_AsGeoJSON","text":"<p>Introduction: Return the GeoJSON string representation of a geometry</p> <p>Format: <code>ST_AsGeoJSON (A:geometry)</code></p> <p>SQL example: <pre><code>SELECT ST_AsGeoJSON(polygondf.countyshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/snowflake/vector-data/Function/#st_asgml","title":"ST_AsGML","text":"<p>Introduction: Return the GML string representation of a geometry</p> <p>Format: <code>ST_AsGML (A:geometry)</code></p> <p>SQL example: <pre><code>SELECT ST_AsGML(polygondf.countyshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/snowflake/vector-data/Function/#st_askml","title":"ST_AsKML","text":"<p>Introduction: Return the KML string representation of a geometry</p> <p>Format: <code>ST_AsKML (A:geometry)</code></p> <p>SQL example: <pre><code>SELECT ST_AsKML(polygondf.countyshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/snowflake/vector-data/Function/#st_astext","title":"ST_AsText","text":"<p>Introduction: Return the Well-Known Text string representation of a geometry</p> <p>Format: <code>ST_AsText (A:geometry)</code></p> <p>SQL example: <pre><code>SELECT ST_AsText(polygondf.countyshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/snowflake/vector-data/Function/#st_azimuth","title":"ST_Azimuth","text":"<p>Introduction: Returns Azimuth for two given points in radians null otherwise.</p> <p>Format: <code>ST_Azimuth(pointA: Point, pointB: Point)</code></p> <p>SQL example: <pre><code>SELECT ST_Azimuth(ST_POINT(0.0, 25.0), ST_POINT(0.0, 0.0))\n</code></pre></p> <p>Output: <code>3.141592653589793</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_boundary","title":"ST_Boundary","text":"<p>Introduction: Returns the closure of the combinatorial boundary of this Geometry.</p> <p>Format: <code>ST_Boundary(geom: geometry)</code></p> <p>SQL example: <pre><code>SELECT ST_Boundary(ST_GeomFromText('POLYGON((1 1,0 0, -1 1, 1 1))'))\n</code></pre></p> <p>Output: <code>LINESTRING (1 1, 0 0, -1 1, 1 1)</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_boundingdiagonal","title":"ST_BoundingDiagonal","text":"<p>Introduction: Returns a linestring spanning minimum and maximum values of each dimension of the given geometry's coordinates as its start and end point respectively. If an empty geometry is provided, the returned LineString is also empty. If a single vertex (POINT) is provided, the returned LineString has both the start and end points same as the points coordinates</p> <p>Format: <code>ST_BoundingDiagonal(geom: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_BoundingDiagonal(ST_GeomFromWKT(geom))\n</code></pre> <p>Input: <code>POLYGON ((1 1 1, 3 3 3, 0 1 4, 4 4 0, 1 1 1))</code></p> <p>Output: <code>LINESTRING Z(0 1 1, 4 4 4)</code></p> <p>Input: <code>POINT (10 10)</code></p> <p>Output: <code>LINESTRING (10 10, 10 10)</code></p> <p>Input: <code>GEOMETRYCOLLECTION(POLYGON ((5 5 5, -1 2 3, -1 -1 0, 5 5 5)), POINT (10 3 3))</code></p> <p>Output: <code>LINESTRING Z(-1 -1 0, 10 5 5)</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_buffer","title":"ST_Buffer","text":"<p>Introduction: Returns a geometry/geography that represents all points whose distance from this Geometry/geography is less than or equal to distance.</p> <p>Format: <code>ST_Buffer (A:geometry, buffer: Double)</code></p> <p>SQL example: <pre><code>SELECT ST_Buffer(polygondf.countyshape, 1)\nFROM polygondf\n</code></pre></p>"},{"location":"api/snowflake/vector-data/Function/#st_buildarea","title":"ST_BuildArea","text":"<p>Introduction: Returns the areal geometry formed by the constituent linework of the input geometry.</p> <p>Format: <code>ST_BuildArea (A:geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_BuildArea(\nST_GeomFromText('MULTILINESTRING((0 0, 20 0, 20 20, 0 20, 0 0),(2 2, 18 2, 18 18, 2 18, 2 2))')\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+----------------------------------------------------------------------------+\n|geom                                                                        |\n+----------------------------------------------------------------------------+\n|POLYGON((0 0,0 20,20 20,20 0,0 0),(2 2,18 2,18 18,2 18,2 2))                |\n+----------------------------------------------------------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_centroid","title":"ST_Centroid","text":"<p>Introduction: Return the centroid point of A</p> <p>Format: <code>ST_Centroid (A:geometry)</code></p> <p>SQL example: <pre><code>SELECT ST_Centroid(polygondf.countyshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/snowflake/vector-data/Function/#st_closestpoint","title":"ST_ClosestPoint","text":"<p>Introduction: Returns the 2-dimensional point on geom1 that is closest to geom2. This is the first point of the shortest line between the geometries. If using 3D geometries, the Z coordinates will be ignored. If you have a 3D Geometry, you may prefer to use ST_3DClosestPoint. It will throw an exception indicates illegal argument if one of the params is an empty geometry.</p> <p>Format: <code>ST_ClosestPoint(g1: Geometry, g2: Geometry)</code></p> <p>SQL Example: <pre><code>SELECT ST_AsText( ST_ClosestPoint(g1, g2)) As ptwkt;\n</code></pre></p> <p>Input: <code>g1: POINT (160 40), g2: LINESTRING (10 30, 50 50, 30 110, 70 90, 180 140, 130 190)</code></p> <p>Output: <code>POINT(160 40)</code></p> <p>Input: <code>g1: LINESTRING (10 30, 50 50, 30 110, 70 90, 180 140, 130 190), g2: POINT (160 40)</code></p> <p>Output: <code>POINT(125.75342465753425 115.34246575342466)</code></p> <p>Input: <code>g1: 'POLYGON ((190 150, 20 10, 160 70, 190 150))', g2: ST_Buffer('POINT(80 160)', 30)</code></p> <p>Output: <code>POINT(131.59149149528952 101.89887534906197)</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_collect","title":"ST_Collect","text":"<p>Introduction: Returns MultiGeometry object based on a geometry column.</p> <p>Format</p> <p><code>ST_Collect(*geom: geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_Collect(\ntbl.geom) AS geom\n</code></pre> <p>Result:</p> <pre><code>+---------------------------------------------------------------+\n|geom                                                           |\n+---------------------------------------------------------------+\n|MULTIPOINT ((21.427834 52.042576573), (45.342524 56.342354355))|\n+---------------------------------------------------------------+\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_Collect(\nArray(\nST_GeomFromText('POINT(21.427834 52.042576573)'),\nST_GeomFromText('POINT(45.342524 56.342354355)')\n)\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+---------------------------------------------------------------+\n|geom                                                           |\n+---------------------------------------------------------------+\n|MULTIPOINT ((21.427834 52.042576573), (45.342524 56.342354355))|\n+---------------------------------------------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_collectionextract","title":"ST_CollectionExtract","text":"<p>Introduction: Returns a homogeneous multi-geometry from a given geometry collection.</p> <p>The type numbers are: 1. POINT 2. LINESTRING 3. POLYGON</p> <p>If the type parameter is omitted a multi-geometry of the highest dimension is returned.</p> <p>Format: <code>ST_CollectionExtract (A:geometry)</code></p> <p>Format: <code>ST_CollectionExtract (A:geometry, type:Int)</code></p> <p>Example:</p> <pre><code>WITH test_data as (\nST_GeomFromText(\n'GEOMETRYCOLLECTION(POINT(40 10), POLYGON((0 0, 0 5, 5 5, 5 0, 0 0)))'\n) as geom\n)\nSELECT ST_CollectionExtract(geom) as c1, ST_CollectionExtract(geom, 1) as c2\nFROM test_data\n</code></pre> <p>Result:</p> <pre><code>+----------------------------------------------------------------------------+\n|c1                                        |c2                               |\n+----------------------------------------------------------------------------+\n|MULTIPOLYGON(((0 0, 0 5, 5 5, 5 0, 0 0))) |MULTIPOINT(40 10)                |              |\n+----------------------------------------------------------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_concavehull","title":"ST_ConcaveHull","text":"<p>Introduction: Return the Concave Hull of polgyon A, with alpha set to pctConvex[0, 1] in the Delaunay Triangulation method, the concave hull will not contain a hole unless allowHoles is set to true</p> <p>Format: <code>ST_ConcaveHull (A:geometry, pctConvex:float)</code></p> <p>Format: <code>ST_ConcaveHull (A:geometry, pctConvex:float, allowHoles:Boolean)</code></p> <p>SQL example: <pre><code>SELECT ST_ConcaveHull(polygondf.countyshape, pctConvex)`\nFROM polygondf\n</code></pre></p>"},{"location":"api/snowflake/vector-data/Function/#st_convexhull","title":"ST_ConvexHull","text":"<p>Introduction: Return the Convex Hull of polgyon A</p> <p>Format: <code>ST_ConvexHull (A:geometry)</code></p> <p>SQL example: <pre><code>SELECT ST_ConvexHull(polygondf.countyshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/snowflake/vector-data/Function/#st_coorddim","title":"ST_CoordDim","text":"<p>Introduction: Returns the coordinate dimensions of the geometry. It is an alias of <code>ST_NDims</code>.</p> <p>Format: <code>ST_CoordDim(geom: Geometry)</code></p> <p>SQL Example with x, y, z coordinate:</p> <pre><code>SELECT ST_CoordDim(ST_GeomFromText('POINT(1 1 2'))\n</code></pre> <p>Output:</p> <pre><code>3\n</code></pre> <p>SQL Example with x, y coordinate:</p> <pre><code>SELECT ST_CoordDim(ST_GeomFromWKT('POINT(3 7)'))\n</code></pre> <p>Output:</p> <pre><code>2\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_degrees","title":"ST_Degrees","text":"<p>Introduction: Convert an angle in radian to degrees.</p> <p>Format: <code>ST_Degrees(angleInRadian)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Degrees(0.19739555984988044)\n</code></pre> <p>Output:</p> <pre><code>11.309932474020195\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_difference","title":"ST_Difference","text":"<p>Introduction: Return the difference between geometry A and B (return part of geometry A that does not intersect geometry B)</p> <p>Format: <code>ST_Difference (A:geometry, B:geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_Difference(ST_GeomFromWKT('POLYGON ((-3 -3, 3 -3, 3 3, -3 3, -3 -3))'), ST_GeomFromWKT('POLYGON ((0 -4, 4 -4, 4 4, 0 4, 0 -4))'))\n</code></pre> <p>Result:</p> <pre><code>POLYGON ((0 -3, -3 -3, -3 3, 0 3, 0 -3))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_dimension","title":"ST_Dimension","text":"<p>Introduction: Return the topological dimension of this Geometry object, which must be less than or equal to the coordinate dimension. OGC SPEC s2.1.1.1 - returns 0 for POINT, 1 for LINESTRING, 2 for POLYGON, and the largest dimension of the components of a GEOMETRYCOLLECTION. If the dimension is unknown (e.g. for an empty GEOMETRYCOLLECTION) 0 is returned.</p> <p>Format: <code>ST_Dimension (A: Geometry) | ST_Dimension (C: Geometrycollection)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Dimension('GEOMETRYCOLLECTION(LINESTRING(1 1,0 0),POINT(0 0))');\n</code></pre> <p>Output:</p> <pre><code>1\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_distance","title":"ST_Distance","text":"<p>Introduction: Return the Euclidean distance between A and B</p> <p>Format: <code>ST_Distance (A:geometry, B:geometry)</code></p> <p>SQL example: <pre><code>SELECT ST_Distance(polygondf.countyshape, polygondf.countyshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/snowflake/vector-data/Function/#st_distancesphere","title":"ST_DistanceSphere","text":"<p>Introduction: Return the haversine / great-circle distance of A using a given earth radius (default radius: 6371008.0). Unit is meter. Compared to <code>ST_Distance</code> + <code>ST_Transform</code>, it works better for datasets that cover large regions such as continents or the entire planet. It is equivalent to PostGIS <code>ST_Distance(geography, use_spheroid=false)</code> and <code>ST_DistanceSphere</code> function and produces nearly identical results. It provides faster but less accurate result compared to <code>ST_DistanceSpheroid</code>.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lat/lon order. You can use ST_FlipCoordinates to swap lat and lon. For non-point data, we first take the centroids of both geometries and then compute the distance.</p> <p>Format: <code>ST_DistanceSphere (A:geometry)</code></p> <p>SQL example 1: <pre><code>SELECT ST_DistanceSphere(ST_GeomFromWKT('POINT (51.3168 -0.56)'), ST_GeomFromWKT('POINT (55.9533 -3.1883)'))\n</code></pre></p> <p>Output: <code>543796.9506134904</code></p> <p>SQL example 2: <pre><code>SELECT ST_DistanceSphere(ST_GeomFromWKT('POINT (51.3168 -0.56)'), ST_GeomFromWKT('POINT (55.9533 -3.1883)'), 6378137.0)\n</code></pre></p> <p>Output: <code>544405.4459192449</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_distancespheroid","title":"ST_DistanceSpheroid","text":"<p>Introduction: Return the geodesic distance of A using WGS84 spheroid. Unit is meter. Compared to <code>ST_Distance</code> + <code>ST_Transform</code>, it works better for datasets that cover large regions such as continents or the entire planet. It is equivalent to PostGIS <code>ST_Distance(geography, use_spheroid=true)</code> and <code>ST_DistanceSpheroid</code> function and produces nearly identical results. It provides slower but more accurate result compared to <code>ST_DistanceSphere</code>.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lat/lon order. You can use ST_FlipCoordinates to swap lat and lon. For non-point data, we first take the centroids of both geometries and then compute the distance.</p> <p>Format: <code>ST_DistanceSpheroid (A:geometry)</code></p> <p>SQL example: <pre><code>SELECT ST_DistanceSpheroid(ST_GeomFromWKT('POINT (51.3168 -0.56)'), ST_GeomFromWKT('POINT (55.9533 -3.1883)'))\n</code></pre></p> <p>Output: <code>544430.9411996207</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_dump","title":"ST_Dump","text":"<p>Introduction: This is an aggregate function that takes a column of of geometries as input, and returns a single GeometryCollection of all these geometries.</p> <p>Format: <code>ST_Dump(geom: geometry)</code></p> <p>SQL example: <pre><code>SELECT ST_Dump(tbl.geom)\n</code></pre></p> <p>Output: <code>GeometryCollection ( (10 40), (40 30), (20 20), (30 10) )</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_dumppoints","title":"ST_DumpPoints","text":"<p>Introduction: Returns a MultiPoint geometry which consists of individual points that compose the input line string.</p> <p>Format: <code>ST_DumpPoints(geom: geometry)</code></p> <p>SQL example: <pre><code>SELECT ST_DumpPoints(ST_GeomFromText('LINESTRING (0 0, 1 1, 1 0)'))\n</code></pre></p> <p>Output: <code>MultiPoint ((0 0), (0 1), (1 1), (1 0), (0 0))</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_endpoint","title":"ST_EndPoint","text":"<p>Introduction: Returns last point of given linestring.</p> <p>Format: <code>ST_EndPoint(geom: geometry)</code></p> <p>SQL example: <pre><code>SELECT ST_EndPoint(ST_GeomFromText('LINESTRING(100 150,50 60, 70 80, 160 170)'))\n</code></pre></p> <p>Output: <code>POINT(160 170)</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_envelope","title":"ST_Envelope","text":"<p>Introduction: Return the envelop boundary of A</p> <p>Format: <code>ST_Envelope (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_Envelope(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_exteriorring","title":"ST_ExteriorRing","text":"<p>Introduction: Returns a line string representing the exterior ring of the POLYGON geometry. Return NULL if the geometry is not a polygon.</p> <p>Format: <code>ST_ExteriorRing(geom: geometry)</code></p> <p>SQL example: <pre><code>SELECT ST_ExteriorRing(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre></p> <p>Output: <code>LINESTRING (0 0, 1 1, 1 2, 1 1, 0 0)</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_flipcoordinates","title":"ST_FlipCoordinates","text":"<p>Introduction: Returns a version of the given geometry with X and Y axis flipped.</p> <p>Format: <code>ST_FlipCoordinates(A:geometry)</code></p> <p>SQL example: <pre><code>SELECT ST_FlipCoordinates(df.geometry)\nFROM df\n</code></pre></p> <p>Input: <code>POINT (1 2)</code></p> <p>Output: <code>POINT (2 1)</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_force_2d","title":"ST_Force_2D","text":"<p>Introduction: Forces the geometries into a \"2-dimensional mode\" so that all output representations will only have the X and Y coordinates</p> <p>Format: <code>ST_Force_2D (A:geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(\nST_Force_2D(ST_GeomFromText('POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))'))\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+---------------------------------------------------------------+\n|geom                                                           |\n+---------------------------------------------------------------+\n|POLYGON((0 0,0 5,5 0,0 0),(1 1,3 1,1 3,1 1))                   |\n+---------------------------------------------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_force3d","title":"ST_Force3D","text":"<p>Introduction: Forces the geometry into a 3-dimensional model so that all output representations will have X, Y and Z coordinates. An optionally given zValue is tacked onto the geometry if the geometry is 2-dimensional. Default value of zValue is 0.0 If the given geometry is 3-dimensional, no change is performed on it. If the given geometry is empty, no change is performed on it.</p> <p>Note</p> <p>Example output is after calling ST_AsText() on returned geometry, which adds Z for in the WKT for 3D geometries</p> <p>Format: <code>ST_Force3D(geometry, zValue)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Force3D(geometry) AS geom\n</code></pre> <p>Input: <code>LINESTRING(0 1, 1 2, 2 1)</code></p> <p>Output: <code>LINESTRING Z(0 1 0, 1 2 0, 2 1 0)</code></p> <p>Input: <code>POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))</code></p> <p>Output: <code>POLYGON Z((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))</code></p> <pre><code>SELECT ST_Force3D(geometry, 2.3) AS geom\n</code></pre> <p>Input: <code>LINESTRING(0 1, 1 2, 2 1)</code></p> <p>Output: <code>LINESTRING Z(0 1 2.3, 1 2 2.3, 2 1 2.3)</code></p> <p>Input: <code>POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))</code></p> <p>Output: <code>POLYGON Z((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))</code></p> <p>Input: <code>LINESTRING EMPTY</code></p> <p>Output: <code>LINESTRING EMPTY</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_frechetdistance","title":"ST_FrechetDistance","text":"<p>Introduction: Computes and returns discrete Frechet Distance between the given two geometries, based on Computing Discrete Frechet Distance</p> <p>If any of the geometries is empty, returns 0.0</p> <p>Format: <code>ST_FrechetDistance(g1: Geometry, g2: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_FrechetDistance(ST_GeomFromWKT('POINT (0 1)'), ST_GeomFromWKT('LINESTRING (0 0, 1 0, 2 0, 3 0, 4 0, 5 0)'))\n</code></pre> <p>Output:</p> <pre><code>5.0990195135927845\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_geohash","title":"ST_GeoHash","text":"<p>Introduction: Returns GeoHash of the geometry with given precision</p> <p>Format: <code>ST_GeoHash(geom: geometry, precision: int)</code></p> <p>Example:</p> <p>Query:</p> <pre><code>SELECT ST_GeoHash(ST_GeomFromText('POINT(21.427834 52.042576573)'), 5) AS geohash\n</code></pre> <p>Result:</p> <pre><code>+-----------------------------+\n|geohash                      |\n+-----------------------------+\n|u3r0p                        |\n+-----------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_geometricmedian","title":"ST_GeometricMedian","text":"<p>Introduction: Computes the approximate geometric median of a MultiPoint geometry using the Weiszfeld algorithm. The geometric median provides a centrality measure that is less sensitive to outlier points than the centroid.</p> <p>The algorithm will iterate until the distance change between successive iterations is less than the supplied <code>tolerance</code> parameter. If this condition has not been met after <code>maxIter</code> iterations, the function will produce an error and exit, unless <code>failIfNotConverged</code> is set to <code>false</code>.</p> <p>If a <code>tolerance</code> value is not provided, a default <code>tolerance</code> value is <code>1e-6</code>.</p> <p>Format: <code>ST_GeometricMedian(geom: geometry, tolerance: float, maxIter: integer, failIfNotConverged: boolean)</code></p> <p>Format: <code>ST_GeometricMedian(geom: geometry, tolerance: float, maxIter: integer)</code></p> <p>Format: <code>ST_GeometricMedian(geom: geometry, tolerance: float)</code></p> <p>Format: <code>ST_GeometricMedian(geom: geometry)</code></p> <p>Default parameters: <code>tolerance: 1e-6, maxIter: 1000, failIfNotConverged: false</code></p> <p>Example: <pre><code>SELECT ST_GeometricMedian(ST_GeomFromWKT('MULTIPOINT((0 0), (1 1), (2 2), (200 200))'))\n</code></pre></p> <p>Output: <pre><code>POINT (1.9761550281255005 1.9761550281255005)\n</code></pre></p>"},{"location":"api/snowflake/vector-data/Function/#st_geometryn","title":"ST_GeometryN","text":"<p>Introduction: Return the 0-based Nth geometry if the geometry is a GEOMETRYCOLLECTION, (MULTI)POINT, (MULTI)LINESTRING, MULTICURVE or (MULTI)POLYGON. Otherwise, return null</p> <p>Format: <code>ST_GeometryN(geom: geometry, n: Int)</code></p> <p>SQL example: <pre><code>SELECT ST_GeometryN(ST_GeomFromText('MULTIPOINT((1 2), (3 4), (5 6), (8 9))'), 1)\n</code></pre></p> <p>Output: <code>POINT (3 4)</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_geometrytype","title":"ST_GeometryType","text":"<p>Introduction: Returns the type of the geometry as a string. EG: 'ST_Linestring', 'ST_Polygon' etc.</p> <p>Format: <code>ST_GeometryType (A:geometry)</code></p> <p>SQL example: <pre><code>SELECT ST_GeometryType(polygondf.countyshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/snowflake/vector-data/Function/#st_hausdorffdistance","title":"ST_HausdorffDistance","text":"<p>Introduction: Returns a discretized (and hence approximate) Hausdorff distance between the given 2 geometries. Optionally, a densityFraction parameter can be specified, which gives more accurate results by densifying segments before computing hausdorff distance between them. Each segment is broken down into equal-length subsegments whose ratio with segment length is closest to the given density fraction.</p> <p>Hence, the lower the densityFrac value, the more accurate is the computed hausdorff distance, and the more time it takes to compute it.</p> <p>If any of the geometry is empty, 0.0 is returned.</p> <p>Note</p> <p>Accepted range of densityFrac is (0.0, 1.0], if any other value is provided, ST_HausdorffDistance throws an IllegalArgumentException</p> <p>Note</p> <p>Even though the function accepts 3D geometry, the z ordinate is ignored and the computed hausdorff distance is equivalent to the geometries not having the z ordinate.</p> <p>Format: <code>ST_HausdorffDistance(g1: Geometry, g2: Geometry, densityFrac: Double)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_HausdorffDistance(ST_GeomFromWKT('POINT (0.0 1.0)'), ST_GeomFromWKT('LINESTRING (0 0, 1 0, 2 0, 3 0, 4 0, 5 0)'), 0.1)\n</code></pre> <p>Output:</p> <pre><code>5.0990195135927845\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_HausdorffDistance(ST_GeomFromText('POLYGON Z((1 0 1, 1 1 2, 2 1 5, 2 0 1, 1 0 1))'), ST_GeomFromText('POLYGON Z((4 0 4, 6 1 4, 6 4 9, 6 1 3, 4 0 4))'))\n</code></pre> <p>Output:</p> <pre><code>5.0\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_interiorringn","title":"ST_InteriorRingN","text":"<p>Introduction: Returns the Nth interior linestring ring of the polygon geometry. Returns NULL if the geometry is not a polygon or the given N is out of range</p> <p>Format: <code>ST_InteriorRingN(geom: geometry, n: Int)</code></p> <p>SQL example: <pre><code>SELECT ST_InteriorRingN(ST_GeomFromText('POLYGON((0 0, 0 5, 5 5, 5 0, 0 0), (1 1, 2 1, 2 2, 1 2, 1 1), (1 3, 2 3, 2 4, 1 4, 1 3), (3 3, 4 3, 4 4, 3 4, 3 3))'), 0)\n</code></pre></p> <p>Output: <code>LINESTRING (1 1, 2 1, 2 2, 1 2, 1 1)</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_intersection","title":"ST_Intersection","text":"<p>Introduction: Return the intersection geometry of A and B</p> <p>Format: <code>ST_Intersection (A:geometry, B:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_Intersection(polygondf.countyshape, polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_isclosed","title":"ST_IsClosed","text":"<p>Introduction: RETURNS true if the LINESTRING start and end point are the same.</p> <p>Format: <code>ST_IsClosed(geom: geometry)</code></p> <p>SQL example: <pre><code>SELECT ST_IsClosed(ST_GeomFromText('LINESTRING(0 0, 1 1, 1 0)'))\n</code></pre></p> <p>Output: <code>false</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_iscollection","title":"ST_IsCollection","text":"<p>Introduction: Returns <code>TRUE</code> if the geometry type of the input is a geometry collection type. Collection types are the following:</p> <ul> <li>GEOMETRYCOLLECTION</li> <li>MULTI{POINT, POLYGON, LINESTRING}</li> </ul> <p>Format: <code>ST_IsCollection(geom: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_IsCollection(ST_GeomFromText('MULTIPOINT(0 0), (6 6)'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_IsCollection(ST_GeomFromText('POINT(5 5)'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_isempty","title":"ST_IsEmpty","text":"<p>Introduction: Test if a geometry is empty geometry</p> <p>Format: <code>ST_IsEmpty (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_IsEmpty(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_isring","title":"ST_IsRing","text":"<p>Introduction: RETURN true if LINESTRING is ST_IsClosed and ST_IsSimple.</p> <p>Format: <code>ST_IsRing(geom: geometry)</code></p> <p>SQL example: <pre><code>SELECT ST_IsRing(ST_GeomFromText('LINESTRING(0 0, 0 1, 1 1, 1 0, 0 0)'))\n</code></pre></p> <p>Output: <code>true</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_issimple","title":"ST_IsSimple","text":"<p>Introduction: Test if geometry's only self-intersections are at boundary points.</p> <p>Format: <code>ST_IsSimple (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_IsSimple(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_isvalid","title":"ST_IsValid","text":"<p>Introduction: Test if a geometry is well formed. The function can be invoked with just the geometry or with an additional flag (from <code>v1.5.1</code>). The flag alters the validity checking behavior. The flags parameter is a bitfield with the following options:</p> <ul> <li>0 (default): Use usual OGC SFS (Simple Features Specification) validity semantics.</li> <li>1: \"ESRI flag\", Accepts certain self-touching rings as valid, which are considered invalid under OGC standards.</li> </ul> <p>Formats: <pre><code>ST_IsValid (A: Geometry)\n</code></pre> <pre><code>ST_IsValid (A: Geometry, flag: Integer)\n</code></pre></p> <p>SQL Example:</p> <pre><code>SELECT ST_IsValid(ST_GeomFromWKT('POLYGON((0 0, 10 0, 10 10, 0 10, 0 0), (15 15, 15 20, 20 20, 20 15, 15 15))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_isvalidreason","title":"ST_IsValidReason","text":"<p>Introduction: Returns text stating if the geometry is valid. If not, it provides a reason why it is invalid. The function can be invoked with just the geometry or with an additional flag. The flag alters the validity checking behavior. The flags parameter is a bitfield with the following options:</p> <ul> <li>0 (default): Use usual OGC SFS (Simple Features Specification) validity semantics.</li> <li>1: \"ESRI flag\", Accepts certain self-touching rings as valid, which are considered invalid under OGC standards.</li> </ul> <p>Formats: <pre><code>ST_IsValidReason (A: Geometry)\n</code></pre> <pre><code>ST_IsValidReason (A: Geometry, flag: Integer)\n</code></pre></p> <p>SQL Example for valid geometry:</p> <pre><code>SELECT ST_IsValidReason(ST_GeomFromWKT('POLYGON ((100 100, 100 300, 300 300, 300 100, 100 100))')) as validity_info\n</code></pre> <p>Output:</p> <pre><code>Valid Geometry\n</code></pre> <p>SQL Example for invalid geometries:</p> <pre><code>SELECT gid, ST_IsValidReason(geom) as validity_info\nFROM Geometry_table\nWHERE ST_IsValid(geom) = false\nORDER BY gid\n</code></pre> <p>Output:</p> <pre><code>gid  |                  validity_info\n-----+----------------------------------------------------\n5330 | Self-intersection at or near point (32.0, 5.0, NaN)\n5340 | Self-intersection at or near point (42.0, 5.0, NaN)\n5350 | Self-intersection at or near point (52.0, 5.0, NaN)\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_length","title":"ST_Length","text":"<p>Introduction: Return the perimeter of A</p> <p>Format: ST_Length (A:geometry)</p> <p>SQL example: <pre><code>SELECT ST_Length(polygondf.countyshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/snowflake/vector-data/Function/#st_lengthspheroid","title":"ST_LengthSpheroid","text":"<p>Introduction: Return the geodesic perimeter of A using WGS84 spheroid. Unit is meter. Works better for large geometries (country level) compared to <code>ST_Length</code> + <code>ST_Transform</code>. It is equivalent to PostGIS <code>ST_Length(geography, use_spheroid=true)</code> and <code>ST_LengthSpheroid</code> function and produces nearly identical results.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lat/lon order. You can use ST_FlipCoordinates to swap lat and lon.</p> <p>Format: <code>ST_LengthSpheroid (A:geometry)</code></p> <p>SQL example: <pre><code>SELECT ST_LengthSpheroid(ST_GeomFromWKT('Polygon ((0 0, 0 90, 0 0))'))\n</code></pre></p> <p>Output: <code>20037508.342789244</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_linefrommultipoint","title":"ST_LineFromMultiPoint","text":"<p>Introduction: Creates a LineString from a MultiPoint geometry.</p> <p>Format: <code>ST_LineFromMultiPoint (A:geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(\nST_LineFromMultiPoint(ST_GeomFromText('MULTIPOINT((10 40), (40 30), (20 20), (30 10))'))\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+---------------------------------------------------------------+\n|geom                                                           |\n+---------------------------------------------------------------+\n|LINESTRING (10 40, 40 30, 20 20, 30 10)                        |\n+---------------------------------------------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_lineinterpolatepoint","title":"ST_LineInterpolatePoint","text":"<p>Introduction: Returns a point interpolated along a line. First argument must be a LINESTRING. Second argument is a Double between 0 and 1 representing fraction of total linestring length the point has to be located.</p> <p>Format: <code>ST_LineInterpolatePoint (geom: geometry, fraction: Double)</code></p> <p>SQL example: <pre><code>SELECT ST_LineInterpolatePoint(ST_GeomFromWKT('LINESTRING(25 50, 100 125, 150 190)'), 0.2) as Interpolated\n</code></pre></p> <p>Output: <pre><code>+-----------------------------------------+\n|Interpolated                             |\n+-----------------------------------------+\n|POINT (51.5974135047432 76.5974135047432)|\n+-----------------------------------------+\n</code></pre></p>"},{"location":"api/snowflake/vector-data/Function/#st_linelocatepoint","title":"ST_LineLocatePoint","text":"<p>Introduction: Returns a double between 0 and 1, representing the location of the closest point on the LineString as a fraction of its total length. The first argument must be a LINESTRING, and the second argument is a POINT geometry.</p> <p>Format: <code>ST_LineLocatePoint(linestring: Geometry, point: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_LineLocatePoint(ST_GeomFromWKT('LINESTRING(0 0, 1 1, 2 2)'), ST_GeomFromWKT('POINT(0 2)'))\n</code></pre> <p>Output: <pre><code>0.5\n</code></pre></p>"},{"location":"api/snowflake/vector-data/Function/#st_linemerge","title":"ST_LineMerge","text":"<p>Introduction: Returns a LineString formed by sewing together the constituent line work of a MULTILINESTRING.</p> <p>Note</p> <p>Only works for MULTILINESTRING. Using other geometry will return a GEOMETRYCOLLECTION EMPTY. If the MultiLineString can't be merged, the original MULTILINESTRING is returned.</p> <p>Format: <code>ST_LineMerge (A:geometry)</code></p> <pre><code>SELECT ST_LineMerge(geometry)\nFROM df\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_linesubstring","title":"ST_LineSubstring","text":"<p>Introduction: Return a linestring being a substring of the input one starting and ending at the given fractions of total 2d length. Second and third arguments are Double values between 0 and 1. This only works with LINESTRINGs.</p> <p>Format: <code>ST_LineSubstring (geom: geometry, startfraction: Double, endfraction: Double)</code></p> <p>SQL example: <pre><code>SELECT ST_LineSubstring(ST_GeomFromWKT('LINESTRING(25 50, 100 125, 150 190)'), 0.333, 0.666) as Substring\n</code></pre></p> <p>Output: <pre><code>+------------------------------------------------------------------------------------------------+\n|Substring                                                                                       |\n+------------------------------------------------------------------------------------------------+\n|LINESTRING (69.28469348539744 94.28469348539744, 100 125, 111.70035626068274 140.21046313888758)|\n+------------------------------------------------------------------------------------------------+\n</code></pre></p>"},{"location":"api/snowflake/vector-data/Function/#st_makeline","title":"ST_MakeLine","text":"<p>Introduction: Creates a LineString containing the points of Point, MultiPoint, or LineString geometries. Other geometry types cause an error.</p> <p>Format:</p> <p><code>ST_MakeLine(geom1: Geometry, geom2: Geometry)</code></p> <p><code>ST_MakeLine(geoms: Geometry)</code> This Geometry must be a GeometryCollection of the geometry types listed above.</p> <p>SQL Example:</p> <pre><code>SELECT ST_AsText( ST_MakeLine(ST_Point(1,2), ST_Point(3,4)) );\n</code></pre> <p>Output:</p> <pre><code>LINESTRING(1 2,3 4)\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_AsText( ST_MakeLine( 'LINESTRING(0 0, 1 1)', 'LINESTRING(2 2, 3 3)' ) );\n</code></pre> <p>Output:</p> <pre><code> LINESTRING(0 0,1 1,2 2,3 3)\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_makepolygon","title":"ST_MakePolygon","text":"<p>Introduction: Function to convert closed linestring to polygon including holes. The holes must be a MultiLinestring.</p> <p>Format: <code>ST_MakePolygon(geom: geometry, holes: &lt;geometry&gt;)</code></p> <p>Example:</p> <p>Query: <pre><code>SELECT\nST_MakePolygon(\nST_GeomFromText('LINESTRING(7 -1, 7 6, 9 6, 9 1, 7 -1)'),\nST_GeomFromText('MultiLINESTRING((6 2, 8 2, 8 1, 6 1, 6 2))')\n) AS polygon\n</code></pre></p> <p>Result:</p> <pre><code>+----------------------------------------------------------------+\n|polygon                                                         |\n+----------------------------------------------------------------+\n|POLYGON ((7 -1, 7 6, 9 6, 9 1, 7 -1), (6 2, 8 2, 8 1, 6 1, 6 2))|\n+----------------------------------------------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_makevalid","title":"ST_MakeValid","text":"<p>Introduction: Given an invalid geometry, create a valid representation of the geometry.</p> <p>Collapsed geometries are either converted to empty (keepCollaped=true) or a valid geometry of lower dimension (keepCollapsed=false). Default is keepCollapsed=false.</p> <p>Format: <code>ST_MakeValid (A:geometry)</code></p> <p>Format: <code>ST_MakeValid (A:geometry, keepCollapsed:Boolean)</code></p> <p>SQL example:</p> <pre><code>WITH linestring AS (\nSELECT ST_GeomFromWKT('LINESTRING(1 1, 1 1)') AS geom\n) SELECT ST_MakeValid(geom), ST_MakeValid(geom, true) FROM linestring\n</code></pre> <p>Result: <pre><code>+------------------+------------------------+\n|st_makevalid(geom)|st_makevalid(geom, true)|\n+------------------+------------------------+\n|  LINESTRING EMPTY|             POINT (1 1)|\n+------------------+------------------------+\n</code></pre></p> <p>Note</p> <p>In Sedona up to and including version 1.2 the behaviour of ST_MakeValid was different. Be sure to check you code when upgrading. The previous implementation only worked for (multi)polygons and had a different interpretation of the second, boolean, argument. It would also sometimes return multiple geometries for a single geometry input.</p>"},{"location":"api/snowflake/vector-data/Function/#st_minimumboundingcircle","title":"ST_MinimumBoundingCircle","text":"<p>Introduction: Returns the smallest circle polygon that contains a geometry.</p> <p>Format: <code>ST_MinimumBoundingCircle(geom: geometry, [Optional] quadrantSegments:int)</code></p> <p>SQL example: <pre><code>SELECT ST_MinimumBoundingCircle(ST_GeomFromText('POLYGON((1 1,0 0, -1 1, 1 1))'))\n</code></pre></p>"},{"location":"api/snowflake/vector-data/Function/#st_minimumboundingradius","title":"ST_MinimumBoundingRadius","text":"<p>Introduction: Returns two columns containing the center point and radius of the smallest circle that contains a geometry.</p> <p>Format: <code>ST_MinimumBoundingRadius(geom: geometry)</code></p> <p>SQL example: <pre><code>SELECT ST_MinimumBoundingRadius(ST_GeomFromText('POLYGON((1 1,0 0, -1 1, 1 1))'))\n</code></pre></p>"},{"location":"api/snowflake/vector-data/Function/#st_multi","title":"ST_Multi","text":"<p>Introduction: Returns a MultiGeometry object based on the geometry input. ST_Multi is basically an alias for ST_Collect with one geometry.</p> <p>Format</p> <p><code>ST_Multi(geom: geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_Multi(\nST_GeomFromText('POINT(1 1)')\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+---------------------------------------------------------------+\n|geom                                                           |\n+---------------------------------------------------------------+\n|MULTIPOINT (1 1)                                               |\n+---------------------------------------------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_ndims","title":"ST_NDims","text":"<p>Introduction: Returns the coordinate dimension of the geometry.</p> <p>Format: <code>ST_NDims(geom: geometry)</code></p> <p>SQL example with z co-rodinate:</p> <pre><code>SELECT ST_NDims(ST_GeomFromEWKT('POINT(1 1 2)'))\n</code></pre> <p>Output: <code>3</code></p> <p>SQL example with x,y coordinate:</p> <pre><code>SELECT ST_NDims(ST_GeomFromText('POINT(1 1)'))\n</code></pre> <p>Output: <code>2</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_normalize","title":"ST_Normalize","text":"<p>Introduction: Returns the input geometry in its normalized form.</p> <p>Format</p> <p><code>ST_Normalize(geom: geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_AsEWKT(ST_Normalize(ST_GeomFromWKT('POLYGON((0 1, 1 1, 1 0, 0 0, 0 1))'))) AS geom\n</code></pre> <p>Result:</p> <pre><code>+-----------------------------------+\n|geom                               |\n+-----------------------------------+\n|POLYGON ((0 0, 0 1, 1 1, 1 0, 0 0))|\n+-----------------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_npoints","title":"ST_NPoints","text":"<p>Introduction: Return points of the geometry</p> <p>Format: <code>ST_NPoints (A:geometry)</code></p> <pre><code>SELECT ST_NPoints(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_nrings","title":"ST_NRings","text":"<p>Introduction: Returns the number of rings in a Polygon or MultiPolygon. Contrary to ST_NumInteriorRings, this function also takes into account the number of  exterior rings.</p> <p>This function returns 0 for an empty Polygon or MultiPolygon. If the geometry is not a Polygon or MultiPolygon, an IllegalArgument Exception is thrown.</p> <p>Format: <code>ST_NRings(geom: geometry)</code></p> <p>Examples:</p> <p>Input: <code>POLYGON ((1 0, 1 1, 2 1, 2 0, 1 0))</code></p> <p>Output: <code>1</code></p> <p>Input: <code>'MULTIPOLYGON (((1 0, 1 6, 6 6, 6 0, 1 0), (2 1, 2 2, 3 2, 3 1, 2 1)), ((10 0, 10 6, 16 6, 16 0, 10 0), (12 1, 12 2, 13 2, 13 1, 12 1)))'</code></p> <p>Output: <code>4</code></p> <p>Input: <code>'POLYGON EMPTY'</code></p> <p>Output: <code>0</code></p> <p>Input: <code>'LINESTRING (1 0, 1 1, 2 1)'</code></p> <p>Output: <code>Unsupported geometry type: LineString, only Polygon or MultiPolygon geometries are supported.</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_numgeometries","title":"ST_NumGeometries","text":"<p>Introduction: Returns the number of Geometries. If geometry is a GEOMETRYCOLLECTION (or MULTI*) return the number of geometries, for single geometries will return 1.</p> <p>Format: <code>ST_NumGeometries (A:geometry)</code></p> <pre><code>SELECT ST_NumGeometries(df.geometry)\nFROM df\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_numinteriorrings","title":"ST_NumInteriorRings","text":"<p>Introduction: RETURNS number of interior rings of polygon geometries.</p> <p>Format: <code>ST_NumInteriorRings(geom: geometry)</code></p> <p>SQL example: <pre><code>SELECT ST_NumInteriorRings(ST_GeomFromText('POLYGON ((0 0, 0 5, 5 5, 5 0, 0 0), (1 1, 2 1, 2 2, 1 2, 1 1))'))\n</code></pre></p> <p>Output: <code>1</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_numpoints","title":"ST_NumPoints","text":"<p>Introduction: Returns number of points in a LineString</p> <p>Format: <code>ST_NumPoints(geom: geometry)</code></p> <p>Note</p> <p>If any other geometry is provided as an argument, an IllegalArgumentException is thrown.</p> <p>SQL Example:</p> <pre><code>SELECT ST_NumPoints(ST_GeomFromWKT('MULTIPOINT ((0 0), (1 1), (0 1), (2 2))'))\n</code></pre> <p>Output:</p> <pre><code>IllegalArgumentException: Unsupported geometry type: MultiPoint, only LineString geometry is supported.\n</code></pre> <p>SQL Example: <pre><code>SELECT ST_NumPoints(ST_GeomFromText('LINESTRING(0 1, 1 0, 2 0)'))\n</code></pre></p> <p>Output: <code>3</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_pointn","title":"ST_PointN","text":"<p>Introduction: Return the Nth point in a single linestring or circular linestring in the geometry. Negative values are counted backwards from the end of the LineString, so that -1 is the last point. Returns NULL if there is no linestring in the geometry.</p> <p>Format: <code>ST_PointN(geom: geometry, n: integer)</code></p> <p>SQL example: <pre><code>SELECT ST_PointN(ST_GeomFromText('LINESTRING(0 0, 1 2, 2 4, 3 6)'), 2) AS geom\n</code></pre></p> <p>Result:</p> <pre><code>+---------------------------------------------------------------+\n|geom                                                           |\n+---------------------------------------------------------------+\n|POINT (1 2)                                                    |\n+---------------------------------------------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_pointonsurface","title":"ST_PointOnSurface","text":"<p>Introduction: Returns a POINT guaranteed to lie on the surface.</p> <p>Format: <code>ST_PointOnSurface(A:geometry)</code></p> <p>Examples:</p> <pre><code>SELECT ST_AsText(ST_PointOnSurface(ST_GeomFromText('POINT(0 5)')));\n st_astext\n------------\n POINT(0 5)\n\nSELECT ST_AsText(ST_PointOnSurface(ST_GeomFromText('LINESTRING(0 5, 0 10)')));\n st_astext\n------------\n POINT(0 5)\n\nSELECT ST_AsText(ST_PointOnSurface(ST_GeomFromText('POLYGON((0 0, 0 5, 5 5, 5 0, 0 0))')));\n   st_astext\n----------------\n POINT(2.5 2.5)\n\nSELECT ST_AsText(ST_PointOnSurface(ST_GeomFromText('LINESTRING(0 5 1, 0 0 1, 0 10 2)')));\n   st_astext\n----------------\n POINT Z(0 0 1)\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_polygon","title":"ST_Polygon","text":"<p>Introduction: Function to create a polygon built from the given LineString and sets the spatial reference system from the srid</p> <p>Format: <code>ST_Polygon(geom: Geometry, srid: Integer)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_AsText( ST_Polygon(ST_GeomFromEWKT('LINESTRING(75 29 1, 77 29 2, 77 29 3, 75 29 1)'), 4326) );\n</code></pre> <p>Output:</p> <pre><code>POLYGON((75 29 1, 77 29 2, 77 29 3, 75 29 1))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_reduceprecision","title":"ST_ReducePrecision","text":"<p>Introduction: Reduce the decimals places in the coordinates of the geometry to the given number of decimal places. The last decimal place will be rounded. This function was called ST_PrecisionReduce in versions prior to v1.5.0.</p> <p>Format: <code>ST_ReducePrecision (A: Geometry, B: Integer)</code></p> <p>SQL Example:</p> <p><pre><code>SELECT ST_ReducePrecision(ST_GeomFromWKT('Point(0.1234567890123456789 0.1234567890123456789)')\n, 9)\n</code></pre> The new coordinates will only have 9 decimal places.</p> <p>Output:</p> <pre><code>POINT (0.123456789 0.123456789)\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_removepoint","title":"ST_RemovePoint","text":"<p>Introduction: RETURN Line with removed point at given index, position can be omitted and then last one will be removed.</p> <p>Format: <code>ST_RemovePoint(geom: geometry, position: integer)</code></p> <p>Format: <code>ST_RemovePoint(geom: geometry)</code></p> <p>SQL example: <pre><code>SELECT ST_RemovePoint(ST_GeomFromText('LINESTRING(0 0, 1 1, 1 0)'), 1)\n</code></pre></p> <p>Output: <code>LINESTRING(0 0, 1 0)</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_reverse","title":"ST_Reverse","text":"<p>Introduction: Return the geometry with vertex order reversed</p> <p>Format: <code>ST_Reverse (A:geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(\nST_Reverse(ST_GeomFromText('LINESTRING(0 0, 1 2, 2 4, 3 6)'))\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+---------------------------------------------------------------+\n|geom                                                           |\n+---------------------------------------------------------------+\n|LINESTRING (3 6, 2 4, 1 2, 0 0)                                |\n+---------------------------------------------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_s2cellids","title":"ST_S2CellIDs","text":"<p>Introduction: Cover the geometry with Google S2 Cells, return the corresponding cell IDs with the given level. The level indicates the size of cells. With a bigger level, the cells will be smaller, the coverage will be more accurate, but the result size will be exponentially increasing.</p> <p>Format: <code>ST_S2CellIDs(geom: geometry, level: Int)</code></p> <p>SQL example: <pre><code>SELECT ST_S2CellIDs(ST_GeomFromText('LINESTRING(1 3 4, 5 6 7)'), 6)\n</code></pre></p> <p>Output: <pre><code>+------------------------------------------------------------------------------------------------------------------------------+\n|st_s2cellids(st_geomfromtext(LINESTRING(1 3 4, 5 6 7), 0), 6)                                                                 |\n+------------------------------------------------------------------------------------------------------------------------------+\n|[1159395429071192064, 1159958379024613376, 1160521328978034688, 1161084278931456000, 1170091478186196992, 1170654428139618304]|\n+------------------------------------------------------------------------------------------------------------------------------+\n</code></pre></p>"},{"location":"api/snowflake/vector-data/Function/#st_setpoint","title":"ST_SetPoint","text":"<p>Introduction: Replace Nth point of linestring with given point. Index is 0-based. Negative index are counted backwards, e.g., -1 is last point.</p> <p>Format: <code>ST_SetPoint (linestring: geometry, index: integer, point: geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_SetPoint(ST_GeomFromText('LINESTRING (0 0, 0 1, 1 1)'), 2, ST_GeomFromText('POINT (1 0)')) AS geom\n</code></pre> <p>Result:</p> <pre><code>+--------------------------+\n|geom                      |\n+--------------------------+\n|LINESTRING (0 0, 0 1, 1 0)|\n+--------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_setsrid","title":"ST_SetSRID","text":"<p>Introduction: Sets the spatial reference system identifier (SRID) of the geometry.</p> <p>Format: <code>ST_SetSRID (A:geometry, srid: Integer)</code></p> <p>SQL example: <pre><code>SELECT ST_SetSRID(polygondf.countyshape, 3021)\nFROM polygondf\n</code></pre></p>"},{"location":"api/snowflake/vector-data/Function/#st_simplifypreservetopology","title":"ST_SimplifyPreserveTopology","text":"<p>Introduction: Simplifies a geometry and ensures that the result is a valid geometry having the same dimension and number of components as the input, and with the components having the same topological relationship.</p> <p>Format: <code>ST_SimplifyPreserveTopology (A:geometry, distanceTolerance: Double)</code></p> <pre><code>SELECT ST_SimplifyPreserveTopology(polygondf.countyshape, 10.0)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_split","title":"ST_Split","text":"<p>Introduction: Split an input geometry by another geometry (called the blade). Linear (LineString or MultiLineString) geometry can be split by a Point, MultiPoint, LineString, MultiLineString, Polygon, or MultiPolygon. Polygonal (Polygon or MultiPolygon) geometry can be split by a LineString, MultiLineString, Polygon, or MultiPolygon. In either case, when a polygonal blade is used then the boundary of the blade is what is actually split by. ST_Split will always return either a MultiLineString or MultiPolygon even if they only contain a single geometry. Homogeneous GeometryCollections are treated as a multi-geometry of the type it contains. For example, if a GeometryCollection of only Point geometries is passed as a blade it is the same as passing a MultiPoint of the same geometries.</p> <p>Format: <code>ST_Split (input: geometry, blade: geometry)</code></p> <p>SQL Example: <pre><code>SELECT ST_Split(\nST_GeomFromWKT('LINESTRING (0 0, 1.5 1.5, 2 2)'),\nST_GeomFromWKT('MULTIPOINT (0.5 0.5, 1 1)'))\n</code></pre></p> <p>Output: <code>MULTILINESTRING ((0 0, 0.5 0.5), (0.5 0.5, 1 1), (1 1, 1.5 1.5, 2 2))</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_srid","title":"ST_SRID","text":"<p>Introduction: Return the spatial reference system identifier (SRID) of the geometry.</p> <p>Format: <code>ST_SRID (A:geometry)</code></p> <p>SQL example: <pre><code>SELECT ST_SRID(polygondf.countyshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/snowflake/vector-data/Function/#st_startpoint","title":"ST_StartPoint","text":"<p>Introduction: Returns first point of given linestring.</p> <p>Format: <code>ST_StartPoint(geom: geometry)</code></p> <p>SQL example: <pre><code>SELECT ST_StartPoint(ST_GeomFromText('LINESTRING(100 150,50 60, 70 80, 160 170)'))\n</code></pre></p> <p>Output: <code>POINT(100 150)</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_subdivide","title":"ST_SubDivide","text":"<p>Introduction: Returns a multi-geometry divided based of given maximum number of vertices.</p> <p>Format: <code>ST_SubDivide(geom: geometry, maxVertices: int)</code></p> <p>SQL example: <pre><code>SELECT Sedona.ST_AsText(Sedona.ST_SubDivide(Sedona.ST_GeomFromText('LINESTRING(0 0, 85 85, 100 100, 120 120, 21 21, 10 10, 5 5)'), 5));\n</code></pre></p> <p>Output: <pre><code>MULTILINESTRING ((0 0, 5 5), (5 5, 10 10), (10 10, 21 21), (21 21, 60 60), (60 60, 85 85), (85 85, 100 100), (100 100, 120 120))\n</code></pre></p>"},{"location":"api/snowflake/vector-data/Function/#st_subdivideexplode","title":"ST_SubDivideExplode","text":"<p>Introduction: It works the same as ST_SubDivide but returns new rows with geometries instead of a multi-geometry.</p> <p>Format: <code>SELECT SEDONA.ST_AsText(GEOM) FROM table(SEDONA.ST_SubDivideExplode(geom: geometry, maxVertices: int))</code></p> <p>Example:</p> <p>Query: <pre><code>SELECT Sedona.ST_AsText(GEOM)\nFROM table(Sedona.ST_SubDivideExplode(Sedona.ST_GeomFromText('LINESTRING(0 0, 85 85, 100 100, 120 120, 21 21, 10 10, 5 5)'), 5));\n</code></pre></p> <p>Result:</p> <pre><code>+-----------------------------+\n|geom                         |\n+-----------------------------+\n|LINESTRING(0 0, 5 5)         |\n|LINESTRING(5 5, 10 10)       |\n|LINESTRING(10 10, 21 21)     |\n|LINESTRING(21 21, 60 60)     |\n|LINESTRING(60 60, 85 85)     |\n|LINESTRING(85 85, 100 100)   |\n|LINESTRING(100 100, 120 120) |\n+-----------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_symdifference","title":"ST_SymDifference","text":"<p>Introduction: Return the symmetrical difference between geometry A and B (return parts of geometries which are in either of the sets, but not in their intersection)</p> <p>Format: <code>ST_SymDifference (A:geometry, B:geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_SymDifference(ST_GeomFromWKT('POLYGON ((-3 -3, 3 -3, 3 3, -3 3, -3 -3))'), ST_GeomFromWKT('POLYGON ((-2 -3, 4 -3, 4 3, -2 3, -2 -3))'))\n</code></pre> <p>Result:</p> <pre><code>MULTIPOLYGON (((-2 -3, -3 -3, -3 3, -2 3, -2 -3)), ((3 -3, 3 3, 4 3, 4 -3, 3 -3)))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_transform","title":"ST_Transform","text":"<p>Introduction:</p> <p>Transform the Spatial Reference System / Coordinate Reference System of A, from SourceCRS to TargetCRS. For SourceCRS and TargetCRS, WKT format is also available.</p> <p>Note</p> <p>By default, this function uses lat/lon order. You can use ST_FlipCoordinates to swap X and Y.</p> <p>Note</p> <p>If ST_Transform throws an Exception called \"Bursa wolf parameters required\", you need to disable the error notification in ST_Transform. You can append a boolean value at the end.</p> <p>Format: <code>ST_Transform (A:geometry, SourceCRS:string, TargetCRS:string ,[Optional] DisableError)</code></p> <p>SQL example (simple): <pre><code>SELECT ST_Transform(polygondf.countyshape, 'epsg:4326','epsg:3857')\nFROM polygondf\n</code></pre></p> <p>SQL example (with optional parameters): <pre><code>SELECT ST_Transform(polygondf.countyshape, 'epsg:4326','epsg:3857', false)\nFROM polygondf\n</code></pre></p> <p>Note</p> <p>The detailed EPSG information can be searched on EPSG.io.</p>"},{"location":"api/snowflake/vector-data/Function/#st_translate","title":"ST_Translate","text":"<p>Introduction: Returns the input geometry with its X, Y and Z coordinates (if present in the geometry) translated by deltaX, deltaY and deltaZ (if specified)</p> <p>If the geometry is 2D, and a deltaZ parameter is specified, no change is done to the Z coordinate of the geometry and the resultant geometry is also 2D.</p> <p>If the geometry is empty, no change is done to it. If the given geometry contains sub-geometries (GEOMETRY COLLECTION, MULTI POLYGON/LINE/POINT), all underlying geometries are individually translated.</p> <p>Format: <code>ST_Translate(geometry: geometry, deltaX: deltaX, deltaY: deltaY, deltaZ: deltaZ)</code></p> <p>Example:</p> <p>Input: <code>ST_Translate(GEOMETRYCOLLECTION(MULTIPOLYGON (((1 0, 1 1, 2 1, 2 0, 1 0)), ((1 2, 3 4, 3 5, 1 2))), POINT(1, 1, 1), LINESTRING EMPTY), 2, 2, 3)</code></p> <p>Output: <code>GEOMETRYCOLLECTION(MULTIPOLYGON (((3 2, 3 3, 4 3, 4 2, 3 2)), ((3 4, 5 6, 5 7, 3 4))), POINT(3, 3, 4), LINESTRING EMPTY)</code></p> <p>Input: <code>ST_Translate(POINT(1, 3, 2), 1, 2)</code></p> <p>Output: <code>POINT(2, 5, 2)</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_union","title":"ST_Union","text":"<p>Introduction: Return the union of geometry A and B</p> <p>Format: <code>ST_Union (A:geometry, B:geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_Union(ST_GeomFromWKT('POLYGON ((-3 -3, 3 -3, 3 3, -3 3, -3 -3))'), ST_GeomFromWKT('POLYGON ((1 -2, 5 0, 1 2, 1 -2))'))\n</code></pre> <p>Result:</p> <pre><code>POLYGON ((3 -1, 3 -3, -3 -3, -3 3, 3 3, 3 1, 5 0, 3 -1))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_voronoipolygons","title":"ST_VoronoiPolygons","text":"<p>Introduction: Returns a two-dimensional Voronoi diagram from the vertices of the supplied geometry. The result is a GeometryCollection of Polygons that covers an envelope larger than the extent of the input vertices. Returns null if input geometry is null. Returns an empty geometry collection if the input geometry contains only one vertex. Returns an empty geometry collection if the extend_to envelope has zero area.</p> <p>Format: <code>ST_VoronoiPolygons(g1: Geometry, tolerance: Double, extend_to: Geometry)</code></p> <p>Optional parameters:</p> <p><code>tolerance</code> : The distance within which vertices will be considered equivalent. Robustness of the algorithm can be improved by supplying a nonzero tolerance distance. (default = 0.0)</p> <p><code>extend_to</code> : If a geometry is supplied as the \"extend_to\" parameter, the diagram will be extended to cover the envelope of the \"extend_to\" geometry, unless that envelope is smaller than the default envelope (default = NULL. By default, we extend the bounding box of the diagram by the max between bounding box's height and bounding box's width).</p> <p>SQL Example:</p> <pre><code>SELECT st_astext(ST_VoronoiPolygons(ST_GeomFromText('MULTIPOINT ((0 0), (1 1))')));\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION(POLYGON((-1 2,2 -1,-1 -1,-1 2)),POLYGON((-1 2,2 2,2 -1,-1 2)))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_x","title":"ST_X","text":"<p>Introduction: Returns X Coordinate of given Point null otherwise.</p> <p>Format: <code>ST_X(pointA: Point)</code></p> <p>SQL example: <pre><code>SELECT ST_X(ST_POINT(0.0 25.0))\n</code></pre></p> <p>Output: <code>0.0</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_xmax","title":"ST_XMax","text":"<p>Introduction: Returns the maximum X coordinate of a geometry</p> <p>Format: <code>ST_XMax (A:geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_XMax(df.geometry) AS xmax\nFROM df\n</code></pre> <p>Input: <code>POLYGON ((-1 -11, 0 10, 1 11, 2 12, -1 -11))</code></p> <p>Output: <code>2</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_xmin","title":"ST_XMin","text":"<p>Introduction: Returns the minimum X coordinate of a geometry</p> <p>Format: <code>ST_XMin (A:geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_XMin(df.geometry) AS xmin\nFROM df\n</code></pre> <p>Input: <code>POLYGON ((-1 -11, 0 10, 1 11, 2 12, -1 -11))</code></p> <p>Output: <code>-1</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_y","title":"ST_Y","text":"<p>Introduction: Returns Y Coordinate of given Point, null otherwise.</p> <p>Format: <code>ST_Y(pointA: Point)</code></p> <p>SQL example: <pre><code>SELECT ST_Y(ST_POINT(0.0 25.0))\n</code></pre></p> <p>Output: <code>25.0</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_ymax","title":"ST_YMax","text":"<p>Introduction: Return the minimum Y coordinate of A</p> <p>Format: <code>ST_YMax (A:geometry)</code></p> <p>SQL example: <pre><code>SELECT ST_YMax(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre></p> <p>Output: 2</p>"},{"location":"api/snowflake/vector-data/Function/#st_ymin","title":"ST_YMin","text":"<p>Introduction: Return the minimum Y coordinate of A</p> <p>Format: <code>ST_Y_Min (A:geometry)</code></p> <p>SQL example: <pre><code>SELECT ST_YMin(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre></p> <p>Output : 0</p>"},{"location":"api/snowflake/vector-data/Function/#st_z","title":"ST_Z","text":"<p>Introduction: Returns Z Coordinate of given Point, null otherwise.</p> <p>Format: <code>ST_Z(pointA: Point)</code></p> <p>SQL example: <pre><code>SELECT ST_Z(ST_POINT(0.0 25.0 11.0))\n</code></pre></p> <p>Output: <code>11.0</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_zmax","title":"ST_ZMax","text":"<p>Introduction: Returns Z maxima of the given geometry or null if there is no Z coordinate.</p> <p>Format: <code>ST_ZMax(geom: geometry)</code></p> <p>SQL example: <pre><code>SELECT ST_ZMax(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre></p> <p>Output: <code>1.0</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_zmin","title":"ST_ZMin","text":"<p>Introduction: Returns Z minima of the given geometry or null if there is no Z coordinate.</p> <p>Format: <code>ST_ZMin(geom: geometry)</code></p> <p>SQL example: <pre><code>SELECT ST_ZMin(ST_GeomFromText('LINESTRING(1 3 4, 5 6 7)'))\n</code></pre></p> <p>Output: <code>4.0</code></p>"},{"location":"api/snowflake/vector-data/Overview/","title":"Overview","text":"<p>Note</p> <p>Please always keep the schema name <code>SEDONA</code> (e.g., <code>SEDONA.ST_GeomFromWKT</code>) when you use Sedona functions to avoid conflicting with Snowflake's built-in functions.</p> <p>SedonaSQL supports SQL/MM Part3 Spatial SQL Standard. It includes four kinds of SQL operators as follows.</p> <ul> <li>Constructor: Construct a Geometry given an input string or coordinates</li> <li>Example: ST_GeomFromWKT (string). Create a Geometry from a WKT String.</li> <li>Documentation: Here</li> <li>Function: Execute a function on the given column or columns</li> <li>Example: ST_Distance (A, B). Given two Geometry A and B, return the Euclidean distance of A and B.</li> <li>Documentation: Here</li> <li>Aggregate function: Return the aggregated value on the given column</li> <li>Example: ST_Envelope_Aggr (Geometry column). Given a Geometry column, calculate the entire envelope boundary of this column.</li> <li>Documentation: Here</li> <li>Predicate: Execute a logic judgement on the given columns and return true or false</li> <li>Example: ST_Contains (A, B). Check if A fully contains B. Return \"True\" if yes, else return \"False\".</li> <li>Documentation: Here</li> </ul>"},{"location":"api/snowflake/vector-data/Predicate/","title":"Predicate","text":"<p>Note</p> <p>Please always keep the schema name <code>SEDONA</code> (e.g., <code>SEDONA.ST_GeomFromWKT</code>) when you use Sedona functions to avoid conflicting with Snowflake's built-in functions.</p>"},{"location":"api/snowflake/vector-data/Predicate/#st_contains","title":"ST_Contains","text":"<p>Introduction: Return true if A fully contains B</p> <p>Format: <code>ST_Contains (A:geometry, B:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT *\nFROM pointdf\nWHERE ST_Contains(ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), pointdf.arealandmark)\n</code></pre>"},{"location":"api/snowflake/vector-data/Predicate/#st_crosses","title":"ST_Crosses","text":"<p>Introduction: Return true if A crosses B</p> <p>Format: <code>ST_Crosses (A:geometry, B:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT *\nFROM pointdf\nWHERE ST_Crosses(pointdf.arealandmark, ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0))\n</code></pre>"},{"location":"api/snowflake/vector-data/Predicate/#st_disjoint","title":"ST_Disjoint","text":"<p>Introduction: Return true if A and B are disjoint</p> <p>Format: <code>ST_Disjoint (A:geometry, B:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT *\nFROM geom\nWHERE ST_Disjoinnt(geom.geom_a, geom.geom_b)\n</code></pre>"},{"location":"api/snowflake/vector-data/Predicate/#st_dwithin","title":"ST_DWithin","text":"<p>Introduction: Returns true if 'leftGeometry' and 'rightGeometry' are within a specified 'distance'. This function essentially checks if the shortest distance between the envelope of the two geometries is &lt;= the provided distance.</p> <p>Format: <code>ST_DWithin (leftGeometry: Geometry, rightGeometry: Geometry, distance: Double)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_DWithin(ST_GeomFromWKT('POINT (0 0)'), ST_GeomFromWKT('POINT (1 0)'), 2.5)\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/snowflake/vector-data/Predicate/#st_equals","title":"ST_Equals","text":"<p>Introduction: Return true if A equals to B</p> <p>Format: <code>ST_Equals (A:geometry, B:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT *\nFROM pointdf\nWHERE ST_Equals(pointdf.arealandmark, ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0))\n</code></pre>"},{"location":"api/snowflake/vector-data/Predicate/#st_intersects","title":"ST_Intersects","text":"<p>Introduction: Return true if A intersects B</p> <p>Format: <code>ST_Intersects (A:geometry, B:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT *\nFROM pointdf\nWHERE ST_Intersects(ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), pointdf.arealandmark)\n</code></pre>"},{"location":"api/snowflake/vector-data/Predicate/#st_orderingequals","title":"ST_OrderingEquals","text":"<p>Introduction: Returns true if the geometries are equal and the coordinates are in the same order</p> <p>Format: <code>ST_OrderingEquals(A: geometry, B: geometry)</code></p> <p>SQL example 1:</p> <pre><code>SELECT ST_OrderingEquals(ST_GeomFromWKT('POLYGON((2 0, 0 2, -2 0, 2 0))'), ST_GeomFromWKT('POLYGON((2 0, 0 2, -2 0, 2 0))'))\n</code></pre> <p>Output: <code>true</code></p> <p>SQL example 2:</p> <pre><code>SELECT ST_OrderingEquals(ST_GeomFromWKT('POLYGON((2 0, 0 2, -2 0, 2 0))'), ST_GeomFromWKT('POLYGON((0 2, -2 0, 2 0, 0 2))'))\n</code></pre> <p>Output: <code>false</code></p>"},{"location":"api/snowflake/vector-data/Predicate/#st_overlaps","title":"ST_Overlaps","text":"<p>Introduction: Return true if A overlaps B</p> <p>Format: <code>ST_Overlaps (A:geometry, B:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT *\nFROM geom\nWHERE ST_Overlaps(geom.geom_a, geom.geom_b)\n</code></pre>"},{"location":"api/snowflake/vector-data/Predicate/#st_touches","title":"ST_Touches","text":"<p>Introduction: Return true if A touches B</p> <p>Format: <code>ST_Touches (A:geometry, B:geometry)</code></p> <pre><code>SELECT *\nFROM pointdf\nWHERE ST_Touches(pointdf.arealandmark, ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0))\n</code></pre>"},{"location":"api/snowflake/vector-data/Predicate/#st_within","title":"ST_Within","text":"<p>Introduction: Return true if A is fully contained by B</p> <p>Format: <code>ST_Within (A:geometry, B:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT *\nFROM pointdf\nWHERE ST_Within(pointdf.arealandmark, ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0))\n</code></pre>"},{"location":"api/snowflake/vector-data/Predicate/#st_covers","title":"ST_Covers","text":"<p>Introduction: Return true if A covers B</p> <p>Format: <code>ST_Covers (A:geometry, B:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT *\nFROM pointdf\nWHERE ST_Covers(ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), pointdf.arealandmark)\n</code></pre>"},{"location":"api/snowflake/vector-data/Predicate/#st_coveredby","title":"ST_CoveredBy","text":"<p>Introduction: Return true if A is covered by B</p> <p>Format: <code>ST_CoveredBy (A:geometry, B:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT *\nFROM pointdf\nWHERE ST_CoveredBy(pointdf.arealandmark, ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0))\n</code></pre>"},{"location":"api/sql/AggregateFunction/","title":"Aggregate function","text":""},{"location":"api/sql/AggregateFunction/#st_envelope_aggr","title":"ST_Envelope_Aggr","text":"<p>Introduction: Return the entire envelope boundary of all geometries in A</p> <p>Format: <code>ST_Envelope_Aggr (A: geometryColumn)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Envelope_Aggr(ST_GeomFromText('MULTIPOINT(1.1 101.1,2.1 102.1,3.1 103.1,4.1 104.1,5.1 105.1,6.1 106.1,7.1 107.1,8.1 108.1,9.1 109.1,10.1 110.1)'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((1.1 101.1, 1.1 120.1, 20.1 120.1, 20.1 101.1, 1.1 101.1))\n</code></pre>"},{"location":"api/sql/AggregateFunction/#st_intersection_aggr","title":"ST_Intersection_Aggr","text":"<p>Introduction: Return the polygon intersection of all polygons in A</p> <p>Format: <code>ST_Intersection_Aggr (A: geometryColumn)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Intersection_Aggr(ST_GeomFromText('MULTIPOINT(1.1 101.1,2.1 102.1,3.1 103.1,4.1 104.1,5.1 105.1,6.1 106.1,7.1 107.1,8.1 108.1,9.1 109.1,10.1 110.1)'))\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT ((1.1 101.1), (2.1 102.1), (3.1 103.1), (4.1 104.1), (5.1 105.1), (6.1 106.1), (7.1 107.1), (8.1 108.1), (9.1 109.1), (10.1 110.1))\n</code></pre>"},{"location":"api/sql/AggregateFunction/#st_union_aggr","title":"ST_Union_Aggr","text":"<p>Introduction: Return the polygon union of all polygons in A</p> <p>Format: <code>ST_Union_Aggr (A: geometryColumn)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Union_Aggr(ST_GeomFromText('MULTIPOINT(1.1 101.1,2.1 102.1,3.1 103.1,4.1 104.1,5.1 105.1,6.1 106.1,7.1 107.1,8.1 108.1,9.1 109.1,10.1 110.1)'))\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT ((1.1 101.1), (2.1 102.1), (3.1 103.1), (4.1 104.1), (5.1 105.1), (6.1 106.1), (7.1 107.1), (8.1 108.1), (9.1 109.1), (10.1 110.1))\n</code></pre>"},{"location":"api/sql/Constructor/","title":"Constructor","text":""},{"location":"api/sql/Constructor/#read-esri-shapefile","title":"Read ESRI Shapefile","text":"<p>Introduction: Construct a DataFrame from a Shapefile</p> <p>Since: <code>v1.0.0</code></p> <p>SparkSQL example:</p> <pre><code>var spatialRDD = new SpatialRDD[Geometry]\nspatialRDD.rawSpatialRDD = ShapefileReader.readToGeometryRDD(sparkSession.sparkContext, shapefileInputLocation)\nvar rawSpatialDf = Adapter.toDf(spatialRDD,sparkSession)\nrawSpatialDf.createOrReplaceTempView(\"rawSpatialDf\")\nvar spatialDf = sparkSession.sql(\"\"\"\n          | ST_GeomFromWKT(rddshape), _c1, _c2\n          | FROM rawSpatialDf\n        \"\"\".stripMargin)\nspatialDf.show()\nspatialDf.printSchema()\n</code></pre> <p>Note</p> <p>The path to the shapefile is the path to the folder that contains the .shp file, not the path to the .shp file itself. The file extensions of .shp, .shx, .dbf must be in lowercase. Assume you have a shape file called myShapefile, the path should be <code>XXX/myShapefile</code>. The file structure should be like this: <pre><code>- shapefile1\n- shapefile2\n- myshapefile\n    - myshapefile.shp\n    - myshapefile.shx\n    - myshapefile.dbf\n    - myshapefile...\n- ...\n</code></pre></p> <p>Warning</p> <p>Please make sure you use ST_GeomFromWKT to create Geometry type column otherwise that column cannot be used in SedonaSQL.</p> <p>If the file you are reading contains non-ASCII characters you'll need to explicitly set the encoding via <code>sedona.global.charset</code> system property before the call to <code>ShapefileReader.readToGeometryRDD</code>.</p> <p>Example:</p> <pre><code>System.setProperty(\"sedona.global.charset\", \"utf8\")\n</code></pre>"},{"location":"api/sql/Constructor/#st_geomfromgeohash","title":"ST_GeomFromGeoHash","text":"<p>Introduction: Create Geometry from geohash string and optional precision</p> <p>Format: <code>ST_GeomFromGeoHash(geohash: String, precision: Integer)</code></p> <p>Since: <code>v1.1.1</code></p> <p>Spark SQL example:</p> <pre><code>SELECT ST_GeomFromGeoHash('s00twy01mt', 4)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0.703125 0.87890625, 0.703125 1.0546875, 1.0546875 1.0546875, 1.0546875 0.87890625, 0.703125 0.87890625))\n</code></pre>"},{"location":"api/sql/Constructor/#st_geomfromgeojson","title":"ST_GeomFromGeoJSON","text":"<p>Introduction: Construct a Geometry from GeoJson</p> <p>Format: <code>ST_GeomFromGeoJSON (GeoJson: String)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example:</p> <pre><code>SELECT ST_GeomFromGeoJSON('{\n   \"type\":\"Feature\",\n   \"properties\":{\n      \"STATEFP\":\"01\",\n      \"COUNTYFP\":\"077\",\n      \"TRACTCE\":\"011501\",\n      \"BLKGRPCE\":\"5\",\n      \"AFFGEOID\":\"1500000US010770115015\",\n      \"GEOID\":\"010770115015\",\n      \"NAME\":\"5\",\n      \"LSAD\":\"BG\",\n      \"ALAND\":6844991,\n      \"AWATER\":32636\n   },\n   \"geometry\":{\n      \"type\":\"Polygon\",\n      \"coordinates\":[\n         [\n            [-87.621765, 34.873444],\n            [-87.617535, 34.873369],\n            [-87.62119, 34.85053],\n            [-87.62144, 34.865379],\n            [-87.621765, 34.873444]\n         ]\n      ]\n   }\n}')\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((-87.621765 34.873444, -87.617535 34.873369, -87.62119 34.85053, -87.62144 34.865379, -87.621765 34.873444))\n</code></pre> <p>Spark SQL example:</p> <pre><code>SELECT ST_GeomFromGeoJSON('{\n   \"type\":\"Polygon\",\n   \"coordinates\":[\n      [\n         [-87.621765, 34.873444],\n         [-87.617535, 34.873369],\n         [-87.62119, 34.85053],\n         [-87.62144, 34.865379],\n         [-87.621765, 34.873444]\n      ]\n   ]\n}')\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((-87.621765 34.873444, -87.617535 34.873369, -87.62119 34.85053, -87.62144 34.865379, -87.621765 34.873444))\n</code></pre> <p>Warning</p> <p>The way that SedonaSQL reads GeoJSON is different from that in SparkSQL</p>"},{"location":"api/sql/Constructor/#st_geomfromgml","title":"ST_GeomFromGML","text":"<p>Introduction: Construct a Geometry from GML.</p> <p>Format: <code>ST_GeomFromGML (gml: String)</code></p> <p>Since: <code>v1.3.0</code></p> <p>SQL example:</p> <pre><code>SELECT ST_GeomFromGML('\n    &lt;gml:LineString srsName=\"EPSG:4269\"&gt;\n        &lt;gml:coordinates&gt;\n            -71.16028,42.258729\n            -71.160837,42.259112\n            -71.161143,42.25932\n        &lt;/gml:coordinates&gt;\n    &lt;/gml:LineString&gt;\n')\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-71.16028 42.258729, -71.160837 42.259112, -71.161143 42.25932)\n</code></pre>"},{"location":"api/sql/Constructor/#st_geomfromkml","title":"ST_GeomFromKML","text":"<p>Introduction: Construct a Geometry from KML.</p> <p>Format: <code>ST_GeomFromKML (kml: String)</code></p> <p>Since: <code>v1.3.0</code></p> <p>SQL example:</p> <pre><code>SELECT ST_GeomFromKML('\n    &lt;LineString&gt;\n        &lt;coordinates&gt;\n            -71.1663,42.2614\n            -71.1667,42.2616\n        &lt;/coordinates&gt;\n    &lt;/LineString&gt;\n')\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-71.1663 42.2614, -71.1667 42.2616)\n</code></pre>"},{"location":"api/sql/Constructor/#st_geomfromtext","title":"ST_GeomFromText","text":"<p>Introduction: Construct a Geometry from WKT. If SRID is not set, it defaults to 0 (unknown). Alias of ST_GeomFromWKT</p> <p>Format:</p> <p><code>ST_GeomFromText (Wkt: String)</code></p> <p><code>ST_GeomFromText (Wkt: String, srid: Integer)</code></p> <p>Since: <code>v1.0.0</code></p> <p>The optional srid parameter was added in <code>v1.3.1</code></p> <p>Spark SQL example:</p> <pre><code>SELECT ST_GeomFromText('POINT(40.7128 -74.0060)')\n</code></pre> <p>Output:</p> <pre><code>POINT(40.7128 -74.006)\n</code></pre>"},{"location":"api/sql/Constructor/#st_geomfromwkb","title":"ST_GeomFromWKB","text":"<p>Introduction: Construct a Geometry from WKB string or Binary. This function also supports EWKB format.</p> <p>Format:</p> <p><code>ST_GeomFromWKB (Wkb: String)</code></p> <p><code>ST_GeomFromWKB (Wkb: Binary)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example:</p> <pre><code>SELECT ST_GeomFromWKB([01 02 00 00 00 02 00 00 00 00 00 00 00 84 D6 00 C0 00 00 00 00 80 B5 D6 BF 00 00 00 60 E1 EF F7 BF 00 00 00 80 07 5D E5 BF])\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-2.1047439575195312 -0.354827880859375, -1.49606454372406 -0.6676061153411865)\n</code></pre> <p>Spark SQL example:</p> <pre><code>SELECT ST_asEWKT(ST_GeomFromWKB('01010000a0e6100000000000000000f03f000000000000f03f000000000000f03f'))\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POINT Z(1 1 1)\n</code></pre>"},{"location":"api/sql/Constructor/#st_geomfromwkt","title":"ST_GeomFromWKT","text":"<p>Introduction: Construct a Geometry from WKT. If SRID is not set, it defaults to 0 (unknown).</p> <p>Format:</p> <p><code>ST_GeomFromWKT (Wkt: String)</code></p> <p><code>ST_GeomFromWKT (Wkt: String, srid: Integer)</code></p> <p>Since: <code>v1.0.0</code></p> <p>The optional srid parameter was added in <code>v1.3.1</code></p> <p>Spark SQL example:</p> <pre><code>SELECT ST_GeomFromWKT('POINT(40.7128 -74.0060)')\n</code></pre> <p>Output:</p> <pre><code>POINT(40.7128 -74.006)\n</code></pre>"},{"location":"api/sql/Constructor/#st_geomfromewkt","title":"ST_GeomFromEWKT","text":"<p>Introduction: Construct a Geometry from OGC Extended WKT</p> <p>Format: <code>ST_GeomFromEWKT (EWkt: String)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL example: <pre><code>SELECT ST_AsText(ST_GeomFromEWKT('SRID=4269;POINT(40.7128 -74.0060)'))\n</code></pre></p> <p>Output:</p> <pre><code>POINT(40.7128 -74.006)\n</code></pre>"},{"location":"api/sql/Constructor/#st_linefromtext","title":"ST_LineFromText","text":"<p>Introduction: Construct a Line from Wkt text</p> <p>Format: <code>ST_LineFromText (Wkt: String)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Spark SQL example:</p> <pre><code>SELECT ST_LineFromText('LINESTRING(1 2,3 4)')\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (1 2, 3 4)\n</code></pre>"},{"location":"api/sql/Constructor/#st_linestringfromtext","title":"ST_LineStringFromText","text":"<p>Introduction: Construct a LineString from Text, delimited by Delimiter</p> <p>Format: <code>ST_LineStringFromText (Text: String, Delimiter: Char)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example:</p> <pre><code>SELECT ST_LineStringFromText('-74.0428197,40.6867969,-74.0421975,40.6921336,-74.0508020,40.6912794', ',')\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-74.0428197 40.6867969, -74.0421975 40.6921336, -74.050802 40.6912794)\n</code></pre>"},{"location":"api/sql/Constructor/#st_makepoint","title":"ST_MakePoint","text":"<p>Introduction: Creates a 2D, 3D Z or 4D ZM Point geometry. Use ST_MakePointM to make points with XYM coordinates. Z and M values are optional.</p> <p>Format: <code>ST_MakePoint (X: Double, Y: Double, Z: Double, M: Double)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_MakePoint(1.2345, 2.3456));\n</code></pre> <p>Output:</p> <pre><code>POINT (1.2345 2.3456)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_MakePoint(1.2345, 2.3456, 3.4567));\n</code></pre> <p>Output:</p> <pre><code>POINT Z (1.2345 2.3456 3.4567)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_MakePoint(1.2345, 2.3456, 3.4567, 4));\n</code></pre> <p>Output:</p> <pre><code>POINT ZM (1.2345 2.3456 3.4567 4)\n</code></pre>"},{"location":"api/sql/Constructor/#st_mlinefromtext","title":"ST_MLineFromText","text":"<p>Introduction: Construct a MultiLineString from Wkt. If srid is not set, it defaults to 0 (unknown).</p> <p>Format:</p> <p><code>ST_MLineFromText (Wkt: String)</code></p> <p><code>ST_MLineFromText (Wkt: String, srid: Integer)</code></p> <p>Since: <code>v1.3.1</code></p> <p>Spark SQL example:</p> <pre><code>SELECT ST_MLineFromText('MULTILINESTRING((1 2, 3 4), (4 5, 6 7))')\n</code></pre> <p>Output:</p> <pre><code>MULTILINESTRING ((1 2, 3 4), (4 5, 6 7))\n</code></pre>"},{"location":"api/sql/Constructor/#st_mpolyfromtext","title":"ST_MPolyFromText","text":"<p>Introduction: Construct a MultiPolygon from Wkt. If srid is not set, it defaults to 0 (unknown).</p> <p>Format:</p> <p><code>ST_MPolyFromText (Wkt: String)</code></p> <p><code>ST_MPolyFromText (Wkt: String, srid: Integer)</code></p> <p>Since: <code>v1.3.1</code></p> <p>Spark SQL example:</p> <pre><code>SELECT ST_MPolyFromText('MULTIPOLYGON(((0 0 1,20 0 1,20 20 1,0 20 1,0 0 1),(5 5 3,5 7 3,7 7 3,7 5 3,5 5 3)))')\n</code></pre> <p>Output:</p> <pre><code>MULTIPOLYGON (((0 0, 20 0, 20 20, 0 20, 0 0), (5 5, 5 7, 7 7, 7 5, 5 5)))\n</code></pre>"},{"location":"api/sql/Constructor/#st_point","title":"ST_Point","text":"<p>Introduction: Construct a Point from X and Y</p> <p>Format: <code>ST_Point (X: Double, Y: Double)</code></p> <p>Since: <code>v1.0.0</code></p> <p>In <code>v1.4.0</code> an optional Z parameter was removed to be more consistent with other spatial SQL implementations. If you are upgrading from an older version of Sedona - please use ST_PointZ to create 3D points.</p> <p>Spark SQL example:</p> <pre><code>SELECT ST_Point(double(1.2345), 2.3456)\n</code></pre> <p>Output:</p> <pre><code>POINT (1.2345 2.3456)\n</code></pre>"},{"location":"api/sql/Constructor/#st_pointz","title":"ST_PointZ","text":"<p>Introduction: Construct a Point from X, Y and Z and an optional srid. If srid is not set, it defaults to 0 (unknown). Must use ST_AsEWKT function to print the Z coordinate.</p> <p>Format:</p> <p><code>ST_PointZ (X: Double, Y: Double, Z: Double)</code></p> <p><code>ST_PointZ (X: Double, Y: Double, Z: Double, srid: Integer)</code></p> <p>Since: <code>v1.4.0</code></p> <p>Spark SQL example:</p> <pre><code>SELECT ST_AsEWKT(ST_PointZ(1.2345, 2.3456, 3.4567))\n</code></pre> <p>Output:</p> <pre><code>POINT Z(1.2345 2.3456 3.4567)\n</code></pre>"},{"location":"api/sql/Constructor/#st_pointfromtext","title":"ST_PointFromText","text":"<p>Introduction: Construct a Point from Text, delimited by Delimiter</p> <p>Format: <code>ST_PointFromText (Text: String, Delimiter: Char)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example:</p> <pre><code>SELECT ST_PointFromText('40.7128,-74.0060', ',')\n</code></pre> <p>Output:</p> <pre><code>POINT (40.7128 -74.006)\n</code></pre>"},{"location":"api/sql/Constructor/#st_polygonfromenvelope","title":"ST_PolygonFromEnvelope","text":"<p>Introduction: Construct a Polygon from MinX, MinY, MaxX, MaxY.</p> <p>Format:</p> <p><code>ST_PolygonFromEnvelope (MinX: Double, MinY: Double, MaxX: Double, MaxY: Double)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example:</p> <pre><code>SELECT ST_PolygonFromEnvelope(double(1.234),double(2.234),double(3.345),double(3.345))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((1.234 2.234, 1.234 3.345, 3.345 3.345, 3.345 2.234, 1.234 2.234))\n</code></pre>"},{"location":"api/sql/Constructor/#st_polygonfromtext","title":"ST_PolygonFromText","text":"<p>Introduction: Construct a Polygon from Text, delimited by Delimiter. Path must be closed</p> <p>Format: <code>ST_PolygonFromText (Text: String, Delimiter: Char)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example:</p> <pre><code>SELECT ST_PolygonFromText('-74.0428197,40.6867969,-74.0421975,40.6921336,-74.0508020,40.6912794,-74.0428197,40.6867969', ',')\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((-74.0428197 40.6867969, -74.0421975 40.6921336, -74.050802 40.6912794, -74.0428197 40.6867969))\n</code></pre>"},{"location":"api/sql/DataFrameAPI/","title":"DataFrame Style functions","text":"<p>Sedona SQL functions can be used in a DataFrame style API similar to Spark functions.</p> <p>The following objects contain the exposed functions: <code>org.apache.spark.sql.sedona_sql.expressions.st_functions</code>, <code>org.apache.spark.sql.sedona_sql.expressions.st_constructors</code>, <code>org.apache.spark.sql.sedona_sql.expressions.st_predicates</code>, and <code>org.apache.spark.sql.sedona_sql.expressions.st_aggregates</code>.</p> <p>Every functions can take all <code>Column</code> arguments. Additionally, overloaded forms can commonly take a mix of <code>String</code> and other Scala types (such as <code>Double</code>) as arguments.</p> <p>In general the following rules apply (although check the documentation of specific functions for any exceptions):</p> ScalaPython <ol> <li>Every function returns a <code>Column</code> so that it can be used interchangeably with Spark functions as well as <code>DataFrame</code> methods such as <code>DataFrame.select</code> or <code>DataFrame.join</code>.</li> <li>Every function has a form that takes all <code>Column</code> arguments. These are the most versatile of the forms.</li> <li>Most functions have a form that takes a mix of <code>String</code> arguments with other Scala types.</li> </ol> <ol> <li><code>Column</code> type arguments are passed straight through and are always accepted.</li> <li><code>str</code> type arguments are always assumed to be names of columns and are wrapped in a <code>Column</code> to support that. If an actual string literal needs to be passed then it will need to be wrapped in a <code>Column</code> using <code>pyspark.sql.functions.lit</code>.</li> <li>Any other types of arguments are checked on a per function basis. Generally, arguments that could reasonably support a python native type are accepted and passed through.   4. Shapely <code>Geometry</code> objects are not currently accepted in any of the functions.</li> </ol> <p>The exact mixture of argument types allowed is function specific. However, in these instances, all <code>String</code> arguments are assumed to be the names of columns and will be wrapped in a <code>Column</code> automatically. Non-<code>String</code> arguments are assumed to be literals that are passed to the sedona function. If you need to pass a <code>String</code> literal then you should use the all <code>Column</code> form of the sedona function and wrap the <code>String</code> literal in a <code>Column</code> with the <code>lit</code> Spark function.</p> <p>A short example of using this API (uses the <code>array_min</code> and <code>array_max</code> Spark functions):</p> ScalaPython <pre><code>val values_df = spark.sql(\"SELECT array(0.0, 1.0, 2.0) AS values\")\nval min_value = array_min(\"values\")\nval max_value = array_max(\"values\")\nval point_df = values_df.select(ST_Point(min_value, max_value).as(\"point\"))\n</code></pre> <pre><code>from pyspark.sql import functions as f\n\nfrom sedona.sql import st_constructors as stc\n\ndf = spark.sql(\"SELECT array(0.0, 1.0, 2.0) AS values\")\n\nmin_value = f.array_min(\"values\")\nmax_value = f.array_max(\"values\")\n\ndf = df.select(stc.ST_Point(min_value, max_value).alias(\"point\"))\n</code></pre> <p>The above code will generate the following dataframe: <pre><code>+-----------+\n|point      |\n+-----------+\n|POINT (0 2)|\n+-----------+\n</code></pre></p> <p>Some functions will take native python values and infer them as literals. For example:</p> <pre><code>df = df.select(stc.ST_Point(1.0, 3.0).alias(\"point\"))\n</code></pre> <p>This will generate a dataframe with a constant point in a column: <pre><code>+-----------+\n|point      |\n+-----------+\n|POINT (1 3)|\n+-----------+\n</code></pre></p>"},{"location":"api/sql/Function/","title":"Function","text":""},{"location":"api/sql/Function/#geometrytype","title":"GeometryType","text":"<p>Introduction: Returns the type of the geometry as a string. Eg: 'LINESTRING', 'POLYGON', 'MULTIPOINT', etc. This function also indicates if the geometry is measured, by returning a string of the form 'POINTM'.</p> <p>Format: <code>GeometryType (A: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT GeometryType(ST_GeomFromText('LINESTRING(77.29 29.07,77.42 29.26,77.27 29.31,77.29 29.07)'));\n</code></pre> <p>Output:</p> <pre><code> geometrytype\n--------------\n LINESTRING\n</code></pre> <pre><code>SELECT GeometryType(ST_GeomFromText('POINTM(0 0 1)'));\n</code></pre> <p>Output:</p> <pre><code> geometrytype\n--------------\n POINTM\n</code></pre>"},{"location":"api/sql/Function/#st_3ddistance","title":"ST_3DDistance","text":"<p>Introduction: Return the 3-dimensional minimum cartesian distance between A and B</p> <p>Format: <code>ST_3DDistance (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_3DDistance(ST_GeomFromText(\"POINT Z (0 0 -5)\"),\nST_GeomFromText(\"POINT Z(1  1 -6\"))\n</code></pre> <p>Output:</p> <pre><code>1.7320508075688772\n</code></pre>"},{"location":"api/sql/Function/#st_addpoint","title":"ST_AddPoint","text":"<p>Introduction: RETURN Linestring with additional point at the given index, if position is not available the point will be added at the end of line.</p> <p>Format:</p> <p><code>ST_AddPoint(geom: Geometry, point: Geometry, position: Integer)</code></p> <p><code>ST_AddPoint(geom: Geometry, point: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_AddPoint(ST_GeomFromText(\"LINESTRING(0 0, 1 1, 1 0)\"), ST_GeomFromText(\"Point(21 52)\"), 1)\n\nSELECT ST_AddPoint(ST_GeomFromText(\"Linestring(0 0, 1 1, 1 0)\"), ST_GeomFromText(\"Point(21 52)\"))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING(0 0, 21 52, 1 1, 1 0)\nLINESTRING(0 0, 1 1, 1 0, 21 52)\n</code></pre>"},{"location":"api/sql/Function/#st_affine","title":"ST_Affine","text":"<p>Introduction: Apply an affine transformation to the given geometry.</p> <p>ST_Affine has 2 overloaded signatures:</p> <p><code>ST_Affine(geometry, a, b, c, d, e, f, g, h, i, xOff, yOff, zOff)</code></p> <p><code>ST_Affine(geometry, a, b, d, e, xOff, yOff)</code></p> <p>Based on the invoked function, the following transformation is applied:</p> <p><code>x = a * x + b * y + c * z + xOff OR x = a * x + b * y + xOff</code></p> <p><code>y = d * x + e * y + f * z + yOff OR y = d * x + e * y + yOff</code></p> <p><code>z = g * x + f * y + i * z + zOff OR z = g * x + f * y + zOff</code></p> <p>If the given geometry is empty, the result is also empty.</p> <p>Format:</p> <p><code>ST_Affine(geometry, a, b, c, d, e, f, g, h, i, xOff, yOff, zOff)</code></p> <p><code>ST_Affine(geometry, a, b, d, e, xOff, yOff)</code></p> <pre><code>ST_Affine(geometry, 1, 2, 4, 1, 1, 2, 3, 2, 5, 4, 8, 3)\n</code></pre> <p>Input: <code>LINESTRING EMPTY</code></p> <p>Output: <code>LINESTRING EMPTY</code></p> <p>Input: <code>POLYGON ((1 0 1, 1 1 1, 2 2 2, 1 0 1))</code></p> <p>Output: <code>POLYGON Z((9 11 11, 11 12 13, 18 16 23, 9 11 11))</code></p> <p>Input: <code>POLYGON ((1 0, 1 1, 2 1, 2 0, 1 0), (1 0.5, 1 0.75, 1.5 0.75, 1.5 0.5, 1 0.5))</code></p> <p>Output: <code>POLYGON((5 9, 7 10, 8 11, 6 10, 5 9), (6 9.5, 6.5 9.75, 7 10.25, 6.5 10, 6 9.5))</code></p> <pre><code>ST_Affine(geometry, 1, 2, 1, 2, 1, 2)\n</code></pre> <p>Input: <code>POLYGON EMPTY</code></p> <p>Output: <code>POLYGON EMPTY</code></p> <p>Input: <code>GEOMETRYCOLLECTION (MULTIPOLYGON (((1 0, 1 1, 2 1, 2 0, 1 0), (1 0.5, 1 0.75, 1.5 0.75, 1.5 0.5, 1 0.5)), ((5 0, 5 5, 7 5, 7 0, 5 0))), POINT (10 10))</code></p> <p>Output: <code>GEOMETRYCOLLECTION (MULTIPOLYGON (((2 3, 4 5, 5 6, 3 4, 2 3), (3 4, 3.5 4.5, 4 5, 3.5 4.5, 3 4)), ((6 7, 16 17, 18 19, 8 9, 6 7))), POINT (31 32))</code></p> <p>Input: <code>POLYGON ((1 0 1, 1 1 1, 2 2 2, 1 0 1))</code></p> <p>Output: <code>POLYGON Z((2 3 1, 4 5 1, 7 8 2, 2 3 1))</code></p>"},{"location":"api/sql/Function/#st_angle","title":"ST_Angle","text":"<p>Introduction: Computes and returns the angle between two vectors represented by the provided points or linestrings.</p> <p>There are three variants possible for ST_Angle:</p> <p><code>ST_Angle(point1: Geometry, point2: Geometry, point3: Geometry, point4: Geometry)</code> Computes the angle formed by vectors represented by point1 - point2 and point3 - point4</p> <p><code>ST_Angle(point1: Geometry, point2: Geometry, point3: Geometry)</code> Computes the angle formed by vectors represented by point2 - point1 and point2 - point3</p> <p><code>ST_Angle(line1: Geometry, line2: Geometry)</code> Computes the angle formed by vectors S1 - E1 and S2 - E2, where S and E denote start and end points respectively</p> <p>Note</p> <p>If any other geometry type is provided, ST_Angle throws an IllegalArgumentException. Additionally, if any of the provided geometry is empty, ST_Angle throws an IllegalArgumentException.</p> <p>Note</p> <p>If a 3D geometry is provided, ST_Angle computes the angle ignoring the z ordinate, equivalent to calling ST_Angle for corresponding 2D geometries.</p> <p>Tip</p> <p>ST_Angle returns the angle in radian between 0 and 2\\Pi. To convert the angle to degrees, use ST_Degrees.</p> <p>Format: <code>ST_Angle(p1, p2, p3, p4) | ST_Angle(p1, p2, p3) | ST_Angle(line1, line2)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Angle(ST_GeomFromWKT('POINT(0 0)'), ST_GeomFromWKT('POINT (1 1)'), ST_GeomFromWKT('POINT(1 0)'), ST_GeomFromWKT('POINT(6 2)'))\n</code></pre> <p>Output:</p> <pre><code>0.4048917862850834\n</code></pre> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Angle(ST_GeomFromWKT('POINT (1 1)'), ST_GeomFromWKT('POINT (0 0)'), ST_GeomFromWKT('POINT(3 2)'))\n</code></pre> <p>Output:</p> <pre><code>0.19739555984988044\n</code></pre> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Angle(ST_GeomFromWKT('LINESTRING (0 0, 1 1)'), ST_GeomFromWKT('LINESTRING (0 0, 3 2)'))\n</code></pre> <p>Output:</p> <pre><code>0.19739555984988044\n</code></pre>"},{"location":"api/sql/Function/#st_area","title":"ST_Area","text":"<p>Introduction: Return the area of A</p> <p>Format: <code>ST_Area (A: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example: <pre><code>SELECT ST_Area(ST_GeomFromText(\"POLYGON(0 0, 0 10, 10 10, 0 10, 0 0)\"))\n</code></pre></p> <p>Output: <pre><code>10\n</code></pre></p>"},{"location":"api/sql/Function/#st_areaspheroid","title":"ST_AreaSpheroid","text":"<p>Introduction: Return the geodesic area of A using WGS84 spheroid. Unit is square meter. Works better for large geometries (country level) compared to <code>ST_Area</code> + <code>ST_Transform</code>. It is equivalent to PostGIS <code>ST_Area(geography, use_spheroid=true)</code> function and produces nearly identical results.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lon/lat order. You can use ST_FlipCoordinates to swap lat and lon.</p> <p>Note</p> <p>By default, this function uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order.</p> <p>Format: <code>ST_AreaSpheroid (A: Geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_AreaSpheroid(ST_GeomFromWKT('Polygon ((34 35, 28 30, 25 34, 34 35))'))\n</code></pre> <p>Output:</p> <pre><code>201824850811.76245\n</code></pre>"},{"location":"api/sql/Function/#st_asbinary","title":"ST_AsBinary","text":"<p>Introduction: Return the Well-Known Binary representation of a geometry</p> <p>Format: <code>ST_AsBinary (A: Geometry)</code></p> <p>Since: <code>v1.1.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_AsBinary(ST_GeomFromWKT('POINT (1 1)'))\n</code></pre> <p>Output:</p> <pre><code>0101000000000000000000f87f000000000000f87f\n</code></pre>"},{"location":"api/sql/Function/#st_asewkb","title":"ST_AsEWKB","text":"<p>Introduction: Return the Extended Well-Known Binary representation of a geometry. EWKB is an extended version of WKB which includes the SRID of the geometry. The format originated in PostGIS but is supported by many GIS tools. If the geometry is lacking SRID a WKB format is produced. Se ST_SetSRID It will ignore the M coordinate if present.</p> <p>Format: <code>ST_AsEWKB (A: Geometry)</code></p> <p>Since: <code>v1.1.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_AsEWKB(ST_SetSrid(ST_GeomFromWKT('POINT (1 1)'), 3021))\n</code></pre> <p>Output:</p> <pre><code>0101000020cd0b0000000000000000f03f000000000000f03f\n</code></pre>"},{"location":"api/sql/Function/#st_asewkt","title":"ST_AsEWKT","text":"<p>Introduction: Return the Extended Well-Known Text representation of a geometry. EWKT is an extended version of WKT which includes the SRID of the geometry. The format originated in PostGIS but is supported by many GIS tools. If the geometry is lacking SRID a WKT format is produced. See ST_SetSRID It will support M coordinate if present since v1.5.0.</p> <p>Format: <code>ST_AsEWKT (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_AsEWKT(ST_SetSrid(ST_GeomFromWKT('POLYGON((0 0,0 1,1 1,1 0,0 0))'), 4326))\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POLYGON ((0 0, 0 1, 1 1, 1 0, 0 0))\n</code></pre> <p>Spark SQL Example:</p> <pre><code>SELECT ST_AsEWKT(ST_MakePointM(1.0, 1.0, 1.0))\n</code></pre> <p>Output:</p> <pre><code>POINT M(1 1 1)\n</code></pre> <p>Spark SQL Example:</p> <pre><code>SELECT ST_AsEWKT(ST_MakePoint(1.0, 1.0, 1.0, 1.0))\n</code></pre> <p>Output:</p> <pre><code>POINT ZM(1 1 1 1)\n</code></pre>"},{"location":"api/sql/Function/#st_asgeojson","title":"ST_AsGeoJSON","text":"<p>Introduction: Return the GeoJSON string representation of a geometry</p> <p>Format: <code>ST_AsGeoJSON (A: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_AsGeoJSON(ST_GeomFromWKT('POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>{\n\"type\":\"Polygon\",\n\"coordinates\":[\n[[1.0,1.0],\n[8.0,1.0],\n[8.0,8.0],\n[1.0,8.0],\n[1.0,1.0]]\n]\n}\n</code></pre>"},{"location":"api/sql/Function/#st_asgml","title":"ST_AsGML","text":"<p>Introduction: Return the GML string representation of a geometry</p> <p>Format: <code>ST_AsGML (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_AsGML(ST_GeomFromWKT('POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>1.0,1.0 8.0,1.0 8.0,8.0 1.0,8.0 1.0,1.0\n</code></pre>"},{"location":"api/sql/Function/#st_askml","title":"ST_AsKML","text":"<p>Introduction: Return the KML string representation of a geometry</p> <p>Format: <code>ST_AsKML (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_AsKML(ST_GeomFromWKT('POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>1.0,1.0 8.0,1.0 8.0,8.0 1.0,8.0 1.0,1.0\n</code></pre>"},{"location":"api/sql/Function/#st_astext","title":"ST_AsText","text":"<p>Introduction: Return the Well-Known Text string representation of a geometry. It will support M coordinate if present since v1.5.0.</p> <p>Format: <code>ST_AsText (A: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_AsText(ST_SetSRID(ST_Point(1.0,1.0), 3021))\n</code></pre> <p>Output:</p> <pre><code>POINT (1 1)\n</code></pre> <p>Spark SQL Example:</p> <pre><code>SELECT ST_AsText(ST_MakePointM(1.0, 1.0, 1.0))\n</code></pre> <p>Output:</p> <pre><code>POINT M(1 1 1)\n</code></pre> <p>Spark SQL Example:</p> <pre><code>SELECT ST_AsText(ST_MakePoint(1.0, 1.0, 1.0, 1.0))\n</code></pre> <p>Output:</p> <pre><code>POINT ZM(1 1 1 1)\n</code></pre>"},{"location":"api/sql/Function/#st_azimuth","title":"ST_Azimuth","text":"<p>Introduction: Returns Azimuth for two given points in radians null otherwise.</p> <p>Format: <code>ST_Azimuth(pointA: Point, pointB: Point)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Azimuth(ST_POINT(0.0, 25.0), ST_POINT(0.0, 0.0))\n</code></pre> <p>Output:</p> <pre><code>3.141592653589793\n</code></pre>"},{"location":"api/sql/Function/#st_boundary","title":"ST_Boundary","text":"<p>Introduction: Returns the closure of the combinatorial boundary of this Geometry.</p> <p>Format: <code>ST_Boundary(geom: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Boundary(ST_GeomFromWKT('POLYGON((1 1,0 0, -1 1, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (1 1, 0 0, -1 1, 1 1)\n</code></pre>"},{"location":"api/sql/Function/#st_boundingdiagonal","title":"ST_BoundingDiagonal","text":"<p>Introduction: Returns a linestring spanning minimum and maximum values of each dimension of the given geometry's coordinates as its start and end point respectively. If an empty geometry is provided, the returned LineString is also empty. If a single vertex (POINT) is provided, the returned LineString has both the start and end points same as the points coordinates</p> <p>Format: <code>ST_BoundingDiagonal(geom: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example: <pre><code>SELECT ST_BoundingDiagonal(ST_GeomFromWKT(geom))\n</code></pre></p> <p>Input: <code>POLYGON ((1 1 1, 3 3 3, 0 1 4, 4 4 0, 1 1 1))</code></p> <p>Output: <code>LINESTRING Z(0 1 1, 4 4 4)</code></p> <p>Input: <code>POINT (10 10)</code></p> <p>Output: <code>LINESTRING (10 10, 10 10)</code></p> <p>Input: <code>GEOMETRYCOLLECTION(POLYGON ((5 5 5, -1 2 3, -1 -1 0, 5 5 5)), POINT (10 3 3))</code></p> <p>Output: <code>LINESTRING Z(-1 -1 0, 10 5 5)</code></p>"},{"location":"api/sql/Function/#st_buffer","title":"ST_Buffer","text":"<p>Introduction: Returns a geometry/geography that represents all points whose distance from this Geometry/geography is less than or equal to distance.</p> <p>The optional third parameter controls the buffer accuracy and style. Buffer accuracy is specified by the number of line segments approximating a quarter circle, with a default of 8 segments. Buffer style can be set by providing blank-separated key=value pairs in a list format.</p> <ul> <li><code>quad_segs=#</code> : Number of line segments utilized to approximate a quarter circle (default is 8).</li> <li><code>endcap=round|flat|square</code> : End cap style (default is <code>round</code>). <code>butt</code> is an accepted synonym for <code>flat</code>.</li> <li><code>join=round|mitre|bevel</code> : Join style (default is <code>round</code>). <code>miter</code> is an accepted synonym for <code>mitre</code>.</li> <li><code>mitre_limit=#.#</code> : mitre ratio limit and it only affects mitred join style. <code>miter_limit</code> is an accepted synonym for <code>mitre_limit</code>.</li> <li><code>side=both|left|right</code> : The option <code>left</code> or <code>right</code> enables a single-sided buffer operation on the geometry, with the buffered side aligned according to the direction of the line. This functionality is specific to LINESTRING geometry and has no impact on POINT or POLYGON geometries. By default, square end caps are applied.</li> </ul> <p>Note</p> <p><code>ST_Buffer</code> throws an <code>IllegalArgumentException</code> if the correct format, parameters, or options are not provided.</p> <p>Format:</p> <pre><code>ST_Buffer (A: Geometry, buffer: Double, bufferStyleParameters: String [Optional])\n</code></pre> <p>Since: <code>v1.5.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Buffer(ST_GeomFromWKT('POINT(0 0)'), 10)\nSELECT ST_Buffer(ST_GeomFromWKT('POINT(0 0)'), 10, 'quad_segs=2')\n</code></pre> <p>Output:</p> <p> </p> <p>8 Segments \u2002 2 Segments</p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Buffer(ST_GeomFromWKT('LINESTRING(0 0, 50 70, 100 100)'), 10, 'side=left')\n</code></pre> <p>Output:</p> <p> </p> <p>Original Linestring \u2003 Left side buffed Linestring</p>"},{"location":"api/sql/Function/#st_buildarea","title":"ST_BuildArea","text":"<p>Introduction: Returns the areal geometry formed by the constituent linework of the input geometry.</p> <p>Format: <code>ST_BuildArea (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_BuildArea(\nST_GeomFromWKT('MULTILINESTRING((0 0, 20 0, 20 20, 0 20, 0 0),(2 2, 18 2, 18 18, 2 18, 2 2))')\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+----------------------------------------------------------------------------+\n|geom                                                                        |\n+----------------------------------------------------------------------------+\n|POLYGON((0 0,0 20,20 20,20 0,0 0),(2 2,18 2,18 18,2 18,2 2))                |\n+----------------------------------------------------------------------------+\n</code></pre>"},{"location":"api/sql/Function/#st_centroid","title":"ST_Centroid","text":"<p>Introduction: Return the centroid point of A</p> <p>Format: <code>ST_Centroid (A: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Centroid(ST_GeomFromWKT('MULTIPOINT(-1  0, -1 2, 7 8, 9 8, 10 6)'))\n</code></pre> <p>Output:</p> <pre><code>POINT (4.8 4.8)\n</code></pre>"},{"location":"api/sql/Function/#st_closestpoint","title":"ST_ClosestPoint","text":"<p>Introduction: Returns the 2-dimensional point on geom1 that is closest to geom2. This is the first point of the shortest line between the geometries. If using 3D geometries, the Z coordinates will be ignored. If you have a 3D Geometry, you may prefer to use ST_3DClosestPoint. It will throw an exception indicates illegal argument if one of the params is an empty geometry.</p> <p>Format: <code>ST_ClosestPoint(g1: Geometry, g2: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example: <pre><code>SELECT ST_AsText( ST_ClosestPoint(g1, g2)) As ptwkt;\n</code></pre></p> <p>Input: <code>g1: POINT (160 40), g2: LINESTRING (10 30, 50 50, 30 110, 70 90, 180 140, 130 190)</code></p> <p>Output: <code>POINT(160 40)</code></p> <p>Input: <code>g1: LINESTRING (10 30, 50 50, 30 110, 70 90, 180 140, 130 190), g2: POINT (160 40)</code></p> <p>Output: <code>POINT(125.75342465753425 115.34246575342466)</code></p> <p>Input: <code>g1: 'POLYGON ((190 150, 20 10, 160 70, 190 150))', g2: ST_Buffer('POINT(80 160)', 30)</code></p> <p>Output: <code>POINT(131.59149149528952 101.89887534906197)</code></p>"},{"location":"api/sql/Function/#st_collect","title":"ST_Collect","text":"<p>Introduction: Returns MultiGeometry object based on geometry column/s or array with geometries</p> <p>Format:</p> <p><code>ST_Collect(*geom: Geometry)</code></p> <p><code>ST_Collect(geom: ARRAY[Geometry])</code></p> <p>Since: <code>v1.2.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Collect(\nST_GeomFromText('POINT(21.427834 52.042576573)'),\nST_GeomFromText('POINT(45.342524 56.342354355)')\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+---------------------------------------------------------------+\n|geom                                                           |\n+---------------------------------------------------------------+\n|MULTIPOINT ((21.427834 52.042576573), (45.342524 56.342354355))|\n+---------------------------------------------------------------+\n</code></pre> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Collect(\nArray(\nST_GeomFromText('POINT(21.427834 52.042576573)'),\nST_GeomFromText('POINT(45.342524 56.342354355)')\n)\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+---------------------------------------------------------------+\n|geom                                                           |\n+---------------------------------------------------------------+\n|MULTIPOINT ((21.427834 52.042576573), (45.342524 56.342354355))|\n+---------------------------------------------------------------+\n</code></pre>"},{"location":"api/sql/Function/#st_collectionextract","title":"ST_CollectionExtract","text":"<p>Introduction: Returns a homogeneous multi-geometry from a given geometry collection.</p> <p>The type numbers are: 1. POINT 2. LINESTRING 3. POLYGON</p> <p>If the type parameter is omitted a multi-geometry of the highest dimension is returned.</p> <p>Format:</p> <p><code>ST_CollectionExtract (A: Geometry)</code></p> <p><code>ST_CollectionExtract (A: Geometry, type: Integer)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Spark SQL Example:</p> <pre><code>WITH test_data as (\nST_GeomFromText(\n'GEOMETRYCOLLECTION(POINT(40 10), POLYGON((0 0, 0 5, 5 5, 5 0, 0 0)))'\n) as geom\n)\nSELECT ST_CollectionExtract(geom) as c1, ST_CollectionExtract(geom, 1) as c2\nFROM test_data\n</code></pre> <p>Result:</p> <pre><code>+----------------------------------------------------------------------------+\n|c1                                        |c2                               |\n+----------------------------------------------------------------------------+\n|MULTIPOLYGON(((0 0, 0 5, 5 5, 5 0, 0 0))) |MULTIPOINT(40 10)                |              |\n+----------------------------------------------------------------------------+\n</code></pre>"},{"location":"api/sql/Function/#st_concavehull","title":"ST_ConcaveHull","text":"<p>Introduction: Return the Concave Hull of polygon A, with alpha set to pctConvex[0, 1] in the Delaunay Triangulation method, the concave hull will not contain a hole unless allowHoles is set to true</p> <p>Format:</p> <p><code>ST_ConcaveHull (A: Geometry, pctConvex: Double)</code></p> <p><code>ST_ConcaveHull (A: Geometry, pctConvex: Double, allowHoles: Boolean)</code></p> <p>Since: <code>v1.4.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_ConcaveHull(ST_GeomFromWKT('POLYGON((175 150, 20 40, 50 60, 125 100, 175 150))'), 1)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((125 100, 20 40, 50 60, 175 150, 125 100))\n</code></pre>"},{"location":"api/sql/Function/#st_convexhull","title":"ST_ConvexHull","text":"<p>Introduction: Return the Convex Hull of polygon A</p> <p>Format: <code>ST_ConvexHull (A: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_ConvexHull(ST_GeomFromText('POLYGON((175 150, 20 40, 50 60, 125 100, 175 150))'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((20 40, 175 150, 125 100, 20 40))\n</code></pre>"},{"location":"api/sql/Function/#st_coorddim","title":"ST_CoordDim","text":"<p>Introduction: Returns the coordinate dimensions of the geometry. It is an alias of <code>ST_NDims</code>.</p> <p>Format: <code>ST_CoordDim(geom: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example with x, y, z coordinate:</p> <pre><code>SELECT ST_CoordDim(ST_GeomFromText('POINT(1 1 2'))\n</code></pre> <p>Output:</p> <pre><code>3\n</code></pre> <p>Spark SQL Example with x, y coordinate:</p> <pre><code>SELECT ST_CoordDim(ST_GeomFromWKT('POINT(3 7)'))\n</code></pre> <p>Output:</p> <pre><code>2\n</code></pre>"},{"location":"api/sql/Function/#st_degrees","title":"ST_Degrees","text":"<p>Introduction: Convert an angle in radian to degrees.</p> <p>Format: <code>ST_Degrees(angleInRadian)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Degrees(0.19739555984988044)\n</code></pre> <p>Output:</p> <pre><code>11.309932474020195\n</code></pre>"},{"location":"api/sql/Function/#st_difference","title":"ST_Difference","text":"<p>Introduction: Return the difference between geometry A and B (return part of geometry A that does not intersect geometry B)</p> <p>Format: <code>ST_Difference (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Difference(ST_GeomFromWKT('POLYGON ((-3 -3, 3 -3, 3 3, -3 3, -3 -3))'), ST_GeomFromWKT('POLYGON ((0 -4, 4 -4, 4 4, 0 4, 0 -4))'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0 -3, -3 -3, -3 3, 0 3, 0 -3))\n</code></pre>"},{"location":"api/sql/Function/#st_dimension","title":"ST_Dimension","text":"<p>Introduction: Return the topological dimension of this Geometry object, which must be less than or equal to the coordinate dimension. OGC SPEC s2.1.1.1 - returns 0 for POINT, 1 for LINESTRING, 2 for POLYGON, and the largest dimension of the components of a GEOMETRYCOLLECTION. If the dimension is unknown (e.g. for an empty GEOMETRYCOLLECTION) 0 is returned.</p> <p>Format: <code>ST_Dimension (A: Geometry) | ST_Dimension (C: Geometrycollection)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Dimension('GEOMETRYCOLLECTION(LINESTRING(1 1,0 0),POINT(0 0))');\n</code></pre> <p>Output:</p> <pre><code>1\n</code></pre>"},{"location":"api/sql/Function/#st_distance","title":"ST_Distance","text":"<p>Introduction: Return the Euclidean distance between A and B</p> <p>Format: <code>ST_Distance (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Distance(ST_GeomFromText('POINT(72 42)'), ST_GeomFromText('LINESTRING(-72 -42, 82 92)'))\n</code></pre> <p>Output:</p> <pre><code>31.155515639003543\n</code></pre>"},{"location":"api/sql/Function/#st_distancesphere","title":"ST_DistanceSphere","text":"<p>Introduction: Return the haversine / great-circle distance of A using a given earth radius (default radius: 6371008.0). Unit is meter. Compared to <code>ST_Distance</code> + <code>ST_Transform</code>, it works better for datasets that cover large regions such as continents or the entire planet. It is equivalent to PostGIS <code>ST_Distance(geography, use_spheroid=false)</code> and <code>ST_DistanceSphere</code> function and produces nearly identical results. It provides faster but less accurate result compared to <code>ST_DistanceSpheroid</code>.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lon/lat order. You can use ST_FlipCoordinates to swap lat and lon. For non-point data, we first take the centroids of both geometries and then compute the distance.</p> <p>Note</p> <p>By default, this function uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order.</p> <p>Format: <code>ST_DistanceSphere (A: Geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_DistanceSphere(ST_GeomFromWKT('POINT (-0.56 51.3168)'), ST_GeomFromWKT('POINT (-3.1883 55.9533)'))\n</code></pre> <p>Output:</p> <pre><code>543796.9506134904\n</code></pre> <p>Spark SQL Example:</p> <pre><code>SELECT ST_DistanceSphere(ST_GeomFromWKT('POINT (-0.56 51.3168)'), ST_GeomFromWKT('POINT (-3.1883 55.9533)'), 6378137.0)\n</code></pre> <p>Output:</p> <pre><code>544405.4459192449\n</code></pre>"},{"location":"api/sql/Function/#st_distancespheroid","title":"ST_DistanceSpheroid","text":"<p>Introduction: Return the geodesic distance of A using WGS84 spheroid. Unit is meter. Compared to <code>ST_Distance</code> + <code>ST_Transform</code>, it works better for datasets that cover large regions such as continents or the entire planet. It is equivalent to PostGIS <code>ST_Distance(geography, use_spheroid=true)</code> and <code>ST_DistanceSpheroid</code> function and produces nearly identical results. It provides slower but more accurate result compared to <code>ST_DistanceSphere</code>.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lon/lat order. You can use ST_FlipCoordinates to swap lat and lon. For non-point data, we first take the centroids of both geometries and then compute the distance.</p> <p>Note</p> <p>By default, this function uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order.</p> <p>Format: <code>ST_DistanceSpheroid (A: Geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_DistanceSpheroid(ST_GeomFromWKT('POINT (-0.56 51.3168)'), ST_GeomFromWKT('POINT (-3.1883 55.9533)'))\n</code></pre> <p>Output:</p> <pre><code>544430.9411996207\n</code></pre>"},{"location":"api/sql/Function/#st_dump","title":"ST_Dump","text":"<p>Introduction: It expands the geometries. If the geometry is simple (Point, Polygon Linestring etc.) it returns the geometry itself, if the geometry is collection or multi it returns record for each of collection components.</p> <p>Format: <code>ST_Dump(geom: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example: <pre><code>SELECT ST_Dump(ST_GeomFromText('MULTIPOINT ((10 40), (40 30), (20 20), (30 10))'))\n</code></pre></p> <p>Output:</p> <pre><code>[POINT (10 40), POINT (40 30), POINT (20 20), POINT (30 10)]\n</code></pre>"},{"location":"api/sql/Function/#st_dumppoints","title":"ST_DumpPoints","text":"<p>Introduction: Returns list of Points which geometry consists of.</p> <p>Format: <code>ST_DumpPoints(geom: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_DumpPoints(ST_GeomFromText('LINESTRING (0 0, 1 1, 1 0)'))\n</code></pre> <p>Output:</p> <pre><code>[POINT (0 0), POINT (0 1), POINT (1 1), POINT (1 0), POINT (0 0)]\n</code></pre>"},{"location":"api/sql/Function/#st_endpoint","title":"ST_EndPoint","text":"<p>Introduction: Returns last point of given linestring.</p> <p>Format: <code>ST_EndPoint(geom: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_EndPoint(ST_GeomFromText('LINESTRING(100 150,50 60, 70 80, 160 170)'))\n</code></pre> <p>Output:</p> <pre><code>POINT(160 170)\n</code></pre>"},{"location":"api/sql/Function/#st_envelope","title":"ST_Envelope","text":"<p>Introduction: Return the envelope boundary of A</p> <p>Format: <code>ST_Envelope (A: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Envelope(ST_GeomFromWKT('LINESTRING(0 0, 1 3)'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0 0, 0 3, 1 3, 1 0, 0 0))\n</code></pre>"},{"location":"api/sql/Function/#st_exteriorring","title":"ST_ExteriorRing","text":"<p>Introduction: Returns a line string representing the exterior ring of the POLYGON geometry. Return NULL if the geometry is not a polygon.</p> <p>Format: <code>ST_ExteriorRing(geom: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_ExteriorRing(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (0 0, 1 1, 1 2, 1 1, 0 0)\n</code></pre>"},{"location":"api/sql/Function/#st_flipcoordinates","title":"ST_FlipCoordinates","text":"<p>Introduction: Returns a version of the given geometry with X and Y axis flipped.</p> <p>Format: <code>ST_FlipCoordinates(A: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_FlipCoordinates(ST_GeomFromWKT(\"POINT (1 2)\"))\n</code></pre> <p>Output:</p> <pre><code>POINT (2 1)\n</code></pre>"},{"location":"api/sql/Function/#st_force_2d","title":"ST_Force_2D","text":"<p>Introduction: Forces the geometries into a \"2-dimensional mode\" so that all output representations will only have the X and Y coordinates</p> <p>Format: <code>ST_Force_2D (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Force_2D(ST_GeomFromText('POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON((0 0,0 5,5 0,0 0),(1 1,3 1,1 3,1 1))\n</code></pre>"},{"location":"api/sql/Function/#st_force3d","title":"ST_Force3D","text":"<p>Introduction: Forces the geometry into a 3-dimensional model so that all output representations will have X, Y and Z coordinates. An optionally given zValue is tacked onto the geometry if the geometry is 2-dimensional. Default value of zValue is 0.0 If the given geometry is 3-dimensional, no change is performed on it. If the given geometry is empty, no change is performed on it.</p> <p>Note</p> <p>Example output is after calling ST_AsText() on returned geometry, which adds Z for in the WKT for 3D geometries</p> <p>Format: <code>ST_Force3D(geometry: Geometry, zValue: Double)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_AsText(ST_Force3D(ST_GeomFromText('POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))'), 2.3))\n</code></pre> <p>Output:</p> <pre><code>POLYGON Z((0 0 2, 0 5 2, 5 0 2, 0 0 2), (1 1 2, 3 1 2, 1 3 2, 1 1 2))\n</code></pre> <p>Spark SQL Example:</p> <pre><code>SELECT ST_AsText(ST_Force3D(ST_GeomFromText('LINESTRING(0 1,1 0,2 0)'), 2.3))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING Z(0 1 2.3, 1 0 2.3, 2 0 2.3)\n</code></pre> <p>Spark SQL Example:</p> <pre><code>SELECT ST_AsText(ST_Force3D(ST_GeomFromText('LINESTRING EMPTY'), 3))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING EMPTY\n</code></pre>"},{"location":"api/sql/Function/#st_frechetdistance","title":"ST_FrechetDistance","text":"<p>Introduction: Computes and returns discrete Frechet Distance between the given two geometries, based on Computing Discrete Frechet Distance</p> <p>If any of the geometries is empty, returns 0.0</p> <p>Format: <code>ST_FrechetDistance(g1: Geometry, g2: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_FrechetDistance(ST_GeomFromWKT('POINT (0 1)'), ST_GeomFromWKT('LINESTRING (0 0, 1 0, 2 0, 3 0, 4 0, 5 0)'))\n</code></pre> <p>Output:</p> <pre><code>5.0990195135927845\n</code></pre>"},{"location":"api/sql/Function/#st_geohash","title":"ST_GeoHash","text":"<p>Introduction: Returns GeoHash of the geometry with given precision</p> <p>Format: <code>ST_GeoHash(geom: Geometry, precision: Integer)</code></p> <p>Since: <code>v1.1.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_GeoHash(ST_GeomFromText('POINT(21.427834 52.042576573)'), 5) AS geohash\n</code></pre> <p>Output:</p> <pre><code>u3r0p\n</code></pre>"},{"location":"api/sql/Function/#st_geometricmedian","title":"ST_GeometricMedian","text":"<p>Introduction: Computes the approximate geometric median of a MultiPoint geometry using the Weiszfeld algorithm. The geometric median provides a centrality measure that is less sensitive to outlier points than the centroid.</p> <p>The algorithm will iterate until the distance change between successive iterations is less than the supplied <code>tolerance</code> parameter. If this condition has not been met after <code>maxIter</code> iterations, the function will produce an error and exit, unless <code>failIfNotConverged</code> is set to <code>false</code>.</p> <p>If a <code>tolerance</code> value is not provided, a default <code>tolerance</code> value is <code>1e-6</code>.</p> <p>Format:</p> <pre><code>ST_GeometricMedian(geom: Geometry, tolerance: Double, maxIter: Integer, failIfNotConverged: Boolean)\n</code></pre> <pre><code>ST_GeometricMedian(geom: Geometry, tolerance: Double, maxIter: Integer)\n</code></pre> <pre><code>ST_GeometricMedian(geom: Geometry, tolerance: Double)\n</code></pre> <pre><code>ST_GeometricMedian(geom: Geometry)\n</code></pre> <p>Default parameters: <code>tolerance: 1e-6, maxIter: 1000, failIfNotConverged: false</code></p> <p>Since: <code>v1.4.1</code></p> <p>Spark SQL Example: <pre><code>SELECT ST_GeometricMedian(ST_GeomFromWKT('MULTIPOINT((0 0), (1 1), (2 2), (200 200))'))\n</code></pre></p> <p>Output: <pre><code>POINT (1.9761550281255005 1.9761550281255005)\n</code></pre></p>"},{"location":"api/sql/Function/#st_geometryn","title":"ST_GeometryN","text":"<p>Introduction: Return the 0-based Nth geometry if the geometry is a GEOMETRYCOLLECTION, (MULTI)POINT, (MULTI)LINESTRING, MULTICURVE or (MULTI)POLYGON. Otherwise, return null</p> <p>Format: <code>ST_GeometryN(geom: Geometry, n: Integer)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_GeometryN(ST_GeomFromText('MULTIPOINT((1 2), (3 4), (5 6), (8 9))'), 1)\n</code></pre> <p>Output:</p> <pre><code>POINT (3 4)\n</code></pre>"},{"location":"api/sql/Function/#st_geometrytype","title":"ST_GeometryType","text":"<p>Introduction: Returns the type of the geometry as a string. EG: 'ST_Linestring', 'ST_Polygon' etc.</p> <p>Format: <code>ST_GeometryType (A: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_GeometryType(ST_GeomFromText('LINESTRING(77.29 29.07,77.42 29.26,77.27 29.31,77.29 29.07)'))\n</code></pre> <p>Output:</p> <pre><code>ST_LINESTRING\n</code></pre>"},{"location":"api/sql/Function/#st_h3celldistance","title":"ST_H3CellDistance","text":"<p>Introduction: return result of h3 function gridDistance(cel1, cell2). As described by H3 documentation</p> <p>Finding the distance can fail because the two indexes are not comparable (different resolutions), too far apart, or are separated by pentagonal distortion. This is the same set of limitations as the local IJ coordinate space functions.</p> <p>In this case, Sedona use in-house implementation of estimation the shortest path and return the size as distance.</p> <p>Format: <code>ST_H3CellDistance(cell1: Long, cell2: Long)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL example: <pre><code>select ST_H3CellDistance(ST_H3CellIDs(ST_GeomFromWKT('POINT(1 2)'), 8, true)[0], ST_H3CellIDs(ST_GeomFromWKT('POINT(1.23 1.59)'), 8, true)[0])\n</code></pre></p> <p>Output: <pre><code>+-----------------------------------------------------------------------------------------------------------------------------------------+\n|st_h3celldistance(st_h3cellids(st_geomfromwkt(POINT(1 2), 0), 8, true)[0], st_h3cellids(st_geomfromwkt(POINT(1.23 1.59), 0), 8, true)[0])|\n+-----------------------------------------------------------------------------------------------------------------------------------------+\n|                                                                                                                                       78|\n+-----------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre></p>"},{"location":"api/sql/Function/#st_h3cellids","title":"ST_H3CellIDs","text":"<p>Introduction: Cover the geometry by H3 cell IDs with the given resolution(level). To understand the cell statistics please refer to H3 Doc H3 native fill functions doesn't guarantee full coverage on the shapes.</p>"},{"location":"api/sql/Function/#cover-polygon","title":"Cover Polygon","text":"<p>When fullCover = false, for polygon sedona will use polygonToCells. This can't guarantee full coverage but will guarantee no false positive.</p> <p>When fullCover = true, sedona will add on extra traversal logic to guarantee full coverage on shapes. This will lead to redundancy but can guarantee full coverage.</p> <p>Choose the option according to your use case.</p>"},{"location":"api/sql/Function/#cover-linestring","title":"Cover LineString","text":"<p>For the lineString, sedona will call gridPathCells(https://h3geo.org/docs/api/traversal#gridpathcells) per segment. From H3's documentation</p> <p>This function may fail to find the line between two indexes, for example if they are very far apart. It may also fail when finding distances for indexes on opposite sides of a pentagon.</p> <p>When the <code>gridPathCells</code> function throw error, Sedona implemented in-house approximate implementation to generate the shortest path, which can cover the corner cases.</p> <p>Both functions can't guarantee full coverage. When the <code>fullCover = true</code>, we'll do extra cell traversal to guarantee full cover. In worst case, sedona will use MBR to guarantee the full coverage.</p> <p>If you seek to get the shortest path between cells, you can call this function with <code>fullCover = false</code></p> <p>Format: <code>ST_H3CellIDs(geom: geometry, level: Int, fullCover: Boolean)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_H3CellIDs(ST_GeomFromText('LINESTRING(1 3 4, 5 6 7)'), 6, true)\n</code></pre></p> <p>Output: <pre><code>+-------------------------------------------------------------+\n|st_h3cellids(st_geomfromtext(LINESTRING(1 3 4, 5 6 7), 0), 6)|\n+-------------------------------------------------------------+\n|                                         [6055475394579005...|\n+-------------------------------------------------------------+\n</code></pre></p>"},{"location":"api/sql/Function/#st_h3kring","title":"ST_H3KRing","text":"<p>Introduction: return the result of H3 function gridDisk(cell, k).</p> <p>K means <code>the distance of the origin index</code>, <code>gridDisk(cell, k)</code> return cells with distance <code>&lt;=k</code> from the original cell.</p> <p><code>exactRing : Boolean</code>, when set to <code>true</code>, sedona will remove the result of <code>gridDisk(cell, k-1)</code> from the original results, means only keep the cells with distance exactly <code>k</code> from the original cell</p> <p>Format: <code>ST_H3KRing(cell: Long, k: Int, exactRing: Boolean)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_H3KRing(ST_H3CellIDs(ST_GeomFromWKT('POINT(1 2)'), 8, true)[0], 1, true) cells union select ST_H3KRing(ST_H3CellIDs(ST_GeomFromWKT('POINT(1 2)'), 8, true)[0], 1, false) cells\n</code></pre></p> <p>Output: <pre><code>+--------------------------------------------------------------------------------------------------------------------------------------------+\n|cells                                                                                                                                       |\n+--------------------------------------------------------------------------------------------------------------------------------------------+\n|[614552597293957119, 614552609329512447, 614552609316929535, 614552609327415295, 614552609287569407, 614552597289762815]                    |\n|[614552609325318143, 614552597293957119, 614552609329512447, 614552609316929535, 614552609327415295, 614552609287569407, 614552597289762815]|\n+--------------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre></p>"},{"location":"api/sql/Function/#st_h3togeom","title":"ST_H3ToGeom","text":"<p>Introduction: return the result of H3 function cellsToMultiPolygon(cells).</p> <p>Reverse the uber h3 cells to MultiPolygon object composed by the geometry hexagons.</p> <p>Format: <code>ST_H3ToGeom(cells: Array[Long])</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_H3ToGeom(ST_H3CellIDs(ST_GeomFromWKT('POINT(1 2)'), 8, true)[0], 1, true))\n</code></pre></p> <p>Output: <pre><code>|st_h3togeom(st_h3cellids(st_geomfromwkt(POINT(1 2), 0), 8, true))                                                                                                                                                                                                                              |\n+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|MULTIPOLYGON (((1.0057629565404935 1.9984665139177658, 1.0037116327309032 2.001832524914011, 0.9997277993570498 2.0011632704656668, 0.9977951427833285 1.99712822839324, 0.9998461908217768 1.9937621529331915, 1.0038301712104252 1.9944311839965554, 1.0057629565404935 1.9984665139177658)))|\n+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre></p>"},{"location":"api/sql/Function/#st_hausdorffdistance","title":"ST_HausdorffDistance","text":"<p>Introduction: Returns a discretized (and hence approximate) Hausdorff distance between the given 2 geometries. Optionally, a densityFraction parameter can be specified, which gives more accurate results by densifying segments before computing hausdorff distance between them. Each segment is broken down into equal-length subsegments whose ratio with segment length is closest to the given density fraction.</p> <p>Hence, the lower the densityFrac value, the more accurate is the computed hausdorff distance, and the more time it takes to compute it.</p> <p>If any of the geometry is empty, 0.0 is returned.</p> <p>Note</p> <p>Accepted range of densityFrac is (0.0, 1.0], if any other value is provided, ST_HausdorffDistance throws an IllegalArgumentException</p> <p>Note</p> <p>Even though the function accepts 3D geometry, the z ordinate is ignored and the computed hausdorff distance is equivalent to the geometries not having the z ordinate.</p> <p>Format: <code>ST_HausdorffDistance(g1: Geometry, g2: Geometry, densityFrac: Double)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_HausdorffDistance(ST_GeomFromWKT('POINT (0.0 1.0)'), ST_GeomFromWKT('LINESTRING (0 0, 1 0, 2 0, 3 0, 4 0, 5 0)'), 0.1)\n</code></pre> <p>Output:</p> <pre><code>5.0990195135927845\n</code></pre> <p>Spark SQL Example:</p> <pre><code>SELECT ST_HausdorffDistance(ST_GeomFromText('POLYGON Z((1 0 1, 1 1 2, 2 1 5, 2 0 1, 1 0 1))'), ST_GeomFromText('POLYGON Z((4 0 4, 6 1 4, 6 4 9, 6 1 3, 4 0 4))'))\n</code></pre> <p>Output:</p> <pre><code>5.0\n</code></pre>"},{"location":"api/sql/Function/#st_interiorringn","title":"ST_InteriorRingN","text":"<p>Introduction: Returns the Nth interior linestring ring of the polygon geometry. Returns NULL if the geometry is not a polygon or the given N is out of range</p> <p>Format: <code>ST_InteriorRingN(geom: Geometry, n: Integer)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_InteriorRingN(ST_GeomFromText('POLYGON((0 0, 0 5, 5 5, 5 0, 0 0), (1 1, 2 1, 2 2, 1 2, 1 1), (1 3, 2 3, 2 4, 1 4, 1 3), (3 3, 4 3, 4 4, 3 4, 3 3))'), 0)\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (1 1, 2 1, 2 2, 1 2, 1 1)\n</code></pre>"},{"location":"api/sql/Function/#st_intersection","title":"ST_Intersection","text":"<p>Introduction: Return the intersection geometry of A and B</p> <p>Format: <code>ST_Intersection (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Intersection(\nST_GeomFromWKT(\"POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))\"),\nST_GeomFromWKT(\"POLYGON((2 2, 9 2, 9 9, 2 9, 2 2))\")\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((2 8, 8 8, 8 2, 2 2, 2 8))\n</code></pre>"},{"location":"api/sql/Function/#st_isclosed","title":"ST_IsClosed","text":"<p>Introduction: RETURNS true if the LINESTRING start and end point are the same.</p> <p>Format: <code>ST_IsClosed(geom: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_IsClosed(ST_GeomFromText('LINESTRING(0 0, 1 1, 1 0)'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/sql/Function/#st_iscollection","title":"ST_IsCollection","text":"<p>Introduction: Returns <code>TRUE</code> if the geometry type of the input is a geometry collection type. Collection types are the following:</p> <ul> <li>GEOMETRYCOLLECTION</li> <li>MULTI{POINT, POLYGON, LINESTRING}</li> </ul> <p>Format: <code>ST_IsCollection(geom: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_IsCollection(ST_GeomFromText('MULTIPOINT(0 0), (6 6)'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre> <p>Spark SQL Example:</p> <pre><code>SELECT ST_IsCollection(ST_GeomFromText('POINT(5 5)'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/sql/Function/#st_isempty","title":"ST_IsEmpty","text":"<p>Introduction: Test if a geometry is empty geometry</p> <p>Format: <code>ST_IsEmpty (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_IsEmpty(ST_GeomFromWKT('POLYGON((0 0,0 1,1 1,1 0,0 0))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/sql/Function/#st_isring","title":"ST_IsRing","text":"<p>Introduction: RETURN true if LINESTRING is ST_IsClosed and ST_IsSimple.</p> <p>Format: <code>ST_IsRing(geom: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_IsRing(ST_GeomFromText(\"LINESTRING(0 0, 0 1, 1 1, 1 0, 0 0)\"))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Function/#st_issimple","title":"ST_IsSimple","text":"<p>Introduction: Test if geometry's only self-intersections are at boundary points.</p> <p>Format: <code>ST_IsSimple (A: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_IsSimple(ST_GeomFromWKT('POLYGON((1 1, 3 1, 3 3, 1 3, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Function/#st_isvalid","title":"ST_IsValid","text":"<p>Introduction: Test if a geometry is well formed. The function can be invoked with just the geometry or with an additional flag (from <code>v1.5.1</code>). The flag alters the validity checking behavior. The flags parameter is a bitfield with the following options:</p> <ul> <li>0 (default): Use usual OGC SFS (Simple Features Specification) validity semantics.</li> <li>1: \"ESRI flag\", Accepts certain self-touching rings as valid, which are considered invalid under OGC standards.</li> </ul> <p>Formats: <pre><code>ST_IsValid (A: Geometry)\n</code></pre> <pre><code>ST_IsValid (A: Geometry, flag: Integer)\n</code></pre></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_IsValid(ST_GeomFromWKT('POLYGON((0 0, 10 0, 10 10, 0 10, 0 0), (15 15, 15 20, 20 20, 20 15, 15 15))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/sql/Function/#st_isvalidreason","title":"ST_IsValidReason","text":"<p>Introduction: Returns text stating if the geometry is valid. If not, it provides a reason why it is invalid. The function can be invoked with just the geometry or with an additional flag. The flag alters the validity checking behavior. The flags parameter is a bitfield with the following options:</p> <ul> <li>0 (default): Use usual OGC SFS (Simple Features Specification) validity semantics.</li> <li>1: \"ESRI flag\", Accepts certain self-touching rings as valid, which are considered invalid under OGC standards.</li> </ul> <p>Formats: <pre><code>ST_IsValidReason (A: Geometry)\n</code></pre> <pre><code>ST_IsValidReason (A: Geometry, flag: Integer)\n</code></pre></p> <p>Since: <code>v1.5.1</code></p> <p>SQL Example for valid geometry:</p> <pre><code>SELECT ST_IsValidReason(ST_GeomFromWKT('POLYGON ((100 100, 100 300, 300 300, 300 100, 100 100))')) as validity_info\n</code></pre> <p>Output:</p> <pre><code>Valid Geometry\n</code></pre> <p>SQL Example for invalid geometries:</p> <pre><code>SELECT gid, ST_IsValidReason(geom) as validity_info\nFROM Geometry_table\nWHERE ST_IsValid(geom) = false\nORDER BY gid\n</code></pre> <p>Output:</p> <pre><code>gid  |                  validity_info\n-----+----------------------------------------------------\n5330 | Self-intersection at or near point (32.0, 5.0, NaN)\n5340 | Self-intersection at or near point (42.0, 5.0, NaN)\n5350 | Self-intersection at or near point (52.0, 5.0, NaN)\n</code></pre>"},{"location":"api/sql/Function/#st_length","title":"ST_Length","text":"<p>Introduction: Return the perimeter of A</p> <p>Format: <code>ST_Length (A: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Length(ST_GeomFromWKT('LINESTRING(38 16,38 50,65 50,66 16,38 16)'))\n</code></pre> <p>Output:</p> <pre><code>123.0147027033899\n</code></pre>"},{"location":"api/sql/Function/#st_lengthspheroid","title":"ST_LengthSpheroid","text":"<p>Introduction: Return the geodesic perimeter of A using WGS84 spheroid. Unit is meter. Works better for large geometries (country level) compared to <code>ST_Length</code> + <code>ST_Transform</code>. It is equivalent to PostGIS <code>ST_Length(geography, use_spheroid=true)</code> and <code>ST_LengthSpheroid</code> function and produces nearly identical results.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lon/lat order. You can use ST_FlipCoordinates to swap lat and lon.</p> <p>Note</p> <p>By default, this function uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order.</p> <p>Format: <code>ST_LengthSpheroid (A: Geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_LengthSpheroid(ST_GeomFromWKT('Polygon ((0 0, 90 0, 0 0))'))\n</code></pre> <p>Output:</p> <pre><code>20037508.342789244\n</code></pre>"},{"location":"api/sql/Function/#st_linefrommultipoint","title":"ST_LineFromMultiPoint","text":"<p>Introduction: Creates a LineString from a MultiPoint geometry.</p> <p>Format: <code>ST_LineFromMultiPoint (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_LineFromMultiPoint(ST_GeomFromText('MULTIPOINT((10 40), (40 30), (20 20), (30 10))'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (10 40, 40 30, 20 20, 30 10)\n</code></pre>"},{"location":"api/sql/Function/#st_lineinterpolatepoint","title":"ST_LineInterpolatePoint","text":"<p>Introduction: Returns a point interpolated along a line. First argument must be a LINESTRING. Second argument is a Double between 0 and 1 representing fraction of total linestring length the point has to be located.</p> <p>Format: <code>ST_LineInterpolatePoint (geom: Geometry, fraction: Double)</code></p> <p>Since: <code>v1.0.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_LineInterpolatePoint(ST_GeomFromWKT('LINESTRING(25 50, 100 125, 150 190)'), 0.2)\n</code></pre> <p>Output: <pre><code>POINT (51.5974135047432 76.5974135047432)\n</code></pre></p>"},{"location":"api/sql/Function/#st_linelocatepoint","title":"ST_LineLocatePoint","text":"<p>Introduction: Returns a double between 0 and 1, representing the location of the closest point on the LineString as a fraction of its total length. The first argument must be a LINESTRING, and the second argument is a POINT geometry.</p> <p>Format: <code>ST_LineLocatePoint(linestring: Geometry, point: Geometry)</code></p> <p>Since: <code>v1.5.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_LineLocatePoint(ST_GeomFromWKT('LINESTRING(0 0, 1 1, 2 2)'), ST_GeomFromWKT('POINT(0 2)'))\n</code></pre> <p>Output: <pre><code>0.5\n</code></pre></p>"},{"location":"api/sql/Function/#st_linemerge","title":"ST_LineMerge","text":"<p>Introduction: Returns a LineString formed by sewing together the constituent line work of a MULTILINESTRING.</p> <p>Note</p> <p>Only works for MULTILINESTRING. Using other geometry will return a GEOMETRYCOLLECTION EMPTY. If the MultiLineString can't be merged, the original MULTILINESTRING is returned.</p> <p>Format: <code>ST_LineMerge (A: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_LineMerge(ST_GeomFromWKT('MULTILINESTRING ((-29 -27, -30 -29.7, -45 -33), (-45 -33, -46 -32))'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-29 -27, -30 -29.7, -45 -33, -46 -32)\n</code></pre>"},{"location":"api/sql/Function/#st_linesubstring","title":"ST_LineSubstring","text":"<p>Introduction: Return a linestring being a substring of the input one starting and ending at the given fractions of total 2d length. Second and third arguments are Double values between 0 and 1. This only works with LINESTRINGs.</p> <p>Format:</p> <p><code>ST_LineSubstring (geom: Geometry, startfraction: Double, endfraction: Double)</code></p> <p>Since: <code>v1.0.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_LineSubstring(ST_GeomFromWKT('LINESTRING(25 50, 100 125, 150 190)'), 0.333, 0.666)\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (69.28469348539744 94.28469348539744, 100 125, 111.70035626068274 140.21046313888758)\n</code></pre>"},{"location":"api/sql/Function/#st_makeline","title":"ST_MakeLine","text":"<p>Introduction: Creates a LineString containing the points of Point, MultiPoint, or LineString geometries. Other geometry types cause an error.</p> <p>Format:</p> <p><code>ST_MakeLine(geom1: Geometry, geom2: Geometry)</code></p> <p><code>ST_MakeLine(geoms: ARRAY[Geometry])</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_AsText( ST_MakeLine(ST_Point(1,2), ST_Point(3,4)) );\n</code></pre> <p>Output:</p> <pre><code>LINESTRING(1 2,3 4)\n</code></pre> <p>Spark SQL Example:</p> <pre><code>SELECT ST_AsText( ST_MakeLine( 'LINESTRING(0 0, 1 1)', 'LINESTRING(2 2, 3 3)' ) );\n</code></pre> <p>Output:</p> <pre><code> LINESTRING(0 0,1 1,2 2,3 3)\n</code></pre>"},{"location":"api/sql/Function/#st_makepolygon","title":"ST_MakePolygon","text":"<p>Introduction: Function to convert closed linestring to polygon including holes</p> <p>Format: <code>ST_MakePolygon(geom: Geometry, holes: ARRAY[Geometry])</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_MakePolygon(\nST_GeomFromText('LINESTRING(7 -1, 7 6, 9 6, 9 1, 7 -1)'),\nARRAY(ST_GeomFromText('LINESTRING(6 2, 8 2, 8 1, 6 1, 6 2)'))\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((7 -1, 7 6, 9 6, 9 1, 7 -1), (6 2, 8 2, 8 1, 6 1, 6 2))\n</code></pre>"},{"location":"api/sql/Function/#st_makevalid","title":"ST_MakeValid","text":"<p>Introduction: Given an invalid geometry, create a valid representation of the geometry.</p> <p>Collapsed geometries are either converted to empty (keepCollapsed=true) or a valid geometry of lower dimension (keepCollapsed=false). Default is keepCollapsed=false.</p> <p>Format:</p> <p><code>ST_MakeValid (A: Geometry)</code></p> <p><code>ST_MakeValid (A: Geometry, keepCollapsed: Boolean)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>WITH linestring AS (\nSELECT ST_GeomFromWKT('LINESTRING(1 1, 1 1)') AS geom\n) SELECT ST_MakeValid(geom), ST_MakeValid(geom, true) FROM linestring\n</code></pre> <p>Result: <pre><code>+------------------+------------------------+\n|st_makevalid(geom)|st_makevalid(geom, true)|\n+------------------+------------------------+\n|  LINESTRING EMPTY|             POINT (1 1)|\n+------------------+------------------------+\n</code></pre></p> <p>Note</p> <p>In Sedona up to and including version 1.2 the behaviour of ST_MakeValid was different.</p> <p>Be sure to check you code when upgrading. The previous implementation only worked for (multi)polygons and had a different interpretation of the second, boolean, argument. It would also sometimes return multiple geometries for a single geometry input.</p>"},{"location":"api/sql/Function/#st_minimumboundingcircle","title":"ST_MinimumBoundingCircle","text":"<p>Introduction: Returns the smallest circle polygon that contains a geometry. The optional quadrantSegments parameter determines how many segments to use per quadrant and the default number of segments has been changed to 48 since v1.5.0.</p> <p>Format:</p> <p><code>ST_MinimumBoundingCircle(geom: Geometry, [Optional] quadrantSegments: Integer)</code></p> <p>Since: <code>v1.0.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_MinimumBoundingCircle(ST_GeomFromWKT('LINESTRING(0 0, 0 1)'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0.5 0.5, 0.4997322937381828 0.4836404585891119, 0.4989294616193017 0.4672984353849285, 0.4975923633360985 0.4509914298352197, 0.4957224306869052 0.4347369038899742, 0.4933216660424395 0.4185522633027057, 0.4903926402016152 0.4024548389919359, 0.4869384896386668 0.3864618684828134, 0.4829629131445342 0.3705904774487396, 0.4784701678661044 0.3548576613727689, 0.4734650647475528 0.3392802673484192, 0.4679529633786629 0.3238749760393833, 0.4619397662556434 0.3086582838174551, 0.4554319124605879 0.2936464850978027, 0.4484363707663442 0.2788556548904993, 0.4409606321741775 0.2643016315870012, 0.4330127018922194 0.25, 0.4246010907632894 0.2359660746748161, 0.4157348061512726 0.2222148834901989, 0.4064233422958076 0.2087611515660989, 0.3966766701456176 0.1956192854956397, 0.3865052266813685 0.1828033579181773, 0.3759199037394887 0.1703270924499656, 0.3649320363489179 0.1582038489885644, 0.3535533905932738 0.1464466094067263, 0.3417961510114357 0.1350679636510822, 0.3296729075500345 0.1240800962605114, 0.3171966420818228 0.1134947733186316, 0.3043807145043603 0.1033233298543824, 0.2912388484339011 0.0935766577041924, 0.2777851165098012 0.0842651938487274, 0.264033925325184 0.0753989092367106, 0.2500000000000001 0.0669872981077807, 0.2356983684129989 0.0590393678258225, 0.2211443451095007 0.0515636292336559, 0.2063535149021975 0.0445680875394122, 0.1913417161825449 0.0380602337443566, 0.1761250239606168 0.0320470366213372, 0.1607197326515808 0.0265349352524472, 0.1451423386272312 0.0215298321338956, 0.1294095225512605 0.0170370868554659, 0.1135381315171867 0.0130615103613332, 0.0975451610080642 0.0096073597983848, 0.0814477366972944 0.0066783339575605, 0.0652630961100259 0.0042775693130948, 0.0490085701647804 0.0024076366639016, 0.0327015646150716 0.0010705383806983, 0.0163595414108882 0.0002677062618172, 0 0, -0.016359541410888 0.0002677062618172, -0.0327015646150715 0.0010705383806983, -0.0490085701647802 0.0024076366639015, -0.0652630961100257 0.0042775693130948, -0.0814477366972942 0.0066783339575605, -0.097545161008064 0.0096073597983847, -0.1135381315171866 0.0130615103613332, -0.1294095225512603 0.0170370868554658, -0.1451423386272311 0.0215298321338955, -0.1607197326515807 0.0265349352524472, -0.1761250239606166 0.0320470366213371, -0.1913417161825448 0.0380602337443566, -0.2063535149021973 0.044568087539412, -0.2211443451095006 0.0515636292336558, -0.2356983684129987 0.0590393678258224, -0.2499999999999999 0.0669872981077806, -0.264033925325184 0.0753989092367106, -0.277785116509801 0.0842651938487273, -0.291238848433901 0.0935766577041924, -0.3043807145043602 0.1033233298543823, -0.3171966420818227 0.1134947733186314, -0.3296729075500343 0.1240800962605111, -0.3417961510114356 0.1350679636510821, -0.3535533905932737 0.1464466094067262, -0.3649320363489177 0.1582038489885642, -0.3759199037394886 0.1703270924499655, -0.3865052266813683 0.1828033579181771, -0.3966766701456175 0.1956192854956396, -0.4064233422958076 0.2087611515660989, -0.4157348061512725 0.2222148834901987, -0.4246010907632894 0.235966074674816, -0.4330127018922192 0.2499999999999998, -0.4409606321741775 0.264301631587001, -0.4484363707663441 0.2788556548904991, -0.4554319124605878 0.2936464850978025, -0.4619397662556434 0.3086582838174551, -0.4679529633786628 0.3238749760393831, -0.4734650647475528 0.3392802673484191, -0.4784701678661044 0.3548576613727686, -0.4829629131445341 0.3705904774487395, -0.4869384896386668 0.3864618684828132, -0.4903926402016152 0.4024548389919357, -0.4933216660424395 0.4185522633027056, -0.4957224306869052 0.434736903889974, -0.4975923633360984 0.4509914298352196, -0.4989294616193017 0.4672984353849282, -0.4997322937381828 0.4836404585891118, -0.5 0.4999999999999999, -0.4997322937381828 0.5163595414108879, -0.4989294616193017 0.5327015646150715, -0.4975923633360985 0.5490085701647801, -0.4957224306869052 0.5652630961100257, -0.4933216660424395 0.5814477366972941, -0.4903926402016153 0.597545161008064, -0.4869384896386668 0.6135381315171865, -0.4829629131445342 0.6294095225512601, -0.4784701678661045 0.645142338627231, -0.4734650647475529 0.6607197326515806, -0.4679529633786629 0.6761250239606166, -0.4619397662556435 0.6913417161825446, -0.455431912460588 0.7063535149021972, -0.4484363707663442 0.7211443451095005, -0.4409606321741776 0.7356983684129986, -0.4330127018922194 0.7499999999999999, -0.4246010907632896 0.7640339253251838, -0.4157348061512727 0.777785116509801, -0.4064233422958078 0.7912388484339008, -0.3966766701456177 0.8043807145043602, -0.3865052266813686 0.8171966420818226, -0.3759199037394889 0.8296729075500342, -0.3649320363489179 0.8417961510114356, -0.353553390593274 0.8535533905932735, -0.3417961510114358 0.8649320363489177, -0.3296729075500345 0.8759199037394887, -0.317196642081823 0.8865052266813683, -0.3043807145043604 0.8966766701456175, -0.2912388484339011 0.9064233422958076, -0.2777851165098015 0.9157348061512725, -0.2640339253251843 0.9246010907632893, -0.2500000000000002 0.9330127018922192, -0.235698368412999 0.9409606321741775, -0.2211443451095007 0.9484363707663441, -0.2063535149021977 0.9554319124605877, -0.1913417161825452 0.9619397662556433, -0.176125023960617 0.9679529633786628, -0.1607197326515809 0.9734650647475528, -0.1451423386272312 0.9784701678661044, -0.1294095225512608 0.9829629131445341, -0.1135381315171869 0.9869384896386668, -0.0975451610080643 0.9903926402016152, -0.0814477366972945 0.9933216660424395, -0.0652630961100262 0.9957224306869051, -0.0490085701647807 0.9975923633360984, -0.0327015646150718 0.9989294616193017, -0.0163595414108883 0.9997322937381828, -0.0000000000000001 1, 0.0163595414108876 0.9997322937381828, 0.0327015646150712 0.9989294616193019, 0.04900857016478 0.9975923633360985, 0.0652630961100256 0.9957224306869052, 0.0814477366972943 0.9933216660424395, 0.0975451610080637 0.9903926402016153, 0.1135381315171863 0.9869384896386669, 0.1294095225512601 0.9829629131445342, 0.145142338627231 0.9784701678661045, 0.1607197326515807 0.9734650647475529, 0.1761250239606164 0.967952963378663, 0.1913417161825446 0.9619397662556435, 0.2063535149021972 0.955431912460588, 0.2211443451095005 0.9484363707663442, 0.2356983684129984 0.9409606321741777, 0.2499999999999997 0.9330127018922195, 0.2640339253251837 0.9246010907632896, 0.2777851165098009 0.9157348061512727, 0.291238848433901 0.9064233422958077, 0.3043807145043599 0.8966766701456179, 0.3171966420818225 0.8865052266813687, 0.3296729075500342 0.8759199037394889, 0.3417961510114355 0.8649320363489179, 0.3535533905932737 0.8535533905932738, 0.3649320363489175 0.841796151011436, 0.3759199037394885 0.8296729075500346, 0.3865052266813683 0.817196642081823, 0.3966766701456175 0.8043807145043604, 0.4064233422958076 0.7912388484339011, 0.4157348061512723 0.7777851165098015, 0.4246010907632893 0.7640339253251842, 0.4330127018922192 0.7500000000000002, 0.4409606321741774 0.735698368412999, 0.4484363707663439 0.7211443451095011, 0.4554319124605877 0.7063535149021978, 0.4619397662556433 0.6913417161825453, 0.4679529633786628 0.676125023960617, 0.4734650647475528 0.6607197326515809, 0.4784701678661043 0.6451423386272317, 0.482962913144534 0.6294095225512608, 0.4869384896386668 0.613538131517187, 0.4903926402016152 0.5975451610080643, 0.4933216660424395 0.5814477366972945, 0.4957224306869051 0.5652630961100262, 0.4975923633360984 0.5490085701647807, 0.4989294616193017 0.5327015646150718, 0.4997322937381828 0.5163595414108882, 0.5 0.5))\n</code></pre>"},{"location":"api/sql/Function/#st_minimumboundingradius","title":"ST_MinimumBoundingRadius","text":"<p>Introduction: Returns a struct containing the center point and radius of the smallest circle that contains a geometry.</p> <p>Format: <code>ST_MinimumBoundingRadius(geom: Geometry)</code></p> <p>Since: <code>v1.0.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_MinimumBoundingRadius(ST_GeomFromText('POLYGON((1 1,0 0, -1 1, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>{POINT (0 1), 1.0}\n</code></pre>"},{"location":"api/sql/Function/#st_multi","title":"ST_Multi","text":"<p>Introduction: Returns a MultiGeometry object based on the geometry input. ST_Multi is basically an alias for ST_Collect with one geometry.</p> <p>Format: <code>ST_Multi(geom: Geometry)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Multi(ST_GeomFromText('POINT(1 1)'))\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT (1 1)\n</code></pre>"},{"location":"api/sql/Function/#st_ndims","title":"ST_NDims","text":"<p>Introduction: Returns the coordinate dimension of the geometry.</p> <p>Format: <code>ST_NDims(geom: Geometry)</code></p> <p>Since: <code>v1.3.1</code></p> <p>Spark SQL example with z coordinate:</p> <pre><code>SELECT ST_NDims(ST_GeomFromEWKT('POINT(1 1 2)'))\n</code></pre> <p>Output:</p> <pre><code>3\n</code></pre> <p>Spark SQL example with x,y coordinate:</p> <pre><code>SELECT ST_NDims(ST_GeomFromText('POINT(1 1)'))\n</code></pre> <p>Output:</p> <pre><code>2\n</code></pre>"},{"location":"api/sql/Function/#st_normalize","title":"ST_Normalize","text":"<p>Introduction: Returns the input geometry in its normalized form.</p> <p>Format:</p> <p><code>ST_Normalize(geom: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_AsEWKT(ST_Normalize(ST_GeomFromWKT('POLYGON((0 1, 1 1, 1 0, 0 0, 0 1))')))\n</code></pre> <p>Result:</p> <pre><code>POLYGON ((0 0, 0 1, 1 1, 1 0, 0 0))\n</code></pre>"},{"location":"api/sql/Function/#st_npoints","title":"ST_NPoints","text":"<p>Introduction: Return points of the geometry</p> <p>Format: <code>ST_NPoints (A: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_NPoints(ST_GeomFromText('LINESTRING(77.29 29.07,77.42 29.26,77.27 29.31,77.29 29.07)'))\n</code></pre> <p>Output:</p> <pre><code>4\n</code></pre>"},{"location":"api/sql/Function/#st_nrings","title":"ST_NRings","text":"<p>Introduction: Returns the number of rings in a Polygon or MultiPolygon. Contrary to ST_NumInteriorRings, this function also takes into account the number of  exterior rings.</p> <p>This function returns 0 for an empty Polygon or MultiPolygon. If the geometry is not a Polygon or MultiPolygon, an IllegalArgument Exception is thrown.</p> <p>Format: <code>ST_NRings(geom: Geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Examples:</p> <p>Input: <code>POLYGON ((1 0, 1 1, 2 1, 2 0, 1 0))</code></p> <p>Output: <code>1</code></p> <p>Input: <code>'MULTIPOLYGON (((1 0, 1 6, 6 6, 6 0, 1 0), (2 1, 2 2, 3 2, 3 1, 2 1)), ((10 0, 10 6, 16 6, 16 0, 10 0), (12 1, 12 2, 13 2, 13 1, 12 1)))'</code></p> <p>Output: <code>4</code></p> <p>Input: <code>'POLYGON EMPTY'</code></p> <p>Output: <code>0</code></p> <p>Input: <code>'LINESTRING (1 0, 1 1, 2 1)'</code></p> <p>Output: <code>Unsupported geometry type: LineString, only Polygon or MultiPolygon geometries are supported.</code></p>"},{"location":"api/sql/Function/#st_numgeometries","title":"ST_NumGeometries","text":"<p>Introduction: Returns the number of Geometries. If geometry is a GEOMETRYCOLLECTION (or MULTI*) return the number of geometries, for single geometries will return 1.</p> <p>Format: <code>ST_NumGeometries (A: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_NumGeometries(ST_GeomFromWKT('LINESTRING (-29 -27, -30 -29.7, -45 -33)'))\n</code></pre> <p>Output:</p> <pre><code>1\n</code></pre>"},{"location":"api/sql/Function/#st_numinteriorrings","title":"ST_NumInteriorRings","text":"<p>Introduction: RETURNS number of interior rings of polygon geometries.</p> <p>Format: <code>ST_NumInteriorRings(geom: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_NumInteriorRings(ST_GeomFromText('POLYGON ((0 0, 0 5, 5 5, 5 0, 0 0), (1 1, 2 1, 2 2, 1 2, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>1\n</code></pre>"},{"location":"api/sql/Function/#st_numpoints","title":"ST_NumPoints","text":"<p>Introduction: Returns number of points in a LineString</p> <p>Note</p> <p>If any other geometry is provided as an argument, an IllegalArgumentException is thrown. Example: <code>SELECT ST_NumPoints(ST_GeomFromWKT('MULTIPOINT ((0 0), (1 1), (0 1), (2 2))'))</code></p> <p>Output: <code>IllegalArgumentException: Unsupported geometry type: MultiPoint, only LineString geometry is supported.</code></p> <p>Format: <code>ST_NumPoints(geom: Geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_NumPoints(ST_GeomFromText('LINESTRING(0 1, 1 0, 2 0)'))\n</code></pre> <p>Output:</p> <pre><code>3\n</code></pre>"},{"location":"api/sql/Function/#st_pointn","title":"ST_PointN","text":"<p>Introduction: Return the Nth point in a single linestring or circular linestring in the geometry. Negative values are counted backwards from the end of the LineString, so that -1 is the last point. Returns NULL if there is no linestring in the geometry.</p> <p>Format: <code>ST_PointN(geom: Geometry, n: Integer)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Spark SQL Example: <pre><code>SELECT ST_PointN(ST_GeomFromText(\"LINESTRING(0 0, 1 2, 2 4, 3 6)\"), 2)\n</code></pre></p> <p>Result:</p> <pre><code>POINT (1 2)\n</code></pre>"},{"location":"api/sql/Function/#st_pointonsurface","title":"ST_PointOnSurface","text":"<p>Introduction: Returns a POINT guaranteed to lie on the surface.</p> <p>Format: <code>ST_PointOnSurface(A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Examples:</p> <pre><code>SELECT ST_AsText(ST_PointOnSurface(ST_GeomFromText('POINT(0 5)')));\n st_astext\n------------\n POINT(0 5)\n\nSELECT ST_AsText(ST_PointOnSurface(ST_GeomFromText('LINESTRING(0 5, 0 10)')));\n st_astext\n------------\n POINT(0 5)\n\nSELECT ST_AsText(ST_PointOnSurface(ST_GeomFromText('POLYGON((0 0, 0 5, 5 5, 5 0, 0 0))')));\n   st_astext\n----------------\n POINT(2.5 2.5)\n\nSELECT ST_AsText(ST_PointOnSurface(ST_GeomFromText('LINESTRING(0 5 1, 0 0 1, 0 10 2)')));\n   st_astext\n----------------\n POINT Z(0 0 1)\n</code></pre>"},{"location":"api/sql/Function/#st_polygon","title":"ST_Polygon","text":"<p>Introduction: Function to create a polygon built from the given LineString and sets the spatial reference system from the srid</p> <p>Format: <code>ST_Polygon(geom: Geometry, srid: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_AsText( ST_Polygon(ST_GeomFromEWKT('LINESTRING(75 29 1, 77 29 2, 77 29 3, 75 29 1)'), 4326) );\n</code></pre> <p>Output:</p> <pre><code>POLYGON((75 29 1, 77 29 2, 77 29 3, 75 29 1))\n</code></pre>"},{"location":"api/sql/Function/#st_reduceprecision","title":"ST_ReducePrecision","text":"<p>Introduction: Reduce the decimals places in the coordinates of the geometry to the given number of decimal places. The last decimal place will be rounded. This function was called ST_PrecisionReduce in versions prior to v1.5.0.</p> <p>Format: <code>ST_ReducePrecision (A: Geometry, B: Integer)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <p><pre><code>SELECT ST_ReducePrecision(ST_GeomFromWKT('Point(0.1234567890123456789 0.1234567890123456789)')\n, 9)\n</code></pre> The new coordinates will only have 9 decimal places.</p> <p>Output:</p> <pre><code>POINT (0.123456789 0.123456789)\n</code></pre>"},{"location":"api/sql/Function/#st_removepoint","title":"ST_RemovePoint","text":"<p>Introduction: RETURN Line with removed point at given index, position can be omitted and then last one will be removed.</p> <p>Format:</p> <p><code>ST_RemovePoint(geom: Geometry, position: Integer)</code></p> <p><code>ST_RemovePoint(geom: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_RemovePoint(ST_GeomFromText(\"LINESTRING(0 0, 1 1, 1 0)\"), 1)\n</code></pre> <p>Output:</p> <pre><code>LINESTRING(0 0, 1 0)\n</code></pre>"},{"location":"api/sql/Function/#st_reverse","title":"ST_Reverse","text":"<p>Introduction: Return the geometry with vertex order reversed</p> <p>Format: <code>ST_Reverse (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Reverse(ST_GeomFromWKT('LINESTRING(0 0, 1 2, 2 4, 3 6)'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (3 6, 2 4, 1 2, 0 0)\n</code></pre>"},{"location":"api/sql/Function/#st_s2cellids","title":"ST_S2CellIDs","text":"<p>Introduction: Cover the geometry with Google S2 Cells, return the corresponding cell IDs with the given level. The level indicates the size of cells. With a bigger level, the cells will be smaller, the coverage will be more accurate, but the result size will be exponentially increasing.</p> <p>Format: <code>ST_S2CellIDs(geom: Geometry, level: Integer)</code></p> <p>Since: <code>v1.4.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_S2CellIDs(ST_GeomFromText('LINESTRING(1 3 4, 5 6 7)'), 6)\n</code></pre> <p>Output: <pre><code>[1159395429071192064, 1159958379024613376, 1160521328978034688, 1161084278931456000, 1170091478186196992, 1170654428139618304]\n</code></pre></p>"},{"location":"api/sql/Function/#st_setpoint","title":"ST_SetPoint","text":"<p>Introduction: Replace Nth point of linestring with given point. Index is 0-based. Negative index are counted backwards, e.g., -1 is last point.</p> <p>Format: <code>ST_SetPoint (linestring: Geometry, index: Integer, point: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_SetPoint(ST_GeomFromText('LINESTRING (0 0, 0 1, 1 1)'), 2, ST_GeomFromText('POINT (1 0)'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (0 0, 0 1, 1 0)\n</code></pre>"},{"location":"api/sql/Function/#st_setsrid","title":"ST_SetSRID","text":"<p>Introduction: Sets the spatial reference system identifier (SRID) of the geometry.</p> <p>Format: <code>ST_SetSRID (A: Geometry, srid: Integer)</code></p> <p>Since: <code>v1.1.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_AsEWKT(ST_SetSRID(ST_GeomFromWKT('POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))'), 3021))\n</code></pre> <p>Output:</p> <pre><code>SRID=3021;POLYGON ((1 1, 8 1, 8 8, 1 8, 1 1))\n</code></pre>"},{"location":"api/sql/Function/#st_simplifypreservetopology","title":"ST_SimplifyPreserveTopology","text":"<p>Introduction: Simplifies a geometry and ensures that the result is a valid geometry having the same dimension and number of components as the input, and with the components having the same topological relationship.</p> <p>Since: <code>v1.0.0</code></p> <p>Format: <code>ST_SimplifyPreserveTopology (A: Geometry, distanceTolerance: Double)</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_SimplifyPreserveTopology(ST_GeomFromText('POLYGON((8 25, 28 22, 28 20, 15 11, 33 3, 56 30, 46 33,46 34, 47 44, 35 36, 45 33, 43 19, 29 21, 29 22,35 26, 24 39, 8 25))'), 10)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((8 25, 28 22, 15 11, 33 3, 56 30, 47 44, 35 36, 43 19, 24 39, 8 25))\n</code></pre>"},{"location":"api/sql/Function/#st_split","title":"ST_Split","text":"<p>Introduction: Split an input geometry by another geometry (called the blade). Linear (LineString or MultiLineString) geometry can be split by a Point, MultiPoint, LineString, MultiLineString, Polygon, or MultiPolygon. Polygonal (Polygon or MultiPolygon) geometry can be split by a LineString, MultiLineString, Polygon, or MultiPolygon. In either case, when a polygonal blade is used then the boundary of the blade is what is actually split by. ST_Split will always return either a MultiLineString or MultiPolygon even if they only contain a single geometry. Homogeneous GeometryCollections are treated as a multi-geometry of the type it contains. For example, if a GeometryCollection of only Point geometries is passed as a blade it is the same as passing a MultiPoint of the same geometries.</p> <p>Since: <code>v1.4.0</code></p> <p>Format: <code>ST_Split (input: Geometry, blade: Geometry)</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Split(\nST_GeomFromWKT('LINESTRING (0 0, 1.5 1.5, 2 2)'),\nST_GeomFromWKT('MULTIPOINT (0.5 0.5, 1 1)'))\n</code></pre> <p>Output:</p> <pre><code>MULTILINESTRING ((0 0, 0.5 0.5), (0.5 0.5, 1 1), (1 1, 1.5 1.5, 2 2))\n</code></pre>"},{"location":"api/sql/Function/#st_srid","title":"ST_SRID","text":"<p>Introduction: Return the spatial reference system identifier (SRID) of the geometry.</p> <p>Format: <code>ST_SRID (A: Geometry)</code></p> <p>Since: <code>v1.1.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_SRID(ST_SetSRID(ST_GeomFromWKT('POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))'), 3021))\n</code></pre> <p>Output:</p> <pre><code>3021\n</code></pre>"},{"location":"api/sql/Function/#st_startpoint","title":"ST_StartPoint","text":"<p>Introduction: Returns first point of given linestring.</p> <p>Format: <code>ST_StartPoint(geom: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_StartPoint(ST_GeomFromText('LINESTRING(100 150,50 60, 70 80, 160 170)'))\n</code></pre> <p>Output:</p> <pre><code>POINT(100 150)\n</code></pre>"},{"location":"api/sql/Function/#st_subdivide","title":"ST_SubDivide","text":"<p>Introduction: Returns list of geometries divided based of given maximum number of vertices.</p> <p>Format: <code>ST_SubDivide(geom: Geometry, maxVertices: Integer)</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL Example: <pre><code>SELECT ST_SubDivide(ST_GeomFromText(\"POLYGON((35 10, 45 45, 15 40, 10 20, 35 10), (20 30, 35 35, 30 20, 20 30))\"), 5)\n</code></pre></p> <p>Output: <pre><code>[\n    POLYGON((37.857142857142854 20, 35 10, 10 20, 37.857142857142854 20)),\n    POLYGON((15 20, 10 20, 15 40, 15 20)),\n    POLYGON((20 20, 15 20, 15 30, 20 30, 20 20)),\n    POLYGON((26.428571428571427 20, 20 20, 20 30, 26.4285714 23.5714285, 26.4285714 20)),\n    POLYGON((15 30, 15 40, 20 40, 20 30, 15 30)),\n    POLYGON((20 40, 26.4285714 40, 26.4285714 32.1428571, 20 30, 20 40)),\n    POLYGON((37.8571428 20, 30 20, 34.0476190 32.1428571, 37.8571428 32.1428571, 37.8571428 20)),\n    POLYGON((34.0476190 34.6825396, 26.4285714 32.1428571, 26.4285714 40, 34.0476190 40, 34.0476190 34.6825396)),\n    POLYGON((34.0476190 32.1428571, 35 35, 37.8571428 35, 37.8571428 32.1428571, 34.0476190 32.1428571)),\n    POLYGON((35 35, 34.0476190 34.6825396, 34.0476190 35, 35 35)),\n    POLYGON((34.0476190 35, 34.0476190 40, 37.8571428 40, 37.8571428 35, 34.0476190 35)),\n    POLYGON((30 20, 26.4285714 20, 26.4285714 23.5714285, 30 20)),\n    POLYGON((15 40, 37.8571428 43.8095238, 37.8571428 40, 15 40)),\n    POLYGON((45 45, 37.8571428 20, 37.8571428 43.8095238, 45 45))\n]\n</code></pre></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_SubDivide(ST_GeomFromText(\"LINESTRING(0 0, 85 85, 100 100, 120 120, 21 21, 10 10, 5 5)\"), 5)\n</code></pre> <p>Output: <pre><code>[\n    LINESTRING(0 0, 5 5)\n    LINESTRING(5 5, 10 10)\n    LINESTRING(10 10, 21 21)\n    LINESTRING(21 21, 60 60)\n    LINESTRING(60 60, 85 85)\n    LINESTRING(85 85, 100 100)\n    LINESTRING(100 100, 120 120)\n]\n</code></pre></p>"},{"location":"api/sql/Function/#st_subdivideexplode","title":"ST_SubDivideExplode","text":"<p>Introduction: It works the same as ST_SubDivide but returns new rows with geometries instead of list.</p> <p>Format: <code>ST_SubDivideExplode(geom: Geometry, maxVertices: Integer)</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL Example:</p> <p>Query: <pre><code>SELECT ST_SubDivideExplode(ST_GeomFromText(\"LINESTRING(0 0, 85 85, 100 100, 120 120, 21 21, 10 10, 5 5)\"), 5)\n</code></pre></p> <p>Result:</p> <pre><code>+-----------------------------+\n|geom                         |\n+-----------------------------+\n|LINESTRING(0 0, 5 5)         |\n|LINESTRING(5 5, 10 10)       |\n|LINESTRING(10 10, 21 21)     |\n|LINESTRING(21 21, 60 60)     |\n|LINESTRING(60 60, 85 85)     |\n|LINESTRING(85 85, 100 100)   |\n|LINESTRING(100 100, 120 120) |\n+-----------------------------+\n</code></pre> <p>Using Lateral View</p> <p>Table: <pre><code>+-------------------------------------------------------------+\n|geometry                                                     |\n+-------------------------------------------------------------+\n|LINESTRING(0 0, 85 85, 100 100, 120 120, 21 21, 10 10, 5 5)  |\n+-------------------------------------------------------------+\n</code></pre></p> <p>Query <pre><code>select geom from geometries LATERAL VIEW ST_SubdivideExplode(geometry, 5) AS geom\n</code></pre></p> <p>Result: <pre><code>+-----------------------------+\n|geom                         |\n+-----------------------------+\n|LINESTRING(0 0, 5 5)         |\n|LINESTRING(5 5, 10 10)       |\n|LINESTRING(10 10, 21 21)     |\n|LINESTRING(21 21, 60 60)     |\n|LINESTRING(60 60, 85 85)     |\n|LINESTRING(85 85, 100 100)   |\n|LINESTRING(100 100, 120 120) |\n+-----------------------------+\n</code></pre></p>"},{"location":"api/sql/Function/#st_symdifference","title":"ST_SymDifference","text":"<p>Introduction: Return the symmetrical difference between geometry A and B (return parts of geometries which are in either of the sets, but not in their intersection)</p> <p>Format: <code>ST_SymDifference (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_SymDifference(ST_GeomFromWKT('POLYGON ((-3 -3, 3 -3, 3 3, -3 3, -3 -3))'), ST_GeomFromWKT('POLYGON ((-2 -3, 4 -3, 4 3, -2 3, -2 -3))'))\n</code></pre> <p>Output:</p> <pre><code>MULTIPOLYGON (((-2 -3, -3 -3, -3 3, -2 3, -2 -3)), ((3 -3, 3 3, 4 3, 4 -3, 3 -3)))\n</code></pre>"},{"location":"api/sql/Function/#st_transform","title":"ST_Transform","text":"<p>Introduction:</p> <p>Transform the Spatial Reference System / Coordinate Reference System of A, from SourceCRS to TargetCRS. For SourceCRS and TargetCRS, WKT format is also available since <code>v1.3.1</code>. Since <code>v1.5.1</code>, if the <code>SourceCRS</code> is not specified, CRS will be fetched from the geometry using ST_SRID.</p> <p>Lon/Lat Order in the input geometry</p> <p>If the input geometry is in lat/lon order, it might throw an error such as <code>too close to pole</code>, <code>latitude or longitude exceeded limits</code>, or give unexpected results. You need to make sure that the input geometry is in lon/lat order. If the input geometry is in lat/lon order, you can use ST_FlipCoordinates to swap X and Y.</p> <p>Lon/Lat Order in the source and target CRS</p> <p>Sedona will make sure the source and target CRS to be in lon/lat order. If the source CRS or target CRS is in lat/lon order, these CRS will be swapped to lon/lat order.</p> <p>CRS code</p> <p>The CRS code is the code of the CRS in the official EPSG database (https://epsg.org/) in the format of <code>EPSG:XXXX</code>. A community tool EPSG.io can help you quick identify a CRS code. For example, the code of WGS84 is <code>EPSG:4326</code>.</p> <p>WKT format</p> <p>You can also use OGC WKT v1 format to specify the source CRS and target CRS. An example OGC WKT v1 CRS of <code>EPGS:3857</code> is as follows:</p> <pre><code>PROJCS[\"WGS 84 / Pseudo-Mercator\",\n    GEOGCS[\"WGS 84\",\n        DATUM[\"WGS_1984\",\n            SPHEROID[\"WGS 84\",6378137,298.257223563,\n                AUTHORITY[\"EPSG\",\"7030\"]],\n            AUTHORITY[\"EPSG\",\"6326\"]],\n        PRIMEM[\"Greenwich\",0,\n            AUTHORITY[\"EPSG\",\"8901\"]],\n        UNIT[\"degree\",0.0174532925199433,\n            AUTHORITY[\"EPSG\",\"9122\"]],\n        AUTHORITY[\"EPSG\",\"4326\"]],\n    PROJECTION[\"Mercator_1SP\"],\n    PARAMETER[\"central_meridian\",0],\n    PARAMETER[\"scale_factor\",1],\n    PARAMETER[\"false_easting\",0],\n    PARAMETER[\"false_northing\",0],\n    UNIT[\"metre\",1,\n        AUTHORITY[\"EPSG\",\"9001\"]],\n    AXIS[\"Easting\",EAST],\n    AXIS[\"Northing\",NORTH],\n    EXTENSION[\"PROJ4\",\"+proj=merc +a=6378137 +b=6378137 +lat_ts=0 +lon_0=0 +x_0=0 +y_0=0 +k=1 +units=m +nadgrids=@null +wktext +no_defs\"],\n    AUTHORITY[\"EPSG\",\"3857\"]]\n</code></pre> <p>Note</p> <p>By default, this function uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order.</p> <p>Note</p> <p>By default, ST_Transform follows the <code>lenient</code> mode which tries to fix issues by itself. You can append a boolean value at the end to enable the <code>strict</code> mode. In <code>strict</code> mode, ST_Transform will throw an error if it finds any issue.</p> <p>Format:</p> <pre><code>ST_Transform (A: Geometry, SourceCRS: String, TargetCRS: String, lenientMode: Boolean)\n</code></pre> <pre><code>ST_Transform (A: Geometry, SourceCRS: String, TargetCRS: String)\n</code></pre> <pre><code>ST_Transform (A: Geometry, TargetCRS: String)\n</code></pre> <p>Since: <code>v1.2.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_AsText(ST_Transform(ST_GeomFromText('POLYGON((170 50,170 72,-130 72,-130 50,170 50))'),'EPSG:4326', 'EPSG:32649'))\n</code></pre> <pre><code>SELECT ST_AsText(ST_Transform(ST_GeomFromText('POLYGON((170 50,170 72,-130 72,-130 50,170 50))'),'EPSG:4326', 'EPSG:32649', false))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((8766047.980342899 17809098.336766362, 5122546.516721856 18580261.912528664, 3240775.0740796793 -13688660.50985159, 4556241.924514083 -12463044.21488129, 8766047.980342899 17809098.336766362))\n</code></pre>"},{"location":"api/sql/Function/#st_translate","title":"ST_Translate","text":"<p>Introduction: Returns the input geometry with its X, Y and Z coordinates (if present in the geometry) translated by deltaX, deltaY and deltaZ (if specified)</p> <p>If the geometry is 2D, and a deltaZ parameter is specified, no change is done to the Z coordinate of the geometry and the resultant geometry is also 2D.</p> <p>If the geometry is empty, no change is done to it. If the given geometry contains sub-geometries (GEOMETRY COLLECTION, MULTI POLYGON/LINE/POINT), all underlying geometries are individually translated.</p> <p>Format:</p> <p><code>ST_Translate(geometry: Geometry, deltaX: Double, deltaY: Double, deltaZ: Double)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Translate(ST_GeomFromText('GEOMETRYCOLLECTION(MULTIPOLYGON(((3 2,3 3,4 3,4 2,3 2)),((3 4,5 6,5 7,3 4))), POINT(1 1 1), LINESTRING EMPTY)'), 2, 2, 3)\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (MULTIPOLYGON (((5 4, 5 5, 6 5, 6 4, 5 4)), ((5 6, 7 8, 7 9, 5 6))), POINT (3 3), LINESTRING EMPTY)\n</code></pre> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Translate(ST_GeomFromText('POINT(-71.01 42.37)'),1,2)\n</code></pre> <p>Output:</p> <pre><code>POINT (-70.01 44.37)\n</code></pre>"},{"location":"api/sql/Function/#st_union","title":"ST_Union","text":"<p>Introduction: Return the union of geometry A and B</p> <p>Format: <code>ST_Union (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Union(ST_GeomFromWKT('POLYGON ((-3 -3, 3 -3, 3 3, -3 3, -3 -3))'), ST_GeomFromWKT('POLYGON ((1 -2, 5 0, 1 2, 1 -2))'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((3 -1, 3 -3, -3 -3, -3 3, 3 3, 3 1, 5 0, 3 -1))\n</code></pre>"},{"location":"api/sql/Function/#st_voronoipolygons","title":"ST_VoronoiPolygons","text":"<p>Introduction: Returns a two-dimensional Voronoi diagram from the vertices of the supplied geometry. The result is a GeometryCollection of Polygons that covers an envelope larger than the extent of the input vertices. Returns null if input geometry is null. Returns an empty geometry collection if the input geometry contains only one vertex. Returns an empty geometry collection if the extend_to envelope has zero area.</p> <p>Format: <code>ST_VoronoiPolygons(g1: Geometry, tolerance: Double, extend_to: Geometry)</code></p> <p>Optional parameters:</p> <p><code>tolerance</code> : The distance within which vertices will be considered equivalent. Robustness of the algorithm can be improved by supplying a nonzero tolerance distance. (default = 0.0)</p> <p><code>extend_to</code> : If a geometry is supplied as the \"extend_to\" parameter, the diagram will be extended to cover the envelope of the \"extend_to\" geometry, unless that envelope is smaller than the default envelope (default = NULL. By default, we extend the bounding box of the diagram by the max between bounding box's height and bounding box's width).</p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT st_astext(ST_VoronoiPolygons(ST_GeomFromText('MULTIPOINT ((0 0), (1 1))')));\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION(POLYGON((-1 2,2 -1,-1 -1,-1 2)),POLYGON((-1 2,2 2,2 -1,-1 2)))\n</code></pre>"},{"location":"api/sql/Function/#st_x","title":"ST_X","text":"<p>Introduction: Returns X Coordinate of given Point null otherwise.</p> <p>Format: <code>ST_X(pointA: Point)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_X(ST_POINT(0.0 25.0))\n</code></pre> <p>Output:</p> <pre><code>0.0\n</code></pre>"},{"location":"api/sql/Function/#st_xmax","title":"ST_XMax","text":"<p>Introduction: Returns the maximum X coordinate of a geometry</p> <p>Format: <code>ST_XMax (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_XMax(ST_GeomFromText('POLYGON ((-1 -11, 0 10, 1 11, 2 12, -1 -11))'))\n</code></pre> <p>Output:</p> <pre><code>2\n</code></pre>"},{"location":"api/sql/Function/#st_xmin","title":"ST_XMin","text":"<p>Introduction: Returns the minimum X coordinate of a geometry</p> <p>Format: <code>ST_XMin (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_XMin(ST_GeomFromText('POLYGON ((-1 -11, 0 10, 1 11, 2 12, -1 -11))'))\n</code></pre> <p>Output:</p> <pre><code>-1\n</code></pre>"},{"location":"api/sql/Function/#st_y","title":"ST_Y","text":"<p>Introduction: Returns Y Coordinate of given Point, null otherwise.</p> <p>Format: <code>ST_Y(pointA: Point)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Y(ST_POINT(0.0 25.0))\n</code></pre> <p>Output:</p> <pre><code>25.0\n</code></pre>"},{"location":"api/sql/Function/#st_ymax","title":"ST_YMax","text":"<p>Introduction: Return the minimum Y coordinate of A</p> <p>Format: <code>ST_YMax (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Spark SQL Example: <pre><code>SELECT ST_YMax(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre></p> <p>Output:</p> <pre><code>2\n</code></pre>"},{"location":"api/sql/Function/#st_ymin","title":"ST_YMin","text":"<p>Introduction: Return the minimum Y coordinate of A</p> <p>Format: <code>ST_Y_Min (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_YMin(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre> <p>Output:</p> <pre><code>0\n</code></pre>"},{"location":"api/sql/Function/#st_z","title":"ST_Z","text":"<p>Introduction: Returns Z Coordinate of given Point, null otherwise.</p> <p>Format: <code>ST_Z(pointA: Point)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Z(ST_POINT(0.0 25.0 11.0))\n</code></pre> <p>Output:</p> <pre><code>11.0\n</code></pre>"},{"location":"api/sql/Function/#st_zmax","title":"ST_ZMax","text":"<p>Introduction: Returns Z maxima of the given geometry or null if there is no Z coordinate.</p> <p>Format: <code>ST_ZMax(geom: Geometry)</code></p> <p>Since: <code>v1.3.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_ZMax(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre> <p>Output:</p> <pre><code>1.0\n</code></pre>"},{"location":"api/sql/Function/#st_zmin","title":"ST_ZMin","text":"<p>Introduction: Returns Z minima of the given geometry or null if there is no Z coordinate.</p> <p>Format: <code>ST_ZMin(geom: Geometry)</code></p> <p>Since: <code>v1.3.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_ZMin(ST_GeomFromText('LINESTRING(1 3 4, 5 6 7)'))\n</code></pre> <p>Output:</p> <pre><code>4.0\n</code></pre>"},{"location":"api/sql/Optimizer/","title":"Query optimization","text":"<p>Sedona Spatial operators fully supports Apache SparkSQL query optimizer. It has the following query optimization features:</p> <ul> <li>Automatically optimizes range join query and distance join query.</li> <li>Automatically performs predicate pushdown.</li> </ul> <p>Tip</p> <p>Sedona join performance is heavily affected by the number of partitions. If the join performance is not ideal, please increase the number of partitions by doing <code>df.repartition(XXX)</code> right after you create the original DataFrame.</p>"},{"location":"api/sql/Optimizer/#range-join","title":"Range join","text":"<p>Introduction: Find geometries from A and geometries from B such that each geometry pair satisfies a certain predicate. Most predicates supported by SedonaSQL can trigger a range join.</p> <p>Spark SQL Example:</p> <pre><code>SELECT *\nFROM polygondf, pointdf\nWHERE ST_Contains(polygondf.polygonshape,pointdf.pointshape)\n</code></pre> <pre><code>SELECT *\nFROM polygondf, pointdf\nWHERE ST_Intersects(polygondf.polygonshape,pointdf.pointshape)\n</code></pre> <pre><code>SELECT *\nFROM pointdf, polygondf\nWHERE ST_Within(pointdf.pointshape, polygondf.polygonshape)\n</code></pre> <pre><code>SELECT *\nFROM pointdf, polygondf\nWHERE ST_DWithin(pointdf.pointshape, polygondf.polygonshape, 10.0)\n</code></pre> <p>Spark SQL Physical plan:</p> <pre><code>== Physical Plan ==\nRangeJoin polygonshape#20: geometry, pointshape#43: geometry, false\n:- Project [st_polygonfromenvelope(cast(_c0#0 as decimal(24,20)), cast(_c1#1 as decimal(24,20)), cast(_c2#2 as decimal(24,20)), cast(_c3#3 as decimal(24,20)), mypolygonid) AS polygonshape#20]\n:  +- *FileScan csv\n+- Project [st_point(cast(_c0#31 as decimal(24,20)), cast(_c1#32 as decimal(24,20)), myPointId) AS pointshape#43]\n   +- *FileScan csv\n</code></pre> <p>Note</p> <p>All join queries in SedonaSQL are inner joins</p>"},{"location":"api/sql/Optimizer/#distance-join","title":"Distance join","text":"<p>Introduction: Find geometries from A and geometries from B such that the distance of each geometry pair is less or equal than a certain distance. It supports the planar Euclidean distance calculators <code>ST_Distance</code>, <code>ST_HausdorffDistance</code>, <code>ST_FrechetDistance</code> and the meter-based geodesic distance calculators <code>ST_DistanceSpheroid</code> and <code>ST_DistanceSphere</code>.</p> <p>Spark SQL Example for planar Euclidean distance:</p> <p>Only consider fully within a certain distance <pre><code>SELECT *\nFROM pointdf1, pointdf2\nWHERE ST_Distance(pointdf1.pointshape1,pointdf2.pointshape2) &lt; 2\n</code></pre></p> <pre><code>SELECT *\nFROM pointDf, polygonDF\nWHERE ST_HausdorffDistance(pointDf.pointshape, polygonDf.polygonshape, 0.3) &lt; 2\n</code></pre> <pre><code>SELECT *\nFROM pointDf, polygonDF\nWHERE ST_FrechetDistance(pointDf.pointshape, polygonDf.polygonshape) &lt; 2\n</code></pre> <p>Consider intersects within a certain distance <pre><code>SELECT *\nFROM pointdf1, pointdf2\nWHERE ST_Distance(pointdf1.pointshape1,pointdf2.pointshape2) &lt;= 2\n</code></pre></p> <pre><code>SELECT *\nFROM pointDf, polygonDF\nWHERE ST_HausdorffDistance(pointDf.pointshape, polygonDf.polygonshape) &lt;= 2\n</code></pre> <pre><code>SELECT *\nFROM pointDf, polygonDF\nWHERE ST_FrechetDistance(pointDf.pointshape, polygonDf.polygonshape) &lt;= 2\n</code></pre> <p>Spark SQL Physical plan: <pre><code>== Physical Plan ==\nDistanceJoin pointshape1#12: geometry, pointshape2#33: geometry, 2.0, true\n:- Project [st_point(cast(_c0#0 as decimal(24,20)), cast(_c1#1 as decimal(24,20)), myPointId) AS pointshape1#12]\n:  +- *FileScan csv\n+- Project [st_point(cast(_c0#21 as decimal(24,20)), cast(_c1#22 as decimal(24,20)), myPointId) AS pointshape2#33]\n   +- *FileScan csv\n</code></pre></p> <p>Warning</p> <p>If you use planar euclidean distance functions like <code>ST_Distance</code>, <code>ST_HausdorffDistance</code> or <code>ST_FrechetDistance</code> as the predicate, Sedona doesn't control the distance's unit (degree or meter). It is same with the geometry. If your coordinates are in the longitude and latitude system, the unit of <code>distance</code> should be degree instead of meter or mile. To change the geometry's unit, please either transform the coordinate reference system to a meter-based system. See ST_Transform. If you don't want to transform your data, please consider using <code>ST_DistanceSpheroid</code> or <code>ST_DistanceSphere</code>.</p> <p>Spark SQL Example for meter-based geodesic distance <code>ST_DistanceSpheroid</code> (works for <code>ST_DistanceSphere</code> too):</p> <p>Less than a certain distance== <pre><code>SELECT *\nFROM pointdf1, pointdf2\nWHERE ST_DistanceSpheroid(pointdf1.pointshape1,pointdf2.pointshape2) &lt; 2\n</code></pre></p> <p>Less than or equal to a certain distance== <pre><code>SELECT *\nFROM pointdf1, pointdf2\nWHERE ST_DistanceSpheroid(pointdf1.pointshape1,pointdf2.pointshape2) &lt;= 2\n</code></pre></p> <p>Warning</p> <p>If you use <code>ST_DistanceSpheroid</code> or <code>ST_DistanceSphere</code> as the predicate, the unit of the distance is meter. Currently, distance join with geodesic distance calculators work best for point data. For non-point data, it only considers their centroids.</p>"},{"location":"api/sql/Optimizer/#broadcast-index-join","title":"Broadcast index join","text":"<p>Introduction: Perform a range join or distance join but broadcast one of the sides of the join. This maintains the partitioning of the non-broadcast side and doesn't require a shuffle.</p> <p>Sedona will create a spatial index on the broadcasted table.</p> <p>Sedona uses broadcast join only if the correct side has a broadcast hint. The supported join type - broadcast side combinations are:</p> <ul> <li>Inner - either side, preferring to broadcast left if both sides have the hint</li> <li>Left semi - broadcast right</li> <li>Left anti - broadcast right</li> <li>Left outer - broadcast right</li> <li>Right outer - broadcast left</li> </ul> <pre><code>pointDf.alias(\"pointDf\").join(broadcast(polygonDf).alias(\"polygonDf\"), expr(\"ST_Contains(polygonDf.polygonshape, pointDf.pointshape)\"))\n</code></pre> <p>Spark SQL Physical plan: <pre><code>== Physical Plan ==\nBroadcastIndexJoin pointshape#52: geometry, BuildRight, BuildRight, false ST_Contains(polygonshape#30, pointshape#52)\n:- Project [st_point(cast(_c0#48 as decimal(24,20)), cast(_c1#49 as decimal(24,20))) AS pointshape#52]\n:  +- FileScan csv\n+- SpatialIndex polygonshape#30: geometry, QUADTREE, [id=#62]\n   +- Project [st_polygonfromenvelope(cast(_c0#22 as decimal(24,20)), cast(_c1#23 as decimal(24,20)), cast(_c2#24 as decimal(24,20)), cast(_c3#25 as decimal(24,20))) AS polygonshape#30]\n      +- FileScan csv\n</code></pre></p> <p>This also works for distance joins with <code>ST_Distance</code>, <code>ST_DistanceSpheroid</code>, <code>ST_DistanceSphere</code>, <code>ST_HausdorffDistance</code> or <code>ST_FrechetDistance</code>:</p> <pre><code>pointDf1.alias(\"pointDf1\").join(broadcast(pointDf2).alias(\"pointDf2\"), expr(\"ST_Distance(pointDf1.pointshape, pointDf2.pointshape) &lt;= 2\"))\n</code></pre> <p>Spark SQL Physical plan: <pre><code>== Physical Plan ==\nBroadcastIndexJoin pointshape#52: geometry, BuildRight, BuildLeft, true, 2.0 ST_Distance(pointshape#52, pointshape#415) &lt;= 2.0\n:- Project [st_point(cast(_c0#48 as decimal(24,20)), cast(_c1#49 as decimal(24,20))) AS pointshape#52]\n:  +- FileScan csv\n+- SpatialIndex pointshape#415: geometry, QUADTREE, [id=#1068]\n   +- Project [st_point(cast(_c0#48 as decimal(24,20)), cast(_c1#49 as decimal(24,20))) AS pointshape#415]\n      +- FileScan csv\n</code></pre></p> <p>Note: If the distance is an expression, it is only evaluated on the first argument to ST_Distance (<code>pointDf1</code> above).</p>"},{"location":"api/sql/Optimizer/#automatic-broadcast-index-join","title":"Automatic broadcast index join","text":"<p>When one table involved a spatial join query is smaller than a threshold, Sedona will automatically choose broadcast index join instead of Sedona optimized join. The current threshold is controlled by sedona.join.autoBroadcastJoinThreshold and set to the same as <code>spark.sql.autoBroadcastJoinThreshold</code>.</p>"},{"location":"api/sql/Optimizer/#raster-join","title":"Raster join","text":"<p>The optimization for spatial join also works for raster predicates, such as <code>RS_Intersects</code>, <code>RS_Contains</code> and <code>RS_Within</code>.</p> <p>SQL Example:</p> <pre><code>-- Raster-geometry join\nSELECT df1.id, df2.id, RS_Value(df1.rast, df2.geom) FROM df1 JOIN df2 ON RS_Intersects(df1.rast, df2.geom)\n\n-- Raster-raster join\nSELECT df1.id, df2.id FROM df1 JOIN df2 ON RS_Intersects(df1.rast, df2.rast)\n</code></pre> <p>These queries could be planned as RangeJoin or BroadcastIndexJoin. Here is an example of the physical plan using RangeJoin:</p> <pre><code>== Physical Plan ==\n*(1) Project [id#14, id#25]\n+- RangeJoin rast#13: raster, geom#24: geometry, INTERSECTS,  **org.apache.spark.sql.sedona_sql.expressions.RS_Intersects**\n   :- LocalTableScan [rast#13, id#14]\n   +- LocalTableScan [geom#24, id#25]\n</code></pre>"},{"location":"api/sql/Optimizer/#google-s2-based-approximate-equi-join","title":"Google S2 based approximate equi-join","text":"<p>If the performance of Sedona optimized join is not ideal, which is possibly caused by  complicated and overlapping geometries, you can resort to Sedona built-in Google S2-based approximate equi-join. This equi-join leverages Spark's internal equi-join algorithm and might be performant given that you can opt to skip the refinement step  by sacrificing query accuracy.</p> <p>Please use the following steps:</p>"},{"location":"api/sql/Optimizer/#1-generate-s2-ids-for-both-tables","title":"1. Generate S2 ids for both tables","text":"<p>Use ST_S2CellIds to generate cell IDs. Each geometry may produce one or more IDs.</p> <pre><code>SELECT id, geom, name, explode(ST_S2CellIDs(geom, 15)) as cellId\nFROM lefts\n</code></pre> <pre><code>SELECT id, geom, name, explode(ST_S2CellIDs(geom, 15)) as cellId\nFROM rights\n</code></pre>"},{"location":"api/sql/Optimizer/#2-perform-equi-join","title":"2. Perform equi-join","text":"<p>Join the two tables by their S2 cellId</p> <pre><code>SELECT lcs.id as lcs_id, lcs.geom as lcs_geom, lcs.name as lcs_name, rcs.id as rcs_id, rcs.geom as rcs_geom, rcs.name as rcs_name\nFROM lcs JOIN rcs ON lcs.cellId = rcs.cellId\n</code></pre>"},{"location":"api/sql/Optimizer/#3-optional-refine-the-result","title":"3. Optional: Refine the result","text":"<p>Due to the nature of S2 Cellid, the equi-join results might have a few false-positives depending on the S2 level you choose. A smaller level indicates bigger cells, less exploded rows, but more false positives.</p> <p>To ensure the correctness, you can use one of the Spatial Predicates to filter out them. Use this query instead of the query in Step 2.</p> <pre><code>SELECT lcs.id as lcs_id, lcs.geom as lcs_geom, lcs.name as lcs_name, rcs.id as rcs_id, rcs.geom as rcs_geom, rcs.name as rcs_name\nFROM lcs, rcs\nWHERE lcs.cellId = rcs.cellId AND ST_Contains(lcs.geom, rcs.geom)\n</code></pre> <p>As you see, compared to the query in Step 2, we added one more filter, which is <code>ST_Contains</code>, to remove false positives. You can also use <code>ST_Intersects</code> and so on.</p> <p>Tip</p> <p>You can skip this step if you don't need 100% accuracy and want faster query speed.</p>"},{"location":"api/sql/Optimizer/#4-optional-de-duplicate","title":"4. Optional: De-duplicate","text":"<p>Due to the explode function used when we generate S2 Cell Ids, the resulting DataFrame may have several duplicate  matches. You can remove them by performing a GroupBy query. <pre><code>SELECT lcs_id, rcs_id, first(lcs_geom), first(lcs_name), first(rcs_geom), first(rcs_name)\nFROM joinresult\nGROUP BY (lcs_id, rcs_id)\n</code></pre> <p>The <code>first</code> function is to take the first value from a number of duplicate values.</p> <p>If you don't have a unique id for each geometry, you can also group by geometry itself. See below:</p> <pre><code>SELECT lcs_geom, rcs_geom, first(lcs_name), first(rcs_name)\nFROM joinresult\nGROUP BY (lcs_geom, rcs_geom)\n</code></pre> <p>Note</p> <p>If you are doing point-in-polygon join, this is not a problem and you can safely discard this issue. This issue only happens when you do polygon-polygon, polygon-linestring, linestring-linestring join.</p>"},{"location":"api/sql/Optimizer/#s2-for-distance-join","title":"S2 for distance join","text":"<p>This also works for distance join. You first need to use <code>ST_Buffer(geometry, distance)</code> to wrap one of your original geometry column. If your original geometry column contains points, this <code>ST_Buffer</code> will make them become circles with a radius of <code>distance</code>.</p> <p>Since the coordinates are in the longitude and latitude system, so the unit of <code>distance</code> should be degree instead of meter or mile. You can get an approximation by performing <code>METER_DISTANCE/111000.0</code>, then filter out false-positives.  Note that this might lead to inaccurate results if your data is close to the poles or antimeridian.</p> <p>In a nutshell, run this query first on the left table before Step 1. Please replace <code>METER_DISTANCE</code> with a meter distance. In Step 1, generate S2 IDs based on the <code>buffered_geom</code> column. Then run Step 2, 3, 4 on the original <code>geom</code> column.</p> <pre><code>SELECT id, geom, ST_Buffer(geom, METER_DISTANCE/111000.0) as buffered_geom, name\nFROM lefts\n</code></pre>"},{"location":"api/sql/Optimizer/#regular-spatial-predicate-pushdown","title":"Regular spatial predicate pushdown","text":"<p>Introduction: Given a join query and a predicate in the same WHERE clause, first executes the Predicate as a filter, then executes the join query.</p> <p>Spark SQL Example:</p> <pre><code>SELECT *\nFROM polygondf, pointdf\nWHERE ST_Contains(polygondf.polygonshape,pointdf.pointshape)\nAND ST_Contains(ST_PolygonFromEnvelope(1.0,101.0,501.0,601.0), polygondf.polygonshape)\n</code></pre> <p>Spark SQL Physical plan:</p> <pre><code>== Physical Plan ==\nRangeJoin polygonshape#20: geometry, pointshape#43: geometry, false\n:- Project [st_polygonfromenvelope(cast(_c0#0 as decimal(24,20)), cast(_c1#1 as decimal(24,20)), cast(_c2#2 as decimal(24,20)), cast(_c3#3 as decimal(24,20)), mypolygonid) AS polygonshape#20]\n:  +- Filter  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains$**\n:     +- *FileScan csv\n+- Project [st_point(cast(_c0#31 as decimal(24,20)), cast(_c1#32 as decimal(24,20)), myPointId) AS pointshape#43]\n   +- *FileScan csv\n</code></pre>"},{"location":"api/sql/Optimizer/#push-spatial-predicates-to-geoparquet","title":"Push spatial predicates to GeoParquet","text":"<p>Sedona supports spatial predicate push-down for GeoParquet files. When spatial filters were applied to dataframes backed by GeoParquet files, Sedona will use the <code>bbox</code> properties in the metadata to determine if all data in the file will be discarded by the spatial predicate. This optimization could reduce the number of files scanned when the queried GeoParquet dataset was partitioned by spatial proximity.</p> <p>To maximize the performance of Sedona GeoParquet filter pushdown, we suggest that you sort the data by their geohash values (see ST_GeoHash) and then save as a GeoParquet file. An example is as follows:</p> <pre><code>SELECT col1, col2, geom, ST_GeoHash(geom, 5) as geohash\nFROM spatialDf\nORDER BY geohash\n</code></pre> <p>The following figure is the visualization of a GeoParquet dataset. <code>bbox</code>es of all GeoParquet files were plotted as blue rectangles and the query window was plotted as a red rectangle. Sedona will only scan 1 of the 6 files to answer queries such as <code>SELECT * FROM geoparquet_dataset WHERE ST_Intersects(geom, &lt;query window&gt;)</code>, thus only part of the data covered by the light green rectangle needs to be scanned.</p> <p></p> <p>We can compare the metrics of querying the GeoParquet dataset with or without the spatial predicate and observe that querying with spatial predicate results in fewer number of rows scanned.</p> Without spatial predicate With spatial predicate"},{"location":"api/sql/Overview/","title":"Introduction","text":""},{"location":"api/sql/Overview/#function-list","title":"Function list","text":"<p>SedonaSQL supports SQL/MM Part3 Spatial SQL Standard. It includes four kinds of SQL operators as follows. All these operators can be directly called through: <pre><code>var myDataFrame = sedona.sql(\"YOUR_SQL\")\n</code></pre></p> <p>Alternatively, <code>expr</code> and <code>selectExpr</code> can be used: <pre><code>myDataFrame.withColumn(\"geometry\", expr(\"ST_*\")).selectExpr(\"ST_*\")\n</code></pre></p> <ul> <li>Constructor: Construct a Geometry given an input string or coordinates<ul> <li>Example: ST_GeomFromWKT (string). Create a Geometry from a WKT String.</li> <li>Documentation: Here</li> </ul> </li> <li>Function: Execute a function on the given column or columns<ul> <li>Example: ST_Distance (A, B). Given two Geometry A and B, return the Euclidean distance of A and B.</li> <li>Documentation: Here</li> </ul> </li> <li>Aggregate function: Return the aggregated value on the given column<ul> <li>Example: ST_Envelope_Aggr (Geometry column). Given a Geometry column, calculate the entire envelope boundary of this column.</li> <li>Documentation: Here</li> </ul> </li> <li>Predicate: Execute a logic judgement on the given columns and return true or false<ul> <li>Example: ST_Contains (A, B). Check if A fully contains B. Return \"True\" if yes, else return \"False\".</li> <li>Documentation: Here</li> </ul> </li> </ul> <p>Sedona also provides an Adapter to convert SpatialRDD &lt;-&gt; DataFrame. Please read Adapter Scaladoc</p> <p>SedonaSQL supports SparkSQL query optimizer, documentation is Here</p>"},{"location":"api/sql/Overview/#quick-start","title":"Quick start","text":"<p>The detailed explanation is here Write a SQL/DataFrame application.</p> <ol> <li>Add Sedona-core and Sedona-SQL into your project pom.xml or build.sbt</li> <li>Create your Sedona config if you want to customize your SparkSession. <pre><code>import org.apache.sedona.spark.SedonaContext\nval config = SedonaContext.builder().\nmaster(\"local[*]\").appName(\"SedonaSQL\")\n.getOrCreate()\n</code></pre></li> <li>Add the following line after your Sedona context declaration: <pre><code>import org.apache.sedona.spark.SedonaContext\nval sedona = SedonaContext.create(config)\n</code></pre></li> </ol>"},{"location":"api/sql/Parameter/","title":"Parameter","text":""},{"location":"api/sql/Parameter/#usage","title":"Usage","text":"<p>SedonaSQL supports many parameters. To change their values,</p> <ol> <li>Set it through SparkConf: <pre><code>sparkSession = SparkSession.builder().\nconfig(\"spark.serializer\",\"org.apache.spark.serializer.KryoSerializer\").\nconfig(\"spark.kryo.registrator\", \"org.apache.sedona.core.serde.SedonaKryoRegistrator\").\nconfig(\"sedona.global.index\",\"true\")\nmaster(\"local[*]\").appName(\"mySedonaSQLdemo\").getOrCreate()\n</code></pre></li> <li>Check your current SedonaSQL configuration: <pre><code>val sedonaConf = new SedonaConf(sparkSession.conf)\nprintln(sedonaConf)\n</code></pre></li> <li>Sedona parameters can be changed at runtime: <pre><code>sparkSession.conf.set(\"sedona.global.index\",\"false\")\n</code></pre></li> </ol>"},{"location":"api/sql/Parameter/#explanation","title":"Explanation","text":"<ul> <li>sedona.global.index<ul> <li>Use spatial index (currently, only supports in SQL range join and SQL distance join)</li> <li>Default: true</li> <li>Possible values: true, false</li> </ul> </li> <li>sedona.global.indextype<ul> <li>Spatial index type, only valid when \"sedona.global.index\" is true</li> <li>Default: rtree</li> <li>Possible values: rtree, quadtree</li> </ul> </li> <li>sedona.join.autoBroadcastJoinThreshold<ul> <li>Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join.   By setting this value to -1 automatic broadcasting can be disabled.</li> <li>Default: The default value is the same as spark.sql.autoBroadcastJoinThreshold</li> <li>Possible values: any integer with a byte suffix i.e. 10MB or 512KB</li> </ul> </li> <li>sedona.join.gridtype<ul> <li>Spatial partitioning grid type for join query</li> <li>Default: kdbtree</li> <li>Possible values: quadtree, kdbtree</li> </ul> </li> <li>sedona.join.indexbuildside (Advanced users only!)<ul> <li>The side which Sedona builds spatial indices on</li> <li>Default: left</li> <li>Possible values: left, right</li> </ul> </li> <li>sedona.join.numpartition (Advanced users only!)<ul> <li>Number of partitions for both sides in a join query</li> <li>Default: -1, which means use the existing partitions</li> <li>Possible values: any integers</li> </ul> </li> <li>sedona.join.spatitionside (Advanced users only!)<ul> <li>The dominant side in spatial partitioning stage</li> <li>Default: left</li> <li>Possible values: left, right</li> </ul> </li> <li>sedona.join.optimizationmode (Advanced users only!)<ul> <li>When should Sedona optimize spatial join SQL queries</li> <li>Default: nonequi</li> <li>Possible values:<ul> <li>all: Always optimize spatial join queries, even for equi-joins.</li> <li>none: Disable optimization for spatial joins.</li> <li>nonequi: Optimize spatial join queries that are not equi-joins.</li> </ul> </li> </ul> </li> </ul>"},{"location":"api/sql/Predicate/","title":"Predicate","text":""},{"location":"api/sql/Predicate/#st_contains","title":"ST_Contains","text":"<p>Introduction: Return true if A fully contains B</p> <p>Format: <code>ST_Contains (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Contains(ST_GeomFromWKT('POLYGON((175 150,20 40,50 60,125 100,175 150))'), ST_GeomFromWKT('POINT(174 149)'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/sql/Predicate/#st_crosses","title":"ST_Crosses","text":"<p>Introduction: Return true if A crosses B</p> <p>Format: <code>ST_Crosses (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Crosses(ST_GeomFromWKT('POLYGON((1 1, 4 1, 4 4, 1 4, 1 1))'),ST_GeomFromWKT('POLYGON((2 2, 5 2, 5 5, 2 5, 2 2))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/sql/Predicate/#st_disjoint","title":"ST_Disjoint","text":"<p>Introduction: Return true if A and B are disjoint</p> <p>Format: <code>ST_Disjoint (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Disjoint(ST_GeomFromWKT('POLYGON((1 4, 4.5 4, 4.5 2, 1 2, 1 4))'),ST_GeomFromWKT('POLYGON((5 4, 6 4, 6 2, 5 2, 5 4))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Predicate/#st_dwithin","title":"ST_DWithin","text":"<p>Introduction: Returns true if 'leftGeometry' and 'rightGeometry' are within a specified 'distance'. This function essentially checks if the shortest distance between the envelope of the two geometries is &lt;= the provided distance.</p> <p>Format: <code>ST_DWithin (leftGeometry: Geometry, rightGeometry: Geometry, distance: Double)</code></p> <p>Since: <code>v1.5.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_DWithin(ST_GeomFromWKT('POINT (0 0)'), ST_GeomFromWKT('POINT (1 0)'), 2.5)\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Predicate/#st_equals","title":"ST_Equals","text":"<p>Introduction: Return true if A equals to B</p> <p>Format: <code>ST_Equals (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Equals(ST_GeomFromWKT('LINESTRING(0 0,10 10)'), ST_GeomFromWKT('LINESTRING(0 0,5 5,10 10)'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Predicate/#st_intersects","title":"ST_Intersects","text":"<p>Introduction: Return true if A intersects B</p> <p>Format: <code>ST_Intersects (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Intersects(ST_GeomFromWKT('LINESTRING(-43.23456 72.4567,-43.23456 72.4568)'), ST_GeomFromWKT('POINT(-43.23456 72.4567772)'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Predicate/#st_orderingequals","title":"ST_OrderingEquals","text":"<p>Introduction: Returns true if the geometries are equal and the coordinates are in the same order</p> <p>Format: <code>ST_OrderingEquals(A: geometry, B: geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_OrderingEquals(ST_GeomFromWKT('POLYGON((2 0, 0 2, -2 0, 2 0))'), ST_GeomFromWKT('POLYGON((2 0, 0 2, -2 0, 2 0))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre> <p>Spark SQL Example:</p> <pre><code>SELECT ST_OrderingEquals(ST_GeomFromWKT('POLYGON((2 0, 0 2, -2 0, 2 0))'), ST_GeomFromWKT('POLYGON((0 2, -2 0, 2 0, 0 2))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/sql/Predicate/#st_overlaps","title":"ST_Overlaps","text":"<p>Introduction: Return true if A overlaps B</p> <p>Format: <code>ST_Overlaps (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Overlaps(ST_GeomFromWKT('POLYGON((2.5 2.5, 2.5 4.5, 4.5 4.5, 4.5 2.5, 2.5 2.5))'), ST_GeomFromWKT('POLYGON((4 4, 4 6, 6 6, 6 4, 4 4))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Predicate/#st_touches","title":"ST_Touches","text":"<p>Introduction: Return true if A touches B</p> <p>Format: <code>ST_Touches (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Touches(ST_GeomFromWKT('LINESTRING(0 0,1 1,0 2)'), ST_GeomFromWKT('POINT(0 2)'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Predicate/#st_within","title":"ST_Within","text":"<p>Introduction: Return true if A is fully contained by B</p> <p>Format: <code>ST_Within (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Within(ST_GeomFromWKT('POLYGON((0 0,3 0,3 3,0 3,0 0))'), ST_GeomFromWKT('POLYGON((1 1,2 1,2 2,1 2,1 1))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/sql/Predicate/#st_covers","title":"ST_Covers","text":"<p>Introduction: Return true if A covers B</p> <p>Format: <code>ST_Covers (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Covers(ST_GeomFromWKT('POLYGON((-2 0,0 2,2 0,-2 0))'), ST_GeomFromWKT('POLYGON((-1 0,0 1,1 0,-1 0))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Predicate/#st_coveredby","title":"ST_CoveredBy","text":"<p>Introduction: Return true if A is covered by B</p> <p>Format: <code>ST_CoveredBy (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_CoveredBy(ST_GeomFromWKT('POLYGON((0 0,3 0,3 3,0 3,0 0))'),  ST_GeomFromWKT('POLYGON((1 1,2 1,2 2,1 2,1 1))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/sql/Raster-aggregate-function/","title":"Raster aggregates","text":""},{"location":"api/sql/Raster-aggregate-function/#rs_union_aggr","title":"RS_Union_Aggr","text":"<p>Introduction: Returns a raster containing bands by specified indexes from all rasters in the provided column. Extracts the first bands from each raster and combines them into the output raster based on the input index values.</p> <p>Note</p> <p>RS_Union_Aggr can take multiple banded rasters as input but it would only extract the first band to the resulting raster. RS_Union_Aggr expects the following input, if not satisfied then will throw an IllegalArgumentException:</p> <ul> <li>Indexes to be in an arithmetic sequence without any gaps.</li> <li>Indexes to be unique and not repeated.</li> <li>Rasters should be of the same shape.</li> </ul> <p>Format: <code>RS_Union_Aggr(A: rasterColumn, B: indexColumn)</code></p> <p>Since: <code>v1.5.1</code></p> <p>Spark SQL Example:</p> <p>Contents of <code>raster_table</code>.</p> <pre><code>+------------------------------+-----+\n|                        raster|index|\n+------------------------------+-----+\n|GridCoverage2D[\"geotiff_cov...|    1|\n|GridCoverage2D[\"geotiff_cov...|    2|\n|GridCoverage2D[\"geotiff_cov...|    3|\n|GridCoverage2D[\"geotiff_cov...|    4|\n|GridCoverage2D[\"geotiff_cov...|    5|\n+------------------------------+-----+\n</code></pre> <pre><code>SELECT RS_Union_Aggr(raster, index) FROM raster_table\n</code></pre> <p>Output:</p> <p>This output raster contains the first band of each raster in the <code>raster_table</code> at specified index.</p> <pre><code>GridCoverage2D[\"geotiff_coverage\", GeneralEnvel...\n</code></pre>"},{"location":"api/sql/Raster-loader/","title":"Raster loader","text":"<p>Note</p> <p>Sedona loader are available in Scala, Java and Python and have the same APIs.</p>"},{"location":"api/sql/Raster-loader/#load-any-raster-to-raster-format","title":"Load any raster to Raster format","text":"<p>The raster loader of Sedona leverages Spark built-in binary data source and works with several RS constructors to produce Raster type. Each raster is a row in the resulting DataFrame and stored in a <code>Raster</code> format.</p> <p>By default, these functions uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order.</p>"},{"location":"api/sql/Raster-loader/#load-raster-to-a-binary-dataframe","title":"Load raster to a binary DataFrame","text":"<p>You can load any type of raster data using the code below. Then use the RS constructors below to create a Raster DataFrame.</p> <pre><code>sedona.read.format(\"binaryFile\").load(\"/some/path/*.asc\")\n</code></pre>"},{"location":"api/sql/Raster-loader/#rs_fromarcinfoasciigrid","title":"RS_FromArcInfoAsciiGrid","text":"<p>Introduction: Returns a raster geometry from an Arc Info Ascii Grid file.</p> <p>Format: <code>RS_FromArcInfoAsciiGrid(asc: ARRAY[Byte])</code></p> <p>Since: <code>v1.4.0</code></p> <p>Spark SQL Example:</p> <pre><code>var df = sedona.read.format(\"binaryFile\").load(\"/some/path/*.asc\")\ndf = df.withColumn(\"raster\", f.expr(\"RS_FromArcInfoAsciiGrid(content)\"))\n</code></pre>"},{"location":"api/sql/Raster-loader/#rs_fromgeotiff","title":"RS_FromGeoTiff","text":"<p>Introduction: Returns a raster geometry from a GeoTiff file.</p> <p>Format: <code>RS_FromGeoTiff(asc: ARRAY[Byte])</code></p> <p>Since: <code>v1.4.0</code></p> <p>Spark SQL Example:</p> <pre><code>var df = sedona.read.format(\"binaryFile\").load(\"/some/path/*.tiff\")\ndf = df.withColumn(\"raster\", f.expr(\"RS_FromGeoTiff(content)\"))\n</code></pre>"},{"location":"api/sql/Raster-loader/#rs_makeemptyraster","title":"RS_MakeEmptyRaster","text":"<p>Introduction: Returns an empty raster geometry. Every band in the raster is initialized to <code>0.0</code>.</p> <p>Since: <code>v1.5.0</code></p> <p>Format:</p> <pre><code>RS_MakeEmptyRaster(numBands: Integer, bandDataType: String = 'D', width: Integer, height: Integer, upperleftX: Double, upperleftY: Double, cellSize: Double)\n</code></pre> <ul> <li>NumBands: The number of bands in the raster. If not specified, the raster will have a single band.</li> <li>BandDataType: Optional parameter specifying the data types of all the bands in the created raster. Accepts one of:<ol> <li>\"D\" - 64 bits Double</li> <li>\"F\" - 32 bits Float</li> <li>\"I\" - 32 bits signed Integer</li> <li>\"S\" - 16 bits signed Short</li> <li>\"US\" - 16 bits unsigned Short</li> <li>\"B\" - 8 bits Byte</li> </ol> </li> <li>Width: The width of the raster in pixels.</li> <li>Height: The height of the raster in pixels.</li> <li>UpperleftX: The X coordinate of the upper left corner of the raster, in terms of the CRS units.</li> <li>UpperleftY: The Y coordinate of the upper left corner of the raster, in terms of the CRS units.</li> <li>Cell Size (pixel size): The size of the cells in the raster, in terms of the CRS units.</li> </ul> <p>It uses the default Cartesian coordinate system.</p> <p>Format:</p> <pre><code>RS_MakeEmptyRaster(numBands: Integer, bandDataType: String = 'D', width: Integer, height: Integer, upperleftX: Double, upperleftY: Double, scaleX: Double, scaleY: Double, skewX: Double, skewY: Double, srid: Integer)\n</code></pre> <ul> <li>NumBands: The number of bands in the raster. If not specified, the raster will have a single band.</li> <li>BandDataType: Optional parameter specifying the data types of all the bands in the created raster. Accepts one of:<ol> <li>\"D\" - 64 bits Double</li> <li>\"F\" - 32 bits Float</li> <li>\"I\" - 32 bits signed Integer</li> <li>\"S\" - 16 bits signed Short</li> <li>\"US\" - 16 bits unsigned Short</li> <li>\"B\" - 8 bits Byte</li> </ol> </li> <li>Width: The width of the raster in pixels.</li> <li>Height: The height of the raster in pixels.</li> <li>UpperleftX: The X coordinate of the upper left corner of the raster, in terms of the CRS units.</li> <li>UpperleftY: The Y coordinate of the upper left corner of the raster, in terms of the CRS units.</li> <li>ScaleX (pixel size on X): The size of the cells on the X axis, in terms of the CRS units.</li> <li>ScaleY (pixel size on Y): The size of the cells on the Y axis, in terms of the CRS units.</li> <li>SkewX: The skew of the raster on the X axis, in terms of the CRS units.</li> <li>SkewY: The skew of the raster on the Y axis, in terms of the CRS units.</li> <li>SRID: The SRID of the raster. Use 0 if you want to use the default Cartesian coordinate system. Use 4326 if you want to use WGS84.</li> </ul> <p>Note</p> <p>If any other value than the accepted values for the bandDataType is provided, RS_MakeEmptyRaster defaults to double as the data type for the raster.</p> <p>Spark SQL example 1 (with 2 bands):</p> <pre><code>SELECT RS_MakeEmptyRaster(2, 10, 10, 0.0, 0.0, 1.0)\n</code></pre> <p>Output:</p> <pre><code>+--------------------------------------------+\n|rs_makeemptyraster(2, 10, 10, 0.0, 0.0, 1.0)|\n+--------------------------------------------+\n|                        GridCoverage2D[\"g...|\n+--------------------------------------------+\n</code></pre> <p>Spark SQL example 2 (with 2 bands and dataType):</p> <pre><code>SELECT RS_MakeEmptyRaster(2, 'I', 10, 10, 0.0, 0.0, 1.0) - Create a raster with integer datatype\n</code></pre> <p>Output:</p> <pre><code>+--------------------------------------------+\n|rs_makeemptyraster(2, 10, 10, 0.0, 0.0, 1.0)|\n+--------------------------------------------+\n|                        GridCoverage2D[\"g...|\n+--------------------------------------------+\n</code></pre> <p>Spark SQL example 3 (with 2 bands, scale, skew, and SRID):</p> <pre><code>SELECT RS_MakeEmptyRaster(2, 10, 10, 0.0, 0.0, 1.0, -1.0, 0.0, 0.0, 4326)\n</code></pre> <p>Output:</p> <pre><code>+------------------------------------------------------------------+\n|rs_makeemptyraster(2, 10, 10, 0.0, 0.0, 1.0, -1.0, 0.0, 0.0, 4326)|\n+------------------------------------------------------------------+\n|                                              GridCoverage2D[\"g...|\n+------------------------------------------------------------------+\n</code></pre> <p>Spark SQL example 4 (with 2 bands, scale, skew, and SRID):</p> <pre><code>SELECT RS_MakeEmptyRaster(2, 'F', 10, 10, 0.0, 0.0, 1.0, -1.0, 0.0, 0.0, 4326) - Create a raster with float datatype\n</code></pre> <p>Output: <pre><code>+------------------------------------------------------------------+\n|rs_makeemptyraster(2, 10, 10, 0.0, 0.0, 1.0, -1.0, 0.0, 0.0, 4326)|\n+------------------------------------------------------------------+\n|                                              GridCoverage2D[\"g...|\n+------------------------------------------------------------------+\n</code></pre></p>"},{"location":"api/sql/Raster-loader/#rs_fromnetcdf","title":"RS_FromNetCDF","text":"<p>Introduction: Returns a raster geometry representing the given record variable short name from a NetCDF file. This API reads the array data of the record variable in memory along with all its dimensions Since the netCDF format has many variants, the reader might not work for your test case, if that is so, please report this using the public forums.</p> <p>This API has been tested for netCDF classic (NetCDF 1, 2, 5) and netCDF4/HDF5 files.</p> <p>This API requires the name of the record variable. It is assumed that a variable of the given name exists, and its last 2 dimensions are 'lat' and 'lon' dimensions respectively.</p> <p>If this assumption does not hold true for your case, you can choose to pass the lonDimensionName and latDimensionName explicitly.</p> <p>You can use RS_NetCDFInfo to get the details of the passed netCDF file (variables and its dimensions).</p> <p>Format 1: <code>RS_FromNetCDF(netCDF: ARRAY[Byte], recordVariableName: String)</code></p> <p>Format 2: <code>RS_FromNetCDF(netCDF: ARRAY[Byte], recordVariableName: String, lonDimensionName: String, latDimensionName: String)</code></p> <p>Since: <code>v1.5.1</code></p> <p>Spark Example:</p> <pre><code>val df = sedona.read.format(\"binaryFile\").load(\"/some/path/test.nc\")\ndf = df.withColumn(\"raster\", f.expr(\"RS_FromNetCDF(content, 'O3')\"))\n</code></pre> <pre><code>val df = sedona.read.format(\"binaryFile\").load(\"/some/path/test.nc\")\ndf = df.withColumn(\"raster\", f.expr(\"RS_FromNetCDF(content, 'O3', 'lon', 'lat')\"))\n</code></pre>"},{"location":"api/sql/Raster-loader/#rs_netcdfinfo","title":"RS_NetCDFInfo","text":"<p>Introduction: Returns a string containing names of the variables in a given netCDF file along with its dimensions.</p> <p>Format: <code>RS_NetCDFInfo(netCDF: ARRAY[Byte])</code></p> <p>Since: <code>1.5.1</code></p> <p>Spark Example:</p> <pre><code>val df = sedona.read.format(\"binaryFile\").load(\"/some/path/test.nc\")\nrecordInfo = df.selectExpr(\"RS_NetCDFInfo(content) as record_info\").first().getString(0)\nprint(recordInfo)\n</code></pre> <p>Output:</p> <pre><code>O3(time=2, z=2, lat=48, lon=80)\n\nNO2(time=2, z=2, lat=48, lon=80)\n</code></pre>"},{"location":"api/sql/Raster-map-algebra/","title":"Raster map algebra","text":""},{"location":"api/sql/Raster-map-algebra/#map-algebra","title":"Map Algebra","text":"<p>Map algebra is a way to perform raster calculations using mathematical expressions. The expression can be a simple arithmetic operation or a complex combination of multiple operations. The expression can be applied to a single raster band or multiple raster bands. The result of the expression is a new raster.</p> <p>Apache Sedona provides two ways to perform map algebra operations:</p> <ol> <li>Using the <code>RS_MapAlgebra</code> function.</li> <li>Using <code>RS_BandAsArray</code> and array based map algebra functions, such as <code>RS_Add</code>, <code>RS_Multiply</code>, etc.</li> </ol> <p>Generally, the <code>RS_MapAlgebra</code> function is more flexible and can be used to perform more complex operations. The function takes three to four arguments:</p> <pre><code>RS_MapAlgebra(rast: Raster, pixelType: String, script: String, [noDataValue: Double])\n</code></pre> <ul> <li><code>rast</code>: The raster to apply the map algebra expression to.</li> <li><code>pixelType</code>: The data type of the output raster. This can be one of <code>D</code> (double), <code>F</code> (float), <code>I</code> (integer), <code>S</code> (short), <code>US</code> (unsigned short) or <code>B</code> (byte). If specified <code>NULL</code>, the output raster will have the same data type as the input raster.</li> <li><code>script</code>: The map algebra script. Refer here for more details on the format.</li> <li><code>noDataValue</code>: (Optional) The nodata value of the output raster.</li> </ul> <p>As of version <code>v1.5.1</code>, the <code>RS_MapAlgebra</code> function allows two raster column inputs, with multi-band rasters supported. The function accepts 5 parameters:</p> <pre><code>RS_MapAlgebra(rast0: Raster, rast1: Raster, pixelType: String, script: String, noDataValue: Double)\n</code></pre> <ul> <li><code>rast0</code>: The first raster to apply the map algebra expression to.</li> <li><code>rast1</code>: The second raster to apply the map algebra expression to.</li> <li><code>pixelType</code>: The data type of the output raster. This can be one of <code>D</code> (double), <code>F</code> (float), <code>I</code> (integer), <code>S</code> (short), <code>US</code> (unsigned short) or <code>B</code> (byte). If specified <code>NULL</code>, the output raster will have the same data type as the input raster.</li> <li><code>script</code>: The map algebra script. Refer here for more details on the format.</li> <li><code>noDataValue</code>: (Not optional) The nodata value of the output raster, <code>null</code> is allowed.</li> </ul> <p>Spark SQL Example for two raster input <code>RS_MapAlgebra</code>:</p> <pre><code>RS_MapAlgebra(rast0, rast1, 'D', 'out = rast0[0] * 0.5 + rast1[0] * 0.5;', null)\n</code></pre> <p><code>RS_MapAlgebra</code> also has good performance, since it is backed by Jiffle and can be compiled to Java bytecode for execution. We'll demonstrate both approaches to implementing commonly used map algebra operations.</p> <p>Note</p> <p>The <code>RS_MapAlgebra</code> function can cast the output raster to a different data type specified by <code>pixelType</code>:</p> <ul> <li> <p>If <code>pixelType</code> is smaller than the input raster data type, narrowing casts will be performed, which may result in loss of data.</p> </li> <li> <p>If <code>pixelType</code> is larger, widening casts will retain data accuracy.</p> </li> <li> <p>If <code>pixelType</code> matches the input raster data type, no casting occurs.</p> </li> </ul> <p>This allows controlling the output pixel data type. Users should consider potential precision impacts when coercing to a smaller type.</p>"},{"location":"api/sql/Raster-map-algebra/#ndvi","title":"NDVI","text":"<p>The Normalized Difference Vegetation Index (NDVI) is a simple graphical indicator that can be used to analyze remote sensing measurements, typically, but not necessarily, from a space platform, and assess whether the target being observed contains live green vegetation or not. NDVI has become a de facto standard index used to determine whether a given area contains live green vegetation or not. The NDVI is calculated from these individual measurements as follows:</p> <pre><code>NDVI = (NIR - Red) / (NIR + Red)\n</code></pre> <p>where NIR is the near-infrared band and Red is the red band.</p> <p>Assume that we have a bunch of rasters with 4 bands: red, green, blue, and near-infrared. We want to calculate the NDVI for each raster. We can use the <code>RS_MapAlgebra</code> function to do this:</p> <pre><code>SELECT RS_MapAlgebra(rast, 'D', 'out = (rast[3] - rast[0]) / (rast[3] + rast[0]);') as ndvi FROM raster_table\n</code></pre> <p>The Jiffle script is <code>out = (rast[3] - rast[0]) / (rast[3] + rast[0]);</code>. The <code>rast</code> variable is always bound to the input raster, and the <code>out</code> variable is bound to the output raster. Jiffle iterates over all the pixels in the input raster and executes the script for each pixel. the <code>rast[3]</code> and <code>rast[0]</code> refers to the current pixel values of the near-infrared and red bands, respectively. The <code>out</code> variable is the current output pixel value.</p> <p>The result of the <code>RS_MapAlgebra</code> function is a raster with a single band. The band is of type double, since we specified <code>D</code> as the <code>pixelType</code> argument.</p> <p>We can implement the same NDVI calculation using the array based map algebra functions:</p> <pre><code>SELECT RS_Divide(\nRS_Subtract(RS_BandAsArray(rast, 1), RS_BandAsArray(rast, 4)),\nRS_Add(RS_BandAsArray(rast, 1), RS_BandAsArray(rast, 4))) as ndvi FROM raster_table\n</code></pre> <p>The <code>RS_BandAsArray</code> function extracts the specified band of the input raster to an array of double, and the <code>RS_Add</code>, <code>RS_Subtract</code>, and <code>RS_Divide</code> functions perform the arithmetic operations on the arrays. The code using the array based map algebra functions is more verbose. However, there is a <code>RS_NormalizedDifference</code> function that can be used to calculate the NDVI more concisely:</p> <pre><code>SELECT RS_NormalizedDifference(RS_BandAsArray(rast, 1), RS_BandAsArray(rast, 4)) as ndvi FROM raster_table\n</code></pre> <p>The result of array based map algebra functions is an array of double. User can use <code>RS_AddBandFromArray</code> to add the array to a raster as a new band.</p>"},{"location":"api/sql/Raster-map-algebra/#awei","title":"AWEI","text":"<p>The Automated Water Extraction Index (AWEI) is a spectral index that can be used to extract water bodies from remote sensing imagery. The AWEI is calculated from these individual measurements as follows:</p> <pre><code>AWEI = 4 * (Green - SWIR2) - (0.25 * NIR + 2.75 * SWIR1)\n</code></pre> <p>AWEI can be implemented easily using <code>RS_MapAlgebra</code>:</p> <pre><code>-- Assume that the raster includes all 13 Sentinel-2 bands\nSELECT RS_MapAlgebra(rast, 'D', 'out = 4 * (rast[2] - rast[11]) - (0.25 * rast[7] + 2.75 * rast[12]);') as awei FROM raster_table\n</code></pre> <p>We can also implement the same AWEI calculation using array based map algebra functions. The code looks more verbose:</p> <pre><code>SELECT RS_Subtract(\nRS_Add(RS_MultiplyFactor(band_nir, 0.25), RS_MultiplyFactor(band_swir1, 2.75)),\nRS_MultiplyFactor(RS_Subtract(band_swir2, band_green), 4)) as awei\nFROM (\nSELECT RS_BandAsArray(rast, 3) AS band_green,\nRS_BandAsArray(rast, 12) AS band_swir2,\nRS_BandAsArray(rast, 13) AS band_swir1,\nRS_BandAsArray(rast, 8) AS band_nir\nFROM raster_table) t\n</code></pre>"},{"location":"api/sql/Raster-map-algebra/#further-reading","title":"Further Reading","text":"<ul> <li>Jiffle language summary</li> <li>Raster operators</li> </ul>"},{"location":"api/sql/Raster-operators/","title":"Raster operators","text":""},{"location":"api/sql/Raster-operators/#pixel-functions","title":"Pixel Functions","text":""},{"location":"api/sql/Raster-operators/#rs_pixelascentroid","title":"RS_PixelAsCentroid","text":"<p>Introduction: Returns the centroid (point geometry) of the specified pixel's area. The pixel coordinates specified are 1-indexed. If <code>colX</code> and <code>rowY</code> are out of bounds for the raster, they are interpolated assuming the same skew and translate values.</p> <p>Format: <code>RS_PixelAsCentroid(raster: Raster, colX: Integer, rowY: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_AsText(RS_PixelAsCentroid(RS_MakeEmptyRaster(1, 12, 13, 134, -53, 9), 3, 3))\n</code></pre> <p>Output:</p> <pre><code>POINT (156.5 -75.5)\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_pixelascentroids","title":"RS_PixelAsCentroids","text":"<p>Introduction: Returns a list of the centroid point geometry, the pixel value and its raster X and Y coordinates for each pixel in the raster at the specified band. Each centroid represents the geometric center of the corresponding pixel's area.</p> <p>Format: <code>RS_PixelAsCentroids(raster: Raster, band: Integer)</code></p> <p>Since: <code>v1.5.1</code></p> <p>Spark SQL Example: <pre><code>SELECT ST_AsText(RS_PixelAsCentroids(raster, 1)) from rasters\n</code></pre></p> <p>Output: <pre><code>[[POINT (-13065222 4021263.75),148.0,0,0], [POINT (-13065151 4021263.75),123.0,0,1], [POINT (-13065077 4021263.75),99.0,1,0], [POINT (-13065007 4021261.75),140.0,1,1]]\n</code></pre></p> <p>Spark SQL example for extracting Point, value, raster x and y coordinates:</p> <pre><code>val pointDf = sedona.read...\nval rasterDf = sedona.read.format(\"binaryFile\").load(\"/some/path/*.tiff\")\nvar df = sedona.read.format(\"binaryFile\").load(\"/some/path/*.tiff\")\ndf = df.selectExpr(\"RS_FromGeoTiff(content) as raster\")\n\ndf.selectExpr(\n\"explode(RS_PixelAsCentroids(raster, 1)) as exploded\"\n).selectExpr(\n\"exploded.geom as geom\",\n\"exploded.value as value\",\n\"exploded.x as x\",\n\"exploded.y as y\"\n).show(3)\n</code></pre> <p>Output:</p> <pre><code>+----------------------------------------------+-----+---+---+\n|geom                                          |value|x  |y  |\n+----------------------------------------------+-----+---+---+\n|POINT (-13095781.835693639 4021226.5856936392)|0.0  |1  |1  |\n|POINT (-13095709.507080918 4021226.5856936392)|0.0  |2  |1  |\n|POINT (-13095637.178468198 4021226.5856936392)|0.0  |3  |1  |\n+----------------------------------------------+-----+---+---+\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_pixelaspoint","title":"RS_PixelAsPoint","text":"<p>Introduction: Returns a point geometry of the specified pixel's upper-left corner. The pixel coordinates specified are 1-indexed.</p> <p>Note</p> <p>If the pixel coordinates specified do not exist in the raster (out of bounds), RS_PixelAsPoint throws an IndexOutOfBoundsException.</p> <p>Format: <code>RS_PixelAsPoint(raster: Raster, colX: Integer, rowY: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_AsText(RS_PixelAsPoint(raster, 2, 1)) from rasters\n</code></pre> <p>Output: <pre><code>POINT (123.19, -12)\n</code></pre></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_AsText(RS_PixelAsPoint(raster, 6, 2)) from rasters\n</code></pre> <p>Output: <pre><code>IndexOutOfBoundsException: Specified pixel coordinates (6, 2) do not lie in the raster\n</code></pre></p>"},{"location":"api/sql/Raster-operators/#rs_pixelaspoints","title":"RS_PixelAsPoints","text":"<p>Introduction: Returns a list of the pixel's upper-left corner point geometry, the pixel value and its raster X and Y coordinates for each pixel in the raster at the specified band.</p> <p>Format: <code>RS_PixelAsPoints(raster: Raster, band: Integer)</code></p> <p>Since: <code>v1.5.1</code></p> <p>Spark SQL Example: <pre><code>SELECT ST_AsText(RS_PixelAsPoints(raster, 1)) from rasters\n</code></pre></p> <p>Output: <pre><code>[[POINT (-13065223 4021262.75),148.0,0,0], [POINT (-13065150 4021262.75),123.0,0,1], [POINT (-13065078 4021262.75),99.0,1,0], [POINT (-13065006 4021262.75),140.0,1,1]]\n</code></pre></p> <p>Spark SQL example for extracting Point, value, raster x and y coordinates:</p> <pre><code>val pointDf = sedona.read...\nval rasterDf = sedona.read.format(\"binaryFile\").load(\"/some/path/*.tiff\")\nvar df = sedona.read.format(\"binaryFile\").load(\"/some/path/*.tiff\")\ndf = df.selectExpr(\"RS_FromGeoTiff(content) as raster\")\n\ndf.selectExpr(\n\"explode(RS_PixelAsPoints(raster, 1)) as exploded\"\n).selectExpr(\n\"exploded.geom as geom\",\n\"exploded.value as value\",\n\"exploded.x as x\",\n\"exploded.y as y\"\n).show(3)\n</code></pre> <p>Output:</p> <pre><code>+--------------------------------------+-----+---+---+\n|geom                                  |value|x  |y  |\n+--------------------------------------+-----+---+---+\n|POINT (-13095818 4021262.75)          |0.0  |1  |1  |\n|POINT (-13095745.67138728 4021262.75) |0.0  |2  |1  |\n|POINT (-13095673.342774557 4021262.75)|0.0  |3  |1  |\n+--------------------------------------+-----+---+---+\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_pixelaspolygon","title":"RS_PixelAsPolygon","text":"<p>Introduction: Returns a polygon geometry that bounds the specified pixel. The pixel coordinates specified are 1-indexed. If <code>colX</code> and <code>rowY</code> are out of bounds for the raster, they are interpolated assuming the same skew and translate values.</p> <p>Format: <code>RS_PixelAsPolygon(raster: Raster, colX: Integer, rowY: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_AsText(RS_PixelAsPolygon(RS_MakeEmptyRaster(1, 5, 10, 123, -230, 8), 2, 3))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((131 -246, 139 -246, 139 -254, 131 -254, 131 -246))\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_pixelaspolygons","title":"RS_PixelAsPolygons","text":"<p>Introduction: Returns a list of the polygon geometry, the pixel value and its raster X and Y coordinates for each pixel in the raster at the specified band.</p> <p>Format: <code>RS_PixelAsPolygons(raster: Raster, band: Integer)</code></p> <p>Since: <code>v1.5.1</code></p> <p>Spark SQL Example: <pre><code>SELECT ST_AsText(RS_PixelAsPolygons(raster, 1)) from rasters\n</code></pre></p> <p>Output: <pre><code>[[POLYGON ((123.19000244140625 -12, 127.19000244140625 -12, 127.19000244140625 -16, 123.19000244140625 -16, 123.19000244140625 -12)),0.0,1,1],\n[POLYGON ((127.19000244140625 -12, 131.19000244140625 -12, 131.19000244140625 -16, 127.19000244140625 -16, 127.19000244140625 -12)),0.0,2,1],\n[POLYGON ((131.19000244140625 -12, 135.19000244140625 -12, 135.19000244140625 -16, 131.19000244140625 -16, 131.19000244140625 -12)),0.0,3,1]]\n</code></pre></p> <p>Spark SQL example for extracting Point, value, raster x and y coordinates:</p> <pre><code>val pointDf = sedona.read...\nval rasterDf = sedona.read.format(\"binaryFile\").load(\"/some/path/*.tiff\")\nvar df = sedona.read.format(\"binaryFile\").load(\"/some/path/*.tiff\")\ndf = df.selectExpr(\"RS_FromGeoTiff(content) as raster\")\n\ndf.selectExpr(\n\"explode(RS_PixelAsPolygons(raster, 1)) as exploded\"\n).selectExpr(\n\"exploded.geom as geom\",\n\"exploded.value as value\",\n\"exploded.x as x\",\n\"exploded.y as y\"\n).show(3)\n</code></pre> <p>Output:</p> <pre><code>+--------------------+-----+---+---+\n|                geom|value|  x|  y|\n+--------------------+-----+---+---+\n|POLYGON ((-130958...|  0.0|  1|  1|\n|POLYGON ((-130957...|  0.0|  2|  1|\n|POLYGON ((-130956...|  0.0|  3|  1|\n+--------------------+-----+---+---+\n</code></pre>"},{"location":"api/sql/Raster-operators/#geometry-functions","title":"Geometry Functions","text":""},{"location":"api/sql/Raster-operators/#rs_envelope","title":"RS_Envelope","text":"<p>Introduction: Returns the envelope of the raster as a Geometry.</p> <p>Format: <code>RS_Envelope (raster: Raster)</code></p> <p>Since: <code>v1.4.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_Envelope(raster) FROM raster_table\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0 0,20 0,20 60,0 60,0 0))\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_convexhull","title":"RS_ConvexHull","text":"<p>Introduction: Return the convex hull geometry of the raster including the NoDataBandValue band pixels. For regular shaped and non-skewed rasters, this gives more or less the same result as RS_Envelope and hence is only useful for irregularly shaped or skewed rasters.</p> <p>Format: <code>RS_ConvexHull(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_ConvexHull(RS_MakeEmptyRaster(1, 5, 10, 156, -132, 5, 10, 3, 5, 0));\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((156 -132, 181 -107, 211 -7, 186 -32, 156 -132))\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_minconvexhull","title":"RS_MinConvexHull","text":"<p>Introduction: Returns the min convex hull geometry of the raster excluding the NoDataBandValue band pixels, in the given band. If no band is specified, all the bands are considered when creating the min convex hull of the raster. The created geometry representing the min convex hull has world coordinates of the raster in its CRS as the corner coordinates.</p> <p>Note</p> <p>If the specified band does not exist in the raster, RS_MinConvexHull throws an IllegalArgumentException</p> <p>Format:</p> <p><code>RS_MinConvexHull(raster: Raster)</code></p> <p><code>RS_MinConvexHull(raster: Raster, band: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>val inputDf = Seq((Seq(0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0),\nSeq(0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0))).toDF(\"values2\", \"values1\")\ninputDf.selectExpr(\"ST_AsText(RS_MinConvexHull(RS_AddBandFromArray(\" +\n\"RS_AddBandFromArray(RS_MakeEmptyRaster(2, 5, 5, 0, 0, 1, -1, 0, 0, 0), values1, 1, 0), values2, 2, 0))) as minConvexHullAll\").show()\n</code></pre> <p>Output: <pre><code>+----------------------------------------+\n|minConvexHullAll                        |\n+----------------------------------------+\n|POLYGON ((0 -1, 4 -1, 4 -5, 0 -5, 0 -1))|\n+----------------------------------------+\n</code></pre></p> <p>Spark SQL Example:</p> <pre><code>val inputDf = Seq((Seq(0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0),\nSeq(0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0))).toDF(\"values2\", \"values1\")\ninputDf.selectExpr(\"ST_AsText(RS_MinConvexHull(RS_AddBandFromArray(\" +\n\"RS_AddBandFromArray(RS_MakeEmptyRaster(2, 5, 5, 0, 0, 1, -1, 0, 0, 0), values1, 1, 0), values2, 2, 0), 1)) as minConvexHull1\").show()\n</code></pre> <p>Output: <pre><code>+----------------------------------------+\n|minConvexHull1                          |\n+----------------------------------------+\n|POLYGON ((1 -1, 4 -1, 4 -5, 1 -5, 1 -1))|\n+----------------------------------------+\n</code></pre></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_MinConvexHull(raster, 3) from rasters;\n</code></pre> <p>Output:</p> <pre><code>Provided band index 3 does not lie in the raster\n</code></pre>"},{"location":"api/sql/Raster-operators/#raster-accessors","title":"Raster Accessors","text":""},{"location":"api/sql/Raster-operators/#rs_georeference","title":"RS_GeoReference","text":"<p>Introduction: Returns the georeference metadata of raster as a string in GDAL or ESRI format. Default is GDAL if not specified.</p> <p>Note</p> <p>If you are using <code>show()</code> to display the output, it will show special characters as escape sequences. To get the expected behavior use the following code:</p> ScalaJavaPython <pre><code>println(df.selectExpr(\"RS_GeoReference(rast)\").sample(0.5).collect().mkString(\"\\n\"))\n</code></pre> <pre><code>System.out.println(String.join(\"\\n\", df.selectExpr(\"RS_GeoReference(rast)\").sample(0.5).collect()))\n</code></pre> <pre><code>print(\"\\n\".join(df.selectExpr(\"RS_GeoReference(rast)\").sample(0.5).collect()))\n</code></pre> <p>The <code>sample()</code> function is only there to reduce the data sent to <code>collect()</code>, you may also use <code>filter()</code> if that's appropriate.</p> <p>Format: <code>RS_GeoReference(raster: Raster, format: String = \"GDAL\")</code></p> <p>Since: <code>v1.5.0</code></p> <p>Difference between format representation is as follows:</p> <p><code>GDAL</code></p> <pre><code>ScaleX\nSkewY\nSkewX\nScaleY\nUpperLeftX\nUpperLeftY\n</code></pre> <p><code>ESRI</code></p> <pre><code>ScaleX\nSkewY\nSkewX\nScaleY\nUpperLeftX + ScaleX * 0.5\nUpperLeftY + ScaleY * 0.5\n</code></pre> <p>Spark SQL Example:</p> <pre><code>SELECT RS_GeoReference(ST_MakeEmptyRaster(1, 100, 100, -53, 51, 2, -2, 4, 5, 4326))\n</code></pre> <p>Output:</p> <pre><code>2.000000\n5.000000\n4.000000\n-2.000000\n-53.000000\n51.000000\n</code></pre> <p>Spark SQL Example:</p> <pre><code>SELECT RS_GeoReferrence(ST_MakeEmptyRaster(1, 3, 4, 100.0, 200.0,2.0, -3.0, 0.1, 0.2, 0), \"GDAL\")\n</code></pre> <p>Output:</p> <pre><code>2.000000\n0.200000\n0.100000\n-3.000000\n100.000000\n200.000000\n</code></pre> <p>Spark SQL Example:</p> <pre><code>SELECT RS_GeoReferrence(ST_MakeEmptyRaster(1, 3, 4, 100.0, 200.0,2.0, -3.0, 0.1, 0.2, 0), \"ERSI\")\n</code></pre> <pre><code>2.000000\n0.200000\n0.100000\n-3.000000\n101.000000\n198.500000\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_geotransform","title":"RS_GeoTransform","text":"<p>Introduction: Returns an array of parameters that represent the GeoTranformation of the raster. The array contains the following values:</p> <ul> <li>0: pixel width along west-east axis (x axis)</li> <li>1: pixel height along north-south axis (y axis)</li> <li>2: Rotation of the raster</li> <li>3: Angular separation between x axis and y axis</li> <li>4: X ordinate of upper-left coordinate</li> <li>5: Y ordinate of upper-left coordinate</li> </ul> <p>Note</p> <p>Refer to this image for a clear understanding between i &amp; j axis and x &amp; y axis.</p> <p>Format: <code>RS_GeoTransform(raster: Raster)</code></p> <p>Since: <code>v1.5.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_GeoTransform(\nRS_MakeEmptyRaster(2, 10, 15, 1, 2, 1, -2, 1, 2, 0)\n)\n</code></pre> <p>Output:</p> <pre><code>[2.23606797749979, 2.23606797749979, -1.1071487177940904, -2.214297435588181, 1.0, 2.0]\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_height","title":"RS_Height","text":"<p>Introduction: Returns the height of the raster.</p> <p>Format: <code>RS_Height(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_Height(raster) FROM rasters\n</code></pre> <p>Output:</p> <pre><code>512\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_rastertoworldcoordx","title":"RS_RasterToWorldCoordX","text":"<p>Introduction: Returns the upper left X coordinate of the given row and column of the given raster geometric units of the geo-referenced raster. If any out of bounds values are given, the X coordinate of the assumed point considering existing raster pixel size and skew values will be returned.</p> <p>Format: <code>RS_RasterToWorldCoordX(raster: Raster, colX: Integer, rowY: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_RasterToWorldCoordX(ST_MakeEmptyRaster(1, 5, 10, -123, 54, 5, -10, 0, 0, 4326), 1, 1) from rasters\n</code></pre> <p>Output:</p> <pre><code>-123\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_rastertoworldcoordy","title":"RS_RasterToWorldCoordY","text":"<p>Introduction: Returns the upper left Y coordinate of the given row and column of the given raster geometric units of the geo-referenced raster. If any out of bounds values are given, the Y coordinate of the assumed point considering existing raster pixel size and skew values will be returned.</p> <p>Format: <code>RS_RasterToWorldCoordY(raster: Raster, colX: Integer, rowY: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_RasterToWorldCoordY(ST_MakeEmptyRaster(1, 5, 10, -123, 54, 5, -10, 0, 0, 4326), 1, 1) from rasters\n</code></pre> <p>Output:</p> <pre><code>54\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_rastertoworldcoord","title":"RS_RasterToWorldCoord","text":"<p>Introduction: Returns the upper left X and Y coordinates of the given row and column of the given raster geometric units of the geo-referenced raster as a Point geometry. If any out of bounds values are given, the X and Y coordinates of the assumed point considering existing raster pixel size and skew values will be returned.</p> <p>Format: <code>RS_RasterToWorldCoord(raster: Raster, colX: Integer, rowY: Integer)</code></p> <p>Since: <code>v1.5.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_RasterToWorldCoord(ST_MakeEmptyRaster(1, 5, 10, -123, 54, 5, -10, 0, 0, 4326), 1, 1) from rasters\n</code></pre> <p>Output:</p> <pre><code>POINT (-123 54)\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_rotation","title":"RS_Rotation","text":"<p>Introduction: Returns the uniform rotation of the raster in radian.</p> <p>Format: <code>RS_Rotation(raster: Raster)</code></p> <p>Since: <code>v1.5.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_Rotation(\nRS_MakeEmptyRaster(2, 10, 15, 1, 2, 1, -2, 1, 2, 0)\n)\n</code></pre> <p>Output:</p> <pre><code>-1.1071487177940904\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_scalex","title":"RS_ScaleX","text":"<p>Introduction: Returns the pixel width of the raster in CRS units.</p> <p>Note</p> <p>RS_ScaleX attempts to get an Affine transform on the grid in order to return scaleX (See World File for more details). If the transform on the geometry is not an Affine transform, RS_ScaleX will throw an UnsupportedException: <pre><code>UnsupportedOperationException(\"Only AffineTransform2D is supported\")\n</code></pre></p> <p>Format: <code>RS_ScaleX(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_ScaleX(raster) FROM rasters\n</code></pre> <p>Output:</p> <pre><code>1\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_scaley","title":"RS_ScaleY","text":"<p>Introduction: Returns the pixel height of the raster in CRS units.</p> <p>Note</p> <p>RS_ScaleY attempts to get an Affine transform on the grid in order to return scaleX (See World File for more details). If the transform on the geometry is not an Affine transform, RS_ScaleY will throw an UnsupportedException: <pre><code>UnsupportedOperationException(\"Only AffineTransform2D is supported\")\n</code></pre></p> <p>Format: <code>RS_ScaleY(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_ScaleY(raster) FROM rasters\n</code></pre> <p>Output: <pre><code>-2\n</code></pre></p>"},{"location":"api/sql/Raster-operators/#rs_skewx","title":"RS_SkewX","text":"<p>Introduction: Returns the X skew or rotation parameter.</p> <p>Format: <code>RS_SkewX(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_SkewX(RS_MakeEmptyRaster(2, 10, 10, 0.0, 0.0, 1.0, -1.0, 0.1, 0.2, 4326))\n</code></pre> <p>Output:</p> <pre><code>0.1\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_skewy","title":"RS_SkewY","text":"<p>Introduction: Returns the Y skew or rotation parameter.</p> <p>Format: <code>RS_SkewY(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_SkewY(RS_MakeEmptyRaster(2, 10, 10, 0.0, 0.0, 1.0, -1.0, 0.1, 0.2, 4326))\n</code></pre> <p>Output:</p> <pre><code>0.2\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_upperleftx","title":"RS_UpperLeftX","text":"<p>Introduction: Returns the X coordinate of the upper-left corner of the raster.</p> <p>Format: <code>RS_UpperLeftX(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_UpperLeftX(raster) FROM rasters\n</code></pre> <p>Output:</p> <pre><code>5\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_upperlefty","title":"RS_UpperLeftY","text":"<p>Introduction: Returns the Y coordinate of the upper-left corner of the raster.</p> <p>Format: <code>RS_UpperLeftY(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_UpperLeftY(raster) FROM rasters\n</code></pre> <p>Output:</p> <pre><code>6\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_width","title":"RS_Width","text":"<p>Introduction: Returns the width of the raster.</p> <p>Format: <code>RS_Width(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_Width(raster) FROM rasters\n</code></pre> <p>Output: <pre><code>517\n</code></pre></p>"},{"location":"api/sql/Raster-operators/#rs_worldtorastercoord","title":"RS_WorldToRasterCoord","text":"<p>Introduction: Returns the grid coordinate of the given world coordinates as a Point.</p> <p>Format:</p> <p><code>RS_WorldToRasterCoord(raster: Raster, point: Geometry)</code></p> <p><code>RS_WorldToRasterCoord(raster: Raster, x: Double, y: Point)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_WorldToRasterCoord(ST_MakeEmptyRaster(1, 5, 5, -53, 51, 1, -1, 0, 0, 4326), -53, 51) from rasters;\n</code></pre> <p>Output:</p> <pre><code>POINT (1 1)\n</code></pre> <p>Spark SQL Example:</p> <pre><code>SELECT RS_WorldToRasterCoord(ST_MakeEmptyRaster(1, 5, 5, -53, 51, 1, -1, 0, 0, 4326), ST_GeomFromText('POINT (-52 51)')) from rasters;\n</code></pre> <p>Output:</p> <pre><code>POINT (2 1)\n</code></pre> <p>Note</p> <p>If the given geometry point is not in the same CRS as the given raster, the given geometry will be transformed to the given raster's CRS. You can use ST_Transform to transform the geometry beforehand.</p>"},{"location":"api/sql/Raster-operators/#rs_worldtorastercoordx","title":"RS_WorldToRasterCoordX","text":"<p>Introduction: Returns the X coordinate of the grid coordinate of the given world coordinates as an integer.</p> <p>Format:</p> <p><code>RS_WorldToRasterCoord(raster: Raster, point: Geometry)</code></p> <p><code>RS_WorldToRasterCoord(raster: Raster, x: Double, y: Double)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_WorldToRasterCoordX(ST_MakeEmptyRaster(1, 5, 5, -53, 51, 1, -1, 0, 0), -53, 51) from rasters;\n</code></pre> <p>Output:</p> <pre><code>1\n</code></pre> <p>Spark SQL Example:</p> <pre><code>SELECT RS_WorldToRasterCoordX(ST_MakeEmptyRaster(1, 5, 5, -53, 51, 1, -1, 0, 0), ST_GeomFromText('POINT (-53 51)')) from rasters;\n</code></pre> <p>Output:</p> <pre><code>1\n</code></pre> <p>Tip</p> <p>For non-skewed rasters, you can provide any value for latitude and the intended value of world longitude, to get the desired answer</p>"},{"location":"api/sql/Raster-operators/#rs_worldtorastercoordy","title":"RS_WorldToRasterCoordY","text":"<p>Introduction: Returns the Y coordinate of the grid coordinate of the given world coordinates as an integer.</p> <p>Format:</p> <p><code>RS_WorldToRasterCoordY(raster: Raster, point: Geometry)</code></p> <p><code>RS_WorldToRasterCoordY(raster: Raster, x: Double, y: Double)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_WorldToRasterCoordY(ST_MakeEmptyRaster(1, 5, 5, -53, 51, 1, -1, 0, 0), ST_GeomFromText('POINT (-50 50)'));\n</code></pre> <p>Output:</p> <pre><code>2\n</code></pre> <p>Spark SQL Example:</p> <pre><code>SELECT RS_WorldToRasterCoordY(ST_MakeEmptyRaster(1, 5, 5, -53, 51, 1, -1, 0, 0), -50, 49);\n</code></pre> <p>Output:</p> <pre><code>3\n</code></pre> <p>Tip</p> <p>For non-skewed rasters, you can provide any value for longitude and the intended value of world latitude, to get the desired answer</p>"},{"location":"api/sql/Raster-operators/#raster-band-accessors","title":"Raster Band Accessors","text":""},{"location":"api/sql/Raster-operators/#rs_band","title":"RS_Band","text":"<p>Introduction: Returns a new raster consisting 1 or more bands of an existing raster. It can build new rasters from existing ones, export only selected bands from a multiband raster, or rearrange the order of bands in a raster dataset.</p> <p>Format:</p> <p><code>RS_Band(raster: Raster, bands: ARRAY[Integer])</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_NumBands(\nRS_Band(\nRS_AddBandFromArray(\nRS_MakeEmptyRaster(2, 5, 5, 3, -215, 2, -2, 2, 2, 0),\nArray(16, 0, 24, 33, 43, 49, 64, 0, 76, 77, 79, 89, 0, 116, 118, 125, 135, 0, 157, 190, 215, 229, 241, 248, 249),\n1, 0d\n), Array(1,1,1)\n)\n)\n</code></pre> <p>Output:</p> <pre><code>3\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_bandnodatavalue","title":"RS_BandNoDataValue","text":"<p>Introduction: Returns the no data value of the given band of the given raster. If no band is given, band 1 is assumed. The band parameter is 1-indexed. If there is no no data value associated with the given band, RS_BandNoDataValue returns null.</p> <p>Note</p> <p>If the given band does not lie in the raster, RS_BandNoDataValue throws an IllegalArgumentException</p> <p>Format: <code>RS_BandNoDataValue (raster: Raster, band: Integer = 1)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_BandNoDataValue(raster, 1) from rasters;\n</code></pre> <p>Output:</p> <pre><code>0.0\n</code></pre> <p>Spark SQL Example:</p> <pre><code>SELECT RS_BandNoDataValue(raster) from rasters_without_nodata;\n</code></pre> <p>Output:</p> <pre><code>null\n</code></pre> <p>Spark SQL Example:</p> <pre><code>SELECT RS_BandNoDataValue(raster, 3) from rasters;\n</code></pre> <p>Output:</p> <pre><code>IllegalArgumentException: Provided band index 3 is not present in the raster.\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_bandisnodata","title":"RS_BandIsNoData","text":"<p>Returns true if the band is filled with only nodata values. Band 1 is assumed if not specified.</p> <p>Format: <code>RS_BandIsNoData(raster: Raster, band: Integer = 1)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>WITH rast_table AS (SELECT RS_AddBandFromArray(RS_MakeEmptyRaster(1, 2, 2, 0, 0, 1), ARRAY(10d, 10d, 10d, 10d), 1, 10d) as rast)\nSELECT RS_BandIsNoData(rast) from rast_table\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_bandpixeltype","title":"RS_BandPixelType","text":"<p>Introduction: Returns the datatype of each pixel in the given band of the given raster in string format. The band parameter is 1-indexed. If no band is specified, band 1 is assumed.</p> <p>Note</p> <p>If the given band index does not exist in the given raster, RS_BandPixelType throws an IllegalArgumentException.</p> <p>Following are the possible values returned by RS_BandPixelType:</p> <ol> <li><code>REAL_64BITS</code> - For Double values</li> <li><code>REAL_32BITS</code> - For Float values</li> <li><code>SIGNED_32BITS</code> - For Integer values</li> <li><code>SIGNED_16BITS</code> - For Short values</li> <li><code>UNSIGNED_16BITS</code> - For unsigned Short values</li> <li><code>UNSIGNED_8BITS</code> - For Byte values</li> </ol> <p>Format: <code>RS_BandPixelType(rast: Raster, band: Integer = 1)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_BandPixelType(RS_MakeEmptyRaster(2, \"D\", 5, 5, 53, 51, 1, 1, 0, 0, 0), 2);\n</code></pre> <p>Output:</p> <pre><code>REAL_64BITS\n</code></pre> <pre><code>SELECT RS_BandPixelType(RS_MakeEmptyRaster(2, \"I\", 5, 5, 53, 51, 1, 1, 0, 0, 0));\n</code></pre> <p>Output:</p> <pre><code>SIGNED_32BITS\n</code></pre> <pre><code>SELECT RS_BandPixelType(RS_MakeEmptyRaster(2, \"I\", 5, 5, 53, 51, 1, 1, 0, 0, 0), 3);\n</code></pre> <p>Output:</p> <pre><code>IllegalArgumentException: Provided band index 3 is not present in the raster\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_count","title":"RS_Count","text":"<p>Introduction: Returns the number of pixels in a given band. If band is not specified then it defaults to <code>1</code>.</p> <p>Note</p> <p>If excludeNoDataValue is set <code>true</code> then it will only count pixels with value not equal to the nodata value of the raster. Set excludeNoDataValue to <code>false</code> to get count of all pixels in raster.</p> <p>Note</p> <p>If the mentioned band index doesn't exist, this will throw an <code>IllegalArgumentException</code>.</p> <p>Format:</p> <p><code>RS_Count(raster: Raster, band: Integer = 1, excludeNoDataValue: Boolean = true)</code></p> <p><code>RS_Count(raster: Raster, band: Integer = 1)</code></p> <p><code>RS_Count(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_Count(RS_MakeEmptyRaster(2, 5, 5, 0, 0, 1, -1, 0, 0, 0), 1, false)\n</code></pre> <p>Output:</p> <pre><code>25\n</code></pre> <p>Spark SQL Example:</p> <pre><code>SELECT RS_Count(RS_MakeEmptyRaster(2, 5, 5, 0, 0, 1, -1, 0, 0, 0), 1)\n</code></pre> <p>Output:</p> <pre><code>6\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_summarystats","title":"RS_SummaryStats","text":"<p>Introduction: Returns summary stats consisting of count, sum, mean, stddev, min, max for a given band in raster. If band is not specified then it defaults to <code>1</code>.</p> <p>Note</p> <p>If excludeNoDataValue is set <code>true</code> then it will only count pixels with value not equal to the nodata value of the raster. Set excludeNoDataValue to <code>false</code> to get count of all pixels in raster.</p> <p>Note</p> <p>If the mentioned band index doesn't exist, this will throw an <code>IllegalArgumentException</code>.</p> <p><code>RS_SummaryStats(raster: Raster, band: Integer = 1, excludeNoDataValue: Boolean = true)</code></p> <p><code>RS_SummaryStats(raster: Raster, band: Integer = 1)</code></p> <p><code>RS_SummaryStats(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_SummaryStats(RS_MakeEmptyRaster(2, 5, 5, 0, 0, 1, -1, 0, 0, 0), 1, false)\n</code></pre> <p>Output:</p> <pre><code>25.0, 204.0, 8.16, 9.4678403028357, 0.0, 25.0\n</code></pre> <p>Spark SQL Example:</p> <pre><code>SELECT RS_SummaryStats(RS_MakeEmptyRaster(2, 5, 5, 0, 0, 1, -1, 0, 0, 0), 1)\n</code></pre> <p>Output:</p> <pre><code>14.0, 204.0, 14.571428571428571, 11.509091348732502, 1.0, 25.0\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_zonalstats","title":"RS_ZonalStats","text":"<p>Introduction: This returns a statistic value specified by <code>statType</code> over the region of interest defined by <code>zone</code>. It computes the statistic from the pixel values within the ROI geometry and returns the result. If the <code>excludeNoData</code> parameter is not specified, it will default to <code>true</code>. This excludes NoData values from the statistic calculation. Additionally, if the <code>band</code> parameter is not provided, band 1 will be used by default for the statistic computation. The valid options for <code>statType</code> are:</p> <ul> <li><code>count</code>: Number of pixels in the region.</li> <li><code>sum</code>: Sum of pixel values.</li> <li><code>mean|average|avg</code>: Arithmetic mean.</li> <li><code>median</code>: Middle value in the region.</li> <li><code>mode</code>: Most occurring value, if there are multiple values with same occurrence then will return the largest number.</li> <li><code>stddev|sd</code>: Standard deviation.</li> <li><code>variance</code>: Variance.</li> <li><code>min</code>: Minimum value in the region.</li> <li><code>max</code>: Maximum value in the region.</li> </ul> <p>Note</p> <p>If the coordinate reference system (CRS) of the input <code>zone</code> geometry differs from that of the <code>raster</code>, then <code>zone</code> will be transformed to match the CRS of the <code>raster</code> before computation.</p> <p>The following conditions will throw an <code>IllegalArgumentException</code> if they are not met:</p> <ul> <li>The provided <code>raster</code> and <code>zone</code> geometry should intersect.</li> <li>The option provided to <code>statType</code> should be valid.</li> </ul> <p>Format:</p> <pre><code>RS_ZonalStats(raster: Raster, zone: Geometry, band: Integer, statType: String, excludeNoData: Boolean)\n</code></pre> <pre><code>RS_ZonalStats(raster: Raster, zone: Geometry, band: Integer, statType: String)\n</code></pre> <pre><code>RS_ZonalStats(raster: Raster, zone: Geometry, statType: String)\n</code></pre> <p>Since: <code>v1.5.1</code></p> <p>Spark SQL Example:</p> <pre><code>RS_ZonalStats(rast1, geom1, 1, 'sum', false)\n</code></pre> <p>Output:</p> <pre><code>10690406\n</code></pre> <p>Spark SQL Example:</p> <pre><code>RS_ZonalStats(rast2, geom2, 1, 'mean', true)\n</code></pre> <p>Output:</p> <pre><code>226.55992667794473\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_zonalstatsall","title":"RS_ZonalStatsAll","text":"<p>Introduction: Returns an array of statistic values, where each statistic is computed over a region defined by the <code>zone</code> geometry. The array contains the following values:</p> <ul> <li>0: Count of the pixels.</li> <li>1: Sum of the pixel values.</li> <li>2: Arithmetic mean.</li> <li>3: Median.</li> <li>4: Mode.</li> <li>5: Standard deviation.</li> <li>6: Variance.</li> <li>7: Minimum value of the zone.</li> <li>8: Maximum value of the zone.</li> </ul> <p>Note</p> <p>If the coordinate reference system (CRS) of the input <code>zone</code> geometry differs from that of the <code>raster</code>, then <code>zone</code> will be transformed to match the CRS of the <code>raster</code> before computation.</p> <p>The following conditions will throw an <code>IllegalArgumentException</code> if they are not met:</p> <ul> <li>The provided <code>raster</code> and <code>zone</code> geometry should intersect.</li> <li>The option provided to <code>statType</code> should be valid.</li> </ul> <p>Format:</p> <pre><code>RS_ZonalStatsAll(raster: Raster, zone: Geometry, band: Integer, excludeNodata: Boolean)\n</code></pre> <pre><code>RS_ZonalStatsAll(raster: Raster, zone: Geometry, band: Integer)\n</code></pre> <pre><code>RS_ZonalStatsAll(raster: Raster, zone: Geometry)\n</code></pre> <p>Since: <code>v1.5.1</code></p> <p>Spark SQL Example:</p> <pre><code>RS_ZonalStatsAll(rast1, geom1, 1, false)\n</code></pre> <p>Output:</p> <pre><code>[184792.0, 1.0690406E7, 57.851021689230684, 0.0, 0.0, 92.13277429243035, 8488.448098819916, 0.0, 255.0]\n</code></pre> <p>Spark SQL Example:</p> <pre><code>RS_ZonalStatsAll(rast2, geom2, 1, true)\n</code></pre> <p>Output:</p> <pre><code>[14184.0, 3213526.0, 226.55992667794473, 255.0, 255.0, 74.87605357255357, 5606.423398599913, 1.0, 255.0]\n</code></pre>"},{"location":"api/sql/Raster-operators/#raster-predicates","title":"Raster Predicates","text":""},{"location":"api/sql/Raster-operators/#rs_contains","title":"RS_Contains","text":"<p>Introduction: Returns true if the geometry or raster on the left side contains the geometry or raster on the right side. The convex hull of the raster is considered in the test.</p> <p>The rules for testing spatial relationship is the same as <code>RS_Intersects</code>.</p> <p>Format:</p> <p><code>RS_Contains(raster: Raster, geom: Geometry)</code></p> <p><code>RS_Contains(geom: Geometry, raster: Raster)</code></p> <p><code>RS_Contains(raster0: Raster, raster1: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_Contains(RS_MakeEmptyRaster(1, 20, 20, 2, 22, 1), ST_GeomFromWKT('POLYGON ((5 5, 5 10, 10 10, 10 5, 5 5))')) rast_geom,\nRS_Contains(RS_MakeEmptyRaster(1, 20, 20, 2, 22, 1), RS_MakeEmptyRaster(1, 10, 10, 2, 22, 1)) rast_rast\n</code></pre> <p>Output: <pre><code>+---------+---------+\n|rast_geom|rast_rast|\n+---------+---------+\n|     true|     true|\n+---------+---------+\n</code></pre></p>"},{"location":"api/sql/Raster-operators/#rs_intersects","title":"RS_Intersects","text":"<p>Introduction: Returns true if raster or geometry on the left side intersects with the raster or geometry on the right side. The convex hull of the raster is considered in the test.</p> <p>Rules for testing spatial relationship:</p> <ul> <li>If the raster or geometry does not have a defined SRID, it is assumed to be in WGS84.</li> <li>If both sides are in the same CRS, then perform the relationship test directly.</li> <li>Otherwise, both sides will be transformed to WGS84 before the relationship test.</li> </ul> <p>Format:</p> <p><code>RS_Intersects(raster: Raster, geom: Geometry)</code></p> <p><code>RS_Intersects(geom: Geometry, raster: Raster)</code></p> <p><code>RS_Intersects(raster0: Raster, raster1: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_Intersects(RS_MakeEmptyRaster(1, 20, 20, 2, 22, 1), ST_SetSRID(ST_PolygonFromEnvelope(0, 0, 10, 10), 4326)) rast_geom,\nRS_Intersects(RS_MakeEmptyRaster(1, 20, 20, 2, 22, 1), RS_MakeEmptyRaster(1, 10, 10, 1, 11, 1)) rast_rast\n</code></pre> <p>Output:</p> <pre><code>+---------+---------+\n|rast_geom|rast_rast|\n+---------+---------+\n|     true|     true|\n+---------+---------+\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_within","title":"RS_Within","text":"<p>Introduction: Returns true if the geometry or raster on the left side is within the geometry or raster on the right side. The convex hull of the raster is considered in the test.</p> <p>The rules for testing spatial relationship is the same as <code>RS_Intersects</code>.</p> <p>Format: <code>RS_Within(raster: Raster, geom: Geometry)</code></p> <p>Format: <code>RS_Within(geom: Geometry, raster: Raster)</code></p> <p>Format: <code>RS_Within(raster0: Raster, raster1: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_Within(RS_MakeEmptyRaster(1, 20, 20, 2, 22, 1), ST_GeomFromWKT('POLYGON ((0 0, 0 50, 100 50, 100 0, 0 0))')) rast_geom,\nRS_Within(RS_MakeEmptyRaster(1, 20, 20, 2, 22, 1), RS_MakeEmptyRaster(1, 30, 30, 2, 22, 1)) rast_rast\n</code></pre> <p>Output: <pre><code>+---------+---------+\n|rast_geom|rast_rast|\n+---------+---------+\n|     true|     true|\n+---------+---------+\n</code></pre></p>"},{"location":"api/sql/Raster-operators/#raster-based-operators","title":"Raster Based Operators","text":""},{"location":"api/sql/Raster-operators/#rs_addband","title":"RS_AddBand","text":"<p>Introduction: Adds a new band to a raster <code>toRaster</code> at a specified index <code>toRasterIndex</code>. The new band's values are copied from <code>fromRaster</code> at a specified band index <code>fromBand</code>. If no <code>toRasterIndex</code> is provided, the new band is appended to the end of <code>toRaster</code>. If no <code>fromBand</code> is specified, band <code>1</code> from <code>fromRaster</code> is copied by default.</p> <p>Note</p> <p>IllegalArgumentException will be thrown in these cases:</p> <ul> <li>The provided Rasters, <code>toRaster</code> &amp; <code>fromRaster</code> don't have same shape.</li> <li>The provided <code>fromBand</code> is not in <code>fromRaster</code>.</li> <li>The provided <code>toRasterIndex</code> is not in or at end of <code>toRaster</code>.</li> </ul> <p>Format:</p> <pre><code>RS_AddBand(toRaster: Raster, fromRaster: Raster, fromBand: Integer = 1, toRasterIndex: Integer = at_end)\n</code></pre> <pre><code>RS_AddBand(toRaster: Raster, fromRaster: Raster, fromBand: Integer = 1)\n</code></pre> <pre><code>RS_AddBand(toRaster: Raster, fromRaster: Raster)\n</code></pre> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_AddBand(raster1, raster2, 2, 1) FROM rasters\n</code></pre> <p>Output:</p> <pre><code>GridCoverage2D[\"g...\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_clip","title":"RS_Clip","text":"<p>Introduction: Returns a raster that is clipped by the given geometry.</p> <p>If <code>crop</code> is not specified then it will default to <code>true</code>, meaning it will make the resulting raster shrink to the geometry's extent and if <code>noDataValue</code> is not specified then the resulting raster will have the minimum possible value for the band pixel data type.</p> <p>Note</p> <p>Since <code>v1.5.1</code>, if the coordinate reference system (CRS) of the input <code>geom</code> geometry differs from that of the <code>raster</code>, then <code>geom</code> will be transformed to match the CRS of the <code>raster</code>. If the <code>raster</code> or <code>geom</code> doesn't have a CRS then it will default to <code>4326/WGS84</code>.</p> <p>Format:</p> <pre><code>RS_Clip(raster: Raster, band: Integer, geom: Geometry, noDataValue: Double, crop: Boolean)\n</code></pre> <pre><code>RS_Clip(raster: Raster, band: Integer, geom: Geometry, noDataValue: Double)\n</code></pre> <pre><code>RS_Clip(raster: Raster, band: Integer, geom: Geometry)\n</code></pre> <p>Since: <code>v1.5.1</code></p> <p>Original Raster:</p> <p></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_Clip(\nRS_FromGeoTiff(content), 1,\nST_GeomFromWKT('POLYGON ((236722 4204770, 243900 4204770, 243900 4197590, 221170 4197590, 236722 4204770))'),\n200, true\n)\n</code></pre> <p>Output:</p> <p></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_Clip(\nRS_FromGeoTiff(content), 1,\nST_GeomFromWKT('POLYGON ((236722 4204770, 243900 4204770, 243900 4197590, 221170 4197590, 236722 4204770))'),\n200, false\n)\n</code></pre> <p>Output:</p> <p></p>"},{"location":"api/sql/Raster-operators/#rs_metadata","title":"RS_MetaData","text":"<p>Introduction: Returns the metadata of the raster as an array of double. The array contains the following values:</p> <ul> <li>0: upper left x coordinate of the raster, in terms of CRS units</li> <li>1: upper left y coordinate of the raster, in terms of CRS units</li> <li>2: width of the raster, in terms of pixels</li> <li>3: height of the raster, in terms of pixels</li> <li>4: width of a pixel, in terms of CRS units (scaleX)</li> <li>5: height of a pixel, in terms of CRS units (scaleY), may be negative</li> <li>6: skew in x direction (rotation x)</li> <li>7: skew in y direction (rotation y)</li> <li>8: srid of the raster</li> <li>9: number of bands</li> </ul> <p>Format: <code>RS_MetaData (raster: Raster)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_MetaData(raster) FROM raster_table\n</code></pre> <p>Output:</p> <pre><code>[-1.3095817809482181E7, 4021262.7487925636, 512.0, 517.0, 72.32861272132695, -72.32861272132695, 0.0, 0.0, 3857.0, 1.0]\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_numbands","title":"RS_NumBands","text":"<p>Introduction: Returns the number of the bands in the raster.</p> <p>Format: <code>RS_NumBands (raster: Raster)</code></p> <p>Since: <code>v1.4.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_NumBands(raster) FROM raster_table\n</code></pre> <p>Output:</p> <pre><code>4\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_resample","title":"RS_Resample","text":"<p>Introduction: Resamples a raster using a given resampling algorithm and new dimensions (width and height), a new grid corner to pivot the raster at (gridX and gridY) and a set of georeferencing attributes (scaleX and scaleY).</p> <p>RS_Resample also provides an option to pass a reference raster to draw the georeferencing attributes out of. However, the SRIDs of the input and reference raster must be same, otherwise RS_Resample throws an IllegalArgumentException.</p> <p>For the purpose of resampling, width-height pair and scaleX-scaleY pair are mutually exclusive, meaning any one of them can be used at a time.</p> <p>The <code>useScale</code> parameter controls whether to use width-height or scaleX-scaleY. If <code>useScale</code> is false, the provided <code>widthOrScale</code> and <code>heightOrScale</code> values will be floored to integers and considered as width and height respectively (floating point width and height are not allowed). Otherwise, they are considered as scaleX and scaleY respectively.</p> <p>Currently, RS_Resample does not support skewed rasters, and hence even if a skewed reference raster is provided, its skew values are ignored. If the input raster is skewed, the output raster geometry and interpolation may be incorrect.</p> <p>The default algorithm used for resampling is <code>NearestNeighbor</code>, and hence if a null, empty or invalid value of algorithm is provided, RS_Resample defaults to using <code>NearestNeighbor</code>. However, the algorithm parameter is non-optional.</p> <p>Following are valid values for the algorithm parameter (Case-insensitive):</p> <ol> <li>NearestNeighbor</li> <li>Bilinear</li> <li>Bicubic</li> </ol> <p>Tip</p> <p>If you just want to resize or rescale an input raster, you can use RS_Resample(raster: Raster, widthOrScale: Double, heightOrScale: Double, useScale: Boolean, algorithm: String)</p> <p>Format:</p> <pre><code>RS_Resample(raster: Raster, widthOrScale: Double, heightOrScale: Double, gridX: Double, gridY: Double, useScale: Boolean, algorithm: String)\n</code></pre> <pre><code>RS_Resample(raster: Raster, widthOrScale: Double, heightOrScale: Double, useScale: Boolean, algorithm: String)\n</code></pre> <pre><code>RS_Resample(raster: Raster, referenceRaster: Raster, useScale: Boolean, algorithm: String)\n</code></pre> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>WITH INPUT_RASTER AS (\nSELECT RS_AddBandFromArray(\nRS_MakeEmptyRaster(1, 'd', 4, 3, 0, 0, 2, -2, 0, 0, 0),\nARRAY(1, 2, 3, 5, 4, 5, 6, 9, 7, 8, 9, 10), 1, null) as rast\n),\nRESAMPLED_RASTER AS (\nSELECT RS_Resample(rast, 6, 5, 1, -1, false, null) as resample_rast from INPUT_RASTER\n)\nSELECT RS_AsMatrix(resample_rast) as rast_matrix, RS_Metadata(resample_rast) as rast_metadata from RESAMPLED_RASTER\n</code></pre> <p>Output:</p> <pre><code>| 1.0   1.0   2.0   3.0   3.0   5.0|\n| 1.0   1.0   2.0   3.0   3.0   5.0|\n| 4.0   4.0   5.0   6.0   6.0   9.0|\n| 7.0   7.0   8.0   9.0   9.0  10.0|\n| 7.0   7.0   8.0   9.0   9.0  10.0|\n\n(-0.33333333333333326,0.19999999999999996,6,5,1.388888888888889,-1.24,0,0,0,1)\n</code></pre> <p>Spark SQL Example:</p> <pre><code> WITH INPUT_RASTER AS (\nSELECT RS_AddBandFromArray(\nRS_MakeEmptyRaster(1, 'd', 4, 3, 0, 0, 2, -2, 0, 0, 0),\nARRAY(1, 2, 3, 5, 4, 5, 6, 9, 7, 8, 9, 10), 1, null) as rast\n),\nRESAMPLED_RASTER AS (\nSELECT RS_Resample(rast, 1.2, -1.4, true, null) as resample_rast from INPUT_RASTER\n)\nSELECT RS_AsMatrix(resample_rast) as rast_matrix, RS_Metadata(resample_rast) as rast_metadata from RESAMPLED_RASTER\n</code></pre> <p>Output:</p> <pre><code>|       NaN         NaN         NaN         NaN         NaN         NaN         NaN|\n|       NaN    3.050000    3.650000    4.250000    5.160000    6.690000    7.200000|\n|       NaN    5.150000    5.750000    6.350000    7.250000    8.750000    9.250000|\n|       NaN    7.250000    7.850000    8.450000    9.070000    9.730000    9.950000|\n|       NaN    7.400000    8.000000    8.600000    9.200000    9.800000   10.000000|\n\n(0.0, 0.0, 7.0, 5.0, 1.2, -1.4, 0.0, 0.0, 0.0, 1.0)\n</code></pre> <p>Spark SQL Example:</p> <pre><code>WITH INPUT_RASTER AS (\nSELECT RS_AddBandFromArray(RS_MakeEmptyRaster(1, 'd', 4, 3, 0, 0, 2, -2, 0, 0, 0), ARRAY(1, 2, 3, 5, 4, 5, 6, 9, 7, 8, 9, 10), 1, null) as rast\n),\nREF_RASTER AS (\nSELECT RS_MakeEmptyRaster(2, 'd', 6, 5, 1, -1, 1.2, -1.4, 0, 0, 0) as ref_rast\n),\nRESAMPLED_RASTER AS (\nSELECT RS_Resample(rast, ref_rast, true, null) as resample_rast from INPUT_RASTER, REF_RASTER\n)\nSELECT RS_AsMatrix(resample_rast) as rast_matrix, RS_Metadata(resample_rast) as rast_metadata from RESAMPLED_RASTER\n</code></pre> <p>Output:</p> <pre><code>| 1.0   1.0   2.0   3.0   3.0   5.0   5.0|\n| 1.0   1.0   2.0   3.0   3.0   5.0   5.0|\n| 4.0   4.0   5.0   6.0   6.0   9.0   9.0|\n| 7.0   7.0   8.0   9.0   9.0  10.0  10.0|\n| 7.0   7.0   8.0   9.0   9.0  10.0  10.0|\n\n(-0.20000000298023224, 0.4000000059604645, 7.0, 5.0, 1.2, -1.4, 0.0, 0.0, 0.0, 1.0)\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_setbandnodatavalue","title":"RS_SetBandNoDataValue","text":"<p>Introduction: This sets the no data value for a specified band in the raster. If the band index is not provided, band 1 is assumed by default. Passing a <code>null</code> value for <code>noDataValue</code> will remove the no data value and that will ensure all pixels are included in functions rather than excluded as no data.</p> <p>Since <code>v1.5.1</code>, this function supports the ability to replace the current no-data value with the new <code>noDataValue</code>.</p> <p>Note</p> <p>When <code>replace</code> is true, any pixels matching the provided <code>noDataValue</code> will be considered as no-data in the output raster.</p> <p>An <code>IllegalArgumentException</code> will be thrown if the input raster does not already have a no-data value defined. Replacing existing values with <code>noDataValue</code> requires a defined no-data baseline to evaluate against.</p> <p>To use this for no-data replacement, the input raster must first set its no-data value, which can then be selectively replaced via this function.</p> <p>Format:</p> <pre><code>RS_SetBandNoDataValue(raster: Raster, bandIndex: Integer, noDataValue: Double, replace: Boolean)\n</code></pre> <pre><code>RS_SetBandNoDataValue(raster: Raster, bandIndex: Integer = 1, noDataValue: Double)\n</code></pre> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_BandNoDataValue(\nRS_SetBandNoDataValue(\nRS_MakeEmptyRaster(1, 20, 20, 2, 22, 2, 3, 1, 1, 0),\n-999\n)\n)\n</code></pre> <p>Output:</p> <pre><code>-999\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_setgeoreference","title":"RS_SetGeoReference","text":"<p>Introduction: Sets the Georeference information of an object in a single call. Accepts inputs in <code>GDAL</code> and <code>ESRI</code> format. Default format is <code>GDAL</code>. If all 6 parameters are not provided then will return null.</p> <p>Format:</p> <pre><code>RS_SetGeoReference(raster: Raster, geoRefCoord: String, format: String = \"GDAL\")\n</code></pre> <pre><code>RS_SetGeoReference(raster: Raster, upperLeftX: Double, upperLeftY: Double, scaleX: Double, scaleY: Double, skewX: Double, skewY: Double)\n</code></pre> <p>Since: <code>v1.5.0</code></p> <p>Difference between format representation is as follows:</p> <p><code>GDAL</code></p> <pre><code>ScaleX SkewY SkewX ScaleY UpperLeftX UpperLeftY\n</code></pre> <p><code>ESRI</code></p> <pre><code>ScaleX SkewY SkewX ScaleY (UpperLeftX + ScaleX * 0.5) (UpperLeftY + ScaleY * 0.5)\n</code></pre> <p>Spark SQL Example:</p> <pre><code>SELECT RS_GeoReference(\nRS_SetGeoReference(\nRS_MakeEmptyRaster(1, 20, 20, 2, 22, 2, 3, 1, 1, 0),\n'3 1.5 1.5 2 22 3'\n)\n)\n</code></pre> <p>Output:</p> <pre><code>3.000000\n1.500000\n1.500000\n2.000000\n22.000000\n3.000000\n</code></pre> <p>Spark SQL Example:</p> <pre><code>SELECT RS_GeoReference(\nRS_SetGeoReference(\nRS_MakeEmptyRaster(1, 20, 20, 2, 22, 2, 3, 1, 1, 0),\n'3 1.5 1.5 2 22 3', 'ESRI'\n)\n)\n</code></pre> <p>Output:</p> <pre><code>3.000000\n1.500000\n1.500000\n2.000000\n20.500000\n2.000000\n</code></pre> <p>Spark SQL Example:</p> <pre><code>SELECT RS_GeoReference(\nRS_SetGeoReference(\nRS_MakeEmptyRaster(2, 5, 5, 0, 0, 1, -1, 0, 0, 0),\n8, -3, 4, 5, 0.2, 0.2\n)\n)\n</code></pre> <p>Output:</p> <pre><code>4.000000\n0.200000\n0.200000\n5.000000\n8.000000\n-3.000000\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_setvalue","title":"RS_SetValue","text":"<p>Introduction: Returns a raster by replacing the value of pixel specified by <code>colX</code> and <code>rowY</code>.</p> <p>Format:</p> <pre><code>RS_SetValue(raster: Raster, bandIndex: Integer = 1, colX: Integer, rowY: Integer, newValue: Double)\n</code></pre> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_BandAsArray(\nRS_SetValue(\nRS_AddBandFromArray(\nRS_MakeEmptyRaster(1, 5, 5, 0, 0, 1, -1, 0, 0, 0),\n[1,1,1,0,0,0,1,2,3,3,5,6,7,0,0,3,0,0,3,0,0,0,0,0,0], 1, 0d\n),\n1, 2, 2, 255\n)\n)\n</code></pre> <p>Output:</p> <pre><code>[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 255.0, 2.0, 3.0, 3.0, 5.0, 6.0, 7.0, 0.0, 0.0, 3.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_setvalues","title":"RS_SetValues","text":"<p>Introduction: Returns a raster by replacing the values of pixels in a specified rectangular region. The top left corner of the region is defined by the <code>colX</code> and <code>rowY</code> coordinates. The <code>width</code> and <code>height</code> parameters specify the dimensions of the rectangular region. The new values to be assigned to the pixels in this region can be specified as an array passed to this function.</p> <p>Note</p> <p>Since <code>v1.5.1</code>, if the coordinate reference system (CRS) of the input <code>geom</code> geometry differs from that of the <code>raster</code>, then <code>geom</code> will be transformed to match the CRS of the <code>raster</code>. If the <code>raster</code> or <code>geom</code> doesn't have a CRS then it will default to <code>4326/WGS84</code>.</p> <p>Format:</p> <pre><code>RS_SetValues(raster: Raster, bandIndex: Integer, colX: Integer, rowY: Integer, width: Integer, height: Integer, newValues: ARRAY[Double], keepNoData: Boolean = false)\n</code></pre> <pre><code>RS_SetValues(raster: Raster, bandIndex: Integer, geom: Geometry, newValue: Double, keepNoData: Boolean = false)\n</code></pre> <p>Since: <code>v1.5.0</code></p> <p>The <code>colX</code>, <code>rowY</code>, and <code>bandIndex</code> are 1-indexed. If <code>keepNoData</code> is <code>true</code>, the pixels with NoData value will not be set to the corresponding value in <code>newValues</code>. The <code>newValues</code> should be provided in rows.</p> <p>The geometry variant of this function accepts all types of Geometries and it sets the <code>newValue</code> in the specified region under the <code>geom</code>.</p> <p>Note</p> <p>If the shape of <code>newValues</code> doesn't match with provided <code>width</code> and <code>height</code>, <code>IllegalArgumentException</code> is thrown.</p> <p>Note</p> <p>If the mentioned <code>bandIndex</code> doesn't exist, this will throw an <code>IllegalArgumentException</code>.</p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_BandAsArray(\nRS_SetValues(\nRS_AddBandFromArray(\nRS_MakeEmptyRaster(1, 5, 5, 0, 0, 1, -1, 0, 0, 0),\nArray(1,1,1,0,0,0,1,2,3,3,5,6,7,0,0,3,0,0,3,0,0,0,0,0,0), 1, 0d\n),\n1, 2, 2, 3, 3, [11,12,13,14,15,16,17,18,19]\n)\n)\n</code></pre> <p>Output:</p> <pre><code>Array(1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 11.0, 12.0, 13.0, 3.0, 5.0, 14.0, 15.0, 16.0, 0.0, 3.0, 17.0, 18.0, 19.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n</code></pre> <p>Spark SQL Example:</p> <pre><code>SELECT RS_BandAsArray(\nRS_SetValues(\nRS_AddBandFromArray(\nRS_MakeEmptyRaster(1, 5, 5, 1, -1, 1, -1, 0, 0, 0),\nArray(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0), 1\n),\n1, ST_GeomFromWKT('POLYGON((1 -1, 3 -3, 6 -6, 4 -1, 1 -1))'), 255, false\n)\n)\n</code></pre> <p>Output:</p> <pre><code>Array(255.0, 255.0, 255.0, 0.0, 0.0, 0.0, 255.0, 255.0, 255.0, 0.0, 0.0, 0.0, 255.0, 255.0, 0.0, 0.0, 0.0, 0.0, 255.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_setsrid","title":"RS_SetSRID","text":"<p>Introduction: Sets the spatial reference system identifier (SRID) of the raster geometry.</p> <p>Format: <code>RS_SetSRID (raster: Raster, srid: Integer)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_SetSRID(raster, 4326)\nFROM raster_table\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_srid","title":"RS_SRID","text":"<p>Introduction: Returns the spatial reference system identifier (SRID) of the raster geometry.</p> <p>Format: <code>RS_SRID (raster: Raster)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_SRID(raster) FROM raster_table\n</code></pre> <p>Output:</p> <pre><code>3857\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_value","title":"RS_Value","text":"<p>Introduction: Returns the value at the given point in the raster. If no band number is specified it defaults to 1.</p> <p>Note</p> <p>Since <code>v1.5.1</code>, if the coordinate reference system (CRS) of the input <code>point</code> geometry differs from that of the <code>raster</code>, then <code>point</code> will be transformed to match the CRS of the <code>raster</code>. If the <code>raster</code> or <code>point</code> doesn't have a CRS then it will default to <code>4326/WGS84</code>.</p> <p>Format:</p> <p><code>RS_Value (raster: Raster, point: Geometry)</code></p> <p><code>RS_Value (raster: Raster, point: Geometry, band: Integer)</code></p> <p><code>RS_Value (raster: Raster, colX: Integer, colY: Integer, band: Integer)</code></p> <p>Since: <code>v1.4.0</code></p> <p>Spark SQL Examples:</p> <ul> <li>For Point Geometry:</li> </ul> <pre><code>SELECT RS_Value(raster, ST_Point(-13077301.685, 4002565.802)) FROM raster_table\n</code></pre> <ul> <li>For Grid Coordinates:</li> </ul> <pre><code>SELECT RS_Value(raster, 3, 4, 1) FROM raster_table\n</code></pre> <p>Output:</p> <pre><code>5.0\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_values","title":"RS_Values","text":"<p>Introduction: Returns the values at the given points or grid coordinates in the raster. If no band number is specified it defaults to 1.</p> <p>RS_Values is similar to RS_Value but operates on an array of points or grid coordinates. RS_Values can be significantly faster since a raster only has to be loaded once for several points.</p> <p>Note</p> <p>Since <code>v1.5.1</code>, if the coordinate reference system (CRS) of the input <code>points</code> geometries differs from that of the <code>raster</code>, then <code>points</code> will be transformed to match the CRS of the <code>raster</code>. If the <code>raster</code> or <code>points</code> doesn't have a CRS then it will default to <code>4326/WGS84</code>.</p> <p>Format:</p> <p><code>RS_Values (raster: Raster, points: ARRAY[Geometry])</code></p> <p><code>RS_Values (raster: Raster, points: ARRAY[Geometry], band: Integer)</code></p> <pre><code>RS_Values (raster: Raster, xCoordinates: ARRAY[Integer], yCoordinates: ARRAY[Integer], band: Integer)\n</code></pre> <p>Since: <code>v1.4.0</code></p> <p>Spark SQL Example:</p> <ul> <li>For Array of Point geometries:</li> </ul> <pre><code>SELECT RS_Values(raster, Array(ST_Point(-1307.5, 400.8), ST_Point(-1403.3, 399.1)))\nFROM raster_table\n</code></pre> <ul> <li>For Arrays of grid coordinates:</li> </ul> <pre><code>SELECT RS_Values(raster, Array(4, 5), Array(3, 2), 1) FROM raster_table\n</code></pre> <p>Output:</p> <pre><code>Array(5.0, 3.0)\n</code></pre> <p>Spark SQL example for joining a point dataset with a raster dataset:</p> <pre><code>val pointDf = sedona.read...\nval rasterDf = sedona.read.format(\"binaryFile\").load(\"/some/path/*.tiff\")\n.withColumn(\"raster\", expr(\"RS_FromGeoTiff(content)\"))\n.withColumn(\"envelope\", expr(\"RS_Envelope(raster)\"))\n\n// Join the points with the raster extent and aggregate points to arrays.\n// We only use the path and envelope of the raster to keep the shuffle as small as possible.\nval df = pointDf.join(rasterDf.select(\"path\", \"envelope\"), expr(\"ST_Within(point_geom, envelope)\"))\n.groupBy(\"path\")\n.agg(collect_list(\"point_geom\").alias(\"point\"), collect_list(\"point_id\").alias(\"id\"))\n\ndf.join(rasterDf, \"path\")\n.selectExpr(\"explode(arrays_zip(id, point, RS_Values(raster, point))) as result\")\n.selectExpr(\"result.*\")\n.show()\n</code></pre> <p>Output:</p> <pre><code>+----+------------+-------+\n| id | point      | value |\n+----+------------+-------+\n|  4 | POINT(1 1) |   3.0 |\n|  5 | POINT(2 2) |   7.0 |\n+----+------------+-------+\n</code></pre>"},{"location":"api/sql/Raster-operators/#raster-tiles","title":"Raster Tiles","text":""},{"location":"api/sql/Raster-operators/#rs_tile","title":"RS_Tile","text":"<p>Introduction: Returns an array of rasters resulting from the split of the input raster based upon the desired dimensions of the output rasters.</p> <p>Format: <code>RS_Tile(raster: Raster, width: Int, height: Int, padWithNoData: Boolean = false, noDataVal: Double = null)</code></p> <p>Format: <code>RS_Tile(raster: Raster, bandIndices: Array[Int], width: Int, height: Int, padWithNoData: Boolean = false, noDataVal: Double = null)</code></p> <p>Since: <code>v1.5.1</code></p> <p><code>width</code> and <code>height</code> specifies the size of generated tiles. If <code>bandIndices</code> is NULL or not specified, all bands will be included in the output tiles, otherwise bands specified by <code>bandIndices</code> will be included. Band indices are 1-based.</p> <p>If <code>padWithNoData</code> = false, edge tiles on the right and bottom sides of the raster may have different dimensions than the rest of the tiles. If <code>padWithNoData</code> = true, all tiles will have the same dimensions with the possibility that edge tiles being padded with NODATA values. If raster band(s) do not have NODATA value(s) specified, one can be specified by setting <code>noDataVal</code>.</p> <p>SQL example:</p> <pre><code>WITH raster_table AS (SELECT RS_MakeEmptyRaster(1, 6, 6, 300, 400, 10) rast)\nSELECT RS_Tile(rast, 2, 2) AS tiles FROM raster_table\n</code></pre> <p>Output:</p> <pre><code>+--------------------+\n|               tiles|\n+--------------------+\n|[GridCoverage2D[\"...|\n+--------------------+\n</code></pre> <p>User can use <code>EXPLODE</code> function to expand the array of tiles into a table of tiles.</p> <pre><code>WITH raster_table AS (SELECT RS_MakeEmptyRaster(1, 6, 6, 300, 400, 10) rast)\nSELECT EXPLODE(RS_Tile(rast, 2, 2)) AS tile FROM raster_table\n</code></pre> <p>Output:</p> <pre><code>+--------------------+\n|                tile|\n+--------------------+\n|GridCoverage2D[\"g...|\n|GridCoverage2D[\"g...|\n|GridCoverage2D[\"g...|\n|GridCoverage2D[\"g...|\n|GridCoverage2D[\"g...|\n|GridCoverage2D[\"g...|\n|GridCoverage2D[\"g...|\n|GridCoverage2D[\"g...|\n|GridCoverage2D[\"g...|\n+--------------------+\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_tileexplode","title":"RS_TileExplode","text":"<p>Introduction: Generates records containing raster tiles resulting from the split of the input raster based upon the desired dimensions of the output rasters.</p> <p>Format: <code>RS_TileExplode(raster: Raster, width: Int, height: Int, padWithNoData: Boolean = false, noDataVal: Double = null)</code></p> <p>Format: <code>RS_TileExplode(raster: Raster, bandIndex: Int, width: Int, height: Int, padWithNoData: Boolean = false, noDataVal: Double = null)</code></p> <p>Format: <code>RS_TileExplode(raster: Raster, bandIndices: Array[Int], width: Int, height: Int, padWithNoData: Boolean = false, noDataVal: Double = null)</code></p> <p>Since: <code>v1.5.0</code></p> <p><code>width</code> and <code>height</code> specifies the size of generated tiles. If <code>bandIndices</code> is NULL or not specified, all bands will be included in the output tiles, otherwise bands specified by <code>bandIndices</code> will be included. <code>bandIndex</code> can be specified if there is only one selected band, which is equivalent to specifying <code>bandIndices</code> as <code>ARRAY(bandIndex)</code>.Band indices are 1-based.</p> <p>If <code>padWithNoData</code> = false, edge tiles on the right and bottom sides of the raster may have different dimensions than the rest of the tiles. If <code>padWithNoData</code> = true, all tiles will have the same dimensions with the possibility that edge tiles being padded with NODATA values. If raster band(s) do not have NODATA value(s) specified, one can be specified by setting <code>noDataVal</code>.</p> <p>The returned records have the following schema:</p> <ul> <li><code>x</code>: The index of the tile along X axis (0-based).</li> <li><code>y</code>: The index of the tile along Y axis (0-based).</li> <li><code>tile</code>: The tile.</li> </ul> <p>SQL example:</p> <pre><code>WITH raster_table AS (SELECT RS_MakeEmptyRaster(1, 6, 6, 300, 400, 10) rast)\nSELECT RS_TileExplode(rast, 2, 2) FROM raster_table\n</code></pre> <p>Output:</p> <pre><code>+---+---+--------------------+\n|  x|  y|                tile|\n+---+---+--------------------+\n|  0|  0|GridCoverage2D[\"g...|\n|  1|  0|GridCoverage2D[\"g...|\n|  2|  0|GridCoverage2D[\"g...|\n|  0|  1|GridCoverage2D[\"g...|\n|  1|  1|GridCoverage2D[\"g...|\n|  2|  1|GridCoverage2D[\"g...|\n|  0|  2|GridCoverage2D[\"g...|\n|  1|  2|GridCoverage2D[\"g...|\n|  2|  2|GridCoverage2D[\"g...|\n+---+---+--------------------+\n</code></pre>"},{"location":"api/sql/Raster-operators/#raster-to-map-algebra-operators","title":"Raster to Map Algebra Operators","text":"<p>To bridge the gap between the raster and map algebra worlds, the following operators are provided. These operators convert a raster to a map algebra object. The map algebra object can then be used with the map algebra operators described in the next section.</p>"},{"location":"api/sql/Raster-operators/#rs_bandasarray","title":"RS_BandAsArray","text":"<p>Introduction: Extract a band from a raster as an array of doubles.</p> <p>Format: <code>RS_BandAsArray (raster: Raster, bandIndex: Integer)</code>.</p> <p>Since: <code>v1.4.1</code></p> <p>BandIndex is 1-based and must be between 1 and RS_NumBands(raster). It returns null if the bandIndex is out of range or the raster is null.</p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_BandAsArray(raster, 1) FROM raster_table\n</code></pre> <p>Output:</p> <pre><code>+--------------------+\n|                band|\n+--------------------+\n|[0.0, 0.0, 0.0, 0...|\n+--------------------+\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_addbandfromarray","title":"RS_AddBandFromArray","text":"<p>Introduction: Add a band to a raster from an array of doubles.</p> <p>Format:</p> <p><code>RS_AddBandFromArray (raster: Raster, band: ARRAY[Double])</code></p> <p><code>RS_AddBandFromArray (raster: Raster, band: ARRAY[Double], bandIndex: Integer)</code></p> <p><code>RS_AddBandFromArray (raster: Raster, band: ARRAY[Double], bandIndex: Integer, noDataValue: Double)</code></p> <p>Since: <code>v1.5.0</code></p> <p>The bandIndex is 1-based and must be between 1 and RS_NumBands(raster) + 1. It throws an exception if the bandIndex is out of range or the raster is null. If not specified, the noDataValue of the band is assumed to be null.</p> <p>When the bandIndex is RS_NumBands(raster) + 1, it appends the band to the end of the raster. Otherwise, it replaces the existing band at the bandIndex.</p> <p>If the bandIndex and noDataValue is not given, a convenience implementation adds a new band with a null noDataValue.</p> <p>Adding a new band with a custom noDataValue requires bandIndex = RS_NumBands(raster) + 1 and non-null noDataValue to be explicitly specified.</p> <p>Modifying or Adding a customNoDataValue is also possible by giving an existing band in RS_AddBandFromArray</p> <p>In order to remove an existing noDataValue from an existing band, pass null as the noDataValue in the RS_AddBandFromArray.</p> <p>Note that: <code>bandIndex == RS_NumBands(raster) + 1</code> is an experimental feature and might lead to the loss of raster metadata and properties such as color models.</p> <p>Note</p> <p>RS_AddBandFromArray typecasts the double band values to the given datatype of the raster. This can lead to overflow values if values beyond the range of the raster's datatype are provided.</p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_AddBandFromArray(raster, RS_MultiplyFactor(RS_BandAsArray(RS_FromGeoTiff(content), 1), 2)) AS raster FROM raster_table\nSELECT RS_AddBandFromArray(raster, RS_MultiplyFactor(RS_BandAsArray(RS_FromGeoTiff(content), 1), 2), 1) AS raster FROM raster_table\nSELECT RS_AddBandFromArray(raster, RS_MultiplyFactor(RS_BandAsArray(RS_FromGeoTiff(content), 1), 2), 1, -999) AS raster FROM raster_table\n</code></pre> <p>Output: <pre><code>+--------------------+\n|              raster|\n+--------------------+\n|GridCoverage2D[\"g...|\n+--------------------+\n</code></pre></p>"},{"location":"api/sql/Raster-operators/#rs_mapalgebra","title":"RS_MapAlgebra","text":"<p>Introduction: Apply a map algebra script on a raster.</p> <p>Format:</p> <pre><code>RS_MapAlgebra (raster: Raster, pixelType: String, script: String)\n</code></pre> <pre><code>RS_MapAlgebra (raster: Raster, pixelType: String, script: String, noDataValue: Double)\n</code></pre> <pre><code>RS_MapAlgebra(rast0: Raster, rast1: Raster, pixelType: String, script: String, noDataValue: Double)\n</code></pre> <p>Since: <code>v1.5.0</code></p> <p><code>RS_MapAlgebra</code> runs a script on a raster. The script is written in a map algebra language called Jiffle. The script takes a raster as input and returns a raster of the same size as output. The script can be used to apply a map algebra expression on a raster. The input raster is named <code>rast</code> in the Jiffle script, and the output raster is named <code>out</code>.</p> <p>Spark SQL Example:</p> <p>Calculate the NDVI of a raster with 4 bands (R, G, B, NIR):</p> <pre><code>-- Assume that the input raster has 4 bands: R, G, B, NIR\n-- rast[0] refers to the R band, rast[3] refers to the NIR band.\nSELECT RS_MapAlgebra(rast, 'D', 'out = (rast[3] - rast[0]) / (rast[3] + rast[0]);') AS ndvi FROM raster_table\n</code></pre> <p>Output: <pre><code>+--------------------+\n|              raster|\n+--------------------+\n|GridCoverage2D[\"g...|\n+--------------------+\n</code></pre></p> <p>Spark SQL Example for two raster input <code>RS_MapAlgebra</code>:</p> <pre><code>RS_MapAlgebra(rast0, rast1, 'D', 'out = rast0[0] * 0.5 + rast1[0] * 0.5;', null)\n</code></pre> <p>For more details and examples about <code>RS_MapAlgebra</code>, please refer to the Map Algebra documentation. To learn how to write map algebra script, please refer to Jiffle language summary.</p>"},{"location":"api/sql/Raster-operators/#map-algebra-operators","title":"Map Algebra Operators","text":"<p>Map algebra operators work on a single band of a raster. Each band is represented as an array of doubles. The operators return an array of doubles.</p>"},{"location":"api/sql/Raster-operators/#rs_add","title":"RS_Add","text":"<p>Introduction: Add two spectral bands in a Geotiff image</p> <p>Format: <code>RS_Add (Band1: ARRAY[Double], Band2: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL Example:</p> <pre><code>val sumDF = spark.sql(\"select RS_Add(band1, band2) as sumOfBands from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_array","title":"RS_Array","text":"<p>Introduction: Create an array that is filled by the given value</p> <p>Format: <code>RS_Array(length: Integer, value: Double)</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_Array(height * width, 0.0)\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_bitwiseand","title":"RS_BitwiseAND","text":"<p>Introduction: Find Bitwise AND between two bands of Geotiff image</p> <p>Format: <code>RS_BitwiseAND (Band1: ARRAY[Double], Band2: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL Example:</p> <pre><code>val biwiseandDF = spark.sql(\"select RS_BitwiseAND(band1, band2) as andvalue from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_bitwiseor","title":"RS_BitwiseOR","text":"<p>Introduction: Find Bitwise OR between two bands of Geotiff image</p> <p>Format: <code>RS_BitwiseOR (Band1: ARRAY[Double], Band2: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL Example:</p> <pre><code>val biwiseorDF = spark.sql(\"select RS_BitwiseOR(band1, band2) as or from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_countvalue","title":"RS_CountValue","text":"<p>Introduction: Returns count of a particular value from a spectral band in a raster image</p> <p>Format: <code>RS_CountValue (Band1: ARRAY[Double], Target: Double)</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL Example:</p> <pre><code>val countDF = spark.sql(\"select RS_CountValue(band1, target) as count from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_divide","title":"RS_Divide","text":"<p>Introduction: Divide band1 with band2 from a geotiff image</p> <p>Format: <code>RS_Divide (Band1: ARRAY[Double], Band2: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL Example:</p> <pre><code>val multiplyDF = spark.sql(\"select RS_Divide(band1, band2) as divideBands from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_fetchregion","title":"RS_FetchRegion","text":"<p>Introduction: Fetch a subset of region from given Geotiff image based on minimumX, minimumY, maximumX and maximumY index as well original height and width of image</p> <p>Format:</p> <pre><code>RS_FetchRegion (Band: ARRAY[Double], coordinates: ARRAY[Integer], dimensions: ARRAY[Integer])\n</code></pre> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL Example:</p> <pre><code>val region = spark.sql(\"select RS_FetchRegion(Band,Array(0, 0, 1, 2),Array(3, 3)) as Region from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_greaterthan","title":"RS_GreaterThan","text":"<p>Introduction: Mask all the values with 1 which are greater than a particular target value</p> <p>Format: <code>RS_GreaterThan (Band: ARRAY[Double], Target: Double)</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL Example:</p> <pre><code>val greaterDF = spark.sql(\"select RS_GreaterThan(band, target) as maskedvalues from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_greaterthanequal","title":"RS_GreaterThanEqual","text":"<p>Introduction: Mask all the values with 1 which are greater than equal to a particular target value</p> <p>Format: <code>RS_GreaterThanEqual (Band: ARRAY[Double], Target: Double)</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL Example:</p> <pre><code>val greaterEqualDF = spark.sql(\"select RS_GreaterThanEqual(band, target) as maskedvalues from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_lessthan","title":"RS_LessThan","text":"<p>Introduction: Mask all the values with 1 which are less than a particular target value</p> <p>Format: <code>RS_LessThan (Band: ARRAY[Double], Target: Double)</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL Example:</p> <pre><code>val lessDF = spark.sql(\"select RS_LessThan(band, target) as maskedvalues from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_lessthanequal","title":"RS_LessThanEqual","text":"<p>Introduction: Mask all the values with 1 which are less than equal to a particular target value</p> <p>Format: <code>RS_LessThanEqual (Band: ARRAY[Double], Target: Double)</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL Example:</p> <pre><code>val lessEqualDF = spark.sql(\"select RS_LessThanEqual(band, target) as maskedvalues from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_logicaldifference","title":"RS_LogicalDifference","text":"<p>Introduction: Return value from band 1 if a value in band1 and band2 are different, else return 0</p> <p>Format: <code>RS_LogicalDifference (Band1: ARRAY[Double], Band2: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL Example:</p> <pre><code>val logicalDifference = spark.sql(\"select RS_LogicalDifference(band1, band2) as logdifference from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_logicalover","title":"RS_LogicalOver","text":"<p>Introduction: Return value from band1 if it's not equal to 0, else return band2 value</p> <p>Format: <code>RS_LogicalOver (Band1: ARRAY[Double], Band2: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL Example:</p> <pre><code>val logicalOver = spark.sql(\"select RS_LogicalOver(band1, band2) as logover from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_mean","title":"RS_Mean","text":"<p>Introduction: Returns Mean value for a spectral band in a Geotiff image</p> <p>Format: <code>RS_Mean (Band: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL Example:</p> <pre><code>val meanDF = spark.sql(\"select RS_Mean(band) as mean from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_mode","title":"RS_Mode","text":"<p>Introduction: Returns Mode from a spectral band in a Geotiff image in form of an array</p> <p>Format: <code>RS_Mode (Band: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL Example:</p> <pre><code>val modeDF = spark.sql(\"select RS_Mode(band) as mode from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_modulo","title":"RS_Modulo","text":"<p>Introduction: Find modulo of pixels with respect to a particular value</p> <p>Format: <code>RS_Modulo (Band: ARRAY[Double], Target: Double)</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL Example:</p> <pre><code>val moduloDF = spark.sql(\"select RS_Modulo(band, target) as modulo from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_multiply","title":"RS_Multiply","text":"<p>Introduction: Multiply two spectral bands in a Geotiff image</p> <p>Format: <code>RS_Multiply (Band1: ARRAY[Double], Band2: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL Example:</p> <pre><code>val multiplyDF = spark.sql(\"select RS_Multiply(band1, band2) as multiplyBands from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_multiplyfactor","title":"RS_MultiplyFactor","text":"<p>Introduction: Multiply a factor to a spectral band in a geotiff image</p> <p>Format: <code>RS_MultiplyFactor (Band1: ARRAY[Double], Factor: Double)</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL Example:</p> <pre><code>val multiplyFactorDF = spark.sql(\"select RS_MultiplyFactor(band1, 2) as multiplyfactor from dataframe\")\n</code></pre> <p>This function only accepts integer as factor before <code>v1.5.0</code>.</p>"},{"location":"api/sql/Raster-operators/#rs_normalize","title":"RS_Normalize","text":"<p>Introduction: Normalize the value in the array to [0, 255]</p> <p>Format: <code>RS_Normalize (Band: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_Normalize(band)\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_normalizeddifference","title":"RS_NormalizedDifference","text":"<p>Introduction: Returns Normalized Difference between two bands(band2 and band1) in a Geotiff image(example: NDVI, NDBI)</p> <p>Format: <code>RS_NormalizedDifference (Band1: ARRAY[Double], Band2: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL Example:</p> <pre><code>val normalizedDF = spark.sql(\"select RS_NormalizedDifference(band1, band2) as normdifference from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_squareroot","title":"RS_SquareRoot","text":"<p>Introduction: Find Square root of band values in a geotiff image</p> <p>Format: <code>RS_SquareRoot (Band: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL Example:</p> <pre><code>val rootDF = spark.sql(\"select RS_SquareRoot(band) as squareroot from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_subtract","title":"RS_Subtract","text":"<p>Introduction: Subtract two spectral bands in a Geotiff image(band2 - band1)</p> <p>Format: <code>RS_Subtract (Band1: ARRAY[Double], Band2: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL Example:</p> <pre><code>val subtractDF = spark.sql(\"select RS_Subtract(band1, band2) as differenceOfOfBands from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-visualizer/","title":"Raster visualization","text":"<p>Sedona offers some APIs to aid in easy visualization of a raster object.</p>"},{"location":"api/sql/Raster-visualizer/#image-based-visualization","title":"Image-based visualization","text":"<p>Sedona offers APIs to visualize a raster in an image form. This API only works for rasters with byte data, and bands &lt;= 4 (Grayscale - RGBA). You can check the data type of an existing raster by using RS_BandPixelType or create your own raster by passing 'B' while using RS_MakeEmptyRaster.</p>"},{"location":"api/sql/Raster-visualizer/#rs_asbase64","title":"RS_AsBase64","text":"<p>Introduction: Returns a base64 encoded string of the given raster. If the datatype is integral then this function internally takes the first 4 bands as RGBA, and converts them to the PNG format, finally produces a base64 string. When the datatype is not integral, the function converts the raster to TIFF format, and then generates a base64 string. To visualize other bands, please use it together with <code>RS_Band</code>. You can take the resulting base64 string in an online viewer to check how the image looks like.</p> <p>Warning</p> <p>This is not recommended for large files.</p> <p>Format: <code>RS_AsBase64(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_AsBase64(raster) from rasters\n</code></pre> <p>Output:</p> <pre><code>iVBORw0KGgoAAAA...\n</code></pre>"},{"location":"api/sql/Raster-visualizer/#rs_asimage","title":"RS_AsImage","text":"<p>Introduction: Returns a HTML that when rendered using an HTML viewer or via a Jupyter Notebook, displays the raster as a square image of side length <code>imageWidth</code>. Optionally, an imageWidth parameter can be passed to RS_AsImage in order to increase the size of the rendered image (default: 200).</p> <p>Format: <code>RS_AsImage(raster: Raster, imageWidth: Integer = 200)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_AsImage(raster, 500) from rasters\nSELECT RS_AsImage(raster) from rasters\n</code></pre> <p>Output:</p> <pre><code>\"&lt;img src=\\\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAUAAAAECAAAAABjWKqcAAAAIElEQVR42mPgPfGfkYUhhfcBNw+DT1KihS6DqLKztjcATWMFp9rkkJgAAAAASUVORK5CYII=\\\" width=\\\"200\\\" /&gt;\";\n</code></pre> <p>Tip</p> <p>RS_AsImage can be paired with SedonaUtils.display_image(df) wrapper inside a Jupyter notebook to directly print the raster as an image in the output, where the 'df' parameter is the dataframe containing the HTML data provided by RS_AsImage</p> <p>Example:</p> <pre><code>df = sedona.read.format('binaryFile').load(DATA_DIR + 'raster.tiff').selectExpr(\\\"RS_FromGeoTiff(content) as raster\\\")\nhtmlDF = df.selectExpr(\\\"RS_AsImage(raster, 500) as raster_image\\\")\nSedonaUtils.display_image(htmlDF)\n</code></pre> <p></p>"},{"location":"api/sql/Raster-visualizer/#text-based-visualization","title":"Text-based visualization","text":""},{"location":"api/sql/Raster-visualizer/#rs_asmatrix","title":"RS_AsMatrix","text":"<p>Introduction: Returns a string, that when printed, outputs the raster band as a pretty printed 2D matrix. All the values of the raster are cast to double for the string. RS_AsMatrix allows specifying the number of digits to be considered after the decimal point. RS_AsMatrix expects a raster, and optionally a band (default: 1) and postDecimalPrecision (default: 6). The band parameter is 1-indexed.</p> <p>Note</p> <p>If the provided band is not present in the raster, RS_AsMatrix throws an IllegalArgumentException</p> <p>Note</p> <p>If the provided raster has integral values, postDecimalPrecision (if any) is simply ignored and integers are printed in the resultant string</p> <p>Note</p> <p>If you are using <code>show()</code> to display the output, it will show special characters as escape sequences. To get the expected behavior use the following code:</p> ScalaJavaPython <pre><code>println(df.selectExpr(\"RS_AsMatrix(rast)\").sample(0.5).collect().mkString(\"\\n\"))\n</code></pre> <pre><code>System.out.println(String.join(\"\\n\", df.selectExpr(\"RS_AsMatrix(rast)\").sample(0.5).collect()))\n</code></pre> <pre><code>print(\"\\n\".join(df.selectExpr(\"RS_AsMatrix(rast)\").sample(0.5).collect()))\n</code></pre> <p>The <code>sample()</code> function is only there to reduce the data sent to <code>collect()</code>, you may also use <code>filter()</code> if that's appropriate.</p> <p>Format:</p> <pre><code>RS_AsMatrix(raster: Raster, band: Integer = 1, postDecimalPrecision: Integer = 6)\n</code></pre> <p>Since: <code>1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>val inputDf = Seq(Seq(1, 3.333333, 4, 0.0001, 2.2222, 9, 10, 11.11111111, 3, 4, 5, 6)).toDF(\"band\")\nprint(inputDf.selectExpr(\"RS_AsMatrix(RS_AddBandFromArray(RS_MakeEmptyRaster(1, 'd', 4, 3, 0, 0, 1, -1, 0, 0, 0), band, 1, 0))\").sample(0.5).collect()(0))\n</code></pre> <p>Output:</p> <pre><code>| 1.00000   3.33333   4.00000   0.00010|\n| 2.22220   9.00000  10.00000  11.11111|\n| 3.00000   4.00000   5.00000   6.00000|\n</code></pre> <p>Spark SQL Example:</p> <pre><code>val inputDf = Seq(Seq(1, 3, 4, 0, 2, 9, 10, 11, 3, 4, 5, 6)).toDF(\"band\")\nprint(inputDf.selectExpr(\"RS_AsMatrix(RS_AddBandFromArray(RS_MakeEmptyRaster(1, 'i', 4, 3, 0, 0, 1, -1, 0, 0, 0), band, 1, 0))\").sample(0.5).collect()(0))\n</code></pre> <p>Output: <pre><code>| 1   3   4   0|\n| 2   9  10  11|\n| 3   4   5   6|\n</code></pre></p>"},{"location":"api/sql/Raster-writer/","title":"Raster writer","text":"<p>Note</p> <p>Sedona writers are available in Scala, Java and Python and have the same APIs.</p>"},{"location":"api/sql/Raster-writer/#write-raster-dataframe-to-raster-files","title":"Write Raster DataFrame to raster files","text":"<p>To write a Sedona Raster DataFrame to raster files, you need to (1) first convert the Raster DataFrame to a binary DataFrame using <code>RS_AsXXX</code> functions and (2) then write the binary DataFrame to raster files using Sedona's built-in <code>raster</code> data source.</p>"},{"location":"api/sql/Raster-writer/#write-raster-dataframe-to-a-binary-dataframe","title":"Write raster DataFrame to a binary DataFrame","text":"<p>You can use the following RS output functions (<code>RS_AsXXX</code>) to convert a Raster DataFrame to a binary DataFrame. Generally the output format of a raster can be different from the original input format. For example, you can use <code>RS_FromGeoTiff</code> to create rasters and save them using <code>RS_AsArcInfoAsciiGrid</code>.</p>"},{"location":"api/sql/Raster-writer/#rs_asarcgrid","title":"RS_AsArcGrid","text":"<p>Introduction: Returns a binary DataFrame from a Raster DataFrame. Each raster object in the resulting DataFrame is an ArcGrid image in binary format. ArcGrid only takes 1 source band. If your raster has multiple bands, you need to specify which band you want to use as the source.</p> <p>Possible values for <code>sourceBand</code>: any non-negative value (&gt;=0). If not given, it will use Band 0.</p> <p>Format:</p> <p><code>RS_AsArcGrid(raster: Raster)</code></p> <p><code>RS_AsArcGrid(raster: Raster, sourceBand: Integer)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_AsArcGrid(raster) FROM my_raster_table\n</code></pre> <p>Spark SQL Example:</p> <pre><code>SELECT RS_AsArcGrid(raster, 1) FROM my_raster_table\n</code></pre> <p>Output:</p> <pre><code>+--------------------+\n|             arcgrid|\n+--------------------+\n|[4D 4D 00 2A 00 0...|\n+--------------------+\n</code></pre> <p>Output schema:</p> <pre><code>root\n|-- arcgrid: binary (nullable = true)\n</code></pre>"},{"location":"api/sql/Raster-writer/#rs_asgeotiff","title":"RS_AsGeoTiff","text":"<p>Introduction: Returns a binary DataFrame from a Raster DataFrame. Each raster object in the resulting DataFrame is a GeoTiff image in binary format.</p> <p>Possible values for <code>compressionType</code>: <code>None</code>, <code>PackBits</code>, <code>Deflate</code>, <code>Huffman</code>, <code>LZW</code> and <code>JPEG</code></p> <p>Possible values for <code>imageQuality</code>: any decimal number between 0 and 1. 0 means the lowest quality and 1 means the highest quality.</p> <p>Format:</p> <p><code>RS_AsGeoTiff(raster: Raster)</code></p> <p><code>RS_AsGeoTiff(raster: Raster, compressionType: String, imageQuality: Double)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_AsGeoTiff(raster) FROM my_raster_table\n</code></pre> <p>Spark SQL Example:</p> <pre><code>SELECT RS_AsGeoTiff(raster, 'LZW', '0.75') FROM my_raster_table\n</code></pre> <p>Output:</p> <pre><code>+--------------------+\n|             geotiff|\n+--------------------+\n|[4D 4D 00 2A 00 0...|\n+--------------------+\n</code></pre> <p>Output schema:</p> <pre><code>root\n|-- geotiff: binary (nullable = true)\n</code></pre>"},{"location":"api/sql/Raster-writer/#rs_aspng","title":"RS_AsPNG","text":"<p>Introduction: Returns a PNG byte array, that can be written to raster files as PNGs using the sedona function. This function can only accept pixel data type of unsigned integer. PNG can accept 1 or 3 bands of data from the raster, refer to RS_Band for more details.</p> <p>Note</p> <p>Raster having <code>UNSIGNED_8BITS</code> pixel data type will have range of <code>0 - 255</code>, whereas rasters having <code>UNSIGNED_16BITS</code> pixel data type will have range of <code>0 - 65535</code>. If provided pixel value is greater than either <code>255</code> for <code>UNSIGNED_8BITS</code> or <code>65535</code> for <code>UNSIGNED_16BITS</code>, then the extra bit will be truncated.</p> <p>Note</p> <p>Raster that have float or double values will result in an empty byte array. PNG only accepts Integer values, if you want to write your raster to an image file, please refer to RS_AsGeoTiff.</p> <p>Format: <code>RS_AsPNG(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_AsPNG(raster) FROM Rasters\n</code></pre> <p>Output:</p> <pre><code>[-119, 80, 78, 71, 13, 10, 26, 10, 0, 0, 0, 13, 73...]\n</code></pre> <p>Spark SQL Example:</p> <pre><code>SELECT RS_AsPNG(RS_Band(raster, Array(3, 1, 2)))\n</code></pre> <p>Output:</p> <pre><code>[-103, 78, 94, -26, 61, -16, -91, -103, -65, -116...]\n</code></pre>"},{"location":"api/sql/Raster-writer/#write-a-binary-dataframe-to-raster-files","title":"Write a binary DataFrame to raster files","text":"<p>Introduction: You can write a Sedona binary DataFrame to external storage using Sedona's built-in <code>raster</code> data source. Note that: <code>raster</code> data source does not support reading rasters. Please use Spark built-in <code>binaryFile</code> and Sedona RS constructors together to read rasters.</p> <p>Since: <code>v1.4.1</code></p> <p>Available options:</p> <ul> <li>rasterField:<ul> <li>Default value: the <code>binary</code> type column in the DataFrame. If the input DataFrame has several binary columns, please specify which column you want to use.</li> <li>Allowed values: the name of the to-be-saved binary type column</li> </ul> </li> <li>fileExtension<ul> <li>Default value: <code>.tiff</code></li> <li>Allowed values: any string values such as <code>.png</code>, <code>.jpeg</code>, <code>.asc</code></li> </ul> </li> <li>pathField<ul> <li>No default value. If you use this option, then the column specified in this option must exist in the DataFrame schema. If this option is not used, each produced raster image will have a random UUID file name.</li> <li>Allowed values: any column name that indicates the paths of each raster file</li> </ul> </li> </ul> <p>The schema of the Raster dataframe to be written can be one of the following two schemas:</p> <pre><code>root\n |-- rs_asgeotiff(raster): binary (nullable = true)\n</code></pre> <p>or</p> <pre><code>root\n |-- rs_asgeotiff(raster): binary (nullable = true)\n |-- path: string (nullable = true)\n</code></pre> <p>Spark SQL example 1:</p> <pre><code>sparkSession.write.format(\"raster\").mode(SaveMode.Overwrite).save(\"my_raster_file\")\n</code></pre> <p>Spark SQL example 2:</p> <pre><code>sparkSession.write.format(\"raster\").option(\"rasterField\", \"raster\").option(\"pathField\", \"path\").option(\"fileExtension\", \".tiff\").mode(SaveMode.Overwrite).save(\"my_raster_file\")\n</code></pre> <p>The produced file structure will look like this:</p> <pre><code>my_raster_file\n- part-00000-6c7af016-c371-4564-886d-1690f3b27ca8-c000\n    - test1.tiff\n    - .test1.tiff.crc\n- part-00001-6c7af016-c371-4564-886d-1690f3b27ca8-c000\n    - test2.tiff\n    - .test2.tiff.crc\n- part-00002-6c7af016-c371-4564-886d-1690f3b27ca8-c000\n    - test3.tiff\n    - .test3.tiff.crc\n- _SUCCESS\n</code></pre> <p>To read it back to Sedona Raster DataFrame, you can use the following command (note the <code>*</code> in the path):</p> <pre><code>sparkSession.read.format(\"binaryFile\").load(\"my_raster_file/*\")\n</code></pre> <p>Then you can create Raster type in Sedona like this <code>RS_FromGeoTiff(content)</code> (if the written data was in GeoTiff format).</p> <p>The newly created DataFrame can be written to disk again but must be under a different name such as <code>my_raster_file_modified</code></p>"},{"location":"api/sql/Raster-writer/#write-geometry-to-raster-dataframe","title":"Write Geometry to Raster dataframe","text":""},{"location":"api/sql/Raster-writer/#rs_asraster","title":"RS_AsRaster","text":"<p>Introduction: Converts a Geometry to a Raster dataset. Defaults to using <code>1.0</code> for cell <code>value</code> and <code>null</code> for <code>noDataValue</code> if not provided. Supports all geometry types. The <code>pixelType</code> argument defines data type of the output raster. This can be one of the following, D (double), F (float), I (integer), S (short), US (unsigned short) or B (byte). The <code>useGeomeryExtent</code> argument defines the extent of the resultant raster. When set to <code>true</code>, it corresponds to the extent of <code>geom</code>, and when set to false, it corresponds to the extent of <code>raster</code>. Default value is <code>true</code> if not set. Format:</p> <pre><code>RS_AsRaster(geom: Geometry, raster: Raster, pixelType: String, value: Double, noDataValue: Double, useGeometryExtent: Boolean)\n</code></pre> <pre><code>RS_AsRaster(geom: Geometry, raster: Raster, pixelType: String, value: Double, noDataValue: Double)\n</code></pre> <pre><code>RS_AsRaster(geom: Geometry, raster: Raster, pixelType: String, value: Double)\n</code></pre> <pre><code>RS_AsRaster(geom: Geometry, raster: Raster, pixelType: String)\n</code></pre> <p>Since: <code>v1.5.0</code></p> <p>Note</p> <p>The function doesn't support rasters that have any one of the following properties: <pre><code>ScaleX &lt; 0\nScaleY &gt; 0\nSkewX != 0\nSkewY != 0\n</code></pre> If a raster is provided with anyone of these properties then IllegalArgumentException is thrown.</p> <p>Spark SQL Example:</p> <pre><code>SELECT RS_AsRaster(\nST_GeomFromWKT('POLYGON((15 15, 18 20, 15 24, 24 25, 15 15))'),\nRS_MakeEmptyRaster(2, 255, 255, 3, -215, 2, -2, 0, 0, 4326),\n'D', 255.0, 0d\n)\n</code></pre> <p>Output:</p> <pre><code>GridCoverage2D[\"g...\n</code></pre> <p>Spark SQL Example:</p> <pre><code>SELECT RS_AsRaster(\nST_GeomFromWKT('POLYGON((15 15, 18 20, 15 24, 24 25, 15 15))'),\nRS_MakeEmptyRaster(2, 255, 255, 3, -215, 2, -2, 0, 0, 4326),\n'D'\n)\n</code></pre> <p>Output:</p> <pre><code>GridCoverage2D[\"g...\n</code></pre> <pre><code>SELECT RS_AsRaster(\nST_GeomFromWKT('POLYGON((15 15, 18 20, 15 24, 24 25, 15 15))'),\nRS_MakeEmptyRaster(2, 255, 255, 3, 215, 2, -2, 0, 0, 0),\n'D',255, 0d, false\n)\n</code></pre> <p>Output:</p> <pre><code>GridCoverage2D[\"g...\n</code></pre>"},{"location":"api/sql/Reading-legacy-parquet/","title":"Reading Legacy Parquet Files","text":"<p>Due to a breaking change in Apache Sedona 1.4.0 to the SQL type of <code>GeometryUDT</code> (SEDONA-205) as well as the serialization format of geometry values (SEDONA-207), Parquet files containing geometry columns written by Apache Sedona 1.3.1 or earlier cannot be read by Apache Sedona 1.4.0 or later.</p> <p>For parquet files written by <code>\"parquet\"</code> format when using Apache Sedona 1.3.1-incubating or earlier:</p> <pre><code>df.write.format(\"parquet\").save(\"path/to/parquet/files\")\n</code></pre> <p>Reading such files with Apache Sedona 1.4.0 or later using <code>spark.read.format(\"parquet\").load(\"path/to/parquet/files\")</code> will result in an exception:</p> <pre><code>24/01/08 12:52:56 ERROR Executor: Exception in task 0.0 in stage 12.0 (TID 11)\norg.apache.spark.sql.AnalysisException: Invalid Spark read type: expected required group geom (LIST) {\n  repeated group list {\n    required int32 element (INTEGER(8,true));\n  }\n} to be list type but found Some(BinaryType)\n    at org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter$.checkConversionRequirement(ParquetSchemaConverter.scala:745)\n    at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.$anonfun$convertGroupField$3(ParquetSchemaConverter.scala:343)\n    at scala.Option.fold(Option.scala:251)\n    at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.convertGroupField(ParquetSchemaConverter.scala:324)\n    at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.convertField(ParquetSchemaConverter.scala:188)\n    at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.$anonfun$convertInternal$3(ParquetSchemaConverter.scala:147)\n    at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.$anonfun$convertInternal$3$adapted(ParquetSchemaConverter.scala:117)\n    at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n    at scala.collection.immutable.Range.foreach(Range.scala:158)\n    at scala.collection.TraversableLike.map(TraversableLike.scala:286)\n    at scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n    at scala.collection.AbstractTraversable.map(Traversable.scala:108)\n    ...\n</code></pre> <p>Since v1.5.1, GeoParquet supports reading legacy Parquet files. you can use <code>\"geoparquet\"</code> format with the <code>.option(\"legacyMode\", \"true\")</code> option. Here is an example:</p> Scala/JavaJavaPython <pre><code>val df = sedona.read.format(\"geoparquet\").option(\"legacyMode\", \"true\").load(\"path/to/legacy-parquet-files\")\n</code></pre> <pre><code>Dataset&lt;Row&gt; df = sedona.read.format(\"geoparquet\").option(\"legacyMode\", \"true\").load(\"path/to/legacy-parquet-files\")\n</code></pre> <pre><code>df = sedona.read.format(\"geoparquet\").option(\"legacyMode\", \"true\").load(\"path/to/legacy-parquet-files\")\n</code></pre>"},{"location":"api/sql/Visualization_SedonaKepler/","title":"SedonaKepler","text":"<p>SedonaKepler offers a number of APIs which aid in quick and interactive visualization of a geospatial data in a Jupyter notebook/lab environment.</p> <p>Inorder to start using SedonaKepler, simply import Sedona using: <pre><code>from sedona.spark import *\n</code></pre></p> <p>Alternatively it can also be imported using: <pre><code>from sedona.maps.SedonaKepler import SedonaKepler\n</code></pre></p> <p>Following are details on all the APIs exposed via SedonaKepler:</p>"},{"location":"api/sql/Visualization_SedonaKepler/#creating-a-map-object-using-sedonakeplercreate_map","title":"Creating a map object using SedonaKepler.create_map","text":"<p>SedonaKepler exposes a create_map API with the following signature:</p> <pre><code>create_map(df: SedonaDataFrame=None, name: str='unnamed', config: dict=None) -&gt; map\n</code></pre> <p>The parameter 'name' is used to associate the passed SedonaDataFrame in the map object and any config applied to the map is linked to this name. It is recommended you pass a unique identifier to the dataframe here.</p> <p>If no SedonaDataFrame object is passed, an empty map (with config applied if passed) is returned. A SedonaDataFrame can be added later using the method <code>add_df</code></p> <p>A map config can be passed optionally to apply pre-apply customizations to the map.</p> <p>Note</p> <p>The map config references every customization with the name assigned to the SedonaDataFrame being displayed, if there is a mismatch in the name, the config will not be applied to the map object.</p> <p>Example usage (Referenced from Sedona Jupyter examples)</p> Python <pre><code>map = SedonaKepler.create_map(df=groupedresult, name=\"AirportCount\")\nmap\n</code></pre>"},{"location":"api/sql/Visualization_SedonaKepler/#adding-sedonadataframe-to-a-map-object-using-sedonakepleradd_df","title":"Adding SedonaDataFrame to a map object using SedonaKepler.add_df","text":"<p>SedonaKepler exposes a add_df API with the following signature:</p> <pre><code>add_df(map, df: SedonaDataFrame, name: str='unnamed')\n</code></pre> <p>This API can be used to add a SedonaDataFrame to an already created map object. The map object passed is directly mutated and nothing is returned.</p> <p>The parameters name has the same conditions as 'create_map'</p> <p>Tip</p> <p>This method can be used to add multiple dataframes to a map object to be able to visualize them together.</p> <p>Example usage (Referenced from Sedona Jupyter examples)</p> Python <pre><code>map = SedonaKepler.create_map()\nSedonaKepler.add_df(map, groupedresult, name=\"AirportCount\")\nmap\n</code></pre>"},{"location":"api/sql/Visualization_SedonaKepler/#setting-a-config-via-the-map","title":"Setting a config via the map","text":"<p>A map rendered by accessing the map object created by SedonaKepler includes a config panel which can be used to customize the map</p>"},{"location":"api/sql/Visualization_SedonaKepler/#saving-and-setting-config","title":"Saving and setting config","text":"<p>A map object's current config can be accessed by accessing its 'config' attribute like <code>map.config</code>. This config can be saved for future use or use across notebooks if the exact same map is to be rendered every time.</p> <p>Note</p> <p>The map config references each applied customization with the name given to the dataframe and hence will work only on maps with the same name of dataframe supplied. For more details refer to keplerGl documentation here</p>"},{"location":"api/sql/Visualization_SedonaPyDeck/","title":"SedonaPyDeck","text":"<p>SedonaPyDeck offers a number of APIs which aid in quick and interactive visualization of a geospatial data in a Jupyter notebook/lab environment.</p> <p>Inorder to start using SedonaPyDeck, simply import Sedona using: <pre><code>from sedona.spark import *\n</code></pre></p> <p>Alternatively it can also be imported using: <pre><code>from sedona.maps.SedonaPyDeck import SedonaPyDeck\n</code></pre></p> <p>Following are details on all the APIs exposed via SedonaPyDeck:</p>"},{"location":"api/sql/Visualization_SedonaPyDeck/#geometry-map","title":"Geometry Map","text":"<pre><code>def create_geometry_map(df, fill_color=\"[85, 183, 177, 255]\", line_color=\"[85, 183, 177, 255]\",\n                   elevation_col=0, initial_view_state=None,\n                   map_style=None, map_provider=None):\n</code></pre> <p>The parameter <code>fill_color</code> can be given a list of RGB/RGBA values, or a string that contains RGB/RGBA values based on a column, and is used to color polygons or point geometries in the map</p> <p>The parameter <code>line_color</code> can be given a list of RGB/RGBA values, or a string that contains RGB/RGBA values based on a column, and is used to color the line geometries in the map.</p> <p>The parameter <code>elevation_col</code> can be given a static elevation or elevation based on column values like <code>fill_color</code>, this only works for the polygon geometries in the map.</p> <p>Optionally, parameters <code>initial_view_state</code>, <code>map_style</code>, <code>map_provider</code> can be passed to configure the map as per user's liking. More details on the parameters and their default values can be found on the pydeck website as well by deck.gl here</p>"},{"location":"api/sql/Visualization_SedonaPyDeck/#choropleth-map","title":"Choropleth Map","text":"<pre><code>def create_choropleth_map(df, fill_color=None, plot_col=None, initial_view_state=None, map_style=None,\n                          map_provider=None, elevation_col=0)\n</code></pre> <p>The parameter <code>fill_color</code> can be given a list of RGB/RGBA values, or a string that contains RGB/RGBA values based on a column.</p> <p>For example, all these are valid values of fill_color: <pre><code>fill_color=[255, 12, 250]\nfill_color=[0, 12, 250, 255]\nfill_color='[0, 12, 240, AirportCount * 10]' ## AirportCount is a column in the passed df\n</code></pre></p> <p>Instead of giving a <code>fill_color</code> parameter, a 'plot_col' can be passed which specifies the column to decide the choropleth. SedonaPyDeck then creates a default color scheme based on the values of the column passed.</p> <p>The parameter <code>elevation_col</code> can be given a numeric or a string value (containing the column with/without operations on it) to set a 3D elevation to the plotted polygons if any.</p> <p>Optionally, parameters <code>initial_view_state</code>, <code>map_style</code>, <code>map_provider</code> can be passed to configure the map as per user's liking. More details on the parameters and their default values can be found on the pydeck website.</p>"},{"location":"api/sql/Visualization_SedonaPyDeck/#scatterplot","title":"Scatterplot","text":"<pre><code>def create_scatterplot_map(df, fill_color=\"[255, 140, 0]\", radius_col=1, radius_min_pixels = 1, radius_max_pixels = 10, radius_scale=1, initial_view_state=None, map_style=None, map_provider=None)\n</code></pre> <p>The parameter <code>fill_color</code> can be given a list of RGB/RGBA values, or a string that contains RGB/RGBA values based on a column.</p> <p>The parameter <code>radius_col</code> can be given a numeric value or a string value consisting of any operations on the column, in order to specify the radius of the plotted point.</p> <p>The parameter <code>radius_min_pixels</code> can be given a numeric value that would set the minimum radius in pixels. This can be used to prevent the plotted circle from getting too small when zoomed out.</p> <p>The parameter <code>radius_max_pixels</code> can be given a numeric value that would set the maximum radius in pixels. This can be used to prevent the circle from getting too big when zoomed in.</p> <p>The parameter <code>radius_scale</code> can be given a numeric value that sets a global radius multiplier for all points.</p> <p>Optionally, parameters <code>initial_view_state</code>, <code>map_style</code>, <code>map_provider</code> can be passed to configure the map as per user's liking. More details on the parameters and their default values can be found on the pydeck website as well by deck.gl here</p>"},{"location":"api/sql/Visualization_SedonaPyDeck/#heatmap","title":"Heatmap","text":"<pre><code>def create_heatmap(df, color_range=None, weight=1, aggregation=\"SUM\", initial_view_state=None, map_style=None,\n                map_provider=None)\n</code></pre> <p>The parameter <code>color_range</code> can be optionally given a list of RGB values, SedonaPyDeck by default uses <code>6-class YlOrRd</code> as color_range. More examples can be found on colorbrewer</p> <p>The parameter <code>weight</code> can be given a numeric value or a string with column and operations on it to determine weight of each point while plotting a heatmap. By default, SedonaPyDeck assigns a weight of 1 to each point</p> <p>The parameter <code>aggregation</code> can be used to define aggregation strategy to use when aggregating heatmap to a lower resolution (zooming out). One of \"MEAN\" or \"SUM\" can be provided. By default, SedonaPyDeck uses \"MEAN\" as the aggregation strategy.</p> <p>Optionally, parameters <code>initial_view_state</code>, <code>map_style</code>, <code>map_provider</code> can be passed to configure the map as per user's liking. More details on the parameters and their default values can be found on the pydeck website as well by deck.gl here</p>"},{"location":"api/viz/java-api/","title":"RDD","text":"<p>Please read Javadoc</p> <p>Note: Scala can call Java APIs seamlessly. That means Scala users use the same APIs with Java users</p>"},{"location":"api/viz/sql/","title":"DataFrame/SQL","text":""},{"location":"api/viz/sql/#quick-start","title":"Quick start","text":"<p>The detailed explanation is here: Visualize Spatial DataFrame/RDD.</p> <ol> <li>Add Sedona-core, Sedona-SQL, Sedona-Viz into your project pom.xml or build.sbt</li> <li>Declare your Spark Session <pre><code>sparkSession = SparkSession.builder().\nconfig(\"spark.serializer\",\"org.apache.spark.serializer.KryoSerializer\").\nconfig(\"spark.kryo.registrator\", \"org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator\").\nmaster(\"local[*]\").appName(\"mySedonaVizDemo\").getOrCreate()\n</code></pre></li> <li>Add the following lines after your SparkSession declaration: <pre><code>SedonaSQLRegistrator.registerAll(sparkSession)\nSedonaVizRegistrator.registerAll(sparkSession)\n</code></pre></li> </ol>"},{"location":"api/viz/sql/#regular-functions","title":"Regular functions","text":""},{"location":"api/viz/sql/#st_colorize","title":"ST_Colorize","text":"<p>Introduction: Given the weight of a pixel, return the corresponding color. The weight can be the spatial aggregation of spatial objects or spatial observations such as temperature and humidity.</p> <p>Note</p> <p>The color is encoded to an Integer type value in DataFrame. When you print it, it will show some nonsense values. You can just treat them as colors in GeoSparkViz.</p> <p>Format:</p> <pre><code>ST_Colorize (weight: Double, maxWeight: Double, mandatory color: String (Optional))\n</code></pre> <p>Since: <code>v1.0.0</code></p>"},{"location":"api/viz/sql/#produce-various-colors-heat-map","title":"Produce various colors - heat map","text":"<p>This function will normalize the weight according to the max weight among all pixels. Different pixel obtains different color.</p> <p>Spark SQL example: <pre><code>SELECT pixels.px, ST_Colorize(pixels.weight, 999) AS color\nFROM pixels\n</code></pre></p>"},{"location":"api/viz/sql/#produce-uniform-colors-scatter-plot","title":"Produce uniform colors - scatter plot","text":"<p>If a mandatory color name is put as the third input argument, this function will directly output this color, without considering the weights. In this case, every pixel will possess the same color.</p> <p>Spark SQL Example:</p> <pre><code>SELECT pixels.px, ST_Colorize(pixels.weight, 999, 'red') AS color\nFROM pixels\n</code></pre> <p>Here are some example color names can be entered: <pre><code>\"firebrick\"\n\"#aa38e0\"\n\"0x40A8CC\"\n\"rgba(112,36,228,0.9)\"\n</code></pre></p> <p>Please refer to AWT Colors for a list of pre-defined colors.</p>"},{"location":"api/viz/sql/#st_encodeimage","title":"ST_EncodeImage","text":"<p>Introduction: Return the base64 string representation of a Java PNG BufferedImage. This is specific for the server-client environment. For example, transfer the base64 string from GeoSparkViz to Apache Zeppelin.</p> <p>Format: <code>ST_EncodeImage (A: Image)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_EncodeImage(images.img)\nFROM images\n</code></pre>"},{"location":"api/viz/sql/#st_pixelize","title":"ST_Pixelize","text":"<p>Introduction: Convert a geometry to an array of pixels given a resolution</p> <p>You should use it together with <code>Lateral View</code> and <code>Explode</code></p> <p>Format:</p> <pre><code>ST_Pixelize (A: Geometry, ResolutionX: Integer, ResolutionY: Integer, Boundary: Geometry)\n</code></pre> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Pixelize(shape, 256, 256, (ST_Envelope_Aggr(shape) FROM pointtable))\nFROM polygondf\n</code></pre>"},{"location":"api/viz/sql/#st_tilename","title":"ST_TileName","text":"<p>Introduction: Return the map tile name for a given zoom level. Please refer to OpenStreetMap ZoomLevel and OpenStreetMap tile name.</p> <p>Note</p> <p>Tile name is formatted as a \"Z-X-Y\" string. Z is zoom level. X is tile coordinate on X axis. Y is tile coordinate on Y axis.</p> <p>Format: <code>ST_TileName (A: Pixel, ZoomLevel: Integer)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_TileName(pixels.px, 3)\nFROM pixels\n</code></pre>"},{"location":"api/viz/sql/#aggregate-functions","title":"Aggregate functions","text":""},{"location":"api/viz/sql/#st_render","title":"ST_Render","text":"<p>Introduction: Given a group of pixels and their colors, return a single Java PNG BufferedImage. The 3<sup>rd</sup> parameter is optional and it is the zoom level. You should use zoom level when you want to render tiles, instead of a single image.</p> <p>Format: <code>ST_Render (A: Pixel, B: Color, C: Integer - optional zoom level)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT tilename, ST_Render(pixels.px, pixels.color) AS tileimg\nFROM pixels\nGROUP BY tilename\n</code></pre>"},{"location":"asf/asf/","title":"Copyright","text":"<p>Apache Sedona, Sedona, Apache, the Apache feather logo, and the Apache Sedona project logo are either registered trademarks or trademarks of The Apache Software Foundation in the United States and other countries. All other marks mentioned may be trademarks or registered trademarks of their respective owners. Please visit Apache Software Foundation for more details.</p>"},{"location":"community/contact/","title":"Community","text":"<p>Every volunteer project obtains its strength from the people involved in it. We invite you to participate as much or as little as you choose.</p> <p>You can participate in the community as follows:</p> <ul> <li>Use our project and provide a feedback.</li> <li>Provide us with the use-cases.</li> <li>Report bugs and submit patches.</li> <li>Contribute code and documentation.</li> </ul>"},{"location":"community/contact/#community-events","title":"Community events","text":"<p>Everyone is welcome to join our community events. We have a community office hour every 4 weeks.</p>"},{"location":"community/contact/#twitter","title":"Twitter","text":"<p>Apache Sedona@Twitter</p>"},{"location":"community/contact/#discord-server","title":"Discord Server","text":"<p>Fill in the form below to join the Sedona Discord Server!</p>"},{"location":"community/contact/#mailing-list","title":"Mailing list","text":"<p>Get help using Sedona or contribute to the project on our mailing lists</p> <p>Sedona Mailing Lists: dev@sedona.apache.org: project development and general questions</p> <ul> <li>Please first subscribe and then post emails. To subscribe, please send an email (leave the subject and content blank) to dev-subscribe@sedona.apache.org</li> </ul>"},{"location":"community/contact/#issue-tracker","title":"Issue tracker","text":""},{"location":"community/contact/#bug-reports","title":"Bug Reports","text":"<p>Found bug? Enter an issue in the Sedona JIRA</p> <p>Before submitting an issue, please:</p> <ul> <li>Verify that the bug does in fact exist.</li> <li>Search the issue tracker to verify there is no existing issue reporting the bug you\u2019ve found.</li> <li>Consider tracking down the bug yourself in the Sedona\u2019s source and submitting a patch along with your bug report. This is a great time saver for the Sedona developers and helps ensure the bug will be fixed quickly.</li> </ul>"},{"location":"community/contact/#feature-requests","title":"Feature Requests","text":"<p>Enhancement requests for new features are also welcome. The more concrete and rationale the request is, the greater the chance it will be incorporated into future releases.</p> <p>Enter an issue in the Sedona JIRA or send an email to dev@sedona.apache.org</p>"},{"location":"community/contributor/","title":"Project Management Committee","text":"<p>Sedona has received numerous help from the community. This page lists the contributors and committers of Apache Sedona. People on this page are ordered by their last name.</p>"},{"location":"community/contributor/#committers","title":"Committers","text":"<p>A contributor who contributes enough code to Sedona will be promoted to a committer. A committer has the write access to Sedona main repository</p>"},{"location":"community/contributor/#project-management-committee-pmc","title":"Project Management Committee (PMC)","text":"<p>A committer will be promoted to a PMC member when the community thinks he/she is able to be in charge at least a major component of this project.</p> <p>Current Sedona PMC members are as follows:</p> Name GitHub ID Apache ID Adam Binford Kimahriman kimahriman@apache.org Kanchan Chowdhury kanchanchy kanchanchy@apache.org Pawe\u0142 Koci\u0144ski Imbruced imbruced@apache.org Yitao Li yitao-li yitaoli@apache.org Netanel Malka netanel246 malka@apache.org Kristin Cowalcijk Kontinuation kontinuation@apache.org Mohamed Sarwat Sarwat mosarwat@apache.org Kengo Seki sekikn sekikn@apache.org Sachio Wakai SW186000 swakai@apache.org Jinxuan Wu jinxuan jinxuanw@apache.org Jia Yu jiayuasu jiayu@apache.org Zongsi Zhang zongsizhang zongsizhang@apache.org Felix Cheung felixcheung@apache.org Von Gosling vongosling@apache.org Jean-Baptiste Onofr\u00e9 jbonofre@apache.org George Percivall percivall@apache.org Sunil Govindan sunilg@apache.org"},{"location":"community/contributor/#become-a-committer","title":"Become a committer","text":"<p>To get started contributing to Sedona, learn how to contribute \u2013 anyone can submit patches, documentation and examples to the project.</p> <p>The PMC regularly adds new committers from the active contributors, based on their contributions to Sedona. The qualifications for new committers include:</p> <ul> <li>Sustained contributions to Sedona: Committers should have a history of major contributions to Sedona.</li> <li>Quality of contributions: Committers more than any other community member should submit simple, well-tested, and well-designed patches. In addition, they should show sufficient expertise to be able to review patches.</li> <li>Community involvement: Committers should have a constructive and friendly attitude in all community interactions. They should also be active on the dev mailing list &amp; Gitter, and help mentor newer contributors and users.</li> </ul> <p>The PMC also adds new PMC members. PMC members are expected to carry out PMC responsibilities as described in Apache Guidance, including helping vote on releases, enforce Apache project trademarks, take responsibility for legal and license issues, and ensure the project follows Apache project mechanics. The PMC periodically adds committers to the PMC who have shown they understand and can help with these activities.</p> <p>Current Sedona Committers are as follows:</p> Name GitHub ID Apache ID Nilesh Gajwani iGN5117 nilesh@apache.org"},{"location":"community/contributor/#nominate-a-committer-or-pmc-member","title":"Nominate a committer or PMC member","text":"<p>Steps are as follows: 1. Call a vote (templates/committerVote.txt) 2. Close the vote. If the result is positive, invite the new committer.</p>"},{"location":"community/contributor/#call-for-a-vote","title":"Call for a vote","text":"<p>We do the vote and discussion on the private@sedona.apache.org to enable a frank discussion.</p> <p>Let the Vote thread run for one week. A positive result is achieved by Consensus Approval: at least 3 +1 votes and no vetoes.</p>"},{"location":"community/contributor/#pmc-vote-template","title":"PMC vote template","text":"<p>This is the email to commence a vote for a new PMC candidate. New PMC members need to be voted for by the existing PMC members and subsequently approved by the Board.</p> <pre><code>To: private@sedona.apache.org\nSubject: [VOTE] New PMC candidate: [New PMC NAME]\n\n[ add the reasons behind your nomination here ]\n\nVoting ends one week from today, or until at least 3 +1 votes are cast.\n</code></pre>"},{"location":"community/contributor/#close-a-vote","title":"Close a vote","text":"<p>This email ends the vote and reports the result to the project.</p> <pre><code>To: private@sedona.apache.org\nSubject: [VOTE][RESULT] New PMC candidate: [New PMC NAME]\n\nThe vote has now closed: [paste the vote thread on https://lists.apache.org/list.html?private@sedona.apache.org]. The results are:\n\nBinding Votes:\n\n+1 [TOTAL BINDING +1 VOTES]\n 0 [TOTAL BINDING +0/-0 VOTES]\n-1 [TOTAL BINDING -1 VOTES]\n\nThe vote is ***successful/not successful***\n</code></pre>"},{"location":"community/contributor/#send-a-notice-to-asf-board","title":"Send a notice to ASF Board","text":"<p>The nominating PMC member should send a message to the ASF Board (board@apache.org) with a reference to the vote result in the following form:</p> <pre><code>To: board at apache.org\nCC: private at sedona.apache.org\nSubject: [NOTICE] New PMC NAME for Apache Sedona PMC\nBody:\n\nNew PMC NAME has been voted as a new member of the Apache Sedona PMC. the vote thread is at: *link to the vote result thread*\n</code></pre>"},{"location":"community/contributor/#send-the-invitation","title":"Send the invitation","text":"<pre><code>To: New PMC Email address\nCC: private@sedona.apache.org\n\nHello [New PMC NAME],\n\nThe Sedona Project Management Committee (PMC)\nhereby offers you committer privileges to the project\n[as well as membership in the PMC]. These privileges are\noffered on the understanding that you'll use them\nreasonably and with common sense. We like to work on trust\nrather than unnecessary constraints.\n\nBeing a committer enables you to more easily make\nchanges without needing to go through the patch\nsubmission process. Being a PMC member enables you\nto guide the direction of the project.\n\nBeing a committer does not require you to\nparticipate any more than you already do. It does\ntend to make one even more committed.  You will\nprobably find that you spend more time here.\n\nOf course, you can decline and instead remain as a\ncontributor, participating as you do now.\n\nA. This personal invitation is a chance for you to\naccept or decline in private.  Either way, please\nlet us know in reply to the private@sedona.apache.org\naddress only.\n\nB. If you accept, the next step is to register an iCLA:\n    1. Details of the iCLA and the forms are found\n    through this link: https://www.apache.org/licenses/#clas\n\n    2. Instructions for its completion and return to\n    the Secretary of the ASF are found at\n    https://www.apache.org/licenses/#submitting\n\n    3. When you transmit the completed iCLA, request\n    to notify the Apache Sedona project and choose a\n    unique Apache ID. Look to see if your preferred\n    ID is already taken at\n    https://people.apache.org/committer-index.html\n    This will allow the Secretary to notify the PMC\n    when your iCLA has been recorded.\n\nWhen recording of your iCLA is noted, you will\nreceive a follow-up message with the next steps for\nestablishing you as a committer.\n</code></pre>"},{"location":"community/contributor/#pmc-accept-and-icla-instruction","title":"PMC Accept and ICLA instruction","text":"<pre><code>To: New PMC Email address\nCc: private@sedona.apache.org\nSubject: Re: invitation to become Apache Sedona PMC\n\nWelcome. Here are the next steps in becoming a project committer. After that we will make an announcement to the dev@sedona.apache.org\n\n1. You need to send a Contributor License Agreement to the ASF.\nNormally you would send an Individual CLA. If you also make\ncontributions done in work time or using work resources,\nsee the Corporate CLA. Ask us if you have any issues.\nhttps://www.apache.org/licenses/#clas.\n\nYou need to choose a preferred ASF user name and alternatives.\nIn order to ensure it is available you can view a list of taken IDs at\nhttps://people.apache.org/committer-index.html\n\nPlease notify us when you have submitted the CLA and by what means\nyou did so. This will enable us to monitor its progress.\n\nWe will arrange for your Apache user account when the CLA has\nbeen recorded.\n\n2. After that is done, please use your ASF email to subscribe to the dev@sedona.apache.org\nand private@sedona.apache.org by sending an email to dev-subscribe@sedona.apache.org and\nprivate-subscribe@sedona.apache.org. We generally discuss everything on the dev list and\nkeep the private@sedona.apache.org list for occasional matters which must be private.\n\nThe developer section of the website describes roles within the ASF and provides other\nresources:\n  https://www.apache.org/foundation/how-it-works.html\n  https://www.apache.org/dev/\n\nJust as before you became a committer, participation in any ASF community\nrequires adherence to the ASF Code of Conduct:\n  https://www.apache.org/foundation/policies/conduct.html\n\nYours,\nThe Apache Sedona PMC\n</code></pre>"},{"location":"community/contributor/#create-asf-account","title":"Create ASF account","text":"<p>Once the ICLA has been filed, use the ASF New Account Request form to generate the request. Sedona mentors will request the account.</p> <p>Once Sedona graduates, the PMC chair will make the request.</p>"},{"location":"community/contributor/#add-to-the-system","title":"Add to the system","text":"<p>Once the new PMC subscribes to the Sedona mailing lists using his/her ASF account, one of the PMC needs to add the new PMC to the Whimsy system (https://whimsy.apache.org/roster/pmc/sedona).</p>"},{"location":"community/contributor/#pmc-announcement","title":"PMC announcement","text":"<p>This is the email to announce the new committer to sedona-dev once the account has been created.</p> <pre><code>To: dev@sedona.apache.org\nSubject: new committer: ###New PMC NAME\n\nThe Project Management Committee (PMC) for Apache Sedona\nhas invited New PMC NAME to become a committer and we are pleased\nto announce that they have accepted.\n\n### add specific details here ###\n\nBeing a committer enables easier contribution to the\nproject since there is no need to go via the patch\nsubmission process. This should enable better productivity.\nA PMC member helps manage and guide the direction of the project.\n</code></pre>"},{"location":"community/contributor/#committer-done-template","title":"Committer Done Template","text":"<p>After the committer account is established.</p> <pre><code>To: New PMC Email\nCC: private@sedona.apache.org\nSubject: account request: New PMC NAME\n\nNew PMC NAME, as you know, the ASF Infrastructure has set up your\ncommitter account with the username '####'.\n\nYou have commit access to specific sections of the\nASF repository, as follows:\nhttps://github.com/apache/sedona\n\nYou need to link your ASF Account with your GitHub account.\n\nHere are the steps\n\n1. Verify you have a GitHub ID enabled with 2FA\n    * https://help.github.com/articles/securing-your-account-with-two-factor-authentication-2fa/\n2. Enter your GitHub ID into your Apache ID profile https://id.apache.org/\n3. Merge your Apache and GitHub accounts using\n    * GitBox (Apache Account Linking utility) https://gitbox.apache.org/setup/\n    * You should see 3 green checks in GitBox.\n    * Wait at least 30  minutes for an email inviting you to Apache GitHub Organization and accept invitation\n4. After accepting the GitHub Invitation verify that you are a\nmember of the team https://github.com/orgs/apache/teams/sedona-committers\n\nOptionally, if you want, please follow the instructions to set up your GitHub, SSH, svn password, svn configuration, email forwarding, etc.\nhttps://www.apache.org/dev/#committers\n\nAdditionally, if you have been elected to the Sedona\n Project Mgmt. Committee (PMC): Verify you are part of the LDAP sedona\n  pmc https://whimsy.apache.org/roster/pmc/sedona\n</code></pre>"},{"location":"community/develop/","title":"Develop Sedona","text":""},{"location":"community/develop/#scalajava-developers","title":"Scala/Java developers","text":""},{"location":"community/develop/#ide","title":"IDE","text":"<p>We recommend Intellij IDEA with Scala plugin installed. Please make sure that the IDE has JDK 1.8 set as project default.</p>"},{"location":"community/develop/#import-the-project","title":"Import the project","text":""},{"location":"community/develop/#choose-open","title":"Choose <code>Open</code>","text":""},{"location":"community/develop/#go-to-the-sedona-root-folder-not-a-submodule-folder-and-choose-open","title":"Go to the Sedona root folder (not a submodule folder) and choose <code>open</code>","text":""},{"location":"community/develop/#the-ide-might-show-errors","title":"The IDE might show errors","text":"<p>The IDE usually has trouble understanding the complex project structure in Sedona.</p> <p></p>"},{"location":"community/develop/#fix-errors-by-changing-pomxml","title":"Fix errors by changing pom.xml","text":"<p>You need to comment out the following lines in <code>pom.xml</code> at the root folder, as follows. Remember that you should NOT submit this change to Sedona.</p> <pre><code>&lt;!--    &lt;parent&gt;--&gt;\n&lt;!--        &lt;groupId&gt;org.apache&lt;/groupId&gt;--&gt;\n&lt;!--        &lt;artifactId&gt;apache&lt;/artifactId&gt;--&gt;\n&lt;!--        &lt;version&gt;23&lt;/version&gt;--&gt;\n&lt;!--        &lt;relativePath /&gt;--&gt;\n&lt;!--    &lt;/parent&gt;--&gt;\n</code></pre>"},{"location":"community/develop/#reload-pomxml","title":"Reload pom.xml","text":"<p>Make sure you reload the pom.xml or reload the maven project. The IDE will ask you to remove some modules. Please select <code>yes</code>.</p> <p></p>"},{"location":"community/develop/#the-final-project-structure-should-be-like-this","title":"The final project structure should be like this:","text":""},{"location":"community/develop/#run-unit-tests","title":"Run unit tests","text":""},{"location":"community/develop/#run-all-unit-tests","title":"Run all unit tests","text":"<p>In a terminal, go to the Sedona root folder. Run <code>mvn clean install</code>. All tests will take more than 15 minutes. To only build the project jars, run <code>mvn clean install -DskipTests</code>.</p> <p>Note</p> <p><code>mvn clean install</code> will compile Sedona with Spark 3.0 and Scala 2.12. If you have a different version of Spark in $SPARK_HOME, make sure to specify that using -Dspark command line arg. For example, to compile sedona with Spark 3.4 and Scala 2.12, use: <code>mvn clean install -Dspark=3.4 -Dscala=2.12</code></p> <p>More details can be found on Compile Sedona</p>"},{"location":"community/develop/#run-a-single-unit-test","title":"Run a single unit test","text":"<p>In the IDE, right-click a test case and run this test case.</p> <p></p> <p>The IDE might tell you that the PATH does not exist as follows:</p> <p></p> <p>Go to <code>Edit Configuration</code></p> <p></p> <p>Append the submodule folder to <code>Working Directory</code>. For example, <code>sedona/sql</code>.</p> <p></p> <p>Re-run the test case. Do NOT right click the test case to re-run. Instead, click the button as shown in the figure below.</p> <p></p>"},{"location":"community/develop/#python-developers","title":"Python developers","text":""},{"location":"community/develop/#run-all-python-tests","title":"Run all python tests","text":"<p>To run all Python test cases, follow steps mentioned here.</p>"},{"location":"community/develop/#run-all-python-tests-in-a-single-test-file","title":"Run all python tests in a single test file","text":"<p>To run a particular python test file, specify the path of the .py file to pipenv.</p> <p>For example, to run all tests in <code>test_function.py</code> located in <code>python/tests/sql/</code>, use: <code>pipenv run pytest tests/sql/test_function.py</code>.</p>"},{"location":"community/develop/#run-a-single-test","title":"Run a single test","text":"<p>To run a particular test in a particular .py test file, specify <code>file_name::class_name::test_name</code> to the pytest command.</p> <p>For example, to run the test on ST_Contains function located in sql/test_predicate.py, use: <code>pipenv run pytest tests/sql/test_predicate.py::TestPredicate::test_st_contains</code></p>"},{"location":"community/develop/#ide_1","title":"IDE","text":"<p>We recommend PyCharm</p>"},{"location":"community/develop/#import-the-project_1","title":"Import the project","text":""},{"location":"community/develop/#r-developers","title":"R developers","text":"<p>More details to come.</p>"},{"location":"community/develop/#ide_2","title":"IDE","text":"<p>We recommend RStudio</p>"},{"location":"community/develop/#import-the-project_2","title":"Import the project","text":""},{"location":"community/publication/","title":"Publication","text":"<p>Apache Sedona was formerly called GeoSpark, initiated by Arizona State University Data Systems Lab.</p>"},{"location":"community/publication/#key-publications","title":"Key publications","text":"<p>\"Spatial Data Management in Apache Spark: The GeoSpark Perspective and Beyond\" is the full research paper that talks about the entire GeoSpark ecosystem. Please cite this paper if your work mentions GeoSpark core system.</p> <p>\"GeoSparkViz: A Scalable Geospatial Data Visualization Framework in the Apache Spark Ecosystem\" is the full research paper that talks about map visualization system in GeoSpark. Please cite this paper if your work mentions GeoSpark visualization system.</p> <p>\"Building A Microscopic Road Network Traffic Simulator in Apache Spark\" is the full research paper that talks about the traffic simulator in GeoSpark. Please cite this paper if your work mentions GeoSparkSim traffic simulator.</p>"},{"location":"community/publication/#third-party-evaluation","title":"Third-party evaluation","text":"<p>GeoSpark were evaluated by papers published on database top venues. It is worth noting that we do not have any collaboration with the authors.</p> <ul> <li>SIGMOD 2020 paper \"Architecting a Query Compiler for Spatial Workloads\" Ruby Y. Tahboub, Tiark  Rompf (Purdue University). <p>In Figure 16a, GeoSpark distance join query runs around 7x - 9x faster than Simba, a spatial extension on Spark, on 1 - 24 core machines.</p> </li> <li>PVLDB 2018 paper \"How Good Are Modern Spatial Analytics Systems?\" Varun Pandey, Andreas Kipf, Thomas Neumann, Alfons Kemper (Technical University of Munich), quoted as follows: <p>GeoSpark comes close to a complete spatial analytics system. It also exhibits the best performance in most cases.</p> </li> </ul>"},{"location":"community/publication/#full-publications","title":"Full publications","text":""},{"location":"community/publication/#geospark-ecosystem","title":"GeoSpark Ecosystem","text":"<p>\"Spatial Data Management in Apache Spark: The GeoSpark Perspective and Beyond\" (research paper). Jia Yu, Zongsi Zhang, Mohamed Sarwat. Geoinformatica Journal 2019.</p> <p>\"A Demonstration of GeoSpark: A Cluster Computing Framework for Processing Big Spatial Data\" (demo paper). Jia Yu, Jinxuan Wu, Mohamed Sarwat. In Proceeding of IEEE International Conference on Data Engineering ICDE 2016, Helsinki, FI, May 2016</p> <p>\"GeoSpark: A Cluster Computing Framework for Processing Large-Scale Spatial Data\" (short paper). Jia Yu, Jinxuan Wu, Mohamed Sarwat. In Proceeding of the ACM International Conference on Advances in Geographic Information Systems ACM SIGSPATIAL GIS 2015, Seattle, WA, USA November 2015</p>"},{"location":"community/publication/#geosparkviz-visualization-system","title":"GeoSparkViz Visualization System","text":"<p>\"GeoSparkViz in Action: A Data System with built-in support for Geospatial Visualization\" (demo paper) Jia Yu, Anique Tahir, and Mohamed Sarwat. In Proceedings of the International Conference on Data Engineering, ICDE, 2019</p> <p>\"GeoSparkViz: A Scalable Geospatial Data Visualization Framework in the Apache Spark Ecosystem\" (research paper). Jia Yu, Zongsi Zhang, Mohamed Sarwat. In Proceedings of the International Conference on Scientific and Statistical Database Management, SSDBM 2018, Bolzano-Bozen, Italy July 2018</p>"},{"location":"community/publication/#geosparksim-traffic-simulator","title":"GeoSparkSim Traffic Simulator","text":"<p>\"Dissecting GeoSparkSim: a scalable microscopic road network traffic simulator in Apache Spark\" (journal paper) Jia Yu, Zishan Fu, Mohamed Sarwat. Distributed Parallel Databases 38(4): 963-994 (2020)</p> <p>\"Demonstrating GeoSparkSim: A Scalable Microscopic Road Network Traffic Simulator Based on Apache Spark\". Zishan Fu, Jia Yu, Mohamed Sarwat. International Symposium on Spatial and Temporal Databases, SSTD, 2019</p> <p>\"Building A Microscopic Road Network Traffic Simulator in Apache Spark\" (research paper) Zishan Fu, Jia Yu, and Mohamed Sarwat. In Proceedings of the International Conference on Mobile Data Management, MDM, 2019</p>"},{"location":"community/publication/#a-tutorial-about-geospatial-data-management-in-spark","title":"A Tutorial about Geospatial Data Management in Spark","text":"<p>\"Geospatial Data Management in Apache Spark: A Tutorial\" (Tutorial) Jia Yu and Mohamed Sarwat.  In Proceedings of the International Conference on Data Engineering, ICDE, 2019</p>"},{"location":"community/publish/","title":"Make a Sedona release","text":"<p>This page is for Sedona PMC to publish Sedona releases.</p> <p>Warning</p> <p>All scripts on this page should be run in your local Sedona Git repo under master branch via a single script file.</p>"},{"location":"community/publish/#0-prepare-an-empty-script-file","title":"0. Prepare an empty script file","text":"<ol> <li>In your local Sedona Git repo under master branch, run <pre><code>echo '#!/bin/bash' &gt; create-release.sh\nchmod 777 create-release.sh\n</code></pre></li> <li>Use your favourite GUI text editor to open <code>create-release.sh</code>.</li> <li>Then keep copying the scripts on this web page to replace all content in this script file.</li> <li>Do NOT directly copy/paste the scripts to your terminal because a bug in <code>clipboard.js</code> will create link breaks in such case.</li> <li>Each time when you copy content to this script file, run <code>./create-release.sh</code> to execute it.</li> </ol>"},{"location":"community/publish/#1-check-asf-copyright-in-all-file-headers","title":"1. Check ASF copyright in all file headers","text":"<ol> <li>Run the following script: <pre><code>#!/bin/bash\nwget -q https://archive.apache.org/dist/creadur/apache-rat-0.15/apache-rat-0.15-bin.tar.gz\ntar -xvf  apache-rat-0.15-bin.tar.gz\ngit clone --shared --branch master https://github.com/apache/sedona.git sedona-src\njava -jar apache-rat-0.15/apache-rat-0.15.jar -d sedona-src &gt; report.txt\n</code></pre></li> <li>Read the generated report.txt file and make sure all source code files have ASF header.</li> <li>Delete the generated report and cloned files <pre><code>#!/bin/bash\nrm -rf apache-rat-0.15\nrm -rf sedona-src\nrm report.txt\n</code></pre></li> </ol>"},{"location":"community/publish/#2-update-sedona-python-r-and-zeppelin-versions","title":"2. Update Sedona Python, R and Zeppelin versions","text":"<p>Make sure the Sedona version in the following files are 1.5.1.</p> <ol> <li>https://github.com/apache/sedona/blob/master/python/sedona/version.py</li> <li>https://github.com/apache/sedona/blob/master/R/DESCRIPTION</li> <li>https://github.com/apache/sedona/blob/99239524f17389fc4ae9548ea88756f8ea538bb9/R/R/dependencies.R#L42</li> <li>https://github.com/apache/sedona/blob/master/zeppelin/package.json</li> </ol>"},{"location":"community/publish/#3-update-mkdocsyml","title":"3. Update mkdocs.yml","text":"<ul> <li>Please change the following variables in <code>mkdocs.yml</code> to the version you want to publish.<ul> <li><code>sedona_create_release.current_version</code></li> <li><code>sedona_create_release.current_rc</code></li> <li><code>sedona_create_release.current_git_tag</code></li> <li><code>sedona_create_release.current_snapshot</code></li> </ul> </li> <li>Then compile the website by <code>mkdocs serve</code>. This will generate the scripts listed on this page in your local browser.</li> <li>You can also publish this website if needed. See the instruction at bottom.</li> </ul>"},{"location":"community/publish/#4-stage-and-upload-release-candidates","title":"4. Stage and upload release candidates","text":"<pre><code>#!/bin/bash\n\ngit checkout master\ngit pull\n\nrm -f release.*\nrm -f pom.xml.*\n\necho \"*****Step 1. Stage the Release Candidate to GitHub.\"\n\nmvn -q -B clean release:prepare -Dtag=sedona-1.5.1-rc1 -DreleaseVersion=1.5.1 -DdevelopmentVersion=1.5.1-SNAPSHOT -Dresume=false -Penable-all-submodules -Darguments=\"-DskipTests\"\nmvn -q -B release:clean -Penable-all-submodules\n\necho \"Now the releases are staged. A tag and two commits have been created on Sedona GitHub repo\"\n\necho \"*****Step 2: Upload the Release Candidate to https://repository.apache.org.\"\n\n# For Spark 3.0 and Scala 2.12\nmvn -q org.apache.maven.plugins:maven-release-plugin:2.3.2:perform -DconnectionUrl=scm:git:https://github.com/apache/sedona.git -Dtag=sedona-1.5.1-rc1 -Dresume=false -Darguments=\"-DskipTests -Dspark=3.0 -Dscala=2.12\" -Dspark=3.0 -Dscala=2.12\n\n# For Spark 3.0 and Scala 2.13\n## Note that we use maven-release-plugin 2.3.2 instead of more recent version (e.g., 3.0.1) to get rid of a bug of maven-release-plugin,\n## which prevent us from cloning git repo with user specified -Dtag=&lt;tag&gt;.\n## Please refer to https://issues.apache.org/jira/browse/MRELEASE-933 and https://issues.apache.org/jira/browse/SCM-729 for details.\n##\n## Please also note that system properties `-Dspark` and `-Dscala` has to be specified both for release:perform and the actual build parameters\n## in `-Darguments`, because the build profiles activated for release:perform task will also affect the actual build task. It is safer to specify\n## these system properties for both tasks.\nmvn -q org.apache.maven.plugins:maven-release-plugin:2.3.2:perform -DconnectionUrl=scm:git:https://github.com/apache/sedona.git -Dtag=sedona-1.5.1-rc1 -Dresume=false -Darguments=\"-DskipTests -Dspark=3.0 -Dscala=2.13\" -Dspark=3.0 -Dscala=2.13\n\n# For Spark 3.4 and Scala 2.12\nmvn -q org.apache.maven.plugins:maven-release-plugin:2.3.2:perform -DconnectionUrl=scm:git:https://github.com/apache/sedona.git -Dtag=sedona-1.5.1-rc1 -Dresume=false -Darguments=\"-DskipTests -Dspark=3.4 -Dscala=2.12\" -Dspark=3.4 -Dscala=2.12\n\n# For Spark 3.4 and Scala 2.13\nmvn -q org.apache.maven.plugins:maven-release-plugin:2.3.2:perform -DconnectionUrl=scm:git:https://github.com/apache/sedona.git -Dtag=sedona-1.5.1-rc1 -Dresume=false -Darguments=\"-DskipTests -Dspark=3.4 -Dscala=2.13\" -Dspark=3.4 -Dscala=2.13\n\n# For Spark 3.5 and Scala 2.12\nmvn -q org.apache.maven.plugins:maven-release-plugin:2.3.2:perform -DconnectionUrl=scm:git:https://github.com/apache/sedona.git -Dtag=sedona-1.5.1-rc1 -Dresume=false -Darguments=\"-DskipTests -Dspark=3.5 -Dscala=2.12\" -Dspark=3.4 -Dscala=2.12\n\n# For Spark 3.5 and Scala 2.13\nmvn -q org.apache.maven.plugins:maven-release-plugin:2.3.2:perform -DconnectionUrl=scm:git:https://github.com/apache/sedona.git -Dtag=sedona-1.5.1-rc1 -Dresume=false -Darguments=\"-DskipTests -Dspark=3.5 -Dscala=2.13\" -Dspark=3.4 -Dscala=2.13\n\necho \"*****Step 3: Upload Release Candidate on ASF SVN: https://dist.apache.org/repos/dist/dev/sedona\"\n\necho \"Creating 1.5.1-rc1 folder on SVN...\"\n\nsvn mkdir -m \"Adding folder\" https://dist.apache.org/repos/dist/dev/sedona/1.5.1-rc1\n\necho \"Creating release files locally...\"\n\necho \"Downloading source code...\"\n\nwget https://github.com/apache/sedona/archive/refs/tags/sedona-1.5.1-rc1.tar.gz\ntar -xvf sedona-1.5.1-rc1.tar.gz\nmkdir apache-sedona-1.5.1-src\ncp -r sedona-sedona-1.5.1-rc1/* apache-sedona-1.5.1-src/\ntar czf apache-sedona-1.5.1-src.tar.gz apache-sedona-1.5.1-src\nrm sedona-1.5.1-rc1.tar.gz\nrm -rf sedona-sedona-1.5.1-rc1\n\necho \"Compiling the source code...\"\n\nmkdir apache-sedona-1.5.1-bin\n\ncd apache-sedona-1.5.1-src &amp;&amp; mvn -q clean install -DskipTests -Dspark=3.0 -Dscala=2.12 &amp;&amp; cd ..\ncp apache-sedona-1.5.1-src/spark-shaded/target/sedona-*1.5.1.jar apache-sedona-1.5.1-bin/\ncp apache-sedona-1.5.1-src/flink-shaded/target/sedona-*1.5.1.jar apache-sedona-1.5.1-bin/\ncp apache-sedona-1.5.1-src/snowflake/target/sedona-*1.5.1.jar apache-sedona-1.5.1-bin/\n\ncd apache-sedona-1.5.1-src &amp;&amp; mvn -q clean install -DskipTests -Dspark=3.0 -Dscala=2.13 &amp;&amp; cd ..\ncp apache-sedona-1.5.1-src/spark-shaded/target/sedona-*1.5.1.jar apache-sedona-1.5.1-bin/\n\ncd apache-sedona-1.5.1-src &amp;&amp; mvn -q clean install -DskipTests -Dspark=3.4 -Dscala=2.12 &amp;&amp; cd ..\ncp apache-sedona-1.5.1-src/spark-shaded/target/sedona-*1.5.1.jar apache-sedona-1.5.1-bin/\n\ncd apache-sedona-1.5.1-src &amp;&amp; mvn -q clean install -DskipTests -Dspark=3.4 -Dscala=2.13 &amp;&amp; cd ..\ncp apache-sedona-1.5.1-src/spark-shaded/target/sedona-*1.5.1.jar apache-sedona-1.5.1-bin/\n\ncd apache-sedona-1.5.1-src &amp;&amp; mvn -q clean install -DskipTests -Dspark=3.5 -Dscala=2.12 &amp;&amp; cd ..\ncp apache-sedona-1.5.1-src/spark-shaded/target/sedona-*1.5.1.jar apache-sedona-1.5.1-bin/\n\ncd apache-sedona-1.5.1-src &amp;&amp; mvn -q clean install -DskipTests -Dspark=3.5 -Dscala=2.13 &amp;&amp; cd ..\ncp apache-sedona-1.5.1-src/spark-shaded/target/sedona-*1.5.1.jar apache-sedona-1.5.1-bin/\n\ntar czf apache-sedona-1.5.1-bin.tar.gz apache-sedona-1.5.1-bin\nshasum -a 512 apache-sedona-1.5.1-src.tar.gz &gt; apache-sedona-1.5.1-src.tar.gz.sha512\nshasum -a 512 apache-sedona-1.5.1-bin.tar.gz &gt; apache-sedona-1.5.1-bin.tar.gz.sha512\ngpg -ab apache-sedona-1.5.1-src.tar.gz\ngpg -ab apache-sedona-1.5.1-bin.tar.gz\n\necho \"Uploading local release files...\"\n\nsvn import -m \"Adding file\" apache-sedona-1.5.1-src.tar.gz https://dist.apache.org/repos/dist/dev/sedona/1.5.1-rc1/apache-sedona-1.5.1-src.tar.gz\nsvn import -m \"Adding file\" apache-sedona-1.5.1-src.tar.gz.asc https://dist.apache.org/repos/dist/dev/sedona/1.5.1-rc1/apache-sedona-1.5.1-src.tar.gz.asc\nsvn import -m \"Adding file\" apache-sedona-1.5.1-src.tar.gz.sha512 https://dist.apache.org/repos/dist/dev/sedona/1.5.1-rc1/apache-sedona-1.5.1-src.tar.gz.sha512\nsvn import -m \"Adding file\" apache-sedona-1.5.1-bin.tar.gz https://dist.apache.org/repos/dist/dev/sedona/1.5.1-rc1/apache-sedona-1.5.1-bin.tar.gz\nsvn import -m \"Adding file\" apache-sedona-1.5.1-bin.tar.gz.asc https://dist.apache.org/repos/dist/dev/sedona/1.5.1-rc1/apache-sedona-1.5.1-bin.tar.gz.asc\nsvn import -m \"Adding file\" apache-sedona-1.5.1-bin.tar.gz.sha512 https://dist.apache.org/repos/dist/dev/sedona/1.5.1-rc1/apache-sedona-1.5.1-bin.tar.gz.sha512\n\necho \"Removing local release files...\"\n\nrm apache-sedona-1.5.1-src.tar.gz\nrm apache-sedona-1.5.1-src.tar.gz.asc\nrm apache-sedona-1.5.1-src.tar.gz.sha512\nrm apache-sedona-1.5.1-bin.tar.gz\nrm apache-sedona-1.5.1-bin.tar.gz.asc\nrm apache-sedona-1.5.1-bin.tar.gz.sha512\nrm -rf apache-sedona-1.5.1-src\nrm -rf apache-sedona-1.5.1-bin\n</code></pre>"},{"location":"community/publish/#5-vote-in-dev-sedonaapacheorg","title":"5. Vote in dev sedona.apache.org","text":""},{"location":"community/publish/#vote-email","title":"Vote email","text":"<p>Please add changes at the end if needed:</p> <pre><code>Subject: [VOTE] Release Apache Sedona 1.5.1-rc1\n\nHi all,\n\nThis is a call for vote on Apache Sedona 1.5.1-rc1. Please refer to the changes listed at the bottom of this email.\n\nRelease notes:\nhttps://github.com/apache/sedona/blob/sedona-1.5.1-rc1/docs/setup/release-notes.md\n\nBuild instructions:\nhttps://github.com/apache/sedona/blob/sedona-1.5.1-rc1/docs/setup/compile.md\n\nGitHub tag:\nhttps://github.com/apache/sedona/releases/tag/sedona-1.5.1-rc1\n\nGPG public key to verify the Release:\nhttps://downloads.apache.org/sedona/KEYS\n\nSource code and binaries:\nhttps://dist.apache.org/repos/dist/dev/sedona/1.5.1-rc1/\n\nThe vote will be open for at least 72 hours or until at least 3 \"+1\" PMC votes are cast\n\nInstruction for checking items on the checklist: https://sedona.apache.org/latest/community/vote/\n\nWe recommend you use this Jupyter notebook on MyBinder to perform this task: https://mybinder.org/v2/gh/jiayuasu/sedona-tools/HEAD?labpath=binder%2Fverify-release.ipynb\n\n**Please vote accordingly and you must provide your checklist for your vote**.\n\n\n[ ] +1 approve\n\n[ ] +0 no opinion\n\n[ ] -1 disapprove with the reason\n\nChecklist:\n\n[ ] Download links are valid.\n\n[ ] Checksums and PGP signatures are valid.\n\n[ ] Source code artifacts have correct names matching the current release.\n\nFor a detailed checklist  please refer to:\nhttps://cwiki.apache.org/confluence/display/INCUBATOR/Incubator+Release+Checklist\n\n------------\n\nChanges according to the comments on the previous release\nOriginal comment (Permalink from https://lists.apache.org/list.html):\n</code></pre>"},{"location":"community/publish/#pass-email","title":"Pass email","text":"<p>Please count the votes and add the Permalink of the vote thread at the end.</p> <pre><code>Subject: [RESULT][VOTE] Release Apache Sedona 1.5.1-rc1\n\nDear all,\n\nThe vote closes now as 72hr have passed. The vote PASSES with\n\n+? (binding): NAME1, NAME2, NAME3\n+? (non-binding): NAME4\nNo -1 votes\n\nThe vote thread (Permalink from https://lists.apache.org/list.html):\n\nI will make an announcement soon.\n</code></pre>"},{"location":"community/publish/#announce-email","title":"Announce email","text":"<ol> <li>This email should be sent to dev@sedona.apache.org</li> <li>Please add the permalink of the vote thread</li> <li>Please add the permalink of the vote result thread</li> </ol> <pre><code>Subject: [ANNOUNCE] Apache Sedona 1.5.1 released\n\nDear all,\n\nWe are happy to report that we have released Apache Sedona 1.5.1. Thank you again for your help.\n\nApache Sedona is a cluster computing system for processing large-scale spatial data.\n\n\nVote thread (Permalink from https://lists.apache.org/list.html):\n\n\nVote result thread (Permalink from https://lists.apache.org/list.html):\n\n\nWebsite:\nhttp://sedona.apache.org/\n\nRelease notes:\nhttps://github.com/apache/sedona/blob/sedona-1.5.1/docs/setup/release-notes.md\n\nDownload links:\nhttps://github.com/apache/sedona/releases/tag/sedona-1.5.1\n\nAdditional resources:\nMailing list: dev@sedona.apache.org\nTwitter: https://twitter.com/ApacheSedona\nGitter: https://gitter.im/apache/sedona\n\nRegards,\nApache Sedona Team\n</code></pre>"},{"location":"community/publish/#7-failed-vote","title":"7. Failed vote","text":"<p>If a vote failed, do the following:</p> <ol> <li>In the vote email, say that we will create another release candidate.</li> <li>Restart from Step 3 <code>Update mkdocs.yml</code>. Please increment the release candidate ID (e.g., <code>1.5.1-rc2</code>) and update <code>sedona_create_release.current_rc</code> and <code>sedona_create_release.current_git_tag</code> in <code>mkdocs.yml</code> to generate the script listed on this webpage.</li> </ol>"},{"location":"community/publish/#8-release-source-code-and-maven-package","title":"8. Release source code and Maven package","text":""},{"location":"community/publish/#upload-releases","title":"Upload releases","text":"<pre><code>#!/bin/bash\n\necho \"Move all files in https://dist.apache.org/repos/dist/dev/sedona to https://dist.apache.org/repos/dist/release/sedona, using svn\"\nsvn mkdir -m \"Adding folder\" https://dist.apache.org/repos/dist/release/sedona/1.5.1\nwget https://dist.apache.org/repos/dist/dev/sedona/1.5.1-rc1/apache-sedona-1.5.1-src.tar.gz\nwget https://dist.apache.org/repos/dist/dev/sedona/1.5.1-rc1/apache-sedona-1.5.1-src.tar.gz.asc\nwget https://dist.apache.org/repos/dist/dev/sedona/1.5.1-rc1/apache-sedona-1.5.1-src.tar.gz.sha512\nwget https://dist.apache.org/repos/dist/dev/sedona/1.5.1-rc1/apache-sedona-1.5.1-bin.tar.gz\nwget https://dist.apache.org/repos/dist/dev/sedona/1.5.1-rc1/apache-sedona-1.5.1-bin.tar.gz.asc\nwget https://dist.apache.org/repos/dist/dev/sedona/1.5.1-rc1/apache-sedona-1.5.1-bin.tar.gz.sha512\nsvn import -m \"Adding file\" apache-sedona-1.5.1-src.tar.gz https://dist.apache.org/repos/dist/release/sedona/1.5.1/apache-sedona-1.5.1-src.tar.gz\nsvn import -m \"Adding file\" apache-sedona-1.5.1-src.tar.gz.asc https://dist.apache.org/repos/dist/release/sedona/1.5.1/apache-sedona-1.5.1-src.tar.gz.asc\nsvn import -m \"Adding file\" apache-sedona-1.5.1-src.tar.gz.sha512 https://dist.apache.org/repos/dist/release/sedona/1.5.1/apache-sedona-1.5.1-src.tar.gz.sha512\nsvn import -m \"Adding file\" apache-sedona-1.5.1-bin.tar.gz https://dist.apache.org/repos/dist/release/sedona/1.5.1/apache-sedona-1.5.1-bin.tar.gz\nsvn import -m \"Adding file\" apache-sedona-1.5.1-bin.tar.gz.asc https://dist.apache.org/repos/dist/release/sedona/1.5.1/apache-sedona-1.5.1-bin.tar.gz.asc\nsvn import -m \"Adding file\" apache-sedona-1.5.1-bin.tar.gz.sha512 https://dist.apache.org/repos/dist/release/sedona/1.5.1/apache-sedona-1.5.1-bin.tar.gz.sha512\nrm apache-sedona-1.5.1-src.tar.gz\nrm apache-sedona-1.5.1-src.tar.gz.asc\nrm apache-sedona-1.5.1-src.tar.gz.sha512\nrm apache-sedona-1.5.1-bin.tar.gz\nrm apache-sedona-1.5.1-bin.tar.gz.asc\nrm apache-sedona-1.5.1-bin.tar.gz.sha512\n</code></pre>"},{"location":"community/publish/#manually-close-and-release-the-package","title":"Manually close and release the package","text":"<ol> <li>Click <code>Close</code> on the Sedona staging repo on https://repository.apache.org under <code>staging repository</code></li> <li>Once the staging repo is closed, click <code>Release</code> on this repo.</li> </ol> <p>NOTICE: The staging repo will be automatically dropped after 3 days without closing. If you find the staging repo being dropped, you can re-stage the release using the following script.</p> <pre><code>#!/bin/bash\n\necho \"Re-staging releases to https://repository.apache.org\"\n\ngit checkout master\ngit pull\n\nrm -f release.*\nrm -f pom.xml.*\n\n# For Spark 3.0 and Scala 2.12\nmvn -q org.apache.maven.plugins:maven-release-plugin:2.3.2:perform -DconnectionUrl=scm:git:https://github.com/apache/sedona.git -Dtag=sedona-1.5.1-rc1 -Dresume=false -Darguments=\"-DskipTests -Dspark=3.0 -Dscala=2.12\" -Dspark=3.0 -Dscala=2.12\n\n# For Spark 3.0 and Scala 2.13\nmvn -q org.apache.maven.plugins:maven-release-plugin:2.3.2:perform -DconnectionUrl=scm:git:https://github.com/apache/sedona.git -Dtag=sedona-1.5.1-rc1 -Dresume=false -Darguments=\"-DskipTests -Dspark=3.0 -Dscala=2.13\" -Dspark=3.0 -Dscala=2.13\n\n# For Spark 3.4 and Scala 2.12\nmvn -q org.apache.maven.plugins:maven-release-plugin:2.3.2:perform -DconnectionUrl=scm:git:https://github.com/apache/sedona.git -Dtag=sedona-1.5.1-rc1 -Dresume=false -Darguments=\"-DskipTests -Dspark=3.4 -Dscala=2.12\" -Dspark=3.4 -Dscala=2.12\n\n# For Spark 3.4 and Scala 2.13\nmvn -q org.apache.maven.plugins:maven-release-plugin:2.3.2:perform -DconnectionUrl=scm:git:https://github.com/apache/sedona.git -Dtag=sedona-1.5.1-rc1 -Dresume=false -Darguments=\"-DskipTests -Dspark=3.4 -Dscala=2.13\" -Dspark=3.4 -Dscala=2.13\n</code></pre>"},{"location":"community/publish/#9-release-sedona-python-and-zeppelin","title":"9. Release Sedona Python and Zeppelin","text":"<p>You must have the maintainer privilege of <code>https://pypi.org/project/apache-sedona/</code> and <code>https://www.npmjs.com/package/apache-sedona</code></p> <p>To publish Sedona pythons, you have to use GitHub actions since we release wheels for different platforms. Please use this repo: https://github.com/jiayuasu/sedona-publish-python</p> <pre><code>#!/bin/bash\n\nwget https://github.com/apache/sedona/archive/refs/tags/sedona-1.5.1-rc1.tar.gz\ntar -xvf sedona-1.5.1-rc1.tar.gz\nmkdir apache-sedona-1.5.1-src\ncp -r sedona-sedona-1.5.1-rc1/* apache-sedona-1.5.1-src/\n\nrm -rf apache-sedona-1.5.1-rc1\n\ncd apache-sedona-1.5.1-src/zeppelin &amp;&amp; npm publish &amp;&amp; cd ..\nrm -rf apache-sedona-1.5.1-src\n</code></pre>"},{"location":"community/publish/#10-release-sedona-r-to-cran","title":"10. Release Sedona R to CRAN.","text":"<pre><code>#!/bin/bash\nR CMD build .\nR CMD check --as-cran apache.sedona_*.tar.gz\n</code></pre> <p>Then submit to CRAN using this web form.</p>"},{"location":"community/publish/#11-publish-the-doc-website","title":"11. Publish the doc website","text":""},{"location":"community/publish/#prepare-the-environment-and-doc-folder","title":"Prepare the environment and doc folder","text":"<ol> <li>Check out the 1.5.1 Git tag on your local repo.</li> <li>Read Compile documentation website to set up your environment. But don't deploy anything yet.</li> <li>Add the download link to Download page.</li> <li>Add the news to <code>docs/index.md</code>.</li> </ol>"},{"location":"community/publish/#generate-javadoc-and-scaladoc","title":"Generate Javadoc and Scaladoc","text":"<p>Run the following script to build Javadoc and Scaladoc of sedona modules and move them to docs/api/javadoc directory.</p> <pre><code>#!/bin/bash\n\nmvn -q clean install -DskipTests\nrm -rf docs/api/javadoc &amp;&amp; mkdir -p docs/api/javadoc\nmkdir -p docs/api/javadoc/spark\nmv spark/common/target/apidocs/* docs/api/javadoc/spark\n</code></pre> <p>Please use Intellij IDEA to generate Scaladoc for the spark-common module and paste to <code>docs/api/scaladoc/spark</code>.</p> <p>Please do not commit these generated docs to Sedona GitHub.</p>"},{"location":"community/publish/#compile-r-html-docs","title":"Compile R html docs","text":"<p>From GitHub Action docs workflow, find generated-docs of the commit which is right after the release candidate tag. Download it and copy this folder <code>docs/api/rdocs</code> to the same location of the Sedona to-be-released source repo.</p>"},{"location":"community/publish/#deploy-the-website","title":"Deploy the website","text":"<ol> <li>Run <code>mike deploy --update-aliases 1.5.1 latest -b website -p</code>. This will deploy this website to Sedona main repo's <code>website</code>.</li> <li>Check out the master branch.</li> <li>Git commit and push your changes in <code>download.md</code> and <code>index.md</code> to master branch. Delete all generated docs.</li> <li>Check out the <code>website</code> branch.</li> <li>In a separate folder, check out GitHub sedona-website asf-site branch</li> <li>Copy all content to in Sedona main repo <code>website</code> branch to Sedona website repo <code>asf-site</code> branch.</li> <li>Commit and push the changes to the remote <code>asf-site</code> branch.</li> </ol>"},{"location":"community/release-manager/","title":"Become a release manager","text":"<p>You only need to perform these steps if this is your first time being a release manager.</p>"},{"location":"community/release-manager/#0-software-requirement","title":"0. Software requirement","text":"<ul> <li>JDK 8: <code>brew install openjdk@8</code></li> <li>Maven 3.X. Your Maven must point to JDK 8 (1.8). Check it by <code>mvn --version</code></li> <li>Git and SVN</li> </ul> <p>If your Maven (<code>mvn --version</code>) points to other JDK versions, you must change it to JDK 8. Steps are as follows:</p> <ol> <li>Find all Java installed on your machine: <code>/usr/libexec/java_home -V</code>. You should see multiple JDK versions including JDK 8.</li> <li>Run <code>whereis mvn</code> to get the installation location of your Maven. The result is a symlink to the actual location.</li> <li>Open it in the terminal (with <code>sudo</code> if needed). It will be like this <pre><code>#!/bin/bash\nJAVA_HOME=\"${JAVA_HOME:-$(/usr/libexec/java_home)}\" exec \"/usr/local/Cellar/maven/3.6.3/libexec/bin/mvn\" \"$@\"\n</code></pre></li> <li>Change <code>JAVA_HOME:-$(/usr/libexec/java_home)}</code> to <code>JAVA_HOME:-$(/usr/libexec/java_home -v 1.8)}</code>.  The resulting content will be like this: <pre><code>#!/bin/bash\nJAVA_HOME=\"${JAVA_HOME:-$(/usr/libexec/java_home -v 1.8)}\" exec \"/usr/local/Cellar/maven/3.6.3/libexec/bin/mvn\" \"$@\"\n</code></pre></li> <li>Run <code>mvn --version</code> again. It should now point to JDK 8.</li> </ol>"},{"location":"community/release-manager/#1-obtain-write-access-to-sedona-github-repo","title":"1. Obtain Write Access to Sedona GitHub repo","text":"<ol> <li>Verify you have a GitHub ID enabled with 2FA https://help.github.com/articles/securing-your-account-with-two-factor-authentication-2fa/</li> <li>Enter your GitHub ID into your Apache ID profile https://id.apache.org/</li> <li>Merge your Apache and GitHub accounts using GitBox (Apache Account Linking utility): https://gitbox.apache.org/setup/<ul> <li>You should see 5 green checks in GitBox</li> <li>Wait at least 30  minutes for an email inviting you to Apache GitHub Organization and accept invitation</li> </ul> </li> <li>After accepting the GitHub Invitation, verify that you are a member of the team https://github.com/orgs/apache/teams/sedona-committers</li> <li>Additionally, if you have been elected to the Sedona PMC, verify you are part of the LDAP Sedona PMC https://whimsy.apache.org/roster/pmc/sedona</li> </ol>"},{"location":"community/release-manager/#2-prepare-secret-gpg-key","title":"2. Prepare Secret GPG key","text":"<ol> <li>Install GNUGPG if it was not installed before. On Mac: <code>brew install gnupg gnupg2</code></li> <li>Generate a secret key. It must be RSA4096 (4096 bits long).</li> <li>Run <code>gpg --full-generate-key</code>. If not work, run <code>gpg --default-new-key-algo rsa4096 --gen-key</code></li> <li>At the prompt, specify the kind of key you want: Select <code>RSA</code>, then press <code>enter</code></li> <li>At the prompt, specify the key size you want: Enter <code>4096</code></li> <li>At the prompt, enter the length of time the key should be valid: Press <code>enter</code> to make the key never expire.</li> <li>Verify that your selections are correct.</li> <li>Enter your user ID information: use your real name and Apache email address.</li> <li>Type a secure passphrase. Make sure you remember this because we will use it later.</li> <li>Use the <code>gpg --list-secret-keys --keyid-format=long</code> command to list the long form of the GPG keys.</li> <li>From the list of GPG keys, copy the long form of the GPG key ID you'd like to use (e.g., <code>3AA5C34371567BD2</code>)</li> <li>Run <code>gpg --export --armor 3AA5C34371567BD2</code>, substituting in the GPG key ID you'd like to use.</li> <li>Copy your GPG key, beginning with <code>-----BEGIN PGP PUBLIC KEY BLOCK-----</code> and ending with <code>-----END PGP PUBLIC KEY BLOCK-----</code>.</li> <li>There must be an empty line between <code>-----BEGIN PGP PUBLIC KEY BLOCK-----</code> and the actual key.</li> <li>Publish your armored key in major key servers: https://keyserver.pgp.com/</li> </ol>"},{"location":"community/release-manager/#3-use-svn-to-update-keys","title":"3. Use SVN to update KEYS","text":"<p>Use SVN to append your armored PGP public key to the <code>KEYS</code> files    * https://dist.apache.org/repos/dist/dev/sedona/KEYS    * https://dist.apache.org/repos/dist/release/sedona/KEYS</p> <ol> <li>Check out both KEYS files <pre><code>svn checkout https://dist.apache.org/repos/dist/dev/sedona/ sedona-dev --depth files\nsvn checkout https://dist.apache.org/repos/dist/release/sedona/ sedona-release --depth files\n</code></pre></li> <li>Use your favorite text editor to open <code>sedona-dev/KEYS</code> and <code>sedona-release/KEYS</code>.</li> <li>Paste your armored key to the end of both files. Note: There must be an empty line between <code>-----BEGIN PGP PUBLIC KEY BLOCK-----</code> and the actual key.</li> <li>Commit both KEYS. SVN might ask you to enter your ASF ID and password. Make sure you do it so SVN can always store your ID and password locally. <pre><code>svn commit -m \"Update KEYS\" sedona-dev/KEYS\nsvn commit -m \"Update KEYS\" sedona-release/KEYS\n</code></pre></li> <li>Then remove both svn folders <pre><code>rm -rf sedona-dev\nrm -rf sedona-release\n</code></pre></li> </ol>"},{"location":"community/release-manager/#4-add-gpg_tty-environment-variable","title":"4. Add GPG_TTY environment variable","text":"<p>In your <code>~/.bashrc</code> file, add the following content. Then restart your terminal.</p> <pre><code>GPG_TTY=$(tty)\nexport GPG_TTY\n</code></pre>"},{"location":"community/release-manager/#5-get-github-personal-access-token-classic","title":"5. Get GitHub personal access token (classic)","text":"<p>You need to create a GitHub personal access token (classic). You can follow the instruction on GitHub.</p> <p>In short:</p> <ol> <li>On your GitHub interface -&gt; Settings</li> <li>In the left sidebar, click Developer settings.</li> <li>In the left sidebar, under  Personal access tokens, click Tokens (classic).</li> <li>Select Generate new token, then click Generate new token (classic).</li> <li>Give your token a descriptive name.</li> <li>To give your token an expiration, select the Expiration drop-down menu. Make sure you set the <code>Expiration</code> to <code>No expiration</code>.</li> <li>Select the scopes you'd like to grant this token. To use your token to access repositories from the command line, select <code>repo</code> and <code>admin:org</code>.</li> <li>Click <code>Generate token</code>.</li> <li>Please save your token somewhere because we will use it in the next step.</li> </ol>"},{"location":"community/release-manager/#6-set-up-credentials-for-maven","title":"6. Set up credentials for Maven","text":"<p>In your <code>~/.m2/settings.xml</code> file, add the following content. Please create this file or <code>.m2</code> folder if it does not exist.</p> <p>Please replace all capitalized text with your own ID and password.</p> <pre><code>&lt;settings&gt;\n  &lt;servers&gt;\n    &lt;server&gt;\n      &lt;id&gt;github&lt;/id&gt;\n      &lt;username&gt;YOUR_GITHUB_USERNAME&lt;/username&gt;\n      &lt;password&gt;YOUR_GITHUB_TOKEN&lt;/password&gt;\n    &lt;/server&gt;\n    &lt;server&gt;\n      &lt;id&gt;apache.snapshots.https&lt;/id&gt;\n      &lt;username&gt;YOUR_ASF_ID&lt;/username&gt;\n      &lt;password&gt;YOUR_ASF_PASSWORD&lt;/password&gt;\n    &lt;/server&gt;\n    &lt;server&gt;\n      &lt;id&gt;apache.releases.https&lt;/id&gt;\n      &lt;username&gt;YOUR_ASF_ID&lt;/username&gt;\n      &lt;password&gt;YOUR_ASF_PASSWORD&lt;/password&gt;\n    &lt;/server&gt;\n  &lt;/servers&gt;\n  &lt;profiles&gt;\n    &lt;profile&gt;\n      &lt;id&gt;gpg&lt;/id&gt;\n      &lt;properties&gt;\n        &lt;gpg.passphrase&gt;YOUR_GPG_PASSPHRASE&lt;/gpg.passphrase&gt;\n      &lt;/properties&gt;\n    &lt;/profile&gt;\n  &lt;/profiles&gt;\n  &lt;activeProfiles&gt;\n    &lt;activeProfile&gt;gpg&lt;/activeProfile&gt;\n  &lt;/activeProfiles&gt;\n&lt;/settings&gt;\n</code></pre>"},{"location":"community/rule/","title":"Contributing to Apache Sedona","text":"<p>The project welcomes contributions. You can contribute to Sedona code or documentation by making Pull Requests on Sedona GitHub Repo.</p> <p>The following sections brief the workflow of how to complete a contribution.</p>"},{"location":"community/rule/#pick-announce-a-task-using-jira","title":"Pick / Announce a task using JIRA","text":"<p>It is important to confirm that your contribution is acceptable. You should create a JIRA ticket or pick an existing ticket. A new JIRA ticket will be automatically sent to <code>dev@sedona.apache.org</code></p>"},{"location":"community/rule/#develop-a-code-contribution","title":"Develop a code contribution","text":"<p>Code contributions should include the following:</p> <ul> <li>Detailed documentations on classes and methods.</li> <li>Unit Tests to demonstrate code correctness and allow this to be maintained going forward.  In the case of bug fixes the unit test should demonstrate the bug in the absence of the fix (if any).  Unit Tests can be JUnit test or Scala test. Some Sedona functions need to be tested in both Scala and Java.</li> <li>Updates on corresponding Sedona documentation if necessary.</li> </ul> <p>Code contributions must include a Apache 2.0 license header at the top of each file.</p>"},{"location":"community/rule/#develop-a-document-contribution","title":"Develop a document contribution","text":"<p>Documentation contributions should satisfy the following requirements:</p> <ul> <li>Detailed explanation with examples.</li> <li>Place a newly added document in a proper folder</li> <li>Change the mkdocs.yml if necessary</li> </ul> <p>Note</p> <p>Please read Compile the source code to learn how to compile Sedona website.</p>"},{"location":"community/rule/#make-a-pull-request","title":"Make a Pull Request","text":"<p>After developing a contribution, the easiest and most visible way to submit a Pull Request (PR) to the GitHub repo.</p> <p>Please use the JIRA ticket ID in the PR name, such as \"[SEDONA-1] my subject\".</p> <p>When creating a PR, please answser the questions in the PR template.</p> <p>When a PR is submitted, GitHub Action will check the build correctness. Please check the PR status, and fix any reported problems.</p>"},{"location":"community/rule/#review-a-pull-request","title":"Review a Pull Request","text":"<ul> <li>Every PR requires (1) at least 1 approval from a committer and (2) no disapproval from a committer. Everyone is welcome to review a PR but only the committer can make the final decision.</li> <li>Other reviewers, including community members and committers, may comment on the changes and suggest modifications. Changes can be added by simply pushing more commits to the same branch.</li> <li>Lively, polite, rapid technical debate is encouraged from everyone in the community even if the outcome may be a rejection of the entire change.</li> <li>Keep in mind that changes to more critical parts of Sedona, like Sedona core and spatial join algorithms, will be subjected to more review, and may require more testing and proof of its correctness than other changes.</li> <li>Sometimes, other changes will be merged which conflict with your pull request\u2019s changes. The PR can\u2019t be merged until the conflict is resolved. This can be resolved by resolving the conflicts by hand, then pushing the result to your branch.</li> </ul>"},{"location":"community/rule/#code-of-conduct","title":"Code of Conduct","text":"<p>Please read Apache Software Foundation Code of Conduct.</p> <p>We expect everyone who participates in the Apache community formally or informally, or claims any affiliation with the Foundation, in any Foundation-related activities and especially when representing the ASF in any role to honor this code of conduct.</p>"},{"location":"community/snapshot/","title":"Publish a SNAPSHOT version","text":"<p>This step is to publish Maven SNAPSHOTs to https://repository.apache.org</p> <p>This is a good practice for a release manager to try out his/her credential setup.</p> <p>The detailed requirement is on ASF Infra website</p> <p>Warning</p> <p>All scripts on this page should be run in your local Sedona Git repo under master branch via a single script file.</p>"},{"location":"community/snapshot/#0-prepare-an-empty-script-file","title":"0. Prepare an empty script file","text":"<ol> <li>In your local Sedona Git repo under master branch, run <pre><code>echo '#!/bin/bash' &gt; create-release.sh\nchmod 777 create-release.sh\n</code></pre></li> <li>Use your favourite GUI text editor to open <code>create-release.sh</code>.</li> <li>Then keep copying the scripts on this web page to replace all content in this text file.</li> <li>Do NOT directly copy/paste the scripts to your terminal because a bug in <code>clipboard.js</code> will create link breaks in such case.</li> <li>Each time when you copy content to this script file, run <code>./create-release.sh</code> to execute it.</li> </ol>"},{"location":"community/snapshot/#1-upload-snapshot-versions","title":"1. Upload snapshot versions","text":"<p>In your Sedona GitHub repo, run this script:</p> <pre><code>#!/bin/bash\n\ngit checkout master\ngit pull\n\nrm -f release.*\nrm -f pom.xml.*\n\n# Validate the POMs and your credential setup\nmvn -q -B clean release:prepare -Dtag=sedona-1.5.1-rc1 -DreleaseVersion=1.5.1 -DdevelopmentVersion=1.5.1-SNAPSHOT -Dresume=false -DdryRun=true -Penable-all-submodules -Darguments=\"-DskipTests\"\nmvn -q -B release:clean -Penable-all-submodules\n\n# Spark 3.0 and Scala 2.12\nmvn -q deploy -DskipTests -Dspark=3.0 -Dscala=2.12\n\n# Spark 3.0 and Scala 2.13\nmvn -q deploy -DskipTests -Dspark=3.0 -Dscala=2.13\n\n# Spark 3.4 and Scala 2.12\nmvn -q deploy -DskipTests -Dspark=3.4 -Dscala=2.12\n\n# Spark 3.4 and Scala 2.13\nmvn -q deploy -DskipTests -Dspark=3.4 -Dscala=2.13\n</code></pre>"},{"location":"community/vote/","title":"Vote a Sedona release","text":"<p>This page is for Sedona community to vote a Sedona release. The script below is tested on MacOS.</p> <p>In order to vote a Sedona release, you must provide your checklist including the following minimum requirement:</p> <ul> <li>Download links are valid</li> <li>Checksums and PGP signatures are valid</li> <li>DISCLAIMER and NOTICE are included</li> <li>Source code artifacts have correct names matching the current release</li> <li>The project can compile from the source code</li> </ul> <p>To make your life easier, we have provided an online Jupyter notebook using MyBinder. Please click this button to open the notebook and verify the release: . Then you can vote <code>+1</code> in the vote email.</p> <p>If you prefer to run the steps on your local machine, please read the steps below. If you can successfully finish the steps, you will pass the items mentioned above. Then you can vote <code>+1</code> in the vote email and provide your checklist.</p>"},{"location":"community/vote/#install-necessary-software","title":"Install necessary software","text":"<ol> <li>GPG: On Mac <code>brew install gnupg gnupg2</code>. You can check in a terminal <code>gpg --version</code>.</li> <li>JDK 1.8 or 1.11. Your Mac might have many different Java versions installed. You can try to use it but not sure if it can pass. You can check in a terminal <code>java --version</code>.</li> <li>Apache Maven 3.3.1+. On Mac <code>brew install maven</code>. You can check it in a terminal <code>mvn -version</code>.</li> <li>Python3 installed on your machine. MacOS comes with Python3 by default. You can check in a terminal <code>python3 --version</code>.</li> </ol> <p>You can skip this step if you installed these software before.</p>"},{"location":"community/vote/#run-the-verify-script","title":"Run the verify script","text":"<p>Please replace SEDONA_CURRENT_RC and SEDONA_CURRENT_VERSION with the correct versions. Then paste the content in a script called <code>verify.sh</code> and re-direct the output to a file. To run a script, do the following:</p> <pre><code>#!/bin/bash\n\n## Change the permission of the script to executable\nchmod 777 verify.sh\n\n## Run and redirect the output to a file\n./verify.sh &amp;&gt; verify.out\n</code></pre> <p>The content of the <code>verify.sh</code> script is as follows. If you copy the following content, a line break is automatically added to a long line of code. Please remove it in your local script.</p> <pre><code>#!/bin/bash\n\nSEDONA_CURRENT_RC=1.5.1-rc1\nSEDONA_CURRENT_VERSION=1.5.1\n\n## Download a Sedona release\nwget -q https://downloads.apache.org/sedona/KEYS\nwget -q https://dist.apache.org/repos/dist/dev/sedona/$SEDONA_CURRENT_RC/apache-sedona-$SEDONA_CURRENT_VERSION-src.tar.gz\nwget -q https://dist.apache.org/repos/dist/dev/sedona/$SEDONA_CURRENT_RC/apache-sedona-$SEDONA_CURRENT_VERSION-src.tar.gz.asc\nwget -q https://dist.apache.org/repos/dist/dev/sedona/$SEDONA_CURRENT_RC/apache-sedona-$SEDONA_CURRENT_VERSION-src.tar.gz.sha512\nwget -q https://dist.apache.org/repos/dist/dev/sedona/$SEDONA_CURRENT_RC/apache-sedona-$SEDONA_CURRENT_VERSION-bin.tar.gz\nwget -q https://dist.apache.org/repos/dist/dev/sedona/$SEDONA_CURRENT_RC/apache-sedona-$SEDONA_CURRENT_VERSION-bin.tar.gz.asc\nwget -q https://dist.apache.org/repos/dist/dev/sedona/$SEDONA_CURRENT_RC/apache-sedona-$SEDONA_CURRENT_VERSION-bin.tar.gz.sha512\n\n## Verify the signature and checksum\ngpg --import KEYS\ngpg --verify apache-sedona-$SEDONA_CURRENT_VERSION-src.tar.gz.asc\ngpg --verify apache-sedona-$SEDONA_CURRENT_VERSION-bin.tar.gz.asc\nshasum -a 512 apache-sedona-$SEDONA_CURRENT_VERSION-src.tar.gz\ncat apache-sedona-$SEDONA_CURRENT_VERSION-src.tar.gz.sha512\nshasum -a 512 apache-sedona-$SEDONA_CURRENT_VERSION-bin.tar.gz\ncat apache-sedona-$SEDONA_CURRENT_VERSION-bin.tar.gz.sha512\n\n## Uncompress the source code folder\ntar -xvf apache-sedona-$SEDONA_CURRENT_VERSION-src.tar.gz\n\n## Compile the project from source\n(cd apache-sedona-$SEDONA_CURRENT_VERSION-src;mvn clean install -DskipTests)\n</code></pre> <ul> <li>If successful, in the output file, you should be able to see something similar to the following text. It should include <code>Good signature from</code> and the final 4 lines should be two pairs of checksum matching each other.</li> </ul> <pre><code>gpg: key 3A79A47AC26FF4CD: \"Jia Yu &lt;jiayu@apache.org&gt;\" not changed\ngpg: key 6C883CA80E7FD299: \"PawelKocinski &lt;imbruced@apache.org&gt;\" not changed\ngpg: Total number processed: 2\ngpg:              unchanged: 2\ngpg: assuming signed data in 'apache-sedona-1.2.0-incubating-src.tar.gz'\ngpg: Signature made Mon Apr  4 11:48:31 2022 PDT\ngpg:                using RSA key 949DD6275C69AB954B1872FC6C883CA80E7FD299\ngpg:                issuer \"imbruced@apache.org\"\ngpg: Good signature from \"PawelKocinski &lt;imbruced@apache.org&gt;\" [unknown]\ngpg: WARNING: The key's User ID is not certified with a trusted signature!\ngpg:          There is no indication that the signature belongs to the owner.\nPrimary key fingerprint: 949D D627 5C69 AB95 4B18  72FC 6C88 3CA8 0E7F D299\ngpg: assuming signed data in 'apache-sedona-1.2.0-incubating-bin.tar.gz'\ngpg: Signature made Mon Apr  4 11:48:42 2022 PDT\ngpg:                using RSA key 949DD6275C69AB954B1872FC6C883CA80E7FD299\ngpg:                issuer \"imbruced@apache.org\"\ngpg: Good signature from \"PawelKocinski &lt;imbruced@apache.org&gt;\" [unknown]\ngpg: WARNING: The key's User ID is not certified with a trusted signature!\ngpg:          There is no indication that the signature belongs to the owner.\nPrimary key fingerprint: 949D D627 5C69 AB95 4B18  72FC 6C88 3CA8 0E7F D299\nd3bdfd4d870838ebe63f21cb93634d2421ec1ac1b8184636206a5dc0d89a78a88257798b1f17371ad3cfcc3b1eb79c69e1410afdefeb4d9b52fc8bb5ea18dd2e  apache-sedona-1.2.0-incubating-src.tar.gz\nd3bdfd4d870838ebe63f21cb93634d2421ec1ac1b8184636206a5dc0d89a78a88257798b1f17371ad3cfcc3b1eb79c69e1410afdefeb4d9b52fc8bb5ea18dd2e  apache-sedona-1.2.0-incubating-src.tar.gz\n64cea38dd3ca171ee4e2a7365dbce999773862f2a11599bd0f27e9551d740659a519a9b976b3e7b0826088010967093e6acc9462f7073e9737c24b007a2df846  apache-sedona-1.2.0-incubating-bin.tar.gz\n64cea38dd3ca171ee4e2a7365dbce999773862f2a11599bd0f27e9551d740659a519a9b976b3e7b0826088010967093e6acc9462f7073e9737c24b007a2df846  apache-sedona-1.2.0-incubating-bin.tar.gz\n</code></pre> <ul> <li>At the end of the output, you should also see the <code>BUILD SUCCESS</code> if you can compile the source code. If this step fails, you can contact Sedona PMC and see if this is just because of your environment.</li> </ul>"},{"location":"community/vote/#check-files-manually","title":"Check files manually","text":"<ol> <li> <p>Check if the downloaded files have the correct version.</p> </li> <li> <p>In the unzipped source code folder, and check if DISCLAIMER and NOTICE files and included and up to date.</p> </li> </ol>"},{"location":"setup/cluster/","title":"Set up your Apache Spark cluster","text":"<p>Download a Spark distribution from Spark download page.</p>"},{"location":"setup/cluster/#preliminary","title":"Preliminary","text":"<ol> <li>Set up password-less SSH on your cluster. Each master-worker pair should have bi-directional password-less SSH.</li> <li>Make sure you have installed JRE 1.8 or later.</li> <li>Add the list of your workers' IP address in ./conf/slaves</li> <li>Besides the necessary Spark settings, you may need to add the following lines in Spark configuration files to avoid Sedona memory errors:</li> </ol> <p>In <code>./conf/spark-defaults.conf</code></p> <pre><code>spark.driver.memory 10g\nspark.network.timeout 1000s\nspark.driver.maxResultSize 5g\n</code></pre> <ul> <li><code>spark.driver.memory</code> tells Spark to allocate enough memory for the driver program because Sedona needs to build global grid files (global index) on the driver program. If you have a large amount of data (normally, over 100 GB), set this parameter to 2~5 GB will be good. Otherwise, you may observe \"out of memory\" error.</li> <li><code>spark.network.timeout</code> is the default timeout for all network interactions. Sometimes, spatial join query takes longer time to shuffle data. This will ensure Spark has enough patience to wait for the result.</li> <li><code>spark.driver.maxResultSize</code> is the limit of total size of serialized results of all partitions for each Spark action. Sometimes, the result size of spatial queries is large. The \"Collect\" operation may throw errors.</li> </ul> <p>For more details of Spark parameters, please visit Spark Website.</p>"},{"location":"setup/cluster/#start-your-cluster","title":"Start your cluster","text":"<p>Go the root folder of the uncompressed Apache Spark folder. Start your spark cluster via a terminal</p> <pre><code>./sbin/start-all.sh\n</code></pre>"},{"location":"setup/compile/","title":"Compile Sedona source code","text":""},{"location":"setup/compile/#compile-scala-java-source-code","title":"Compile Scala / Java source code","text":"<p>Sedona Scala/Java code is a project with multiple modules. Each module is a Scala/Java mixed project which is managed by Apache Maven 3.</p> <ul> <li>Make sure your Linux/Mac machine has Java 1.8, Apache Maven 3.3.1+, and Python3.7+. The compilation of Sedona is not tested on Windows machines.</li> </ul> <p>To compile all modules, please make sure you are in the root folder of all modules. Then enter the following command in the terminal:</p> Without unit testsWith unit testsWith Geotools jars packaged <p><pre><code>mvn clean install -DskipTests\n</code></pre> This command will first delete the old binary files and compile all modules. This compilation will skip the unit tests. To compile a single module, please make sure you are in the folder of that module. Then enter the same command.</p> <p><pre><code>mvn clean install\n</code></pre> The maven unit tests of all modules may take up to 30 minutes.</p> <p><pre><code>mvn clean install -DskipTests -Dgeotools\n</code></pre> Geotools jars will be packaged into the produced fat jars.</p> <p>Note</p> <p>By default, this command will compile Sedona with Spark 3.0 and Scala 2.12</p>"},{"location":"setup/compile/#compile-with-different-targets","title":"Compile with different targets","text":"<p>User can specify <code>-Dspark</code> and <code>-Dscala</code> command line options to compile with different targets. Available targets are:</p> <ul> <li><code>-Dspark</code>: <code>3.0</code> for Spark 3.0 to 3.3; <code>{major}.{minor}</code> for Spark 3.4 or later. For example, specify <code>-Dspark=3.4</code> to build for Spark 3.4.</li> <li><code>-Dscala</code>: <code>2.12</code> or <code>2.13</code></li> </ul> Spark 3.0 to 3.3 Scala 2.12Spark 3.4+ Scala 2.12Spark 3.0 to 3.3 Scala 2.13Spark 3.4+ Scala 2.13 <pre><code>mvn clean install -DskipTests -Dspark=3.0 -Dscala=2.12\n</code></pre> <p><pre><code>mvn clean install -DskipTests -Dspark=3.4 -Dscala=2.12\n</code></pre> Please replace <code>3.4</code> with Spark major.minor version when building for higher Spark versions.</p> <pre><code>mvn clean install -DskipTests -Dspark=3.0 -Dscala=2.13\n</code></pre> <p><pre><code>mvn clean install -DskipTests -Dspark=3.4 -Dscala=2.13\n</code></pre> Please replace <code>3.4</code> with Spark major.minor version when building for higher Spark versions.</p> <p>Tip</p> <p>To get the Sedona Spark Shaded jar with all GeoTools jars included, simply append <code>-Dgeotools</code> option. The command is like this:<code>mvn clean install -DskipTests -Dscala=2.12 -Dspark=3.0 -Dgeotools</code></p>"},{"location":"setup/compile/#download-staged-jars","title":"Download staged jars","text":"<p>Sedona uses GitHub Actions to automatically generate jars per commit. You can go here and download the jars by clicking the commits Artifacts tag.</p>"},{"location":"setup/compile/#run-python-test","title":"Run Python test","text":"<ol> <li>Set up the environment variable SPARK_HOME and PYTHONPATH</li> </ol> <p>For example, <pre><code>export SPARK_HOME=$PWD/spark-3.0.1-bin-hadoop2.7\nexport PYTHONPATH=$SPARK_HOME/python\n</code></pre> 2. Compile the Sedona Scala and Java code with <code>-Dgeotools</code> and then copy the sedona-spark-shaded-1.5.1.jar to SPARK_HOME/jars/ folder. <pre><code>cp spark-shaded/target/sedona-spark-shaded-xxx.jar $SPARK_HOME/jars/\n</code></pre> 3. Install the following libraries <pre><code>sudo apt-get -y install python3-pip python-dev libgeos-dev\nsudo pip3 install -U setuptools\nsudo pip3 install -U wheel\nsudo pip3 install -U virtualenvwrapper\nsudo pip3 install -U pipenv\n</code></pre> Homebrew can be used to install libgeos-dev in macOS: <code>brew install geos</code> 4. Set up pipenv to the desired Python version: 3.7, 3.8, or 3.9 <pre><code>cd python\npipenv --python 3.7\n</code></pre> 5. Install the PySpark version and the other dependency <pre><code>cd python\npipenv install pyspark\npipenv install --dev\n</code></pre> <code>pipenv install pyspark</code> installs the latest version of pyspark. In order to remain consistent with the installed spark version, use <code>pipenv install pyspark==&lt;spark_version&gt;</code> 6. Run the Python tests <pre><code>cd python\npipenv run python setup.py build_ext --inplace\npipenv run pytest tests\n</code></pre></p>"},{"location":"setup/compile/#compile-the-documentation","title":"Compile the documentation","text":"<p>The website is automatically built after each commit. The built website can be downloaded here:</p>"},{"location":"setup/compile/#mkdocs-website","title":"MkDocs website","text":"<p>The source code of the documentation website is written in Markdown and then compiled by MkDocs. The website is built upon the Material for MkDocs template.</p> <p>In the Sedona repository, the MkDocs configuration file mkdocs.yml is in the root folder and all documentation source code is in docs folder.</p> <p>To compile the source code and test the website on your local machine, please read the MkDocs Tutorial and Materials for MkDocs Tutorial.</p> <p>In short, you need to run:</p> <pre><code>pip install mkdocs\npip install mkdocs-material\npip install mkdocs-macros-plugin\npip install mkdocs-git-revision-date-localized-plugin\npip install mike\n</code></pre> <p>After installing MkDocs and MkDocs-Material, run these commands in the Sedona root folder:</p> <pre><code>mkdocs build\nmike deploy --update-aliases latest-snapshot -b website -p\nmike serve\n</code></pre>"},{"location":"setup/databricks/","title":"Install on Databricks","text":""},{"location":"setup/databricks/#community-edition-free-tier","title":"Community edition (free-tier)","text":"<p>You just need to install the Sedona jars and Sedona Python on Databricks using Databricks default web UI. Then everything will work.</p>"},{"location":"setup/databricks/#advanced-editions","title":"Advanced editions","text":"<p>We recommend Databricks 10.x+.</p> <p>Tip</p> <p>Wherobots Cloud provides a free tool to deploy Apache Sedona to Databricks. Please sign up here.</p> <ul> <li>Sedona 1.0.1 &amp; 1.1.0 is compiled against Spark 3.1 (~ Databricks DBR 9 LTS, DBR 7 is Spark 3.0)</li> <li>Sedona 1.1.1, 1.2.0 are compiled against Spark 3.2 (~ DBR 10 &amp; 11)</li> <li>Sedona 1.2.1, 1.3.1, 1.4.0 are complied against Spark 3.3</li> <li>1.4.1, 1.5.0 are complied against Spark 3.3, 3.4, 3.5</li> </ul> <p>In Spark 3.2, <code>org.apache.spark.sql.catalyst.expressions.Generator</code> class added a field <code>nodePatterns</code>. Any SQL functions that rely on Generator class may have issues if compiled for a runtime with a differing spark version. For Sedona, those functions are:    * ST_MakeValid    * ST_SubDivideExplode</p> <p>Note</p> <p>If you are using Spark 3.4+ and Scala 2.12, please use <code>sedona-spark-shaded-3.4_2.12</code>. Please pay attention to the Spark version postfix and Scala version postfix. Sedona is not able to support <code>Databricks photon acceleration</code>. Sedona requires Spark internal APIs to inject many optimization strategies, which is not accessible in <code>Photon</code>.</p>"},{"location":"setup/databricks/#install-sedona-from-the-web-ui-not-recommended","title":"Install Sedona from the web UI (not recommended)","text":"<p>This method cannot achieve the best performance of Sedona and does not work for pure SQL environment.</p>"},{"location":"setup/databricks/#install-libraries","title":"Install libraries","text":"<p>1) From the Libraries tab install from Maven Coordinates     <pre><code>org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.5.1\norg.datasyslab:geotools-wrapper:1.5.1-28.2\n</code></pre></p> <p>2) For enabling python support, from the Libraries tab install from PyPI     <pre><code>apache-sedona\nkeplergl==0.3.2\npydeck==0.8.0\n</code></pre></p>"},{"location":"setup/databricks/#initialize","title":"Initialize","text":"<p>After you have installed the libraries and started the cluster, you can initialize the Sedona <code>ST_*</code> functions and types by running from your code:</p> <p>(scala) <pre><code>import org.apache.sedona.sql.utils.SedonaSQLRegistrator\nSedonaSQLRegistrator.registerAll(spark)\n</code></pre></p> <p>(or python) <pre><code>from sedona.register.geo_registrator import SedonaRegistrator\nSedonaRegistrator.registerAll(spark)\n</code></pre></p>"},{"location":"setup/databricks/#install-sedona-from-the-init-script","title":"Install Sedona from the init script","text":"<p>In order to activate the Kryo serializer (this speeds up the serialization and deserialization of geometry types) you need to install the libraries via init script as described below.</p> <p>In order to use the Sedona <code>ST_*/RS_*</code> functions from SQL without having to register the Sedona functions from a python/scala cell, you need to install the Sedona libraries from the cluster init-scripts as follows.</p>"},{"location":"setup/databricks/#download-sedona-jars","title":"Download Sedona jars","text":"<p>Download the Sedona jars to a DBFS location. You can do that manually via UI or from a notebook by executing this code in a cell:</p> <pre><code>%sh\n# Create JAR directory for Sedona\nmkdir -p /Workspace/Shared/sedona/1.5.1\n\n# Download the dependencies from Maven into DBFS\ncurl -o /Workspace/Shared/sedona/1.5.1/geotools-wrapper-1.5.1-28.2.jar \"https://repo1.maven.org/maven2/org/datasyslab/geotools-wrapper/1.5.1-28.2/geotools-wrapper-1.5.1-28.2.jar\"\n\ncurl -o /Workspace/Shared/sedona/1.5.1/sedona-spark-shaded-3.4_2.12-1.5.1.jar \"https://repo1.maven.org/maven2/org/apache/sedona/sedona-spark-shaded-3.4_2.12/1.5.1/sedona-spark-shaded-3.4_2.12-1.5.1.jar\"\n</code></pre>"},{"location":"setup/databricks/#create-an-init-script","title":"Create an init script","text":"<p>Warning</p> <p>Starting from December 2023, Databricks has disabled all DBFS based init script (/dbfs/XXX/.sh).  So you will have to store the init script from a workspace level (<code>/Users/&lt;user-name&gt;/&lt;script-name&gt;.sh</code>) or Unity Catalog volume (<code>/Volumes/&lt;catalog&gt;/&lt;schema&gt;/&lt;volume&gt;/&lt;path-to-script&gt;/&lt;script-name&gt;.sh</code>). Please see https://docs.databricks.com/en/init-scripts/cluster-scoped.html#configure-a-cluster-scoped-init-script-using-the-ui <p>Create an init script in <code>Workspace</code> that loads the Sedona jars into the cluster's default jar directory. You can create that from any notebook by running:</p> <pre><code>%sh\n\n# Create init script directory for Sedona\nmkdir -p /Workspace/Shared/sedona/\n\n# Create init script\ncat &gt; /Workspace/Shared/sedona/sedona-init.sh &lt;&lt;'EOF'\n#!/bin/bash\n#\n# File: sedona-init.sh\n#\n# On cluster startup, this script will copy the Sedona jars to the cluster's default jar directory.\n# In order to activate Sedona functions, remember to add to your spark configuration the Sedona extensions: \"spark.sql.extensions org.apache.sedona.viz.sql.SedonaVizExtensions,org.apache.sedona.sql.SedonaSqlExtensions\"\n\ncp /Workspace/Shared/sedona/1.5.1/*.jar /databricks/jars\n\nEOF\n</code></pre>"},{"location":"setup/databricks/#set-up-cluster-config","title":"Set up cluster config","text":"<p>From your cluster configuration (<code>Cluster</code> -&gt; <code>Edit</code> -&gt; <code>Configuration</code> -&gt; <code>Advanced options</code> -&gt; <code>Spark</code>) activate the Sedona functions and the kryo serializer by adding to the Spark Config <pre><code>spark.sql.extensions org.apache.sedona.viz.sql.SedonaVizExtensions,org.apache.sedona.sql.SedonaSqlExtensions\nspark.serializer org.apache.spark.serializer.KryoSerializer\nspark.kryo.registrator org.apache.sedona.core.serde.SedonaKryoRegistrator\n</code></pre></p> <p>From your cluster configuration (<code>Cluster</code> -&gt; <code>Edit</code> -&gt; <code>Configuration</code> -&gt; <code>Advanced options</code> -&gt; <code>Init Scripts</code>) add the newly created <code>Workspace</code> init script <pre><code>/Workspace/sedona/sedona-init.sh\n</code></pre></p> <p>For enabling python support, from the Libraries tab install from PyPI <pre><code>apache-sedona==1.5.1\ngeopandas==0.11.1\nkeplergl==0.3.2\npydeck==0.8.0\n</code></pre></p> <p>Tips</p> <p>You need to install the Sedona libraries via init script because the libraries installed via UI are installed after the cluster has already started, and therefore the classes specified by the config <code>spark.sql.extensions</code>, <code>spark.serializer</code>, and <code>spark.kryo.registrator</code> are not available at startup time.*</p>"},{"location":"setup/docker/","title":"Sedona JupyterLab Docker Image","text":"<p>Sedona Docker images are available on Sedona official DockerHub repo.</p> <p>We provide a Docker image for Apache Sedona with Python JupyterLab and 1 master node and 1 worker node.</p>"},{"location":"setup/docker/#how-to-use","title":"How to use","text":""},{"location":"setup/docker/#pull-the-image-from-dockerhub","title":"Pull the image from DockerHub","text":"<p>Format:</p> <pre><code>docker pull apache/sedona:&lt;sedona_version&gt;\n</code></pre> <p>Example 1: Pull the latest image of Sedona master branch</p> <pre><code>docker pull apache/sedona:latest\n</code></pre> <p>Example 2: Pull the image of a specific Sedona release</p> <pre><code>docker pull apache/sedona:1.5.1\n</code></pre>"},{"location":"setup/docker/#start-the-container","title":"Start the container","text":"<p>Format:</p> <p><pre><code>docker run -e DRIVER_MEM=&lt;driver_mem&gt; -e EXECUTOR_MEM=&lt;executor_mem&gt; -p 8888:8888 -p 8080:8080 -p 8081:8081 -p 4040:4040 apache/sedona:&lt;sedona_version&gt;\n</code></pre> Driver memory and executor memory are optional. If their values are not given, the container will take 4GB RAM for the driver and 4GB RAM for the executor.</p> <p>Example 1:</p> <pre><code>docker run -e DRIVER_MEM=6g -e EXECUTOR_MEM=8g -p 8888:8888 -p 8080:8080 -p 8081:8081 -p 4040:4040 apache/sedona:latest\n</code></pre> <p>This command will start a container with 6GB RAM for the driver and 8GB RAM for the executor and use the latest Sedona image.</p> <p>This command will bind the container's ports 8888, 8080, 8081, 4040 to the host's ports 8888, 8080, 8081, 4040 respectively.</p> <p>Example 2:</p> <pre><code>docker run -p 8888:8888 -p 8080:8080 -p 8081:8081 -p 4040:4040 apache/sedona:1.5.1\n</code></pre> <p>This command will start a container with 4GB RAM for the driver and 4GB RAM for the executor and use Sedona 1.5.1 image.</p> <p>This command will bind the container's ports 8888, 8080, 8081, 4040 to the host's ports 8888, 8080, 8081, 4040 respectively.</p>"},{"location":"setup/docker/#start-coding","title":"Start coding","text":"<p>Open your browser and go to http://localhost:8888/ to start coding with Sedona.</p>"},{"location":"setup/docker/#notes","title":"Notes","text":"<ul> <li>This container assumes you have at least 8GB RAM and takes all your CPU cores and 8GM RAM. The 1 worker will take 4GB and the Jupyter program will take the remaining 4GB.</li> <li>Sedona in this container runs in the cluster mode. Only 1 notebook can be run at a time. If you want to run another notebook, please shut down the kernel of the current notebook first (How?).</li> </ul>"},{"location":"setup/docker/#how-to-build","title":"How to build","text":"<p>Clone the Sedona GitHub repository</p>"},{"location":"setup/docker/#build-the-image-against-a-sedona-release","title":"Build the image against a Sedona release","text":"<p>Requirements: docker (How?)</p> <p>Format:</p> <pre><code>./docker/sedona-spark-jupyterlab/build.sh &lt;spark_version&gt; &lt;sedona_version&gt; &lt;build_mode&gt;\n</code></pre> <p>Example:</p> <pre><code>./docker/sedona-spark-jupyterlab/build.sh 3.4.1 1.5.1\n</code></pre> <p><code>build_mode</code> is optional. If its value is not given or is <code>local</code>, the script will build the image locally. Otherwise, it will start a cross-platform compilation and push images directly to DockerHub.</p>"},{"location":"setup/docker/#build-the-image-against-the-latest-sedona-master","title":"Build the image against the latest Sedona master","text":"<p>Requirements: docker (How?), JDK &lt;= 19, maven3</p> <p>Format:</p> <pre><code>./docker/sedona-spark-jupyterlab/build.sh &lt;spark_version&gt; latest &lt;build_mode&gt;\n</code></pre> <p>Example:</p> <pre><code>./docker/sedona-spark-jupyterlab/build.sh 3.4.1 latest\n</code></pre> <p><code>build_mode</code> is optional. If its value is not given or is <code>local</code>, the script will build the image locally. Otherwise, it will start a cross-platform compilation and push images directly to DockerHub.</p>"},{"location":"setup/docker/#notes_1","title":"Notes","text":"<p>This docker image can only be built against Sedona 1.4.1+ and Spark 3.0+</p>"},{"location":"setup/docker/#cluster-configuration","title":"Cluster Configuration","text":""},{"location":"setup/docker/#software","title":"Software","text":"<ul> <li>OS: Ubuntu 22.02</li> <li>JDK: openjdk-19</li> <li>Python: 3.10</li> <li>Spark 3.4.1</li> </ul>"},{"location":"setup/docker/#web-ui","title":"Web UI","text":"<ul> <li>JupyterLab: http://localhost:8888/</li> <li>Spark master URL: spark://localhost:7077</li> <li>Spark job UI: http://localhost:4040</li> <li>Spark master web UI: http://localhost:8080/</li> <li>Spark work web UI: http://localhost:8081/</li> </ul>"},{"location":"setup/docker/#how-to-push-to-dockerhub","title":"How to push to DockerHub","text":"<p>Format:</p> <pre><code>docker login\n./docker/sedona-spark-jupyterlab/build.sh &lt;spark_version&gt; &lt;sedona_version&gt; release\n</code></pre> <p>Example:</p> <pre><code>docker login\n./docker/sedona-spark-jupyterlab/build.sh 3.4.1 1.5.1 release\n</code></pre>"},{"location":"setup/emr/","title":"Install on AWS EMR","text":"<p>We recommend Sedona-1.3.1-incuabting and above for EMR. In the tutorial, we use AWS Elastic MapReduce (EMR) 6.9.0. It has the following applications installed: Hadoop 3.3.3, JupyterEnterpriseGateway 2.6.0, Livy 0.7.1, Spark 3.3.0.</p> <p>Tip</p> <p>Wherobots Cloud provides a free tool to deploy Apache Sedona to AWS EMR. Please sign up here.</p> <p>This tutorial is tested on EMR on EC2 with EMR Studio (notebooks). EMR on EC2 uses YARN to manage resources.</p> <p>Note</p> <p>If you are using Spark 3.4+ and Scala 2.12, please use <code>sedona-spark-shaded-3.4_2.12</code>. Please pay attention to the Spark version postfix and Scala version postfix.</p>"},{"location":"setup/emr/#prepare-initialization-script","title":"Prepare initialization script","text":"<p>In your S3 bucket, add a script that has the following content:</p> <pre><code>#!/bin/bash\n\n# EMR clusters only have ephemeral local storage. It does not really matter where we store the jars.\nsudo mkdir /jars\n\n# Download Sedona jar\nsudo curl -o /jars/sedona-spark-shaded-3.0_2.12-1.5.1.jar \"https://repo1.maven.org/maven2/org/apache/sedona/sedona-spark-shaded-3.0_2.12/1.5.1/sedona-spark-shaded-3.0_2.12-1.5.1.jar\"\n\n# Download GeoTools jar\nsudo curl -o /jars/geotools-wrapper-1.5.1-28.2.jar \"https://repo1.maven.org/maven2/org/datasyslab/geotools-wrapper/1.5.1-28.2/geotools-wrapper-1.5.1-28.2.jar\"\n\n# Install necessary python libraries\nsudo python3 -m pip install pandas==1.3.5\nsudo python3 -m pip install shapely==1.8.5\nsudo python3 -m pip install geopandas==0.11.1\nsudo python3 -m pip install keplergl==0.3.2\nsudo python3 -m pip install pydeck==0.8.0\nsudo python3 -m pip install attrs matplotlib descartes apache-sedona==1.5.1\n</code></pre> <p>When you create a EMR cluster, in the <code>bootstrap action</code>, specify the location of this script.</p>"},{"location":"setup/emr/#add-software-configuration","title":"Add software configuration","text":"<p>When you create a EMR cluster, in the software configuration, add the following content:</p> <pre><code>[\n{\n\"Classification\":\"spark-defaults\",\n    \"Properties\":{\n\"spark.yarn.dist.jars\": \"/jars/sedona-spark-shaded-3.0_2.12-1.5.1.jar,/jars/geotools-wrapper-1.5.1-28.2.jar\",\n      \"spark.serializer\": \"org.apache.spark.serializer.KryoSerializer\",\n      \"spark.kryo.registrator\": \"org.apache.sedona.core.serde.SedonaKryoRegistrator\",\n      \"spark.sql.extensions\": \"org.apache.sedona.viz.sql.SedonaVizExtensions,org.apache.sedona.sql.SedonaSqlExtensions\"\n}\n}\n]\n</code></pre> <p>Note</p> <p>If you use Sedona 1.3.1-incubating, please use <code>sedona-python-adpater-3.0_2.12</code> jar in the content above, instead of <code>sedona-spark-shaded-3.0_2.12</code>.</p>"},{"location":"setup/install-python/","title":"Install Sedona Python","text":"<p>Click  and play the interactive Sedona Python Jupyter Notebook immediately!</p> <p>Apache Sedona extends pyspark functions which depends on libraries:</p> <ul> <li>pyspark</li> <li>shapely</li> <li>attrs</li> </ul> <p>You need to install necessary packages if your system does not have them installed. See \"packages\" in our Pipfile.</p>"},{"location":"setup/install-python/#install-sedona","title":"Install sedona","text":"<ul> <li>Installing from PyPI repositories. You can find the latest Sedona Python on PyPI. There is an known issue in Sedona v1.0.1 and earlier versions.</li> </ul> <pre><code>pip install apache-sedona\n</code></pre> <ul> <li>Since Sedona v1.1.0, pyspark is an optional dependency of Sedona Python because spark comes pre-installed on many spark platforms. To install pyspark along with Sedona Python in one go, use the <code>spark</code> extra:</li> </ul> <pre><code>pip install apache-sedona[spark]\n</code></pre> <ul> <li>Installing from Sedona Python source</li> </ul> <p>Clone Sedona GitHub source code and run the following command</p> <pre><code>cd python\npython3 setup.py install\n</code></pre>"},{"location":"setup/install-python/#prepare-sedona-spark-shaded-jar","title":"Prepare sedona-spark-shaded jar","text":"<p>Sedona Python needs one additional jar file called <code>sedona-spark-shaded</code> to work properly. Please make sure you use the correct version for Spark and Scala.</p> <ul> <li>For Spark 3.0 to 3.3 and Scala 2.12, it is called <code>sedona-spark-shaded-3.0_2.12-1.5.1.jar</code></li> <li>For Spark 3.4+ and Scala 2.12, it is called <code>sedona-spark-shaded-3.4_2.12-1.5.1.jar</code>. If you are using Spark versions higher than 3.4, please replace the <code>3.4</code> in artifact names with the corresponding major.minor version numbers.</li> </ul> <p>You can get it using one of the following methods:</p> <ol> <li>Download sedona-spark-shaded jar and geotools-wrapper jar from Maven Central, and put them in SPARK_HOME/jars/ folder.</li> <li>Call the Maven Central coordinate in your python program. For example, Sedona &gt;= 1.4.1</li> </ol> <pre><code>from sedona.spark import *\nconfig = SedonaContext.builder(). \\\n    config('spark.jars.packages',\n           'org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.5.1,'\n           'org.datasyslab:geotools-wrapper:1.5.1-28.2'). \\\n    getOrCreate()\nsedona = SedonaContext.create(config)\n</code></pre> <p>Sedona &lt; 1.4.1</p> <p>SedonaRegistrator is deprecated in Sedona 1.4.1 and later versions. Please use the above method instead.</p> <pre><code>from pyspark.sql import SparkSession\nfrom sedona.register import SedonaRegistrator\nfrom sedona.utils import SedonaKryoRegistrator, KryoSerializer\nspark = SparkSession. \\\n    builder. \\\n    appName('appName'). \\\n    config(\"spark.serializer\", KryoSerializer.getName). \\\n    config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName). \\\n    config('spark.jars.packages',\n           'org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.5.1,'\n           'org.datasyslab:geotools-wrapper:1.5.1-28.2'). \\\n    getOrCreate()\nSedonaRegistrator.registerAll(spark)\n</code></pre>"},{"location":"setup/install-python/#setup-environment-variables","title":"Setup environment variables","text":"<p>If you manually copy the sedona-spark-shaded jar to <code>SPARK_HOME/jars/</code> folder, you need to setup two environment variables</p> <ul> <li>SPARK_HOME. For example, run the command in your terminal</li> </ul> <pre><code>export SPARK_HOME=~/Downloads/spark-3.0.1-bin-hadoop2.7\n</code></pre> <ul> <li>PYTHONPATH. For example, run the command in your terminal</li> </ul> <pre><code>export PYTHONPATH=$SPARK_HOME/python\n</code></pre> <p>You can then play with Sedona Python Jupyter notebook.</p>"},{"location":"setup/install-scala/","title":"Install Sedona Scala/Java","text":"<p>Before starting the Sedona journey, you need to make sure your Apache Spark cluster is ready.</p> <p>There are two ways to use a Scala or Java library with Apache Spark. You can user either one to run Sedona.</p> <ul> <li>Spark interactive Scala or SQL shell: easy to start, good for new learners to try simple functions</li> <li>Self-contained Scala / Java project: a steep learning curve of package management, but good for large projects</li> </ul>"},{"location":"setup/install-scala/#spark-scala-shell","title":"Spark Scala shell","text":""},{"location":"setup/install-scala/#download-sedona-jar-automatically","title":"Download Sedona jar automatically","text":"<ol> <li> <p>Have your Spark cluster ready.</p> </li> <li> <p>Run Spark shell with <code>--packages</code> option. This command will automatically download Sedona jars from Maven Central. <pre><code>./bin/spark-shell --packages MavenCoordinates\n</code></pre> Please refer to Sedona Maven Central coordinates to select the corresponding Sedona packages for your Spark version.</p> <ul> <li> <p>Local mode: test Sedona without setting up a cluster <pre><code>./bin/spark-shell --packages org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.5.1,org.datasyslab:geotools-wrapper:1.5.1-28.2\n</code></pre></p> </li> <li> <p>Cluster mode: you need to specify Spark Master IP <pre><code>./bin/spark-shell --master spark://localhost:7077 --packages org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.5.1,org.datasyslab:geotools-wrapper:1.5.1-28.2\n</code></pre></p> </li> </ul> </li> </ol>"},{"location":"setup/install-scala/#download-sedona-jar-manually","title":"Download Sedona jar manually","text":"<ol> <li> <p>Have your Spark cluster ready.</p> </li> <li> <p>Download Sedona jars:</p> <ul> <li>Download the pre-compiled jars from Sedona Releases</li> <li>Download / Git clone Sedona source code and compile the code by yourself (see Compile Sedona)</li> </ul> </li> <li> <p>Run Spark shell with <code>--jars</code> option. <pre><code>./bin/spark-shell --jars /Path/To/SedonaJars.jar\n</code></pre> If you are using Spark 3.0 to 3.3, please use jars with filenames containing <code>3.0</code>, such as <code>sedona-spark-shaded-3.0_2.12-1.5.1</code>; If you are using Spark 3.4 or higher versions, please use jars with Spark major.minor versions in the filename, such as <code>sedona-spark-shaded-3.4_2.12-1.5.1</code>.</p> <ul> <li> <p>Local mode: test Sedona without setting up a cluster <pre><code>./bin/spark-shell --jars /path/to/sedona-spark-shaded-3.0_2.12-1.5.1.jar,/path/to/geotools-wrapper-1.5.1-28.2.jar\n</code></pre></p> </li> <li> <p>Cluster mode: you need to specify Spark Master IP <pre><code>./bin/spark-shell --master spark://localhost:7077 --jars /path/to/sedona-spark-shaded-3.0_2.12-1.5.1.jar,/path/to/geotools-wrapper-1.5.1-28.2.jar\n</code></pre></p> </li> </ul> </li> </ol>"},{"location":"setup/install-scala/#spark-sql-shell","title":"Spark SQL shell","text":"<p>Please see Use Sedona in a pure SQL environment</p>"},{"location":"setup/install-scala/#self-contained-spark-projects","title":"Self-contained Spark projects","text":"<p>A self-contained project allows you to create multiple Scala / Java files and write complex logics in one place. To use Sedona in your self-contained Spark project, you just need to add Sedona as a dependency in your pom.xml or build.sbt.</p> <ol> <li>To add Sedona as dependencies, please read Sedona Maven Central coordinates</li> <li>Use Sedona Template project to start: Sedona Template Project</li> <li>Compile your project using SBT. Make sure you obtain the fat jar which packages all dependencies.</li> <li>Submit your compiled fat jar to Spark cluster. Make sure you are in the root folder of Spark distribution. Then run the following command: <pre><code>./bin/spark-submit --master spark://YOUR-IP:7077 /Path/To/YourJar.jar\n</code></pre></li> </ol> <p>Note</p> <p>The detailed explanation of spark-submit is available on Spark website.</p>"},{"location":"setup/maven-coordinates/","title":"Maven Coordinates","text":""},{"location":"setup/maven-coordinates/#use-sedona-shaded-fat-jars","title":"Use Sedona shaded (fat) jars","text":"<p>Warning</p> <p>For Scala/Java/Python users, this is the most common way to use Sedona in your environment. Do not use separate Sedona jars unless you are sure that you do not need shaded jars.</p> <p>Warning</p> <p>For R users, this is the only way to use Sedona in your environment.</p> <p>Apache Sedona provides different packages for each supported version of Spark.</p> <ul> <li>For Spark 3.0 to 3.3, the artifact to use should be <code>sedona-spark-shaded-3.0_2.12</code>.</li> <li>For Spark 3.4 or higher versions, please use the artifact with Spark major.minor version in the artifact name. For example, for Spark 3.4, the artifacts to use should be <code>sedona-spark-shaded-3.4_2.12</code>.</li> </ul> <p>If you are using the Scala 2.13 builds of Spark, please use the corresponding packages for Scala 2.13, which are suffixed by <code>_2.13</code>.</p> <p>The optional GeoTools library is required if you want to use CRS transformation, ShapefileReader or GeoTiff reader. This wrapper library is a re-distribution of GeoTools official jars. The only purpose of this library is to bring GeoTools jars from OSGEO repository to Maven Central. This library is under GNU Lesser General Public License (LGPL) license so we cannot package it in Sedona official release.</p> <p>Sedona with Apache Spark and Scala 2.12</p> Spark 3.0 to 3.3 and Scala 2.12Spark 3.4+ and Scala 2.12Spark 3.5 and Scala 2.12 <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-spark-shaded-3.0_2.12&lt;/artifactId&gt;\n&lt;version&gt;1.5.1&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- Optional: https://mvnrepository.com/artifact/org.datasyslab/geotools-wrapper --&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n&lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n&lt;version&gt;1.5.1-28.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-spark-shaded-3.4_2.12&lt;/artifactId&gt;\n&lt;version&gt;1.5.1&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- Optional: https://mvnrepository.com/artifact/org.datasyslab/geotools-wrapper --&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n&lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n&lt;version&gt;1.5.1-28.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-spark-shaded-3.5_2.12&lt;/artifactId&gt;\n&lt;version&gt;1.5.1&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- Optional: https://mvnrepository.com/artifact/org.datasyslab/geotools-wrapper --&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n&lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n&lt;version&gt;1.5.1-28.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Sedona with Apache Spark and Scala 2.13</p> Spark 3.0 to 3.3 and Scala 2.13Spark 3.4+ and Scala 2.13Spark 3.5 and Scala 2.13 <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-spark-shaded-3.0_2.13&lt;/artifactId&gt;\n&lt;version&gt;1.5.1&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- Optional: https://mvnrepository.com/artifact/org.datasyslab/geotools-wrapper --&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n&lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n&lt;version&gt;1.5.1-28.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-spark-shaded-3.4_2.13&lt;/artifactId&gt;\n&lt;version&gt;1.5.1&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- Optional: https://mvnrepository.com/artifact/org.datasyslab/geotools-wrapper --&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n&lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n&lt;version&gt;1.5.1-28.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-spark-shaded-3.5_2.13&lt;/artifactId&gt;\n&lt;version&gt;1.5.1&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- Optional: https://mvnrepository.com/artifact/org.datasyslab/geotools-wrapper --&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n&lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n&lt;version&gt;1.5.1-28.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Sedona with Apache Flink</p> Flink 1.12+ and Scala 2.12 <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-flink-shaded_2.12&lt;/artifactId&gt;\n&lt;version&gt;1.5.1&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- Optional: https://mvnrepository.com/artifact/org.datasyslab/geotools-wrapper --&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n&lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n&lt;version&gt;1.5.1-28.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Sedona with Snowflake</p> Snowflake 7.0+ (Year 2023 and later) <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-snowflake&lt;/artifactId&gt;\n&lt;version&gt;1.5.1&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- Optional: https://mvnrepository.com/artifact/org.datasyslab/geotools-wrapper --&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n&lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n&lt;version&gt;1.5.1-28.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"setup/maven-coordinates/#netcdf-java-542","title":"netCDF-Java 5.4.2","text":"<p>This is required only if you want to read HDF/NetCDF files using <code>RS_FromNetCDF</code>. Note that this JAR is not in Maven Central so you will need to add this repository to your pom.xml or build.sbt, or specify the URL in Spark Config <code>spark.jars.repositories</code> or spark-submit <code>--repositories</code> option.</p> <p>Under BSD 3-clause (compatible with Apache 2.0 license)</p> <p>Add HDF/NetCDF dependency</p> Sedona 1.3.1+Before Sedona 1.3.1 <p>Add unidata repo to your pom.xml</p> <pre><code>&lt;repositories&gt;\n    &lt;repository&gt;\n        &lt;id&gt;unidata-all&lt;/id&gt;\n        &lt;name&gt;Unidata All&lt;/name&gt;\n        &lt;url&gt;https://artifacts.unidata.ucar.edu/repository/unidata-all/&lt;/url&gt;\n    &lt;/repository&gt;\n&lt;/repositories&gt;\n</code></pre> <p>Then add cdm-core to your POM dependency.</p> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;edu.ucar&lt;/groupId&gt;\n&lt;artifactId&gt;cdm-core&lt;/artifactId&gt;\n&lt;version&gt;5.4.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>&lt;!-- https://mvnrepository.com/artifact/org.datasyslab/sernetcdf --&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n&lt;artifactId&gt;sernetcdf&lt;/artifactId&gt;\n&lt;version&gt;0.1.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"setup/maven-coordinates/#use-sedona-unshaded-jars","title":"Use Sedona unshaded jars","text":"<p>Warning</p> <p>For Scala, Java, Python users, please use the following jars only if you satisfy these conditions: (1) you know how to exclude transient dependencies in a complex application. (2) your environment has internet access (3) you are using some sort of Maven package resolver, or pom.xml, or build.sbt. It usually directly takes an input like this <code>GroupID:ArtifactID:Version</code>. If you don't understand what we are talking about, the following jars are not for you.</p> <p>Apache Sedona provides different packages for each supported version of Spark.</p> <ul> <li>For Spark 3.0 to 3.3, the artifacts to use should be <code>sedona-spark-3.0_2.12</code>.</li> <li>For Spark 3.4 or higher versions, please use the artifacts with Spark major.minor version in the artifact name. For example, for Spark 3.4, the artifacts to use should be <code>sedona-spark-3.4_2.12</code>.</li> </ul> <p>If you are using the Scala 2.13 builds of Spark, please use the corresponding packages for Scala 2.13, which are suffixed by <code>_2.13</code>.</p> <p>The optional GeoTools library is required if you want to use CRS transformation, ShapefileReader or GeoTiff reader. This wrapper library is a re-distribution of GeoTools official jars. The only purpose of this library is to bring GeoTools jars from OSGEO repository to Maven Central. This library is under GNU Lesser General Public License (LGPL) license so we cannot package it in Sedona official release.</p> <p>Sedona with Apache Spark and Scala 2.12</p> Spark 3.0 to 3.3 and Scala 2.12Spark 3.4+ and Scala 2.12Spark 3.5 and Scala 2.12 <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-spark-3.0_2.12&lt;/artifactId&gt;\n&lt;version&gt;1.5.1&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n&lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n&lt;version&gt;1.5.1-28.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-spark-3.4_2.12&lt;/artifactId&gt;\n&lt;version&gt;1.5.1&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n&lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n&lt;version&gt;1.5.1-28.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-spark-3.5_2.12&lt;/artifactId&gt;\n&lt;version&gt;1.5.1&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n&lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n&lt;version&gt;1.5.1-28.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Sedona with Apache Spark and Scala 2.13</p> Spark 3.0+ and Scala 2.13Spark 3.4+ and Scala 2.13Spark 3.5 and Scala 2.13 <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-spark-3.0_2.13&lt;/artifactId&gt;\n&lt;version&gt;1.5.1&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n&lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n&lt;version&gt;1.5.1-28.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-spark-3.4_2.13&lt;/artifactId&gt;\n&lt;version&gt;1.5.1&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n&lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n&lt;version&gt;1.5.1-28.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-spark-3.5_2.13&lt;/artifactId&gt;\n&lt;version&gt;1.5.1&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n&lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n&lt;version&gt;1.5.1-28.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Sedona with Apache Flink</p> Flink 1.12+ and Scala 2.12 <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-flink_2.12&lt;/artifactId&gt;\n&lt;version&gt;1.5.1&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n&lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n&lt;version&gt;1.5.1-28.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Sedona Snowflake does not have an unshaded version.</p>"},{"location":"setup/maven-coordinates/#netcdf-java-542_1","title":"netCDF-Java 5.4.2","text":"<p>This is required only if you want to read HDF/NetCDF files using <code>RS_FromNetCDF</code>. Note that this JAR is not in Maven Central so you will need to add this repository to your pom.xml or build.sbt, or specify the URL in Spark Config <code>spark.jars.repositories</code> or spark-submit <code>--repositories</code> option.</p> <p>Under BSD 3-clause (compatible with Apache 2.0 license)</p> <p>Add HDF/NetCDF dependency</p> Sedona 1.3.1+Before Sedona 1.3.1 <p>Add unidata repo to your pom.xml</p> <pre><code>&lt;repositories&gt;\n    &lt;repository&gt;\n        &lt;id&gt;unidata-all&lt;/id&gt;\n        &lt;name&gt;Unidata All&lt;/name&gt;\n        &lt;url&gt;https://artifacts.unidata.ucar.edu/repository/unidata-all/&lt;/url&gt;\n    &lt;/repository&gt;\n&lt;/repositories&gt;\n</code></pre> <p>Then add cdm-core to your POM dependency.</p> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;edu.ucar&lt;/groupId&gt;\n&lt;artifactId&gt;cdm-core&lt;/artifactId&gt;\n&lt;version&gt;5.4.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>&lt;!-- https://mvnrepository.com/artifact/org.datasyslab/sernetcdf --&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n&lt;artifactId&gt;sernetcdf&lt;/artifactId&gt;\n&lt;version&gt;0.1.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"setup/maven-coordinates/#snapshot-versions","title":"SNAPSHOT versions","text":"<p>Sometimes Sedona has a SNAPSHOT version for the upcoming release. It follows the same naming conversion but has \"SNAPSHOT\" as suffix in the version. For example, <code>1.5.1-SNAPSHOT</code></p> <p>In order to download SNAPSHOTs, you need to add the following repositories in your pom.xml or build.sbt</p>"},{"location":"setup/maven-coordinates/#buildsbt","title":"build.sbt","text":"<p>resolvers +=   \"Apache Software Foundation Snapshots\" at \"https://repository.apache.org/content/groups/snapshots\"</p>"},{"location":"setup/maven-coordinates/#pomxml","title":"pom.xml","text":"<pre><code>&lt;repositories&gt;\n&lt;repository&gt;\n&lt;id&gt;snapshots-repo&lt;/id&gt;\n&lt;url&gt;https://repository.apache.org/content/groups/snapshots&lt;/url&gt;\n&lt;releases&gt;&lt;enabled&gt;false&lt;/enabled&gt;&lt;/releases&gt;\n&lt;snapshots&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/snapshots&gt;\n&lt;/repository&gt;\n&lt;/repositories&gt;\n</code></pre>"},{"location":"setup/modules/","title":"Sedona modules for Apache Spark","text":"Name API Introduction spark RDD / SQL / DataFrame SpatialRDD and Spatial DataFrame spark-shaded Shaded version python Python interface for SpatialRDD and Spatial DataFrame Zeppelin Apache Zeppelin Plugin for Apache Zeppelin 0.8.1+"},{"location":"setup/modules/#api-availability","title":"API availability","text":"Core/RDD DataFrame/SQL Viz RDD/SQL Scala/Java \u2705 \u2705 \u2705 Python \u2705 \u2705 SQL only R \u2705 \u2705 \u2705"},{"location":"setup/overview/","title":"Download statistics","text":"Download statistics Maven PyPI Conda-forge CRAN DockerHub Apache Sedona 225k/month Archived GeoSpark releases 10k/month"},{"location":"setup/overview/#what-can-sedona-do","title":"What can Sedona do?","text":""},{"location":"setup/overview/#distributed-spatial-datasets","title":"Distributed spatial datasets","text":"<ul> <li> Spatial RDD on Spark</li> <li> Spatial DataFrame/SQL on Spark</li> <li> Spatial DataStream on Flink</li> <li> Spatial Table/SQL on Flink</li> </ul>"},{"location":"setup/overview/#complex-spatial-objects","title":"Complex spatial objects","text":"<ul> <li> Vector geometries / trajectories</li> <li> Raster images with Map Algebra</li> <li> Various input formats: CSV, TSV, WKT, WKB, GeoJSON, Shapefile, GeoTIFF, ArcGrid, NetCDF/HDF</li> </ul>"},{"location":"setup/overview/#distributed-spatial-queries","title":"Distributed spatial queries","text":"<ul> <li> Spatial query: range query, range join query, distance join query, K Nearest Neighbor query</li> <li> Spatial index: R-Tree, Quad-Tree</li> </ul>"},{"location":"setup/overview/#rich-spatial-analytics-tools","title":"Rich spatial analytics tools","text":"<ul> <li> Coordinate Reference System / Spatial Reference System Transformation</li> <li> High resolution map generation: Visualize Spatial DataFrame/RDD</li> <li> Apache Zeppelin integration</li> <li> Support Scala, Java, Python, R</li> </ul>"},{"location":"setup/platform/","title":"Language wrappers","text":"<p>Sedona binary releases are compiled by Java 1.8 and Scala 2.11/2.12 and tested in the following environments:</p> <p>Warning</p> <p>Support of Spark 2.X and Scala 2.11 was removed in Sedona 1.3.0+ although some parts of the source code might still be compatible. Sedona 1.3.0+ release binary for both Scala 2.12 and 2.13.</p> Sedona Scala/JavaSedona PythonSedona R Spark 2.4 Spark 3.0 Spark 3.1 Spark 3.2 Spark 3.3 Spark 3.4 Spark 3.5 Scala 2.11 not tested not tested not tested not tested not tested not tested not tested Scala 2.12 not tested \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Scala 2.13 not tested not tested not tested not tested \u2705 \u2705 \u2705 Spark 2.4 (Scala 2.11) Spark 3.0 (Scala 2.12) Spark 3.1 (Scala 2.12) Spark 3.2 (Scala 2.12) Spark 3.3 (Scala 2.12) Spark 3.4 (Scala 2.12) Spark 3.5 (Scala 2.12) Python 3.7 not tested \u2705 \u2705 \u2705 \u2705 \u2705 not tested Python 3.8 not tested not tested not tested not tested \u2705 \u2705 \u2705 Python 3.9 not tested not tested not tested not tested \u2705 \u2705 \u2705 Python 3.10 not tested not tested not tested not tested \u2705 \u2705 \u2705 Spark 2.4 Spark 3.0 Spark 3.1 Spark 3.2 Spark 3.3 Spark 3.4 Spark 3.5 Scala 2.11 not tested not tested not tested not tested not tested not tested not tested Scala 2.12 not tested \u2705 \u2705 \u2705 \u2705 \u2705 \u2705"},{"location":"setup/release-notes/","title":"Release notes","text":"<p>Note</p> <p>Support of Spark 2.X and Scala 2.11 was removed in Sedona 1.3.0+ although some parts of the source code might still be compatible. Sedona 1.3.0+ releases binary for both Scala 2.12 and 2.13.</p> <p>Note</p> <p>Sedona Python currently only works with Shapely 1.x. If you use GeoPandas, please use &lt;= GeoPandas <code>0.11.1</code>. GeoPandas &gt; 0.11.1 will automatically install Shapely 2.0. If you use Shapely, please use &lt;= <code>1.8.4</code>.</p>"},{"location":"setup/release-notes/#sedona-151","title":"Sedona 1.5.1","text":"<p>Sedona 1.5.1 is compiled against Spark 3.3 / Spark 3.4 / Spark 3.5, Flink 1.12, Snowflake 7+, Java 8.</p>"},{"location":"setup/release-notes/#highlights","title":"Highlights","text":"<ul> <li> Sedona Snowflake Add support for Snowflake</li> <li> Sedona Spark Support Spark 3.5</li> <li> Sedona Spark Support Snowflake 7+</li> <li> Sedona Spark Added 20+ raster functions (or variants)</li> <li> Sedona Spark/Flink/Snowflake Added 7 vector functions (or variants)</li> <li> Sedona Spark GeoParquet reader and writer supports projjson in metadata</li> <li> Sedona Spark GeoParquet reader and writer conform to GeoParquet spec 1.0.0 instead of 1.0.0-beta1</li> <li> Sedona Spark Added a legacyMode in GeoParquet reader for 1.5.1+ users to read Parquet files written by Sedona 1.3.1 and earlier</li> <li> Sedona Spark Fixed a bug in GeoParquet writer so 1.3.1 and earlier users can read Parquet files written by 1.5.1+</li> </ul>"},{"location":"setup/release-notes/#behavior-change","title":"Behavior change","text":"<ul> <li>All raster functions that take a geometry will implicitly transform the CRS of the geometry if needed.</li> <li>The default CRS for these functions is 4326 for raster and geometry involved in raster functions, if not specified.</li> <li>KeplerGL and DeckGL become optional dependencies for Sedona Spark Python.</li> </ul>"},{"location":"setup/release-notes/#new-contributors","title":"New Contributors","text":"<ul> <li>@hongbo-miao made their first contribution in https://github.com/apache/sedona/pull/1063</li> <li>@prantogg made their first contribution in https://github.com/apache/sedona/pull/1122</li> <li>@MyEnthusiastic made their first contribution in https://github.com/apache/sedona/pull/1130</li> <li>@duhaode520 made their first contribution in https://github.com/apache/sedona/pull/1193</li> </ul>"},{"location":"setup/release-notes/#bug","title":"Bug","text":"<ul> <li>[SEDONA-414] -         ST_MakeLine in sedona-spark does not work with array inputs </li> <li>[SEDONA-417] -         Fix SedonaUtils.display_image </li> <li>[SEDONA-419] -         SedonaKepler and SedonaPyDeck should not be in `sedona.spark` </li> <li>[SEDONA-420] -         Make SedonaKepler and SedonaPydeck optional dependencies </li> <li>[SEDONA-424] -         Specify jt-jiffle as a provided dependency </li> <li>[SEDONA-426] -         Change cloning of rasters to be able to include metadata. </li> <li>[SEDONA-440] -         GeoParquet reader should support filter pushdown on nested fields </li> <li>[SEDONA-443] -         Upload-artifact leads to 503 error </li> <li>[SEDONA-453] -         Performance degrade when indexing points using Quadtree </li> <li>[SEDONA-456] -         SedonaKepler cannot work with geopandas &gt;= 0.13.0 correctly </li> </ul>"},{"location":"setup/release-notes/#new-feature","title":"New Feature","text":"<ul> <li>[SEDONA-369] -         Add ST_DWITHIN </li> <li>[SEDONA-411] -         Add RS_Rotation </li> <li>[SEDONA-413] -         Add buffer parameters to ST_Buffer </li> <li>[SEDONA-415] -         Add optional parameter to ST_Transform </li> <li>[SEDONA-421] -         Add RS_Clip </li> <li>[SEDONA-422] -         Add a feature in RS_SetBandNoDataValue and fix NoDataValue in RS_Clip </li> <li>[SEDONA-427] -         Add RS_RasterToWorldCoord </li> <li>[SEDONA-428] -         Add RS_ZonalStats &amp; RS_ZonalStatsAll </li> <li>[SEDONA-430] -         geoparquet writer should have an option called `writeToCrs` </li> <li>[SEDONA-431] -         Add RS_PixelAsPoints </li> <li>[SEDONA-432] -         Add RS_PixelAsCentroids </li> <li>[SEDONA-433] -         Improve RS_SummaryStats performance </li> <li>[SEDONA-435] -         Add RS_PixelAsPolygons </li> <li>[SEDONA-438] -         Add NetCDF reader to Sedona </li> <li>[SEDONA-439] -         Add RS_Union_Aggr </li> <li>[SEDONA-441] -         Implement ST_LineLocatePoint </li> <li>[SEDONA-449] -         Add two raster column support to RS_MapAlgebra </li> <li>[SEDONA-455] -         Add a new data source namely geoparquet.metadata </li> <li>[SEDONA-459] -         Add Snowflake support </li> <li>[SEDONA-460] -         RS_Tile and RS_TileExplode </li> <li>[SEDONA-461] -         ST_IsValidReason </li> <li>[SEDONA-465] -         Support reading legacy parquet files written by Apache Sedona &lt;= 1.3.1-incubating </li> </ul>"},{"location":"setup/release-notes/#improvement","title":"Improvement","text":"<ul> <li>[SEDONA-339] -         Skip irrelevant GitHub actions </li> <li>[SEDONA-416] -         importing SedonaContext, kepler.gl is not found. </li> <li>[SEDONA-429] -         geoparquet reader/writer should print \"1.0.0\" in its version </li> <li>[SEDONA-434] -         Improve reliability by resolve the nondeterministic of the order of the Map </li> <li>[SEDONA-436] -         Fix RS_SetValues bug </li> <li>[SEDONA-437] -         Add implicit CRS transformation </li> <li>[SEDONA-446] -         Add floating point datatype support in RS_AsBase64 </li> <li>[SEDONA-448] -         RS_SetBandNoDataValue should have `replace` option </li> <li>[SEDONA-454] -         Change the default value of sedona.global.indextype from quadtree to rtree </li> <li>[SEDONA-457] -         Don't write GeometryUDT into org.apache.spark.sql.parquet.row.metadata when writing GeoParquet files </li> <li>[SEDONA-464] -         ST_Valid should have integer flags </li> <li>[SEDONA-466] -         RS_AsRaster does not use the weight and height of the raster in its parameters. </li> </ul>"},{"location":"setup/release-notes/#test","title":"Test","text":"<li>[SEDONA-410] -         pre-commit: check that scripts with shebangs are executable </li> <li>[SEDONA-412] -         pre-commit: add hook `end-of-file-fixer` </li> <li>[SEDONA-423] -         pre-commit: apply hook `end-of-file-fixer` to more files </li> <li>[SEDONA-442] -         pre-commit: add hook markdown-lint </li> <li>[SEDONA-444] -         pre-commit: add hook to trim trailing whitespace </li> <li>[SEDONA-445] -         pre-commit: apply hook end-of-file-fixer to more files </li> <li>[SEDONA-447] -         pre-commit: apply end-of-file-fixer to more files </li> <li>[SEDONA-463] -         Add a Makefile for convenience </li>"},{"location":"setup/release-notes/#task","title":"Task","text":"<ul> <li>[SEDONA-450] -         Support Spark 3.5 </li> <li>[SEDONA-458] -         The docs should have examples for UDF </li> </ul>"},{"location":"setup/release-notes/#sedona-150","title":"Sedona 1.5.0","text":"<p>Sedona 1.5.0 is compiled against Spark 3.3 / Spark 3.4 / Flink 1.12, Java 8.</p>"},{"location":"setup/release-notes/#highlights_1","title":"Highlights","text":"<p>API breaking changes:</p> <ul> <li>The following functions in Sedona requires the input data must be in longitude/latitude order otherwise they might throw errors. You can use <code>FlipCoordinates</code> to swap X and Y.<ul> <li>ST_Transform</li> <li>ST_DistanceSphere</li> <li>ST_DistanceSpheroid</li> <li>ST_GeoHash</li> <li>All ST_H3 functions</li> <li>All ST_S2 functions</li> <li>All RS constructors</li> <li>All RS predicates</li> <li>Spark RDD: CRStransform</li> </ul> </li> <li>Rename <code>RS_Count</code> to <code>RS_CountValue</code></li> <li>Drop <code>RS_HTML</code></li> <li>Unshaded Sedona Spark code are all merged to a single jar <code>sedona-spark</code></li> </ul> <p>New features</p> <ul> <li>Add 18 more ST functions for vector data processing in Sedona Spark and Sedona Flink</li> <li>Add 36 more RS functions in Sedona Spark to support comprehensive raster data ETL and analytics<ul> <li>You can now directly join vector and raster datasets together</li> <li>Flexible map algebra equations: <code>SELECT RS_MapAlgebra(rast, 'D', 'out = (rast[3] - rast[0]) / (rast[3] + rast[0]);') as ndvi FROM raster_table</code></li> </ul> </li> <li>Add native support of Uber H3 functions in Sedona Spark and Sedona Flink.</li> <li>Add SedonaKepler and SedonaPyDeck for interactive map visualization on Sedona Spark.</li> </ul>"},{"location":"setup/release-notes/#bug_1","title":"Bug","text":"<ul> <li>[SEDONA-318] -         SerDe for RasterUDT performs poorly </li> <li>[SEDONA-319] -         RS_AddBandFromArray does not always produce serializable rasters </li> <li>[SEDONA-322] -         The \"Scala and Java build\" CI job occasionally fail </li> <li>[SEDONA-325] -         RS_FromGeoTiff is leaking file descriptors </li> <li>[SEDONA-329] -         Remove geometry_col parameter from SedonaKepler APIs </li> <li>[SEDONA-330] -         Fix bugs in SedonaPyDeck </li> <li>[SEDONA-332] -         RS_Value and RS_Values don't need to fetch all the pixel data </li> <li>[SEDONA-337] -         Failure falling back to pure python implementation when geomserde_speedup is unavailable </li> <li>[SEDONA-338] -         Refactor Raster construction in sedona to use AffineTransform instead of envelope </li> <li>[SEDONA-358] -         Refactor Functions to remove geotools dependency for most vector functions </li> <li>[SEDONA-362] -         RS_BandAsArray truncates the decimal part of float/double pixel values. </li> <li>[SEDONA-373] -         Move RasterPredicates to correct raster package to prevent redundant imports </li> <li>[SEDONA-394] -         fix RS_Band data type bug </li> <li>[SEDONA-401] -         Handle null values in RS_AsMatrix </li> <li>[SEDONA-402] -         Floor grid coordinates received from geotools </li> <li>[SEDONA-403] -         Add Null tolerance to RS_AddBandFromArray </li> <li>[SEDONA-405] -         Sedona driver Out of Memory on 1.4.1 </li> </ul>"},{"location":"setup/release-notes/#new-feature_1","title":"New Feature","text":"<ul> <li>[SEDONA-200] -         Add ST_CoordDim to Sedona </li> <li>[SEDONA-213] -         Add ST_BoundingDiagonal to Sedona </li> <li>[SEDONA-237] -         Implement ST_Dimension </li> <li>[SEDONA-238] -         Implement OGC GeometryType </li> <li>[SEDONA-293] -         Implement ST_IsCollection </li> <li>[SEDONA-294] -         Implement ST_Angle </li> <li>[SEDONA-295] -         Implement ST_LineInterpolatePoint in Flink </li> <li>[SEDONA-296] -         Implement ST_Multi in Sedona Flink </li> <li>[SEDONA-298] -         Implement ST_ClosestPoint </li> <li>[SEDONA-299] -         Implement ST_FrechetDistance </li> <li>[SEDONA-300] -         Implement ST_HausdorffDistance </li> <li>[SEDONA-301] -         Implement ST_Affine </li> <li>[SEDONA-303] -         Port all Sedona Spark functions to Sedona Flink </li> <li>[SEDONA-310] -         Add ST_Degrees to sedona </li> <li>[SEDONA-314] -         Support Optimized join on ST_HausdorffDistance </li> <li>[SEDONA-315] -         Support Optimized join on ST_FrechetDistance </li> <li>[SEDONA-321] -         Implement RS_Intersects(raster, geom) </li> <li>[SEDONA-323] -         Add wrapper for KeplerGl visualization in sedona </li> <li>[SEDONA-328] -         Add wrapper for pydeck visualizations in sedona </li> <li>[SEDONA-331] -         Add RS_Height and RS_Width </li> <li>[SEDONA-334] -         Add ScaleX and ScaleY </li> <li>[SEDONA-335] -         Add RS_PixelAsPoint </li> <li>[SEDONA-336] -         Add RS_UpperLeftX and RS_UpperLeftY </li> <li>[SEDONA-340] -         Add RS_ConvexHull </li> <li>[SEDONA-343] -         Add raster predicates: Contains and Within </li> <li>[SEDONA-344] -         Add RS_RasterToWorldCoordX, RS_RasterToWorldCoordY </li> <li>[SEDONA-346] -         Add RS_WorldToRaster APIs </li> <li>[SEDONA-353] -         Add RS_BandNoDataValue </li> <li>[SEDONA-354] -         Add RS_SkewX and RS_SkewY </li> <li>[SEDONA-355] -         Add RS_BandPixelType </li> <li>[SEDONA-357] -         Implement ST_VoronoiPolygons </li> <li>[SEDONA-359] -         Add RS_GeoReference </li> <li>[SEDONA-361] -         Add RS_MapAlgebra for performing map algebra operations using simple expressions </li> <li>[SEDONA-363] -         Add RS_PixelAsPolygon </li> <li>[SEDONA-364] -         Add RS_MinConvexHull </li> <li>[SEDONA-366] -         Add RS_Count </li> <li>[SEDONA-367] -         Add RS_PixelAsCentroid </li> <li>[SEDONA-368] -         Add RS_SummaryStats </li> <li>[SEDONA-371] -         Add optimized join support for raster-vector and raster-raster(if any) joins </li> <li>[SEDONA-372] -         Add RS_SetGeoReference </li> <li>[SEDONA-375] -         Add RS_SetBandNoDataValue </li> <li>[SEDONA-376] -         Add RS_SetValues </li> <li>[SEDONA-378] -         Add RS_SetValue </li> <li>[SEDONA-379] -         Add RS_AsBase64 </li> <li>[SEDONA-383] -         Add RS_Band </li> <li>[SEDONA-387] -         Add RS_BandIsNoData </li> <li>[SEDONA-388] -         Add RS_AsRaster </li> <li>[SEDONA-391] -         Add RS_AsMatrix </li> <li>[SEDONA-393] -         Add RS_AsPNG </li> <li>[SEDONA-395] -         Add RS_AsImage </li> <li>[SEDONA-396] -         Add RS_SetValues Geometry variant </li> <li>[SEDONA-398] -         Add RS_AddBand </li> <li>[SEDONA-404] -         Add RS_Resample </li> </ul>"},{"location":"setup/release-notes/#improvement_1","title":"Improvement","text":"<ul> <li>[SEDONA-39] -         Fix the Lon/lat order issue in Sedona </li> <li>[SEDONA-114] -         Add ST_MakeLine to Apache Sedona </li> <li>[SEDONA-142] -         Add ST_Collect to Flink Catalog </li> <li>[SEDONA-311] -         Refactor InferredExpression to handle functions with arbitrary arity </li> <li>[SEDONA-313] -         Refactor ST_Affine to support signature like PostGIS </li> <li>[SEDONA-324] -         R \u2013 Fix failing tests </li> <li>[SEDONA-326] -         Improve raster band algebra functions for easier preprocessing of raster data </li> <li>[SEDONA-327] -         Refactor InferredExpression to handle GridCoverage2D </li> <li>[SEDONA-333] -         Support EWKT parser in ST_GeomFromWKT </li> <li>[SEDONA-347] -         Centralize usages of transform() </li> <li>[SEDONA-350] -         Refactor RS_AddBandFromArray to allow adding a custom noDataValue </li> <li>[SEDONA-352] -         Refactor MakeEmptyRaster to allow setting custom datatype for the raster </li> <li>[SEDONA-360] -         Handle nodata values of raster bands in a more concise way </li> <li>[SEDONA-365] -         Refactor RS_Count to RS_CountValue </li> <li>[SEDONA-374] -         RS predicates should support (geom, rast) and (rast, rast) as arguments, and use the convex hull of rasters for spatial relationship testing </li> <li>[SEDONA-385] -         Set the Maven Central to be the first repository to check </li> <li>[SEDONA-386] -         Speed up GridCoverage2D serialization </li> <li>[SEDONA-392] -         Add five more pre-commit hooks </li> <li>[SEDONA-399] -         Support Uber H3 cells </li> <li>[SEDONA-400] -         pre-commit add hook to ensure that links to vcs websites are permalinks </li> <li>[SEDONA-408] -         Set a reasonable default size for RasterUDT </li> </ul>"},{"location":"setup/release-notes/#task_1","title":"Task","text":"<ul> <li>[SEDONA-316] -         Refactor Sedona Jupyter notebook examples with unified SedonaContext entrypoint </li> <li>[SEDONA-317] -         Change map visualization in Jupyter notebooks with KeplerGL </li> <li>[SEDONA-341] -         Move RS_Envelope to GeometryFunctions </li> <li>[SEDONA-356] -         Change CRS transformation from lat/lon to lon/lat order </li> <li>[SEDONA-370] -         Completely drop the old GeoTiff reader and writer </li> <li>[SEDONA-377] -         Change sphere/spheroid functions to work with coordinates in lon/lat order </li> <li>[SEDONA-380] -         Merge all Sedona Spark module to a single module </li> <li>[SEDONA-381] -         Merge python-adapter to sql module </li> <li>[SEDONA-382] -         Merge SQL and Core module to a single Spark module </li> <li>[SEDONA-384] -         Merge viz module to the spark module </li> <li>[SEDONA-397] -         Move Map Algebra functions </li> </ul>"},{"location":"setup/release-notes/#sedona-141","title":"Sedona 1.4.1","text":"<p>Sedona 1.4.1 is compiled against Spark 3.3 / Spark 3.4 / Flink 1.12, Java 8.</p>"},{"location":"setup/release-notes/#highlights_2","title":"Highlights","text":"<ul> <li> Sedona Spark More raster functions and bridge RasterUDT and Map Algebra operators. See Raster based operators and Raster to Map Algebra operators.</li> <li> Sedona Spark &amp; Flink Added geodesic / geography functions:<ul> <li>ST_DistanceSphere</li> <li>ST_DistanceSpheroid</li> <li>ST_AreaSpheroid</li> <li>ST_LengthSpheroid</li> </ul> </li> <li> Sedona Spark &amp; Flink Introduced <code>SedonaContext</code> to unify Sedona entry points.</li> <li> Sedona Spark Support Spark 3.4.</li> <li> Sedona Spark Added a number of new ST functions.</li> <li> Zeppelin Zeppelin helium plugin supports plotting geometries like linestring, polygon.</li> </ul>"},{"location":"setup/release-notes/#api-change","title":"API change","text":"<ul> <li>Sedona Spark &amp; Flink Introduced a new entry point called SedonaContext to unify all Sedona entry points in different compute engines and deprecate old Sedona register entry points. Users no longer have to register Sedona kryo serializer and import many tedious Python classes.<ul> <li>Sedona Spark:<ul> <li>Scala: <pre><code>import org.apache.sedona.spark.SedonaContext\nval sedona = SedonaContext.create(SedonaContext.builder().master(\"local[*]\").getOrCreate())\nsedona.sql(\"SELECT ST_GeomFromWKT(XXX) FROM\")\n</code></pre></li> <li>Python: <pre><code>from sedona.spark import *\n\nconfig = SedonaContext.builder().\\\n   config('spark.jars.packages',\n       'org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.4.1,'\n       'org.datasyslab:geotools-wrapper:1.4.0-28.2'). \\\n   getOrCreate()\nsedona = SedonaContext.create(config)\nsedona.sql(\"SELECT ST_GeomFromWKT(XXX) FROM\")\n</code></pre></li> </ul> </li> <li>Sedona Flink: <pre><code>import org.apache.sedona.flink.SedonaContext\nStreamTableEnvironment sedona = SedonaContext.create(env, tableEnv);\nsedona.sqlQuery(\"SELECT ST_GeomFromWKT(XXX) FROM\")\n</code></pre></li> </ul> </li> </ul>"},{"location":"setup/release-notes/#bug_2","title":"Bug","text":"<ul> <li>[SEDONA-266] -         RS_Values throws UnsupportedOperationException for shuffled point arrays </li> <li>[SEDONA-267] -         Cannot pip install apache-sedona 1.4.0 from source distribution </li> <li>[SEDONA-273] -         Set a upper bound for Shapely, Pandas and GeoPandas </li> <li>[SEDONA-277] -         Sedona spark artifacts for scala 2.13 do not have proper POMs </li> <li>[SEDONA-283] -         Artifacts were deployed twice when running mvn clean deploy </li> <li>[SEDONA-284] -         Property values in dependency deduced POMs for shaded modules were not substituted </li> </ul>"},{"location":"setup/release-notes/#new-feature_2","title":"New Feature","text":"<ul> <li>[SEDONA-196] -         Add ST_Force3D to Sedona </li> <li>[SEDONA-239] -         Implement ST_NumPoints </li> <li>[SEDONA-264] -         zeppelin helium plugin supports plotting geometry like linestring, polygon </li> <li>[SEDONA-280] -         Add ST_GeometricMedian </li> <li>[SEDONA-281] -         Support geodesic / geography functions </li> <li>[SEDONA-286] -         Support optimized distance join on ST_DistanceSpheroid and ST_DistanceSphere </li> <li>[SEDONA-287] -         Use SedonaContext to unify Sedona entry points </li> <li>[SEDONA-292] -         Bridge Sedona Raster and Map Algebra operators </li> <li>[SEDONA-297] -         Implement ST_NRings </li> <li>[SEDONA-302] -         Implement ST_Translate </li> </ul>"},{"location":"setup/release-notes/#improvement_2","title":"Improvement","text":"<ul> <li>[SEDONA-167] -         Add __pycache__ to Python .gitignore </li> <li>[SEDONA-265] -         Migrate all ST functions to Sedona Inferred Expressions </li> <li>[SEDONA-269] -         Add data source for writing binary files </li> <li>[SEDONA-270] -         Remove redundant serialization for rasters </li> <li>[SEDONA-271] -         Add raster function RS_SRID </li> <li>[SEDONA-274] -         Move all ST function logics to Sedona common </li> <li>[SEDONA-275] -         Add raster function RS_SetSRID </li> <li>[SEDONA-276] -         Add support for Spark 3.4 </li> <li>[SEDONA-279] -         Sedona-Flink should not depend on Sedona-Spark modules </li> <li>[SEDONA-282] -         R \u2013 Add raster write function </li> <li>[SEDONA-290] -         RDD Spatial Joins should follow the iterator model </li> </ul>"},{"location":"setup/release-notes/#sedona-140","title":"Sedona 1.4.0","text":"<p>Sedona 1.4.0 is compiled against, Spark 3.3 / Flink 1.12, Java 8.</p>"},{"location":"setup/release-notes/#highlights_3","title":"Highlights","text":"<ul> <li> Sedona Spark &amp; Flink Serialize and deserialize geometries 3 - 7X faster</li> <li> Sedona Spark &amp; Flink Google S2 based spatial join for fast approximate point-in-polygon join. See Join query in Spark and Join query in Flink</li> <li> Sedona Spark Pushdown spatial predicate on GeoParquet to reduce memory consumption by 10X: see explanation</li> <li> Sedona Spark Automatically use broadcast index spatial join for small datasets</li> <li> Sedona Spark New RasterUDT added to Sedona GeoTiff reader.</li> <li> Sedona Spark A number of bug fixes and improvement to the Sedona R module.</li> </ul>"},{"location":"setup/release-notes/#api-change_1","title":"API change","text":"<ul> <li>Sedona Spark &amp; Flink Packaging strategy changed. See Maven Coordinate. Please change your Sedona dependencies if needed. We recommend <code>sedona-spark-shaded-3.0_2.12-1.4.0</code> and <code>sedona-flink-shaded_2.12-1.4.0</code></li> <li>Sedona Spark &amp; Flink GeoTools-wrapper version upgraded. Please use <code>geotools-wrapper-1.4.0-28.2</code>.</li> </ul>"},{"location":"setup/release-notes/#behavior-change_1","title":"Behavior change","text":"<ul> <li>Sedona Flink Sedona Flink no longer outputs any LinearRing type geometry. All LinearRing are changed to LineString.</li> <li>Sedona Spark Join optimization strategy changed. Sedona no longer optimizes spatial join when use a spatial predicate together with a equijoin predicate. By default, it prefers equijoin whenever possible. SedonaConf adds a config option called <code>sedona.join.optimizationmode</code>, it can be configured as one of the following values:<ul> <li><code>all</code>: optimize all joins having spatial predicate in join conditions. This was the behavior of Apache Sedona prior to 1.4.0.</li> <li><code>none</code>: disable spatial join optimization.</li> <li><code>nonequi</code>: only enable spatial join optimization on non-equi joins. This is the default mode.</li> </ul> </li> </ul> <p>When <code>sedona.join.optimizationmode</code> is configured as <code>nonequi</code>, it won't optimize join queries such as <code>SELECT * FROM A, B WHERE A.x = B.x AND ST_Contains(A.geom, B.geom)</code>, since it is an equi-join with equi-condition <code>A.x = B.x</code>. Sedona will optimize for <code>SELECT * FROM A, B WHERE ST_Contains(A.geom, B.geom)</code></p>"},{"location":"setup/release-notes/#bug_3","title":"Bug","text":"<ul> <li>[SEDONA-218] -         Flaky test caused by improper handling of null struct values in Adapter.toDf </li> <li>[SEDONA-221] -         Outer join throws NPE for null geometries </li> <li>[SEDONA-222] -         GeoParquet reader does not work in non-local mode </li> <li>[SEDONA-224] -         java.lang.NoSuchMethodError when loading GeoParquet files using Spark 3.0.x ~ 3.2.x </li> <li>[SEDONA-225] -         Cannot count dataframes loaded from GeoParquet files </li> <li>[SEDONA-227] -         Python SerDe Performance Degradation </li> <li>[SEDONA-230] -         rdd.saveAsGeoJSON should generate feature properties with field names </li> <li>[SEDONA-233] -         Incorrect results for several joins in a single stage </li> <li>[SEDONA-236] -         Flakey python tests in tests.serialization.test_[de]serializers </li> <li>[SEDONA-242] -         Update jars dependencies in Sedona R to Sedona 1.4.0 version </li> <li>[SEDONA-250] -         R Deprecate use of Spark 2.4 </li> <li>[SEDONA-252] -         Fix disabled RS_Base64 test </li> <li>[SEDONA-255] -         R \u2013 Translation issue for ST_Point and ST_PolygonFromEnvelope </li> <li>[SEDONA-258] -         Cannot directly assign raw spatial RDD to CircleRDD using Python binding </li> <li>[SEDONA-259] -         Adapter.toSpatialRdd in Python binding does not have valid implementation for specifying custom field names for user data </li> <li>[SEDONA-261] -         Cannot run distance join using broadcast index join when the distance expression references to attributes from the right-side relation </li> </ul>"},{"location":"setup/release-notes/#new-feature_3","title":"New Feature","text":"<ul> <li>[SEDONA-156] -         predicate pushdown support for GeoParquet </li> <li>[SEDONA-215] -         Add ST_ConcaveHull </li> <li>[SEDONA-216] -         Upgrade jts version to 1.19.0 </li> <li>[SEDONA-235] -         Create ST_S2CellIds in Sedona </li> <li>[SEDONA-246] -         R GeoTiff read/write </li> <li>[SEDONA-254] -         R \u2013 Add raster type </li> <li>[SEDONA-262] -         Don't optimize equi-join by default, add an option to configure when to optimize spatial joins </li> </ul>         Improvement  <ul> <li>[SEDONA-205] -         Use BinaryType in GeometryUDT in Sedona Spark </li> <li>[SEDONA-207] -         Faster serialization/deserialization of geometry objects </li> <li>[SEDONA-212] -         Move shading to separate maven modules </li> <li>[SEDONA-217] -         Automatically broadcast small datasets </li> <li>[SEDONA-220] -         Upgrade Ubuntu build image from 18.04 to 20.04 </li> <li>[SEDONA-226] -         Support reading and writing GeoParquet file metadata </li> <li>[SEDONA-228] -         Standardize logging dependencies </li> <li>[SEDONA-231] -         Redundant Serde Removal </li> <li>[SEDONA-234] -         ST_Point inconsistencies </li> <li>[SEDONA-243] -         Improve Sedona R file readers: GeoParquet and Shapefile </li> <li>[SEDONA-244] -         Align R read/write functions with the Sparklyr framework </li> <li>[SEDONA-249] -         Add jvm flags for running tests on Java 17 </li> <li>[SEDONA-251] -         Add raster type to Sedona </li> <li>[SEDONA-253] -         Upgrade geotools to version 28.2 </li> <li>[SEDONA-260] -         More intuitive configuration of partition and index-build side of spatial joins in Sedona SQL </li> </ul>"},{"location":"setup/release-notes/#sedona-131","title":"Sedona 1.3.1","text":"<p>This version is a minor release on Sedoma 1.3.0 line. It fixes a few critical bugs in 1.3.0. We suggest all 1.3.0 users to migrate to this version.</p>"},{"location":"setup/release-notes/#bug-fixes","title":"Bug fixes","text":"<ul> <li>SEDONA-204 - Init value in X/Y/Z max should be -Double.MAX</li> <li>SEDONA-206 - Performance regression of ST_Transform in 1.3.0-incubating</li> <li>SEDONA-210 - 1.3.0-incubating doesn't work with Scala 2.12 sbt projects</li> <li>SEDONA-211 - Enforce release managers to use JDK 8</li> <li>SEDONA-201 - Implement ST_MLineFromText and ST_MPolyFromText methods</li> </ul>"},{"location":"setup/release-notes/#new-feature_4","title":"New Feature","text":"<ul> <li>SEDONA-196 - Add ST_Force3D to Sedona</li> <li>SEDONA-197 - Add ST_ZMin, ST_ZMax to Sedona</li> <li>SEDONA-199 - Add ST_NDims to Sedona</li> </ul>"},{"location":"setup/release-notes/#improvement_3","title":"Improvement","text":"<ul> <li>SEDONA-194 - Merge org.datasyslab.sernetcdf into Sedona</li> <li>SEDONA-208 - Use Spark RuntimeConfig in SedonaConf</li> </ul>"},{"location":"setup/release-notes/#sedona-130","title":"Sedona 1.3.0","text":"<p>This version is a major release on Sedona 1.3.0 line and consists of 50 PRs. It includes many new functions, optimization and bug fixes.</p>"},{"location":"setup/release-notes/#highlights_4","title":"Highlights","text":"<ul> <li> Sedona on Spark in this release is compiled against Spark 3.3.</li> <li> Sedona on Flink in this release is compiled against Flink 1.14.</li> <li> Scala 2.11 support is removed.</li> <li> Spark 2.X support is removed.</li> <li> Python 3.10 support is added.</li> <li> Aggregators in Flink are added</li> <li> Correctness fixes for corner cases in range join and distance join.</li> <li> Native GeoParquet read and write (../../tutorial/sql/#load-geoparquet).<ul> <li><code>df = spark.read.format(\"geoparquet\").option(\"fieldGeometry\", \"myGeometryColumn\").load(\"PATH/TO/MYFILE.parquet\")</code></li> <li><code>df.write.format(\"geoparquet\").save(\"PATH/TO/MYFILE.parquet\")</code></li> </ul> </li> <li> DataFrame style API (../../tutorial/sql/#dataframe-style-api)<ul> <li><code>df.select(ST_Point(min_value, max_value).as(\"point\"))</code></li> </ul> </li> <li> Allow WKT format CRS in ST_Transform<ul> <li><code>ST_Transform(geom, \"srcWktString\", \"tgtWktString\")</code></li> </ul> </li> </ul> <pre><code>GEOGCS[\"WGS 84\",\nDATUM[\"WGS_1984\",\nSPHEROID[\"WGS 84\",6378137,298.257223563,\nAUTHORITY[\"EPSG\",\"7030\"]],\nAUTHORITY[\"EPSG\",\"6326\"]],\nPRIMEM[\"Greenwich\",0,\nAUTHORITY[\"EPSG\",\"8901\"]],\nUNIT[\"degree\",0.0174532925199433,\nAUTHORITY[\"EPSG\",\"9122\"]],\nAUTHORITY[\"EPSG\",\"4326\"]]\n</code></pre>"},{"location":"setup/release-notes/#bug-fixes_1","title":"Bug fixes","text":"<ul> <li>SEDONA-119 - ST_Touches join query returns true for polygons whose interiors intersect</li> <li>SEDONA-136 - Enable testAsEWKT for Flink</li> <li>SEDONA-137 - Fix ST_Buffer for Flink to work</li> <li>SEDONA-138 - Fix ST_GeoHash for Flink to work</li> <li>SEDONA-153 - Python Serialization Fails with Nulls</li> <li>SEDONA-158 - Fix wrong description about ST_GeometryN in the API docs</li> <li>SEDONA-169 - Fix ST_RemovePoint in accordance with the API document</li> <li>SEDONA-178 - Correctness issue in distance join queries</li> <li>SEDONA-182 - ST_AsText should not return SRID</li> <li>SEDONA-186 - collecting result rows of a spatial join query with SELECT * fails with serde error</li> <li>SEDONA-188 - Python warns about missing <code>jars</code> even when some are found</li> <li>SEDONA-193 - ST_AsBinary produces EWKB by mistake</li> </ul>"},{"location":"setup/release-notes/#new-features","title":"New Features","text":"<ul> <li>SEDONA-94 - GeoParquet  Support For Sedona</li> <li>SEDONA-125 - Allows customized CRS in ST_Transform</li> <li>SEDONA-166 - Provide Type-safe DataFrame Style API</li> <li>SEDONA-168 - Add ST_Normalize to Apache Sedona</li> <li>SEDONA-171 - Add ST_SetPoint to Apache Sedona</li> </ul>"},{"location":"setup/release-notes/#improvement_4","title":"Improvement","text":"<ul> <li>SEDONA-121 - Add equivalent constructors left over from Spark to Flink</li> <li>SEDONA-132 - Create common module for SQL functions</li> <li>SEDONA-133 - Allow user-defined schemas in Adapter.toDf()</li> <li>SEDONA-139 - Fix wrong argument order in Flink unit tests</li> <li>SEDONA-140 - Update Sedona Dependencies in R Package</li> <li>SEDONA-143 - Add missing unit tests for the Flink predicates</li> <li>SEDONA-144 - Add ST_AsGeoJSON to the Flink API</li> <li>SEDONA-145 - Fix ST_AsEWKT to reserve the Z coordinate</li> <li>SEDONA-146 - Add missing output functions to the Flink API</li> <li>SEDONA-147 - Add SRID functions to the Flink API</li> <li>SEDONA-148 - Add boolean functions to the Flink API</li> <li>SEDONA-149 - Add Python 3.10 support</li> <li>SEDONA-151 - Add ST aggregators to Sedona Flink</li> <li>SEDONA-152 - Add reader/writer functions for GML and KML</li> <li>SEDONA-154 - Add measurement functions to the Flink API</li> <li>SEDONA-157 - Add coordinate accessors to the Flink API</li> <li>SEDONA-159 - Add Nth accessor functions to the Flink API</li> <li>SEDONA-160 - Fix geoparquetIOTests.scala to cleanup after test</li> <li>SEDONA-161 - Add ST_Boundary to the Flink API</li> <li>SEDONA-162 - Add ST_Envelope to the Flink API</li> <li>SEDONA-163 - Better handle of unsupported types in shapefile reader</li> <li>SEDONA-164 - Add geometry count functions to the Flink API</li> <li>SEDONA-165 - Upgrade Apache Rat to 0.14</li> <li>SEDONA-170 - Add ST_AddPoint and ST_RemovePoint to the Flink API</li> <li>SEDONA-172 - Add ST_LineFromMultiPoint to Apache Sedona</li> <li>SEDONA-176 - Make ST_Contains conform with OGC standard, and add ST_Covers and ST_CoveredBy functions.</li> <li>SEDONA-177 - Support spatial predicates other than INTERSECTS and COVERS/COVERED_BY in RangeQuery.SpatialRangeQuery and JoinQuery.SpatialJoinQuery</li> <li>SEDONA-181 - Build fails with java.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$</li> <li>SEDONA-189 - Prepare geometries in broadcast join</li> <li>SEDONA-192 - Null handling in predicates</li> <li>SEDONA-195 - Add wkt validation and an optional srid to ST_GeomFromWKT/ST_GeomFromText</li> </ul>"},{"location":"setup/release-notes/#task_2","title":"Task","text":"<ul> <li>SEDONA-150 - Drop Spark 2.4 and Scala 2.11 support</li> </ul>"},{"location":"setup/release-notes/#sedona-121","title":"Sedona 1.2.1","text":"<p>This version is a maintenance release on Sedona 1.2.0 line. It includes bug fixes.</p> <p>Sedona on Spark is now compiled against Spark 3.3, instead of Spark 3.2.</p>"},{"location":"setup/release-notes/#sql-for-spark","title":"SQL (for Spark)","text":"<p>Bug fixes:</p> <ul> <li>SEDONA-104: Bug in reading band values of GeoTiff images</li> <li>SEDONA-118: Fix the wrong result in ST_Within</li> <li>SEDONA-123: Fix the check for invalid lat/lon in ST_GeoHash</li> </ul> <p>Improvement:</p> <ul> <li>SEDONA-96: Refactor ST_MakeValid to use GeometryFixer</li> <li>SEDONA-108: Write support for GeoTiff images</li> <li>SEDONA-122: Overload ST_GeomFromWKB for BYTES column</li> <li>SEDONA-127: Add null safety to ST_GeomFromWKT/WKB/Text</li> <li>SEDONA-129: Support Spark 3.3</li> <li>SEDONA-135: Consolidate and upgrade hadoop dependency</li> </ul> <p>New features:</p> <ul> <li>SEDONA-107: Add St_Reverse function</li> <li>SEDONA-105: Add ST_PointOnSurface function</li> <li>SEDONA-95: Add ST_Disjoint predicate</li> <li>SEDONA-112: Add ST_AsEWKT</li> <li>SEDONA-106: Add ST_LineFromText</li> <li>SEDONA-117: Add RS_AppendNormalizedDifference</li> <li>SEDONA-97: Add ST_Force_2D</li> <li>SEDONA-98: Add ST_IsEmpty</li> <li>SEDONA-116: Add ST_YMax and ST_Ymin</li> <li>SEDONA-115: Add ST_XMax and ST_Min</li> <li>SEDONA-120: Add ST_BuildArea</li> <li>SEDONA-113: Add ST_PointN</li> <li>SEDONA-124: Add ST_CollectionExtract</li> <li>SEDONA-109: Add ST_OrderingEquals</li> </ul>"},{"location":"setup/release-notes/#flink","title":"Flink","text":"<p>New features:</p> <ul> <li>SEDONA-107: Add St_Reverse function</li> <li>SEDONA-105: Add ST_PointOnSurface function</li> <li>SEDONA-95: Add ST_Disjoint predicate</li> <li>SEDONA-112: Add ST_AsEWKT</li> <li>SEDONA-97: Add ST_Force_2D</li> <li>SEDONA-98: Add ST_IsEmpty</li> <li>SEDONA-116: Add ST_YMax and ST_Ymin</li> <li>SEDONA-115: Add ST_XMax and ST_Min</li> <li>SEDONA-120: Add ST_BuildArea</li> <li>SEDONA-113: Add ST_PointN</li> <li>SEDONA-110: Add ST_GeomFromGeoHash</li> <li>SEDONA-121: More ST constructors to Flink</li> <li>SEDONA-122: Overload ST_GeomFromWKB for BYTES column</li> </ul>"},{"location":"setup/release-notes/#sedona-120","title":"Sedona 1.2.0","text":"<p>This version is a major release on Sedona 1.2.0 line. It includes bug fixes and new features: Sedona with Apache Flink.</p>"},{"location":"setup/release-notes/#rdd","title":"RDD","text":"<p>Bug fix:</p> <ul> <li>SEDONA-18: Fix an error reading Shapefile</li> <li>SEDONA-73: Exclude scala-library from scala-collection-compat</li> </ul> <p>Improvement:</p> <ul> <li>SEDONA-77: Refactor Format readers and spatial partitioning functions to be standalone libraries. So they can be used by Flink and others.</li> </ul>"},{"location":"setup/release-notes/#sql","title":"SQL","text":"<p>New features:</p> <ul> <li>SEDONA-4: Handle nulls in SQL functions</li> <li>SEDONA-65: Create ST_Difference function</li> <li>SEDONA-68 Add St_Collect function.</li> <li>SEDONA-82: Create ST_SymmDifference function</li> <li>SEDONA-75: Add support for \"3D\" geometries: Preserve Z coordinates on geometries when serializing, ST_AsText, ST_Z, ST_3DDistance</li> <li>SEDONA-86: Support empty geometries in ST_AsBinary and ST_AsEWKB</li> <li>SEDONA-90: Add ST_Union</li> <li>SEDONA-100: Add st_multi function</li> </ul> <p>Bug fix:</p> <ul> <li>SEDONA-89: GeometryUDT equals should test equivalence of the other object</li> </ul>"},{"location":"setup/release-notes/#flink_1","title":"Flink","text":"<p>Major update:</p> <ul> <li>SEDONA-80: Geospatial stream processing support in Flink Table API</li> <li>SEDONA-85: ST_Geohash function in Flink</li> <li>SEDONA-87: Support Flink Table and DataStream conversion</li> <li>SEDONA-93: Add ST_GeomFromGeoJSON</li> </ul>"},{"location":"setup/release-notes/#sedona-111","title":"Sedona 1.1.1","text":"<p>This version is a maintenance release on Sedona 1.1.X line. It includes bug fixes and a few new functions.</p>"},{"location":"setup/release-notes/#global","title":"Global","text":"<p>New feature:</p> <ul> <li>SEDONA-73: Scala source code supports Scala 2.13</li> </ul>"},{"location":"setup/release-notes/#sql_1","title":"SQL","text":"<p>Bug fix:</p> <ul> <li>SEDONA-67: Support Spark 3.2</li> </ul> <p>New features:</p> <ul> <li>SEDONA-43: Add ST_GeoHash and ST_GeomFromGeoHash</li> <li>SEDONA-45: Add ST_MakePolygon</li> <li>SEDONA-71: Add ST_AsBinary, ST_AsEWKB, ST_SRID, ST_SetSRID</li> </ul>"},{"location":"setup/release-notes/#sedona-110","title":"Sedona 1.1.0","text":"<p>This version is a major release on Sedona 1.1.0 line. It includes bug fixes and new features: R language API, Raster data and Map algebra support</p>"},{"location":"setup/release-notes/#global_1","title":"Global","text":"<p>Dependency upgrade:</p> <ul> <li>SEDONA-30: Use Geotools-wrapper 1.1.0-24.1 to include geotools GeoTiff libraries.</li> </ul> <p>Improvement on join queries in core and SQL:</p> <ul> <li>SEDONA-63: Skip empty partitions in NestedLoopJudgement</li> <li>SEDONA-64: Broadcast dedupParams to improve performance</li> </ul> <p>Behavior change:</p> <ul> <li>SEDONA-62: Ignore HDF test in order to avoid NASA copyright issue</li> </ul>"},{"location":"setup/release-notes/#core","title":"Core","text":"<p>Bug fix:</p> <ul> <li>SEDONA-41: Fix rangeFilter bug when the leftCoveredByRight para is false</li> <li>SEDONA-53: Fix SpatialKnnQuery NullPointerException</li> </ul>"},{"location":"setup/release-notes/#sql_2","title":"SQL","text":"<p>Major update:</p> <ul> <li>SEDONA-30: Add GeoTiff raster I/O and Map Algebra function</li> </ul> <p>New function:</p> <ul> <li>SEDONA-27: Add ST_Subdivide and ST_SubdivideExplode functions</li> </ul> <p>Bug fix:</p> <ul> <li>SEDONA-56: Fix broadcast join with Adapter Query Engine enabled</li> <li>SEDONA-22, SEDONA-60: Fix join queries in SparkSQL when one side has no rows or only one row</li> </ul>"},{"location":"setup/release-notes/#viz","title":"Viz","text":"<p>N/A</p>"},{"location":"setup/release-notes/#python","title":"Python","text":"<p>Improvement:</p> <ul> <li>SEDONA-59: Make pyspark dependency of Sedona Python optional</li> </ul> <p>Bug fix:</p> <ul> <li>SEDONA-50: Remove problematic logging conf that leads to errors on Databricks</li> <li>Fix the issue: Spark dependency in setup.py was configured to be &lt; v3.1.0 by mistake.</li> </ul>"},{"location":"setup/release-notes/#r","title":"R","text":"<p>Major update:</p> <ul> <li>SEDONA-31: Add R interface for Sedona</li> </ul>"},{"location":"setup/release-notes/#sedona-101","title":"Sedona 1.0.1","text":"<p>This version is a maintenance release on Sedona 1.0.0 line. It includes bug fixes, some new features, one API change</p>"},{"location":"setup/release-notes/#known-issue","title":"Known issue","text":"<p>In Sedona v1.0.1 and earlier versions, the Spark dependency in setup.py was configured to be &lt; v3.1.0 by mistake. When you install Sedona Python (apache-sedona v1.0.1) from PyPI, pip might uninstall PySpark 3.1.1 and install PySpark 3.0.2 on your machine.</p> <p>Three ways to fix this:</p> <ol> <li> <p>After install apache-sedona v1.0.1, uninstall PySpark 3.0.2 and reinstall PySpark 3.1.1</p> </li> <li> <p>Ask pip not to install Sedona dependencies: <code>pip install --no-deps apache-sedona</code></p> </li> <li> <p>Install Sedona from the latest setup.py (on GitHub) manually.</p> </li> </ol>"},{"location":"setup/release-notes/#global_2","title":"Global","text":"<p>Dependency upgrade:</p> <ul> <li>SEDONA-16: Use a GeoTools Maven Central wrapper to fix failed Jupyter notebook examples</li> <li>SEDONA-29: upgrade to Spark 3.1.1</li> <li>SEDONA-33: jts2geojson version from 0.14.3 to 0.16.1</li> </ul>"},{"location":"setup/release-notes/#core_1","title":"Core","text":"<p>Bug fix:</p> <ul> <li>SEDONA-35: Address user-data mutability issue with Adapter.toDF()</li> </ul>"},{"location":"setup/release-notes/#sql_3","title":"SQL","text":"<p>Bug fix:</p> <ul> <li>SEDONA-14: Saving dataframe to CSV or Parquet fails due to unknown type</li> <li>SEDONA-15: Add ST_MinimumBoundingRadius and ST_MinimumBoundingCircle functions</li> <li>SEDONA-19: Global indexing does not work with SQL joins</li> <li>SEDONA-20: Case object GeometryUDT and GeometryUDT instance not equal in Spark 3.0.2</li> </ul> <p>New function:</p> <ul> <li>SEDONA-21: allows Sedona to be used in pure SQL environment</li> <li>SEDONA-24: Add ST_LineSubString and ST_LineInterpolatePoint</li> <li>SEDONA-26: Add broadcast join support</li> </ul>"},{"location":"setup/release-notes/#viz_1","title":"Viz","text":"<p>Improvement:</p> <ul> <li>SEDONA-32: Speed up ST_Render</li> </ul> <p>API change:</p> <ul> <li>SEDONA-29: Upgrade to Spark 3.1.1 and fix ST_Pixelize</li> </ul>"},{"location":"setup/release-notes/#python_1","title":"Python","text":"<p>Bug fix:</p> <ul> <li>SEDONA-19: Global indexing does not work with SQL joins</li> </ul>"},{"location":"setup/release-notes/#sedona-100","title":"Sedona 1.0.0","text":"<p>This version is the first Sedona release since it joins the Apache Incubator. It includes new functions, bug fixes, and API changes.</p>"},{"location":"setup/release-notes/#global_3","title":"Global","text":"<p>Key dependency upgrade:</p> <ul> <li>SEDONA-1: upgrade to JTS 1.18</li> <li>upgrade to GeoTools 24.0</li> <li>upgrade to jts2geojson 0.14.3</li> </ul> <p>Key dependency packaging strategy change:</p> <ul> <li>JTS, GeoTools, jts2geojson are no longer packaged in Sedona jars. End users need to add them manually. See here.</li> </ul> <p>Key compilation target change:</p> <ul> <li>SEDONA-3: Paths and class names have been changed to Apache Sedona</li> <li>SEDONA-7: build the source code for Spark 2.4, 3.0, Scala 2.11, 2.12, Python 3.7, 3.8, 3.9. See here.</li> </ul>"},{"location":"setup/release-notes/#sedona-core","title":"Sedona-core","text":"<p>Bug fix:</p> <ul> <li>PR 443: read multiple Shape Files by multiPartitions</li> <li>PR 451 (API change): modify CRSTransform to ignore datum shift</li> </ul> <p>New function:</p> <ul> <li>SEDONA-8: spatialRDD.flipCoordinates()</li> </ul> <p>API / behavior change:</p> <ul> <li>PR 488: JoinQuery.SpatialJoinQuery/DistanceJoinQuery now returns <code>&lt;Geometry, List&gt;</code> instead of <code>&lt;Geometry, HashSet&gt;</code> because we can no longer use HashSet in Sedona for duplicates removal. All original duplicates in both input RDDs will be preserved in the output.</li> </ul>"},{"location":"setup/release-notes/#sedona-sql","title":"Sedona-sql","text":"<p>Bug fix:</p> <ul> <li>SEDONA-8 (API change): ST_Transform slow due to lock contention.</li> <li>PR 427: ST_Point and ST_PolygonFromEnvelope now allows Double type</li> </ul> <p>New function:</p> <ul> <li>PR 499: ST_Azimuth, ST_X, ST_Y, ST_StartPoint, ST_Boundary, ST_EndPoint, ST_ExteriorRing, ST_GeometryN, ST_InteriorRingN, ST_Dump, ST_DumpPoints, ST_IsClosed, ST_NumInteriorRings, ST_AddPoint, ST_RemovePoint, ST_IsRing</li> <li>PR 459: ST_LineMerge</li> <li>PR 460: ST_NumGeometries</li> <li>PR 469: ST_AsGeoJSON</li> <li>SEDONA-8: ST_FlipCoordinates</li> </ul> <p>Behavior change:</p> <ul> <li>PR 480: Aggregate Functions rewrite for new Aggregator API. The functions can be used as typed functions in code and enable compilation-time type check.</li> </ul> <p>API change:</p> <ul> <li>SEDONA-11: Adapter.toDf() will directly generate a geometry type column. ST_GeomFromWKT is no longer needed.</li> </ul>"},{"location":"setup/release-notes/#sedona-viz","title":"Sedona-viz","text":"<p>API change: Drop the function which can generate SVG vector images because the required library has an incompatible license and the SVG image is not good at plotting big data</p>"},{"location":"setup/release-notes/#sedona-python","title":"Sedona Python","text":"<p>API/Behavior change:</p> <ul> <li>Python-to-Sedona adapter is moved to a separate module. To use Sedona Python, see here</li> </ul> <p>New function:</p> <ul> <li>PR 448: Add support for partition number in spatialPartitioning function <code>spatial_rdd.spatialPartitioning(grid_type, NUM_PARTITION)</code></li> </ul>"},{"location":"setup/wherobots/","title":"Install on Wherobots","text":""},{"location":"setup/wherobots/#sedonadb","title":"SedonaDB","text":"<p>Wherobots Cloud offers fully-managed and fully provisioned cloud services for SedonaDB, a comprehensive spatial analytics database system. You can play with it using Wherobots Jupyter Scala and Python kernel. No installation is needed.</p> <p>SedonaDB is 100% compatible with Apache Sedona 1.5.0+ in terms of public APIs but provides more functionalities.</p> <p>It is easy to migrate your existing Sedona workflow to Wherobots Cloud. Please sign up at Wherobots Cloud.</p>"},{"location":"setup/zeppelin/","title":"Install Sedona-Zeppelin","text":"<p>Warning</p> <p>Known issue: due to an issue in Leaflet JS, Sedona can only plot each geometry (point, line string and polygon) as a point on Zeppelin map. To enjoy the scalable and full-fleged visualization, please use SedonaViz to plot scatter plots and heat maps on Zeppelin map.</p>"},{"location":"setup/zeppelin/#compatibility","title":"Compatibility","text":"<p>Apache Spark 2.3+</p> <p>Apache Zeppelin 0.8.1+</p> <p>Sedona 1.0.0+: Sedona-core, Sedona-SQL, Sedona-Viz</p>"},{"location":"setup/zeppelin/#installation","title":"Installation","text":"<p>Note</p> <p>You only need to do Step 1 and 2 only if you cannot see Apache-sedona or GeoSpark Zeppelin in Zeppelin Helium package list.</p>"},{"location":"setup/zeppelin/#create-helium-folder-optional","title":"Create Helium folder (optional)","text":"<p>Create a folder called <code>helium</code> in Zeppelin root folder.</p>"},{"location":"setup/zeppelin/#add-sedona-zeppelin-description-optional","title":"Add Sedona-Zeppelin description (optional)","text":"<p>Create a file called <code>sedona-zeppelin.json</code> in this folder and put the following content in this file. You need to change the artifact path!</p> <pre><code>{\n  \"type\": \"VISUALIZATION\",\n  \"name\": \"sedona-zeppelin\",\n  \"description\": \"Zeppelin visualization support for Sedona\",\n  \"artifact\": \"/Absolute/Path/sedona/zeppelin\",\n  \"license\": \"BSD-2-Clause\",\n  \"icon\": \"&lt;i class='fa fa-globe'&gt;&lt;/i&gt;\"\n}\n</code></pre>"},{"location":"setup/zeppelin/#enable-sedona-zeppelin","title":"Enable Sedona-Zeppelin","text":"<p>Restart Zeppelin then open Zeppelin Helium interface and enable Sedona-Zeppelin.</p> <p></p>"},{"location":"setup/zeppelin/#add-sedona-dependencies-in-zeppelin-spark-interpreter","title":"Add Sedona dependencies in Zeppelin Spark Interpreter","text":""},{"location":"setup/zeppelin/#visualize-sedonasql-results","title":"Visualize SedonaSQL results","text":""},{"location":"setup/zeppelin/#display-sedonaviz-results","title":"Display SedonaViz results","text":"<p>Now, you are good to go! Please read Sedona-Zeppelin tutorial for a hands-on tutorial.</p>"},{"location":"setup/flink/install-scala/","title":"Install Sedona Scala/Java","text":"<p>Before starting the Sedona journey, you need to make sure your Apache Flink cluster is ready.</p> <p>Then you can create a self-contained Scala / Java project. A self-contained project allows you to create multiple Scala / Java files and write complex logics in one place.</p> <p>To use Sedona in your self-contained Flink project, you just need to add Sedona as a dependency in your pom.xml or build.sbt.</p> <ol> <li>To add Sedona as dependencies, please read Sedona Maven Central coordinates</li> <li>Read Sedona Flink guide and use Sedona Template project to start: Sedona Template Project</li> <li>Compile your project using Maven. Make sure you obtain the fat jar which packages all dependencies.</li> <li>Submit your compiled fat jar to Flink cluster. Make sure you are in the root folder of Flink distribution. Then run the following command: <pre><code>./bin/flink run /Path/To/YourJar.jar\n</code></pre></li> </ol>"},{"location":"setup/flink/modules/","title":"Sedona modules for Apache Flink","text":"Name Introduction flink Spatial Table and DataStream implementation flink-shaded shaded version"},{"location":"setup/flink/modules/#api-availability","title":"API availability","text":"DataStream Table Scala/Java \u2705 \u2705 Python no no R no no"},{"location":"setup/flink/platform/","title":"Language wrappers","text":"<p>Sedona Flink binary releases are compiled by Java 1.8 and Scala 2.12, and tested in the following environments:</p> Sedona Scala/Java Flink 1.12 Flink 1.13 Flink 1.14 Scala 2.12 \u2705 \u2705 \u2705 Scala 2.11 not tested not tested not tested"},{"location":"setup/snowflake/install/","title":"Install Sedona SQL","text":"<p>Note</p> <p>This tutorial is for you to manually install Sedona on Snowflake. If you want to use Sedona on Snowflake without manually installing it, you can use the free SedonaSnow native app shipped by Wherobots.</p>"},{"location":"setup/snowflake/install/#prerequisites","title":"Prerequisites","text":"<p>To install Sedona on Snowflake, you need to prepare a Snowflake account and a Snowflake user that can access at least one <code>DATABASE</code> and run at least one <code>WAREHOUSE</code>. Then you can follow the steps below to install Sedona on Snowflake.</p> <p>You can refer to Snowflake Documentation to how to create a DATABASE.</p> <p>In this tutorial, we will use a database created by the following SQL statement. But you can use any database you want.</p> <pre><code>CREATE DATABASE SEDONA_TEST;\n</code></pre>"},{"location":"setup/snowflake/install/#step-1-create-a-stage-in-the-database","title":"Step 1: Create a stage in the database","text":"<p>A stage is a Snowflake object that maps to a location in a cloud storage provider, such as Amazon S3, Azure Blob Storage, or Google Cloud Storage. You can use a stage to load data into a table, or unload data from a table.</p> <p>In this case, we will create a stage named <code>ApacheSedona</code> in the <code>public</code> schema of the database created in the previous step. The stage will be used to load Sedona's JAR files into the database. We will choose a <code>Snowflake managed</code> stage.</p> <p></p> <p>After creating the stage, you should be able to see the stage in the database.</p> <p></p> <p>You can refer to Snowflake Documentation to how to create a stage.</p>"},{"location":"setup/snowflake/install/#step-2-upload-sedonas-jar-files-to-the-stage","title":"Step 2: Upload Sedona's JAR files to the stage","text":"<p>You will need to download the following 2 JAR files:</p> <ul> <li>sedona-snowflake-1.5.1.jar: Sedona's Maven Central repository</li> <li>geotools-wrapper-1.5.1-28.2.jar: GeoTools-wrapper's Maven Central repository</li> </ul> <p>Then you can upload the 2 JAR files to the stage created in the previous step.</p> <p>After uploading the 2 JAR files, you should be able to see the 2 JAR files in the stage.</p> <p></p> <p>You can refer to Snowflake Documentation to how to upload files to a stage.</p>"},{"location":"setup/snowflake/install/#step-3-create-a-schema-in-the-database","title":"Step 3: Create a schema in the database","text":"<p>A schema is a Snowflake object that maps to a database. You can use a schema to organize your tables into groups based on business functions or other categories.</p> <p>In this case, we will create a schema named <code>SEDONA</code> in the database created in the previous step. The schema will be used to create Sedona's functions.</p> <p></p> <p>You can find your schema in the database as follows:</p> <p></p> <p>You can refer to Snowflake Documentation to how to create a schema.</p>"},{"location":"setup/snowflake/install/#step-4-get-the-sql-script-for-creating-sedonas-functions","title":"Step 4: Get the SQL script for creating Sedona's functions","text":"<p>You will need to download sedona-snowflake.sql to create Sedona's functions in the schema created in the previous step.</p> <p>You can also get this SQL script by running the following command:</p> <pre><code>java -jar sedona-snowflake-1.5.1.jar --geotools-version 1.5.1-28.2 &gt; sedona-snowflake.sql\n</code></pre> <p>sedona-snowflake-1.5.1.jar is the JAR file downloaded in Step 2.</p>"},{"location":"setup/snowflake/install/#step-5-run-the-sql-script-to-create-sedonas-functions","title":"Step 5: Run the SQL script to create Sedona's functions","text":"<p>We will create a worksheet in the database created in the previous step, and run the SQL script to create Sedona's functions.</p> <p>In this case, we will choose the option <code>Create Worksheet from SQL File</code>.</p> <p></p> <p>In the worksheet, choose <code>SEDONA_TEST</code> as the database, and <code>PUBLIC</code> as the schema. The SQL script should be in the worksheet. Then right click the worksheet and choose <code>Run All</code>. Snowflake will take 3 minutes to create Sedona's functions.</p> <p></p>"},{"location":"setup/snowflake/install/#step-6-verify-the-installation","title":"Step 6: Verify the installation","text":"<p>Open a new worksheet, choose <code>SEDONA_TEST</code> as the database, and any schema as the schema. Then run the following SQL statement:</p> <pre><code>SELECT SEDONA.ST_AsEWKT(SEDONA.ST_SETSRID(SEDONA.ST_POINT(1, 2), 4326));\n</code></pre> <p>You should be able to see the following result:</p> <pre><code>SRID=4326;POINT (1 2)\n</code></pre> <p>The worksheet should look like this:</p> <p></p>"},{"location":"setup/snowflake/modules/","title":"Sedona modules for Snowflake","text":"Name Introduction snowflake Spatial SQL functions for Snowflake snowflake-tester Automated tester of Sedona Snowflake functions"},{"location":"setup/snowflake/modules/#api-availability","title":"API availability","text":"Table SnowSQL \u2705 Snowpark \u2705"},{"location":"tutorial/Advanced-Tutorial-Tune-your-Application/","title":"Advanced tutorial: Tune your Sedona RDD application","text":"<p>Before getting into this advanced tutorial, please make sure that you have tried several Sedona functions on your local machine.</p>"},{"location":"tutorial/Advanced-Tutorial-Tune-your-Application/#pick-a-proper-sedona-version","title":"Pick a proper Sedona version","text":"<p>The versions of Sedona have three levels: X.X.X (i.e., 0.8.1)</p> <p>The first level means that this version contains big structure redesign which may bring big changes in APIs and performance.</p> <p>The second level (i.e., 0.8) indicates that this version contains significant performance enhancement, big new features and API changes. An old Sedona user who wants to pick this version needs to be careful about the API changes. Before you move to this version, please read Sedona version release notes and make sure you are ready to accept the API changes.</p> <p>The third level (i.e., 0.8.1) tells that this version only contains bug fixes, some small new features and slight performance enhancement. This version will not contain any API changes. Moving to this version is safe. We highly suggest all Sedona users that stay at the same level move to the latest version in this level.</p>"},{"location":"tutorial/Advanced-Tutorial-Tune-your-Application/#choose-a-proper-spatial-rdd-constructor","title":"Choose a proper Spatial RDD constructor","text":"<p>Sedona provides a number of constructors for each SpatialRDD (PointRDD, PolygonRDD and LineStringRDD). In general, you have two options to start with.</p> <ol> <li>Initialize a SpatialRDD from your data source such as HDFS and S3. A typical example is as follows: <pre><code>public PointRDD(JavaSparkContext sparkContext, String InputLocation, Integer Offset, FileDataSplitter splitter, boolean carryInputData, Integer partitions, StorageLevel newLevel)\n</code></pre></li> <li>Initialize a SpatialRDD from an existing RDD. A typical example is as follows: <pre><code>public PointRDD(JavaRDD&lt;Point&gt; rawSpatialRDD, StorageLevel newLevel)\n</code></pre></li> </ol> <p>You may notice that these constructors all take as input a \"StorageLevel\" parameter. This is to tell Apache Spark cache the \"rawSpatialRDD\", one attribute of SpatialRDD. The reason why Sedona does this is that Sedona wants to calculate the dataset boundary and approximate total count using several Apache Spark \"Action\"s. These information are useful when doing Spatial Join Query and Distance Join Query.</p> <p>However, in some cases, you may know well about your datasets. If so, you can manually provide these information by calling this kind of Spatial RDD constructors:</p> <p><pre><code>public PointRDD(JavaSparkContext sparkContext, String InputLocation, Integer Offset, FileDataSplitter splitter, boolean carryInputData, Integer partitions, Envelope datasetBoundary, Integer approximateTotalCount) {\n</code></pre> Manually providing the dataset boundary and approximate total count helps Sedona avoiding several slow \"Action\"s during initialization.</p>"},{"location":"tutorial/Advanced-Tutorial-Tune-your-Application/#cache-the-spatial-rdd-that-is-repeatedly-used","title":"Cache the Spatial RDD that is repeatedly used","text":"<p>Each SpatialRDD (PointRDD, PolygonRDD and LineStringRDD) possesses four RDD attributes. They are:</p> <ol> <li>rawSpatialRDD: The RDD generated by SpatialRDD constructors.</li> <li>spatialPartitionedRDD: The RDD generated by spatial partition a rawSpatialRDD. Note that: this RDD has replicated spatial objects.</li> <li>indexedRawRDD: The RDD generated by indexing a rawSpatialRDD.</li> <li>indexedRDD: The RDD generated by indexing a spatialPartitionedRDD. Note that: this RDD has replicated spatial objects.</li> </ol> <p>These four RDDs don't co-exist so you don't need to worry about the memory issue. These four RDDs are invoked in different queries:</p> <ol> <li>Spatial Range Query / KNN Query, no index: rawSpatialRDD is used.</li> <li>Spatial Range Query / KNN Query, use index: indexedRawRDD is used.</li> <li>Spatial Join Query / Distance Join Query, no index: spatialPartitionedRDD is used.</li> <li>Spatial Join Query / Distance Join Query, use index: indexed RDD is used.</li> </ol> <p>Therefore, if you use one of the queries above many times, you'd better cache the associated RDD into memory. There are several possible use cases:</p> <ol> <li>In Spatial Data Mining such as Spatial Autocorrelation and Spatial Co-location Pattern Mining, you may need to use Spatial Join / Spatial Self-join iteratively in order to calculate the adjacency matrix. If so, please cache the spatialPartitionedRDD/indexedRDD which is queries many times.</li> <li>In Spark RDD sharing applications such as Livy and Spark Job Server, many users may do Spatial Range Query / KNN Query on the same Spatial RDD with different query predicates. You'd better cache the rawSpatialRDD/indexedRawRDD.</li> </ol>"},{"location":"tutorial/Advanced-Tutorial-Tune-your-Application/#be-aware-of-spatial-rdd-partitions","title":"Be aware of Spatial RDD partitions","text":"<p>Sometimes users complain that the execution time is slow in some cases. As the first step, you should always consider increasing the number of your SpatialRDD partitions (2 - 8 times more than the original number). You can do this when you initialize a SpatialRDD. This may significantly improve your performance.</p> <p>After that, you may consider tuning some other parameters in Apache Spark. For example, you may use Kyro serializer or change the RDD fraction that is cached into memory.</p>"},{"location":"tutorial/benchmark/","title":"Benchmark","text":""},{"location":"tutorial/benchmark/#benchmark","title":"Benchmark","text":"<p>We welcome people to use Sedona for benchmark purpose. To achieve the best performance or enjoy all features of Sedona,</p> <ul> <li>Please always use the latest version or state the version used in your benchmark so that we can trace back to the issues.</li> <li>Please consider using Sedona core instead of Sedona SQL. Due to the limitation of SparkSQL (for instance, not support clustered index), we are not able to expose all features to SparkSQL.</li> <li>Please open Sedona kryo serializer to reduce the memory footprint.</li> </ul>"},{"location":"tutorial/demo/","title":"Scala and Java Examples","text":"<p>Scala and Java Examples contains template projects for Sedona Spark (RDD, SQL and Viz) and Sedona Flink. The template projects have been configured properly.</p> <p>Note that, although the template projects are written in Scala, the same APIs can be  used in Java as well.</p>"},{"location":"tutorial/demo/#folder-structure","title":"Folder structure","text":"<p>The folder structure of this repository is as follows.</p> <ul> <li>spark-sql: a Scala template shows how to use Sedona RDD, DataFrame and SQL API</li> <li>flink-sql: a Java template show how to use Sedona SQL via Flink Table APIs</li> </ul>"},{"location":"tutorial/demo/#compile-and-package","title":"Compile and package","text":""},{"location":"tutorial/demo/#prerequisites","title":"Prerequisites","text":"<p>Please make sure you have the following software installed on your local machine:</p> <ul> <li>For Scala: Scala 2.12</li> <li>For Java: JDK 1.8, Apache Maven 3</li> </ul>"},{"location":"tutorial/demo/#compile","title":"Compile","text":"<p>Run a terminal command <code>mvn clean package</code> within the folder of each template</p>"},{"location":"tutorial/demo/#submit-your-fat-jar-to-spark","title":"Submit your fat jar to Spark","text":"<p>After running the command mentioned above, you are able to see a fat jar in <code>./target</code> folder. Please take it and use <code>./bin/spark-submit</code> to submit this jar.</p> <p>To run the jar in this way, you need to:</p> <ul> <li> <p>Either change Spark Master Address in template projects or simply delete it. Currently, they are hard coded to <code>local[*]</code> which means run locally with all cores.</p> </li> <li> <p>Change the dependency packaging scope of Apache Spark from \"compile\" to \"provided\". This is a common packaging strategy in Maven and SBT which means do not package Spark into your fat jar. Otherwise, this may lead to a huge jar and version conflicts!</p> </li> <li> <p>Make sure the dependency versions in build.sbt are consistent with your Spark version.</p> </li> </ul>"},{"location":"tutorial/demo/#run-template-projects-locally","title":"Run template projects locally","text":"<p>We highly suggest you use IDEs to run template projects on your local machine. For Scala, we recommend IntelliJ IDEA with Scala plug-in. For Java, we recommend IntelliJ IDEA and Eclipse. With the help of IDEs, you don't have to prepare anything (even don't need to download and set up Spark!). As long as you have Scala and Java, everything works properly!</p>"},{"location":"tutorial/demo/#scala","title":"Scala","text":"<p>Import the Scala template project as SBT project. Then run the Main file in this project.</p>"},{"location":"tutorial/geopandas-shapely/","title":"Work with GeoPandas and Shapely","text":"<p>Danger</p> <p>Sedona Python currently only works with Shapely 1.x. If you use GeoPandas, please use &lt;= GeoPandas <code>0.11.1</code>. GeoPandas &gt; 0.11.1 will automatically install Shapely 2.0. If you use Shapely, please use &lt;= <code>1.8.4</code>.</p>"},{"location":"tutorial/geopandas-shapely/#interoperate-with-geopandas","title":"Interoperate with GeoPandas","text":"<p>Sedona Python has implemented serializers and deserializers which allows to convert Sedona Geometry objects into Shapely BaseGeometry objects. Based on that it is possible to load the data with geopandas from file (look at Fiona possible drivers) and create Spark DataFrame based on GeoDataFrame object.</p>"},{"location":"tutorial/geopandas-shapely/#from-geopandas-to-sedona-dataframe","title":"From GeoPandas to Sedona DataFrame","text":"<p>Loading the data from shapefile using geopandas read_file method and create Spark DataFrame based on GeoDataFrame:</p> <pre><code>import geopandas as gpd\nfrom sedona.spark import *\n\nconfig = SedonaContext.builder().\\\n      getOrCreate()\n\nsedona = SedonaContext.create(config)\n\ngdf = gpd.read_file(\"gis_osm_pois_free_1.shp\")\n\nsedona.createDataFrame(\n  gdf\n).show()\n</code></pre> <p>This query will show the following outputs:</p> <pre><code>+---------+----+-----------+--------------------+--------------------+\n|   osm_id|code|     fclass|                name|            geometry|\n+---------+----+-----------+--------------------+--------------------+\n| 26860257|2422|  camp_site|            de Kroon|POINT (15.3393145...|\n| 26860294|2406|     chalet|      Le\u015bne Ustronie|POINT (14.8709625...|\n| 29947493|2402|      motel|                null|POINT (15.0946636...|\n| 29947498|2602|        atm|                null|POINT (15.0732014...|\n| 29947499|2401|      hotel|                null|POINT (15.0696777...|\n| 29947505|2401|      hotel|                null|POINT (15.0155749...|\n+---------+----+-----------+--------------------+--------------------+\n</code></pre>"},{"location":"tutorial/geopandas-shapely/#from-sedona-dataframe-to-geopandas","title":"From Sedona DataFrame to GeoPandas","text":"<p>Reading data with Spark and converting to GeoPandas</p> <p><pre><code>import geopandas as gpd\nfrom sedona.spark import *\n\nconfig = SedonaContext.builder().\n    getOrCreate()\n\nsedona = SedonaContext.create(config)\n\ncounties = sedona.\\\n    read.\\\n    option(\"delimiter\", \"|\").\\\n    option(\"header\", \"true\").\\\n    csv(\"counties.csv\")\n\ncounties.createOrReplaceTempView(\"county\")\n\ncounties_geom = sedona.sql(\n    \"SELECT *, st_geomFromWKT(geom) as geometry from county\"\n)\n\ndf = counties_geom.toPandas()\ngdf = gpd.GeoDataFrame(df, geometry=\"geometry\")\n\ngdf.plot(\n    figsize=(10, 8),\n    column=\"value\",\n    legend=True,\n    cmap='YlOrBr',\n    scheme='quantiles',\n    edgecolor='lightgray'\n)\n</code></pre> </p> <p></p> <p> </p>"},{"location":"tutorial/geopandas-shapely/#interoperate-with-shapely-objects","title":"Interoperate with shapely objects","text":""},{"location":"tutorial/geopandas-shapely/#supported-shapely-objects","title":"Supported Shapely objects","text":"shapely object Available Point MultiPoint LineString MultiLinestring Polygon MultiPolygon <p>To create Spark DataFrame based on mentioned Geometry types, please use  GeometryType  from   sedona.sql.types  module. Converting works for list or tuple with shapely objects.</p> <p>Schema for target table with integer id and geometry type can be defined as follow:</p> <pre><code>from pyspark.sql.types import IntegerType, StructField, StructType\n\nfrom sedona.spark import *\n\nschema = StructType(\n    [\n        StructField(\"id\", IntegerType(), False),\n        StructField(\"geom\", GeometryType(), False)\n    ]\n)\n</code></pre> <p>Also Spark DataFrame with geometry type can be converted to list of shapely objects with  collect  method.</p>"},{"location":"tutorial/geopandas-shapely/#point-example","title":"Point example","text":"<pre><code>from shapely.geometry import Point\n\ndata = [\n    [1, Point(21.0, 52.0)],\n    [1, Point(23.0, 42.0)],\n    [1, Point(26.0, 32.0)]\n]\n\n\ngdf = sedona.createDataFrame(\n    data,\n    schema\n)\n\ngdf.show()\n</code></pre> <pre><code>+---+-------------+\n| id|         geom|\n+---+-------------+\n|  1|POINT (21 52)|\n|  1|POINT (23 42)|\n|  1|POINT (26 32)|\n+---+-------------+\n</code></pre> <pre><code>gdf.printSchema()\n</code></pre> <pre><code>root\n |-- id: integer (nullable = false)\n |-- geom: geometry (nullable = false)\n</code></pre>"},{"location":"tutorial/geopandas-shapely/#multipoint-example","title":"MultiPoint example","text":"<pre><code>from shapely.geometry import MultiPoint\n\ndata = [\n    [1, MultiPoint([[19.511463, 51.765158], [19.446408, 51.779752]])]\n]\n\ngdf = sedona.createDataFrame(\n    data,\n    schema\n).show(1, False)\n</code></pre> <pre><code>+---+---------------------------------------------------------+\n|id |geom                                                     |\n+---+---------------------------------------------------------+\n|1  |MULTIPOINT ((19.511463 51.765158), (19.446408 51.779752))|\n+---+---------------------------------------------------------+\n</code></pre>"},{"location":"tutorial/geopandas-shapely/#linestring-example","title":"LineString example","text":"<pre><code>from shapely.geometry import LineString\n\nline = [(40, 40), (30, 30), (40, 20), (30, 10)]\n\ndata = [\n    [1, LineString(line)]\n]\n\ngdf = sedona.createDataFrame(\n    data,\n    schema\n)\n\ngdf.show(1, False)\n</code></pre> <pre><code>+---+--------------------------------+\n|id |geom                            |\n+---+--------------------------------+\n|1  |LINESTRING (10 10, 20 20, 10 40)|\n+---+--------------------------------+\n</code></pre>"},{"location":"tutorial/geopandas-shapely/#multilinestring-example","title":"MultiLineString example","text":"<pre><code>from shapely.geometry import MultiLineString\n\nline1 = [(10, 10), (20, 20), (10, 40)]\nline2 = [(40, 40), (30, 30), (40, 20), (30, 10)]\n\ndata = [\n    [1, MultiLineString([line1, line2])]\n]\n\ngdf = sedona.createDataFrame(\n    data,\n    schema\n)\n\ngdf.show(1, False)\n</code></pre> <pre><code>+---+---------------------------------------------------------------------+\n|id |geom                                                                 |\n+---+---------------------------------------------------------------------+\n|1  |MULTILINESTRING ((10 10, 20 20, 10 40), (40 40, 30 30, 40 20, 30 10))|\n+---+---------------------------------------------------------------------+\n</code></pre>"},{"location":"tutorial/geopandas-shapely/#polygon-example","title":"Polygon example","text":"<pre><code>from shapely.geometry import Polygon\n\npolygon = Polygon(\n    [\n         [19.51121, 51.76426],\n         [19.51056, 51.76583],\n         [19.51216, 51.76599],\n         [19.51280, 51.76448],\n         [19.51121, 51.76426]\n    ]\n)\n\ndata = [\n    [1, polygon]\n]\n\ngdf = sedona.createDataFrame(\n    data,\n    schema\n)\n\ngdf.show(1, False)\n</code></pre> <pre><code>+---+--------------------------------------------------------------------------------------------------------+\n|id |geom                                                                                                    |\n+---+--------------------------------------------------------------------------------------------------------+\n|1  |POLYGON ((19.51121 51.76426, 19.51056 51.76583, 19.51216 51.76599, 19.5128 51.76448, 19.51121 51.76426))|\n+---+--------------------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"tutorial/geopandas-shapely/#multipolygon-example","title":"MultiPolygon example","text":"<pre><code>from shapely.geometry import MultiPolygon\n\nexterior_p1 = [(0, 0), (0, 2), (2, 2), (2, 0), (0, 0)]\ninterior_p1 = [(1, 1), (1, 1.5), (1.5, 1.5), (1.5, 1), (1, 1)]\n\nexterior_p2 = [(0, 0), (1, 0), (1, 1), (0, 1), (0, 0)]\n\npolygons = [\n    Polygon(exterior_p1, [interior_p1]),\n    Polygon(exterior_p2)\n]\n\ndata = [\n    [1, MultiPolygon(polygons)]\n]\n\ngdf = sedona.createDataFrame(\n    data,\n    schema\n)\n\ngdf.show(1, False)\n</code></pre> <pre><code>+---+----------------------------------------------------------------------------------------------------------+\n|id |geom                                                                                                      |\n+---+----------------------------------------------------------------------------------------------------------+\n|1  |MULTIPOLYGON (((0 0, 0 2, 2 2, 2 0, 0 0), (1 1, 1.5 1, 1.5 1.5, 1 1.5, 1 1)), ((0 0, 0 1, 1 1, 1 0, 0 0)))|\n+---+----------------------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"tutorial/jupyter-notebook/","title":"Python Jupyter Notebook Examples","text":"<p>Click  and play the interactive Sedona Python Jupyter Notebook immediately!</p> <p>Sedona Python provides a number of Jupyter Notebook examples.</p> <p>Please use the following steps to run Jupyter notebook with Pipenv on your machine</p> <ol> <li>Clone Sedona GitHub repo or download the source code</li> <li>Install Sedona Python from PyPI or GitHub source: Read Install Sedona Python to learn.</li> <li>Prepare spark-shaded jar: Read Install Sedona Python to learn.</li> <li>Setup pipenv python version. Please use your desired Python version. <pre><code>cd binder\npipenv --python 3.8\n</code></pre></li> <li>Install dependencies <pre><code>cd binder\npipenv install\n</code></pre></li> <li>Install jupyter notebook kernel for pipenv <pre><code>pipenv install ipykernel\npipenv shell\n</code></pre></li> <li>In the pipenv shell, do <pre><code>python -m ipykernel install --user --name=apache-sedona\n</code></pre></li> <li>Setup environment variables <code>SPARK_HOME</code> and <code>PYTHONPATH</code> if you didn't do it before. Read Install Sedona Python to learn.</li> <li>Launch jupyter notebook: <code>jupyter notebook</code></li> <li>Select Sedona notebook. In your notebook, Kernel -&gt; Change Kernel. Your kernel should now be an option.</li> </ol>"},{"location":"tutorial/python-vector-osm/","title":"Example of spark + sedona + hdfs with slave nodes and OSM vector data consults","text":"<pre><code>from IPython.display import display, HTML\nfrom pyspark.sql import SparkSession\nfrom pyspark import StorageLevel\nimport pandas as pd\nfrom pyspark.sql.types import StructType, StructField,StringType, LongType, IntegerType, DoubleType, ArrayType\nfrom pyspark.sql.functions import regexp_replace\nfrom sedona.register import SedonaRegistrator\nfrom sedona.utils import SedonaKryoRegistrator, KryoSerializer\nfrom pyspark.sql.functions import col, split, expr\nfrom pyspark.sql.functions import udf, lit\nfrom sedona.utils import SedonaKryoRegistrator, KryoSerializer\nfrom pyspark.sql.functions import col, split, expr\nfrom pyspark.sql.functions import udf, lit, flatten\nfrom pywebhdfs.webhdfs import PyWebHdfsClient\nfrom datetime import date\nfrom pyspark.sql.functions import monotonically_increasing_id\nimport json\n</code></pre>"},{"location":"tutorial/python-vector-osm/#registering-spark-session-adding-node-executor-configurations-and-sedona-registrator","title":"Registering spark session, adding node executor configurations and sedona registrator","text":"<pre><code>spark = SparkSession.\\\n    builder.\\\n    appName(\"Overpass-API\").\\\n    enableHiveSupport().\\\n    master(\"local[*]\").\\\n    master(\"spark://spark-master:7077\").\\\n    config(\"spark.executor.memory\", \"15G\").\\\n    config(\"spark.driver.maxResultSize\", \"135G\").\\\n    config(\"spark.sql.shuffle.partitions\", \"500\").\\\n    config(' spark.sql.adaptive.coalescePartitions.enabled', True).\\\n    config('spark.sql.adaptive.enabled', True).\\\n    config('spark.sql.adaptive.coalescePartitions.initialPartitionNum', 125).\\\n    config(\"spark.sql.execution.arrow.pyspark.enabled\", True).\\\n    config(\"spark.sql.execution.arrow.fallback.enabled\", True).\\\n    config('spark.kryoserializer.buffer.max', 2047).\\\n    config(\"spark.serializer\", KryoSerializer.getName).\\\n    config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName).\\\n    config(\"spark.jars.packages\", \"org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.4.0,org.datasyslab:geotools-wrapper:1.4.0-28.2\") .\\\n    enableHiveSupport().\\\n    getOrCreate()\n\nSedonaRegistrator.registerAll(spark)\nsc = spark.sparkContext\n</code></pre>"},{"location":"tutorial/python-vector-osm/#connecting-to-overpass-api-to-search-and-downloading-data-for-saving-into-hdfs","title":"Connecting to Overpass API to search and downloading data for saving into HDFS","text":"<pre><code>import requests\nimport json\n\noverpass_url = \"http://overpass-api.de/api/interpreter\"\noverpass_query = \"\"\"\n[out:json];\narea[name = \"Foz do Igua\u00e7u\"];\nway(area)[\"highway\"~\"\"];\nout geom;\n&gt;;\nout skel qt;\n\"\"\"\n\nresponse = requests.get(overpass_url,\n                         params={'data': overpass_query})\ndata = response.json()\nhdfs = PyWebHdfsClient(host='179.106.229.159',port='50070', user_name='root')\nfile_name = \"foz_roads_osm.json\"\nhdfs.delete_file_dir(file_name)\nhdfs.create_file(file_name, json.dumps(data))\n</code></pre>"},{"location":"tutorial/python-vector-osm/#connecting-spark-sedona-with-saved-hdfs-file","title":"Connecting spark sedona with saved hdfs file","text":"<pre><code>path = \"hdfs://776faf4d6a1e:8020/\"+file_name\ndf = spark.read.json(path, multiLine = \"true\")\n</code></pre>"},{"location":"tutorial/python-vector-osm/#consulting-and-organizing-data-for-analysis","title":"Consulting and organizing data for analysis","text":"<pre><code>from pyspark.sql.functions import explode, arrays_zip\n\ndf.createOrReplaceTempView(\"df\")\ntb = spark.sql(\"select *, size(elements) total_nodes from df\")\ntb.show(5)\n\nisolate_total_nodes = tb.select(\"total_nodes\").toPandas()\ntotal_nodes = isolate_total_nodes[\"total_nodes\"].iloc[0]\nprint(total_nodes)\n\nisolate_ids = tb.select(\"elements.id\").toPandas()\nids = pd.DataFrame(isolate_ids[\"id\"].iloc[0]).drop_duplicates()\nprint(ids[0].iloc[1])\n\nformatted_df = tb\\\n.withColumn(\"id\", explode(\"elements.id\"))\n\nformatted_df.show(5)\n\nformatted_df = tb\\\n.withColumn(\"new\", arrays_zip(\"elements.id\", \"elements.geometry\", \"elements.nodes\", \"elements.tags\"))\\\n.withColumn(\"new\", explode(\"new\"))\n\nformatted_df.show(5)\n\n# formatted_df.printSchema()\n\nformatted_df = formatted_df.select(\"new.0\",\"new.1\",\"new.2\",\"new.3.maxspeed\",\"new.3.incline\",\"new.3.surface\", \"new.3.name\", \"total_nodes\")\nformatted_df = formatted_df.withColumnRenamed(\"0\",\"id\").withColumnRenamed(\"1\",\"geom\").withColumnRenamed(\"2\",\"nodes\").withColumnRenamed(\"3\",\"tags\")\nformatted_df.createOrReplaceTempView(\"formatted_df\")\nformatted_df.show(5)\n# TODO atualizar daqui para baixo para considerar a linha inteira na l\u00f3gica\npoints_tb = spark.sql(\"select geom, id from formatted_df where geom IS NOT NULL\")\npoints_tb = points_tb\\\n.withColumn(\"new\", arrays_zip(\"geom.lat\", \"geom.lon\"))\\\n.withColumn(\"new\", explode(\"new\"))\n\npoints_tb = points_tb.select(\"new.0\",\"new.1\", \"id\")\n\npoints_tb = points_tb.withColumnRenamed(\"0\",\"lat\").withColumnRenamed(\"1\",\"lon\")\npoints_tb.printSchema()\n\npoints_tb.createOrReplaceTempView(\"points_tb\")\n\npoints_tb.show(5)\n\ncoordinates_tb = spark.sql(\"select (select collect_list(CONCAT(p1.lat,',',p1.lon)) from points_tb p1 where p1.id = p2.id group by p1.id) as coordinates, p2.id, p2.maxspeed, p2.incline, p2.surface, p2.name, p2.nodes, p2.total_nodes from formatted_df p2\")\ncoordinates_tb.createOrReplaceTempView(\"coordinates_tb\")\ncoordinates_tb.show(5)\n\nroads_tb = spark.sql(\"SELECT ST_LineStringFromText(REPLACE(REPLACE(CAST(coordinates as string),'[',''),']',''), ',') as geom, id, maxspeed, incline, surface, name, nodes, total_nodes FROM coordinates_tb WHERE coordinates IS NOT NULL\")\nroads_tb.createOrReplaceTempView(\"roads_tb\")\nroads_tb.show(5)\n</code></pre>"},{"location":"tutorial/raster/","title":"Raster SQL app","text":"<p>Note</p> <p>Sedona uses 1-based indexing for all raster functions except map algebra function, which uses 0-based indexing.</p> <p>Note</p> <p>Since v<code>1.5.0</code>, Sedona assumes geographic coordinates to be in longitude/latitude order. If your data is lat/lon order, please use <code>ST_FlipCoordinates</code> to swap X and Y.</p> <p>Starting from <code>v1.1.0</code>, Sedona SQL supports raster data sources and raster operators in DataFrame and SQL. Raster support is available in all Sedona language bindings including Scala, Java, Python, and R.</p> <p>This page outlines the steps to manage raster data using SedonaSQL.</p> ScalaJavaPython <pre><code>var myDataFrame = sedona.sql(\"YOUR_SQL\")\nmyDataFrame.createOrReplaceTempView(\"rasterDf\")\n</code></pre> <pre><code>Dataset&lt;Row&gt; myDataFrame = sedona.sql(\"YOUR_SQL\")\nmyDataFrame.createOrReplaceTempView(\"rasterDf\")\n</code></pre> <pre><code>myDataFrame = sedona.sql(\"YOUR_SQL\")\nmyDataFrame.createOrReplaceTempView(\"rasterDf\")\n</code></pre> <p>Detailed SedonaSQL APIs are available here: SedonaSQL API. You can find example raster data in Sedona GitHub repo.</p>"},{"location":"tutorial/raster/#set-up-dependencies","title":"Set up dependencies","text":"Scala/JavaPython <ol> <li>Read Sedona Maven Central coordinates and add Sedona dependencies in build.sbt or pom.xml.</li> <li>Add Apache Spark core, Apache SparkSQL in build.sbt or pom.xml.</li> <li>Please see SQL example project</li> </ol> <ol> <li>Please read Quick start to install Sedona Python.</li> <li>This tutorial is based on Sedona SQL Jupyter Notebook example. You can interact with Sedona Python Jupyter Notebook immediately on Binder. Click  to interact with Sedona Python Jupyter notebook immediately on Binder.</li> </ol>"},{"location":"tutorial/raster/#create-sedona-config","title":"Create Sedona config","text":"<p>Use the following code to create your Sedona config at the beginning. If you already have a SparkSession (usually named <code>spark</code>) created by Wherobots/AWS EMR/Databricks, please skip this step and use <code>spark</code> directly.</p> <p>Sedona &gt;= 1.4.1</p> <p>You can add additional Spark runtime config to the config builder. For example, <code>SedonaContext.builder().config(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\")</code></p> ScalaJavaPython <p><pre><code>import org.apache.sedona.spark.SedonaContext\n\nval config = SedonaContext.builder()\n.master(\"local[*]\") // Delete this if run in cluster mode\n.appName(\"readTestScala\") // Change this to a proper name\n.getOrCreate()\n</code></pre> If you use SedonaViz together with SedonaSQL, please add the following line after <code>SedonaContext.builder()</code> to enable Sedona Kryo serializer: <pre><code>.config(\"spark.kryo.registrator\", classOf[SedonaVizKryoRegistrator].getName) // org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator\n</code></pre></p> <p><pre><code>import org.apache.sedona.spark.SedonaContext;\n\nSparkSession config = SedonaContext.builder()\n.master(\"local[*]\") // Delete this if run in cluster mode\n.appName(\"readTestScala\") // Change this to a proper name\n.getOrCreate()\n</code></pre> If you use SedonaViz together with SedonaSQL, please add the following line after <code>SedonaContext.builder()</code> to enable Sedona Kryo serializer: <pre><code>.config(\"spark.kryo.registrator\", SedonaVizKryoRegistrator.class.getName) // org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator\n</code></pre></p> <p><pre><code>from sedona.spark import *\n\nconfig = SedonaContext.builder() .\\\n    config('spark.jars.packages',\n           'org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.5.1,'\n           'org.datasyslab:geotools-wrapper:1.5.1-28.2'). \\\n    getOrCreate()\n</code></pre> If you are using Spark versions &gt;= 3.4, please replace the <code>3.0</code> in the package name of sedona-spark-shaded with the corresponding major.minor version of Spark, such as <code>sedona-spark-shaded-3.4_2.12:1.5.1</code>.</p> <p>Sedona &lt; 1.4.1</p> <p>The following method has been deprecated since Sedona 1.4.1. Please use the method above to create your Sedona config.</p> ScalaJavaPython <p><pre><code>var sparkSession = SparkSession.builder()\n.master(\"local[*]\") // Delete this if run in cluster mode\n.appName(\"readTestScala\") // Change this to a proper name\n// Enable Sedona custom Kryo serializer\n.config(\"spark.serializer\", classOf[KryoSerializer].getName) // org.apache.spark.serializer.KryoSerializer\n.config(\"spark.kryo.registrator\", classOf[SedonaKryoRegistrator].getName)\n.getOrCreate() // org.apache.sedona.core.serde.SedonaKryoRegistrator\n</code></pre> If you use SedonaViz together with SedonaSQL, please use the following two lines to enable Sedona Kryo serializer instead: <pre><code>.config(\"spark.serializer\", classOf[KryoSerializer].getName) // org.apache.spark.serializer.KryoSerializer\n.config(\"spark.kryo.registrator\", classOf[SedonaVizKryoRegistrator].getName) // org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator\n</code></pre></p> <p><pre><code>SparkSession sparkSession = SparkSession.builder()\n.master(\"local[*]\") // Delete this if run in cluster mode\n.appName(\"readTestScala\") // Change this to a proper name\n// Enable Sedona custom Kryo serializer\n.config(\"spark.serializer\", KryoSerializer.class.getName) // org.apache.spark.serializer.KryoSerializer\n.config(\"spark.kryo.registrator\", SedonaKryoRegistrator.class.getName)\n.getOrCreate() // org.apache.sedona.core.serde.SedonaKryoRegistrator\n</code></pre> If you use SedonaViz together with SedonaSQL, please use the following two lines to enable Sedona Kryo serializer instead: <pre><code>.config(\"spark.serializer\", KryoSerializer.class.getName) // org.apache.spark.serializer.KryoSerializer\n.config(\"spark.kryo.registrator\", SedonaVizKryoRegistrator.class.getName) // org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator\n</code></pre></p> <p><pre><code>sparkSession = SparkSession. \\\n    builder. \\\n    appName('appName'). \\\n    config(\"spark.serializer\", KryoSerializer.getName). \\\n    config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName). \\\n    config('spark.jars.packages',\n           'org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.5.1,'\n           'org.datasyslab:geotools-wrapper:1.5.1-28.2'). \\\n    getOrCreate()\n</code></pre> If you are using Spark versions &gt;= 3.4, please replace the <code>3.0</code> in the package name of sedona-spark-shaded with the corresponding major.minor version of Spark, such as <code>sedona-spark-shaded-3.4_2.12:1.5.1</code>.</p>"},{"location":"tutorial/raster/#initiate-sedonacontext","title":"Initiate SedonaContext","text":"<p>Add the following line after creating the Sedona config. If you already have a SparkSession (usually named <code>spark</code>) created by Wherobots/AWS EMR/Databricks, please call <code>SedonaContext.create(spark)</code> instead.</p> <p>Sedona &gt;= 1.4.1</p> ScalaJavaPython <pre><code>import org.apache.sedona.spark.SedonaContext\n\nval sedona = SedonaContext.create(config)\n</code></pre> <pre><code>import org.apache.sedona.spark.SedonaContext;\n\nSparkSession sedona = SedonaContext.create(config)\n</code></pre> <pre><code>from sedona.spark import *\n\nsedona = SedonaContext.create(config)\n</code></pre> <p>Sedona &lt; 1.4.1</p> <p>The following method has been deprecated since Sedona 1.4.1. Please use the method above to create your SedonaContext.</p> ScalaJavaPython <pre><code>SedonaSQLRegistrator.registerAll(sparkSession)\n</code></pre> <pre><code>SedonaSQLRegistrator.registerAll(sparkSession)\n</code></pre> <pre><code>from sedona.register import SedonaRegistrator\n\nSedonaRegistrator.registerAll(spark)\n</code></pre> <p>You can also register everything by passing <code>--conf spark.sql.extensions=org.apache.sedona.sql.SedonaSqlExtensions</code> to <code>spark-submit</code> or <code>spark-shell</code>.</p>"},{"location":"tutorial/raster/#load-data-from-files","title":"Load data from files","text":"<p>Assume we have a single raster data file called rasterData.tiff, at Path.</p> <p>Use the following code to load the data and create a raw Dataframe.</p> ScalaJavaPython <pre><code>var rawDf = sedona.read.format(\"binaryFile\").load(path_to_raster_data)\nrawDf.createOrReplaceTempView(\"rawdf\")\nrawDf.show()\n</code></pre> <pre><code>Dataset&lt;Row&gt; rawDf = sedona.read.format(\"binaryFile\").load(path_to_raster_data)\nrawDf.createOrReplaceTempView(\"rawdf\")\nrawDf.show()\n</code></pre> <pre><code>rawDf = sedona.read.format(\"binaryFile\").load(path_to_raster_data)\nrawDf.createOrReplaceTempView(\"rawdf\")\nrawDf.show()\n</code></pre> <p>The output will look like this:</p> <pre><code>|                path|    modificationTime|length|             content|\n+--------------------+--------------------+------+--------------------+\n|file:/Download/ra...|2023-09-06 16:24:...|174803|[49 49 2A 00 08 0...|\n</code></pre> <p>For multiple raster data files use the following code to load the data from path and create raw DataFrame.</p> <p>Note</p> <p>The above code works too for loading multiple raster data files.  if the raster files are in separate directories and the option also makes sure that only <code>.tif</code> or <code>.tiff</code> files are being loaded.</p> ScalaJavaPython <pre><code>var rawDf = sedona.read.format(\"binaryFile\").option(\"recursiveFileLookup\", \"true\").option(\"pathGlobFilter\", \"*.tif*\").load(path_to_raster_data_folder)\nrawDf.createOrReplaceTempView(\"rawdf\")\nrawDf.show()\n</code></pre> <pre><code>Dataset&lt;Row&gt; rawDf = sedona.read.format(\"binaryFile\").option(\"recursiveFileLookup\", \"true\").option(\"pathGlobFilter\", \"*.tif*\").load(path_to_raster_data_folder);\nrawDf.createOrReplaceTempView(\"rawdf\");\nrawDf.show();\n</code></pre> <pre><code>rawDf = sedona.read.format(\"binaryFile\").option(\"recursiveFileLookup\", \"true\").option(\"pathGlobFilter\", \"*.tif*\").load(path_to_raster_data_folder)\nrawDf.createOrReplaceTempView(\"rawdf\")\nrawDf.show()\n</code></pre> <p>The output will look like this:</p> <pre><code>|                path|    modificationTime|length|             content|\n+--------------------+--------------------+------+--------------------+\n|file:/Download/ra...|2023-09-06 16:24:...|209199|[4D 4D 00 2A 00 0...|\n|file:/Download/ra...|2023-09-06 16:24:...|174803|[49 49 2A 00 08 0...|\n|file:/Download/ra...|2023-09-06 16:24:...|174803|[49 49 2A 00 08 0...|\n|file:/Download/ra...|2023-09-06 16:24:...|  6619|[49 49 2A 00 08 0...|\n</code></pre> <p>The content column in the raster table is still in the raw form, binary form.</p>"},{"location":"tutorial/raster/#create-a-raster-type-column","title":"Create a Raster type column","text":"<p>All raster operations in SedonaSQL require Raster type objects. Therefore, this should be the next step after loading the data.</p>"},{"location":"tutorial/raster/#from-geotiff","title":"From Geotiff","text":"<pre><code>SELECT RS_FromGeoTiff(content) AS rast, modificationTime, length, path FROM rawdf\n</code></pre> <p>To verify this, use the following code to print the schema of the DataFrame:</p> <pre><code>rasterDf.printSchema()\n</code></pre> <p>The output will be like this:</p> <pre><code>root\n |-- rast: raster (nullable = true)\n |-- modificationTime: timestamp (nullable = true)\n |-- length: long (nullable = true)\n |-- path: string (nullable = true)\n</code></pre>"},{"location":"tutorial/raster/#from-arc-grid","title":"From Arc Grid","text":"<p>The raster data is loaded the same way as <code>tiff</code> file, but the raster data is stored with the extension <code>.asc</code>, ASCII format. The following code creates a Raster type objects from binary data:</p> <pre><code>SELECT RS_FromArcInfoAsciiGrid(content) AS rast, modificationTime, length, path FROM rawdf\n</code></pre>"},{"location":"tutorial/raster/#rasters-metadata","title":"Raster's metadata","text":"<p>Sedona has a function to get the metadata for the raster, and also a function to get the world file of the raster.</p>"},{"location":"tutorial/raster/#metadata","title":"Metadata","text":"<p>This function will return an array of metadata, it will have all the necessary information about the raster, Please refer to RS_MetaData.</p> <pre><code>SELECT RS_MetaData(rast) FROM rasterDf\n</code></pre> <p>Output for the following function will be:</p> <pre><code>[-1.3095817809482181E7, 4021262.7487925636, 512.0, 517.0, 72.32861272132695, -72.32861272132695, 0.0, 0.0, 3857.0, 1.0]\n</code></pre> <p>The first two elements of the array represent the real-world geographic coordinates (like longitude/latitude) of the raster image's top left pixel, while the next two elements represent the pixel dimensions of the raster.</p>"},{"location":"tutorial/raster/#world-file","title":"World File","text":"<p>There are two kinds of georeferences, GDAL and ESRI seen in world files. For more information please refer to RS_GeoReference.</p> <pre><code>SELECT RS_GeoReference(rast, \"ESRI\") FROM rasterDf\n</code></pre> <p>The Output will be as follows:</p> <pre><code>72.328613\n0.000000\n0.000000\n-72.328613\n-13095781.645176\n4021226.584486\n</code></pre> <p>World files are used to georeference and geo-locate images by establishing an image-to-world coordinate transformation that assigns real-world geographic coordinates to the pixels of the image.</p>"},{"location":"tutorial/raster/#raster-manipulation","title":"Raster Manipulation","text":"<p>Since <code>v1.5.0</code> there have been many additions to manipulate raster data, we will show you a few example queries.</p> <p>Note</p> <p>Read SedonaSQL Raster operators to learn how you can use Sedona for raster manipulation.</p>"},{"location":"tutorial/raster/#coordinate-translation","title":"Coordinate translation","text":"<p>Sedona allows you to translate coordinates as per your needs. It can translate pixel locations to world coordinates and vice versa.</p>"},{"location":"tutorial/raster/#pixelaspoint","title":"PixelAsPoint","text":"<p>Use RS_PixelAsPoint to translate pixel coordinates to world location.</p> <pre><code>SELECT RS_PixelAsPoint(rast, 450, 400) FROM rasterDf\n</code></pre> <p>Output:</p> <pre><code>POINT (-13063342 3992403.75)\n</code></pre>"},{"location":"tutorial/raster/#world-to-raster-coordinate","title":"World to Raster Coordinate","text":"<p>Use RS_WorldToRasterCoord to translate world location to pixel coordinates. To just get X coordinate use RS_WorldToRasterCoordX and for just Y coordinate use RS_WorldToRasterCoordY.</p> <pre><code>SELECT RS_WorldToRasterCoord(rast, -1.3063342E7, 3992403.75)\n</code></pre> <p>Output:</p> <pre><code>POINT (450 400)\n</code></pre>"},{"location":"tutorial/raster/#pixel-manipulation","title":"Pixel Manipulation","text":"<p>Use RS_Values to fetch values for a specified array of Point Geometries. The coordinates in the point geometry are indicative of real-world location.</p> <pre><code>SELECT RS_Values(rast, Array(ST_Point(-13063342, 3992403.75), ST_Point(-13074192, 3996020)))\n</code></pre> <p>Output:</p> <pre><code>[132.0, 148.0]\n</code></pre> <p>To change values over a grid or area defined by geometry, we will use RS_SetValues.</p> <pre><code>SELECT RS_SetValues(\nrast, 1, 250, 260, 3, 3,\nArray(10, 12, 17, 26, 28, 37, 43, 64, 66)\n)\n</code></pre> <p>Follow the links to get more information on how to use the functions appropriately.</p>"},{"location":"tutorial/raster/#band-manipulation","title":"Band Manipulation","text":"<p>Sedona provides APIs to select specific bands from a raster image and create a new raster. For example, to select 2 bands from a raster, you can use the RS_Band API to retrieve the desired multi-band raster.</p> <p>Let's use a multi-band raster for this example. The process of loading and converting it to raster type is the same.</p> <pre><code>SELECT RS_Band(colorRaster, Array(1, 2))\n</code></pre> <p>Let's say you have many single-banded rasters and want to add a band to the raster to perform map algebra operations. You can do so using RS_AddBand Sedona function.</p> <pre><code>SELECT RS_AddBand(raster1, raster2, 1, 2)\n</code></pre> <p>This will result in <code>raster1</code> having <code>raster2</code>'s specified band.</p>"},{"location":"tutorial/raster/#resample-raster-data","title":"Resample raster data","text":"<p>Sedona allows you to resample raster data using different interpolation methods like the nearest neighbor, bilinear, and bicubic to change the cell size or align raster grids, using RS_Resample.</p> <pre><code>SELECT RS_Resample(rast, 50, -50, -13063342, 3992403.75, true, \"bicubic\")\n</code></pre> <p>For more information please follow the link.</p>"},{"location":"tutorial/raster/#execute-map-algebra-operations","title":"Execute map algebra operations","text":"<p>Map algebra is a way to perform raster calculations using mathematical expressions. The expression can be a simple arithmetic operation or a complex combination of multiple operations.</p> <p>The Normalized Difference Vegetation Index (NDVI) is a simple graphical indicator that can be used to analyze remote sensing measurements from a space platform and assess whether the target being observed contains live green vegetation or not.</p> <pre><code>NDVI = (NIR - Red) / (NIR + Red)\n</code></pre> <p>where NIR is the near-infrared band and Red is the red band.</p> <pre><code>SELECT RS_MapAlgebra(raster, 'D', 'out = (rast[3] - rast[0]) / (rast[3] + rast[0]);') as ndvi FROM raster_table\n</code></pre> <p>For more information please refer to Map Algebra API.</p>"},{"location":"tutorial/raster/#interoperability-between-raster-and-vector-data","title":"Interoperability between raster and vector data","text":""},{"location":"tutorial/raster/#geometry-as-raster","title":"Geometry As Raster","text":"<p>Sedona allows you to rasterize a geometry by using RS_AsRaster.</p> <pre><code>SELECT RS_AsRaster(\nST_GeomFromWKT('POLYGON((150 150, 220 260, 190 300, 300 220, 150 150))'),\nRS_MakeEmptyRaster(1, 'b', 4, 6, 1, -1, 1),\n'b', 230\n)\n</code></pre> <p>The image created is as below for the vector:</p> <p></p> <p>Note</p> <p>The vector coordinates are buffed up to showcase the output, the real use case, may or may not match the example.</p>"},{"location":"tutorial/raster/#spatial-range-query","title":"Spatial range query","text":"<p>Sedona provides raster predicates to do a range query using a geometry window, for example, let's use RS_Intersects.</p> <pre><code>SELECT rast FROM rasterDf WHERE RS_Intersect(rast, ST_GeomFromWKT('POLYGON((0 0, 0 10, 10 10, 10 0, 0 0))'))\n</code></pre>"},{"location":"tutorial/raster/#spatial-join-query","title":"Spatial join query","text":"<p>Sedona's raster predicates also can do a spatial join using the raster column and geometry column, using the same function as above.</p> <pre><code>SELECT r.rast, g.geom FROM rasterDf r, geomDf g WHERE RS_Interest(r.rast, g.geom)\n</code></pre> <p>Note</p> <p>These range and join queries will filter rasters using the provided geometric boundary and the spatial boundary of the raster.</p> <p>Sedona offers more raster predicates to do spatial range queries and spatial join queries. Please refer to raster predicates docs.</p>"},{"location":"tutorial/raster/#visualize-raster-images","title":"Visualize raster images","text":"<p>Sedona provides APIs to visualize raster data in an image form.</p>"},{"location":"tutorial/raster/#base64-string","title":"Base64 String","text":"<p>The RS_AsBase64 encodes the raster data as a Base64 string and can be visualized using online decoder.</p> <pre><code>SELECT RS_AsBase64(rast) FROM rasterDf\n</code></pre>"},{"location":"tutorial/raster/#html-image","title":"HTML Image","text":"<p>The RS_AsImage returns an HTML image tag, that can be visualized using an HTML viewer or in Jupyter Notebook. For more information please click on the link.</p> <pre><code>SELECT RS_AsImage(rast, 500) FROM rasterDf\n</code></pre> <p>The output looks like this:</p> <p></p>"},{"location":"tutorial/raster/#2-d-matrix","title":"2-D Matrix","text":"<p>Sedona offers an API to visualize raster data that is not sufficient for the other APIs mentioned above.</p> <pre><code>SELECT RS_AsMatrix(rast) FROM rasterDf\n</code></pre> <p>Output will be as follows:</p> <pre><code>| 1   3   4   0|\n| 2   9  10  11|\n| 3   4   5   6|\n</code></pre> <p>Please refer to Raster visualizer docs to learn how to make the most of the visualizing APIs.</p>"},{"location":"tutorial/raster/#save-to-permanent-storage","title":"Save to permanent storage","text":"<p>Sedona has APIs that can save an entire raster column to files in a specified location. Before saving, the raster type column needs to be converted to a binary format. Sedona provides several functions to convert a raster column into a binary column suitable for file storage. Once in binary format, the raster data can then be written to files on disk using the Sedona file storage APIs.</p> <pre><code>rasterDf.write.format(\"raster\").option(\"rasterField\", \"raster\").option(\"fileExtension\", \".tiff\").mode(SaveMode.Overwrite).save(dirPath)\n</code></pre> <p>Sedona has a few writer functions that create the binary DataFrame necessary for saving the raster images.</p>"},{"location":"tutorial/raster/#as-arc-grid","title":"As Arc Grid","text":"<p>Use RS_AsArcGrid to get the binary Dataframe of the raster in Arc Grid format.</p> <pre><code>SELECT RS_AsArcGrid(raster)\n</code></pre>"},{"location":"tutorial/raster/#as-geotiff","title":"As GeoTiff","text":"<p>Use RS_AsGeoTiff to get the binary Dataframe of the raster in GeoTiff format.</p> <pre><code>SELECT RS_AsGeoTiff(raster)\n</code></pre>"},{"location":"tutorial/raster/#as-png","title":"As PNG","text":"<p>Use RS_AsPNG to get the binary Dataframe of the raster in PNG format.</p> <pre><code>SELECT RS_AsPNG(raster)\n</code></pre> <p>Please refer to Raster writer docs for more details.</p>"},{"location":"tutorial/raster/#performance-optimization","title":"Performance optimization","text":"<p>When working with large raster datasets, refer to the documentation on storing raster geometries in Parquet format for recommendations to optimize performance.</p>"},{"location":"tutorial/rdd/","title":"Spatial RDD app","text":"<p>The page outlines the steps to create Spatial RDDs and run spatial queries using Sedona-core.</p>"},{"location":"tutorial/rdd/#set-up-dependencies","title":"Set up dependencies","text":"<p>Please refer to Set up dependencies to set up dependencies.</p>"},{"location":"tutorial/rdd/#create-sedona-config","title":"Create Sedona config","text":"<p>Please refer to Create Sedona config to create a Sedona config.</p>"},{"location":"tutorial/rdd/#initiate-sedonacontext","title":"Initiate SedonaContext","text":"<p>Please refer to Initiate SedonaContext to initiate a SedonaContext.</p>"},{"location":"tutorial/rdd/#create-a-spatialrdd","title":"Create a SpatialRDD","text":""},{"location":"tutorial/rdd/#create-a-typed-spatialrdd","title":"Create a typed SpatialRDD","text":"<p>Sedona-core provides three special SpatialRDDs: PointRDD, PolygonRDD, and LineStringRDD.</p> <p>Warning</p> <p>Typed SpatialRDD has been deprecated for a long time. We do NOT recommend it anymore.</p>"},{"location":"tutorial/rdd/#create-a-generic-spatialrdd","title":"Create a generic SpatialRDD","text":"<p>A generic SpatialRDD is not typed to a certain geometry type and open to more scenarios. It allows an input data file contains mixed types of geometries. For instance, a WKT file contains three types gemetries LineString, Polygon and MultiPolygon.</p>"},{"location":"tutorial/rdd/#from-wktwkb","title":"From WKT/WKB","text":"<p>Geometries in a WKT and WKB file always occupy a single column no matter how many coordinates they have. Sedona provides <code>WktReader</code> and <code>WkbReader</code> to create generic SpatialRDD.</p> <p>Suppose we have a <code>checkin.tsv</code> WKT TSV file at Path <code>/Download/checkin.tsv</code> as follows: <pre><code>POINT (-88.331492 32.324142)    hotel\nPOINT (-88.175933 32.360763)    gas\nPOINT (-88.388954 32.357073)    bar\nPOINT (-88.221102 32.35078) restaurant\n</code></pre> This file has two columns and corresponding offsets(Column IDs) are 0, 1. Column 0 is the WKT string and Column 1 is the checkin business type.</p> <p>Use the following code to create a SpatialRDD</p> ScalaJavaPython <pre><code>val inputLocation = \"/Download/checkin.tsv\"\nval wktColumn = 0 // The WKT string starts from Column 0\nval allowTopologyInvalidGeometries = true // Optional\nval skipSyntaxInvalidGeometries = false // Optional\nval spatialRDD = WktReader.readToGeometryRDD(sedona.sparkContext, inputLocation, wktColumn, allowTopologyInvalidGeometries, skipSyntaxInvalidGeometries)\n</code></pre> <pre><code>String inputLocation = \"/Download/checkin.tsv\"\nint wktColumn = 0 // The WKT string starts from Column 0\nboolean allowTopologyInvalidGeometries = true // Optional\nboolean skipSyntaxInvalidGeometries = false // Optional\nSpatialRDD spatialRDD = WktReader.readToGeometryRDD(sedona.sparkContext, inputLocation, wktColumn, allowTopologyInvalidGeometries, skipSyntaxInvalidGeometries)\n</code></pre> <pre><code>from sedona.core.formatMapper import WktReader\nfrom sedona.core.formatMapper import WkbReader\n\nWktReader.readToGeometryRDD(sc, wkt_geometries_location, 0, True, False)\n\nWkbReader.readToGeometryRDD(sc, wkb_geometries_location, 0, True, False)\n</code></pre>"},{"location":"tutorial/rdd/#from-geojson","title":"From GeoJSON","text":"<p>Geometries in GeoJSON is similar to WKT/WKB. However, a GeoJSON file must be beaked into multiple lines.</p> <p>Suppose we have a <code>polygon.json</code> GeoJSON file at Path <code>/Download/polygon.json</code> as follows:</p> <pre><code>{ \"type\": \"Feature\", \"properties\": { \"STATEFP\": \"01\", \"COUNTYFP\": \"077\", \"TRACTCE\": \"011501\", \"BLKGRPCE\": \"5\", \"AFFGEOID\": \"1500000US010770115015\", \"GEOID\": \"010770115015\", \"NAME\": \"5\", \"LSAD\": \"BG\", \"ALAND\": 6844991, \"AWATER\": 32636 }, \"geometry\": { \"type\": \"Polygon\", \"coordinates\": [ [ [ -87.621765, 34.873444 ], [ -87.617535, 34.873369 ], [ -87.6123, 34.873337 ], [ -87.604049, 34.873303 ], [ -87.604033, 34.872316 ], [ -87.60415, 34.867502 ], [ -87.604218, 34.865687 ], [ -87.604409, 34.858537 ], [ -87.604018, 34.851336 ], [ -87.603716, 34.844829 ], [ -87.603696, 34.844307 ], [ -87.603673, 34.841884 ], [ -87.60372, 34.841003 ], [ -87.603879, 34.838423 ], [ -87.603888, 34.837682 ], [ -87.603889, 34.83763 ], [ -87.613127, 34.833938 ], [ -87.616451, 34.832699 ], [ -87.621041, 34.831431 ], [ -87.621056, 34.831526 ], [ -87.62112, 34.831925 ], [ -87.621603, 34.8352 ], [ -87.62158, 34.836087 ], [ -87.621383, 34.84329 ], [ -87.621359, 34.844438 ], [ -87.62129, 34.846387 ], [ -87.62119, 34.85053 ], [ -87.62144, 34.865379 ], [ -87.621765, 34.873444 ] ] ] } },\n{ \"type\": \"Feature\", \"properties\": { \"STATEFP\": \"01\", \"COUNTYFP\": \"045\", \"TRACTCE\": \"021102\", \"BLKGRPCE\": \"4\", \"AFFGEOID\": \"1500000US010450211024\", \"GEOID\": \"010450211024\", \"NAME\": \"4\", \"LSAD\": \"BG\", \"ALAND\": 11360854, \"AWATER\": 0 }, \"geometry\": { \"type\": \"Polygon\", \"coordinates\": [ [ [ -85.719017, 31.297901 ], [ -85.715626, 31.305203 ], [ -85.714271, 31.307096 ], [ -85.69999, 31.307552 ], [ -85.697419, 31.307951 ], [ -85.675603, 31.31218 ], [ -85.672733, 31.312876 ], [ -85.672275, 31.311977 ], [ -85.67145, 31.310988 ], [ -85.670622, 31.309524 ], [ -85.670729, 31.307622 ], [ -85.669876, 31.30666 ], [ -85.669796, 31.306224 ], [ -85.670356, 31.306178 ], [ -85.671664, 31.305583 ], [ -85.67177, 31.305299 ], [ -85.671878, 31.302764 ], [ -85.671344, 31.302123 ], [ -85.668276, 31.302076 ], [ -85.66566, 31.30093 ], [ -85.665687, 31.30022 ], [ -85.669183, 31.297677 ], [ -85.668703, 31.295638 ], [ -85.671985, 31.29314 ], [ -85.677177, 31.288211 ], [ -85.678452, 31.286376 ], [ -85.679236, 31.28285 ], [ -85.679195, 31.281426 ], [ -85.676865, 31.281049 ], [ -85.674661, 31.28008 ], [ -85.674377, 31.27935 ], [ -85.675714, 31.276882 ], [ -85.677938, 31.275168 ], [ -85.680348, 31.276814 ], [ -85.684032, 31.278848 ], [ -85.684387, 31.279082 ], [ -85.692398, 31.283499 ], [ -85.705032, 31.289718 ], [ -85.706755, 31.290476 ], [ -85.718102, 31.295204 ], [ -85.719132, 31.29689 ], [ -85.719017, 31.297901 ] ] ] } },\n{ \"type\": \"Feature\", \"properties\": { \"STATEFP\": \"01\", \"COUNTYFP\": \"055\", \"TRACTCE\": \"001300\", \"BLKGRPCE\": \"3\", \"AFFGEOID\": \"1500000US010550013003\", \"GEOID\": \"010550013003\", \"NAME\": \"3\", \"LSAD\": \"BG\", \"ALAND\": 1378742, \"AWATER\": 247387 }, \"geometry\": { \"type\": \"Polygon\", \"coordinates\": [ [ [ -86.000685, 34.00537 ], [ -85.998837, 34.009768 ], [ -85.998012, 34.010398 ], [ -85.987865, 34.005426 ], [ -85.986656, 34.004552 ], [ -85.985, 34.002659 ], [ -85.98851, 34.001502 ], [ -85.987567, 33.999488 ], [ -85.988666, 33.99913 ], [ -85.992568, 33.999131 ], [ -85.993144, 33.999714 ], [ -85.994876, 33.995153 ], [ -85.998823, 33.989548 ], [ -85.999925, 33.994237 ], [ -86.000616, 34.000028 ], [ -86.000685, 34.00537 ] ] ] } },\n{ \"type\": \"Feature\", \"properties\": { \"STATEFP\": \"01\", \"COUNTYFP\": \"089\", \"TRACTCE\": \"001700\", \"BLKGRPCE\": \"2\", \"AFFGEOID\": \"1500000US010890017002\", \"GEOID\": \"010890017002\", \"NAME\": \"2\", \"LSAD\": \"BG\", \"ALAND\": 1040641, \"AWATER\": 0 }, \"geometry\": { \"type\": \"Polygon\", \"coordinates\": [ [ [ -86.574172, 34.727375 ], [ -86.562684, 34.727131 ], [ -86.562797, 34.723865 ], [ -86.562957, 34.723168 ], [ -86.562336, 34.719766 ], [ -86.557381, 34.719143 ], [ -86.557352, 34.718322 ], [ -86.559921, 34.717363 ], [ -86.564827, 34.718513 ], [ -86.567582, 34.718565 ], [ -86.570572, 34.718577 ], [ -86.573618, 34.719377 ], [ -86.574172, 34.727375 ] ] ] } },\n</code></pre> <p>Use the following code to create a generic SpatialRDD:</p> ScalaJavaPython <pre><code>val inputLocation = \"/Download/polygon.json\"\nval allowTopologyInvalidGeometries = true // Optional\nval skipSyntaxInvalidGeometries = false // Optional\nval spatialRDD = GeoJsonReader.readToGeometryRDD(sedona.sparkContext, inputLocation, allowTopologyInvalidGeometries, skipSyntaxInvalidGeometries)\n</code></pre> <pre><code>String inputLocation = \"/Download/polygon.json\"\nboolean allowTopologyInvalidGeometries = true // Optional\nboolean skipSyntaxInvalidGeometries = false // Optional\nSpatialRDD spatialRDD = GeoJsonReader.readToGeometryRDD(sedona.sparkContext, inputLocation, allowTopologyInvalidGeometries, skipSyntaxInvalidGeometries)\n</code></pre> <pre><code>from sedona.core.formatMapper import GeoJsonReader\n\nGeoJsonReader.readToGeometryRDD(sc, geo_json_file_location)\n</code></pre> <p>Warning</p> <p>The way that Sedona reads JSON file is different from SparkSQL</p>"},{"location":"tutorial/rdd/#from-shapefile","title":"From Shapefile","text":"ScalaJavaPython <pre><code>val shapefileInputLocation=\"/Download/myshapefile\"\nval spatialRDD = ShapefileReader.readToGeometryRDD(sedona.sparkContext, shapefileInputLocation)\n</code></pre> <pre><code>String shapefileInputLocation=\"/Download/myshapefile\"\nSpatialRDD spatialRDD = ShapefileReader.readToGeometryRDD(sedona.sparkContext, shapefileInputLocation)\n</code></pre> <pre><code>from sedona.core.formatMapper.shapefileParser import ShapefileReader\n\nShapefileReader.readToGeometryRDD(sc, shape_file_location)\n</code></pre> <p>Note</p> <p>The path to the shapefile is the path to the folder that contains the .shp file, not the path to the .shp file itself. The file extensions of .shp, .shx, .dbf must be in lowercase. Assume you have a shape file called myShapefile, the path should be <code>XXX/myShapefile</code>. The file structure should be like this:</p> <pre><code>- shapefile1\n- shapefile2\n- myshapefile\n    - myshapefile.shp\n    - myshapefile.shx\n    - myshapefile.dbf\n    - myshapefile...\n    - ...\n</code></pre> <p>If the file you are reading contains non-ASCII characters you'll need to explicitly set the encoding via <code>sedona.global.charset</code> system property before creating your Spark context.</p> <p>Example:</p> <pre><code>System.setProperty(\"sedona.global.charset\", \"utf8\")\n\nval sc = new SparkContext(...)\n</code></pre>"},{"location":"tutorial/rdd/#from-sedonasql-dataframe","title":"From SedonaSQL DataFrame","text":"<p>Note</p> <p>More details about SedonaSQL, please read the SedonaSQL tutorial.</p> <p>To create a generic SpatialRDD from CSV, TSV, WKT, WKB and GeoJSON input formats, you can use SedonaSQL.</p> <p>We use checkin.csv CSV file as the example. You can create a generic SpatialRDD using the following steps:</p> <ol> <li>Load data in SedonaSQL. <pre><code>var df = sedona.read.format(\"csv\").option(\"header\", \"false\").load(csvPointInputLocation)\ndf.createOrReplaceTempView(\"inputtable\")\n</code></pre></li> <li>Create a Geometry type column in SedonaSQL <pre><code>var spatialDf = sedona.sql(\n\"\"\"\n        |SELECT ST_Point(CAST(inputtable._c0 AS Decimal(24,20)),CAST(inputtable._c1 AS Decimal(24,20))) AS checkin\n        |FROM inputtable\n    \"\"\".stripMargin)\n</code></pre></li> <li>Use SedonaSQL DataFrame-RDD Adapter to convert a DataFrame to an SpatialRDD <pre><code>var spatialRDD = Adapter.toSpatialRdd(spatialDf, \"checkin\")\n</code></pre></li> </ol> <p>\"checkin\" is the name of the geometry column</p> <p>For WKT/WKB/GeoJSON data, please use ST_GeomFromWKT / ST_GeomFromWKB / ST_GeomFromGeoJSON instead.</p>"},{"location":"tutorial/rdd/#transform-the-coordinate-reference-system","title":"Transform the Coordinate Reference System","text":"<p>Sedona doesn't control the coordinate unit (degree-based or meter-based) of all geometries in an SpatialRDD. The unit of all related distances in Sedona is same as the unit of all geometries in an SpatialRDD.</p> <p>By default, this function uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order. You can use spatialRDD.flipCoordinates to swap X and Y.</p> <p>To convert Coordinate Reference System of an SpatialRDD, use the following code:</p> ScalaJavaPython <pre><code>val sourceCrsCode = \"epsg:4326\" // WGS84, the most common degree-based CRS\nval targetCrsCode = \"epsg:3857\" // The most common meter-based CRS\nobjectRDD.CRSTransform(sourceCrsCode, targetCrsCode, false)\n</code></pre> <pre><code>String sourceCrsCode = \"epsg:4326\" // WGS84, the most common degree-based CRS\nString targetCrsCode = \"epsg:3857\" // The most common meter-based CRS\nobjectRDD.CRSTransform(sourceCrsCode, targetCrsCode, false)\n</code></pre> <pre><code>sourceCrsCode = \"epsg:4326\" // WGS84, the most common degree-based CRS\ntargetCrsCode = \"epsg:3857\" // The most common meter-based CRS\nobjectRDD.CRSTransform(sourceCrsCode, targetCrsCode, False)\n</code></pre> <p><code>false</code> in CRSTransform(sourceCrsCode, targetCrsCode, false) means that it will not tolerate Datum shift. If you want it to be lenient, use <code>true</code> instead.</p> <p>Warning</p> <p>CRS transformation should be done right after creating each SpatialRDD, otherwise it will lead to wrong query results. For instance, use something like this:</p> ScalaJavaPython <pre><code>val objectRDD = WktReader.readToGeometryRDD(sedona.sparkContext, inputLocation, wktColumn, allowTopologyInvalidGeometries, skipSyntaxInvalidGeometries)\nobjectRDD.CRSTransform(\"epsg:4326\", \"epsg:3857\", false)\n</code></pre> <pre><code>SpatialRDD objectRDD = WktReader.readToGeometryRDD(sedona.sparkContext, inputLocation, wktColumn, allowTopologyInvalidGeometries, skipSyntaxInvalidGeometries)\nobjectRDD.CRSTransform(\"epsg:4326\", \"epsg:3857\", false)\n</code></pre> <pre><code>objectRDD = WktReader.readToGeometryRDD(sedona.sparkContext, inputLocation, wktColumn, allowTopologyInvalidGeometries, skipSyntaxInvalidGeometries)\nobjectRDD.CRSTransform(\"epsg:4326\", \"epsg:3857\", False)\n</code></pre> <p>The details CRS information can be found on EPSG.io</p>"},{"location":"tutorial/rdd/#read-other-attributes-in-an-spatialrdd","title":"Read other attributes in an SpatialRDD","text":"<p>Each SpatialRDD can carry non-spatial attributes such as price, age and name.</p> <p>The other attributes are combined together to a string and stored in UserData field of each geometry.</p> <p>To retrieve the UserData field, use the following code:</p> ScalaJavaPython <pre><code>val rddWithOtherAttributes = objectRDD.rawSpatialRDD.rdd.map[String](f=&gt;f.getUserData.asInstanceOf[String])\n</code></pre> <pre><code>SpatialRDD&lt;Geometry&gt; spatialRDD = Adapter.toSpatialRdd(spatialDf, \"arealandmark\");\nspatialRDD.rawSpatialRDD.map(obj -&gt; {return obj.getUserData();});\n</code></pre> <pre><code>rdd_with_other_attributes = object_rdd.rawSpatialRDD.map(lambda x: x.getUserData())\n</code></pre>"},{"location":"tutorial/rdd/#write-a-spatial-range-query","title":"Write a Spatial Range Query","text":"<p>A spatial range query takes as input a range query window and an SpatialRDD and returns all geometries that have specified relationship with the query window.</p> <p>Assume you now have an SpatialRDD (typed or generic). You can use the following code to issue an Spatial Range Query on it.</p> <p>spatialPredicate can be set to <code>SpatialPredicate.INTERSECTS</code> to return all geometries intersect with query window. Supported spatial predicates are:</p> <ul> <li><code>CONTAINS</code>: geometry is completely inside the query window</li> <li><code>INTERSECTS</code>: geometry have at least one point in common with the query window</li> <li><code>WITHIN</code>: geometry is completely within the query window (no touching edges)</li> <li><code>COVERS</code>: query window has no point outside of the geometry</li> <li><code>COVERED_BY</code>: geometry has no point outside of the query window</li> <li><code>OVERLAPS</code>: geometry and the query window spatially overlap</li> <li><code>CROSSES</code>: geometry and the query window spatially cross</li> <li><code>TOUCHES</code>: the only points shared between geometry and the query window are on the boundary of geometry and the query window</li> <li><code>EQUALS</code>: geometry and the query window are spatially equal</li> </ul> <p>Note</p> <p>Spatial range query is equivalent with a SELECT query with spatial predicate as search condition in Spatial SQL. An example query is as follows: <pre><code>SELECT *\nFROM checkin\nWHERE ST_Intersects(checkin.location, queryWindow)\n</code></pre></p> ScalaJavaPython <pre><code>val rangeQueryWindow = new Envelope(-90.01, -80.01, 30.01, 40.01)\nval spatialPredicate = SpatialPredicate.COVERED_BY // Only return gemeotries fully covered by the window\nval usingIndex = false\nvar queryResult = RangeQuery.SpatialRangeQuery(spatialRDD, rangeQueryWindow, spatialPredicate, usingIndex)\n</code></pre> <pre><code>Envelope rangeQueryWindow = new Envelope(-90.01, -80.01, 30.01, 40.01)\nSpatialPredicate spatialPredicate = SpatialPredicate.COVERED_BY // Only return gemeotries fully covered by the window\nboolean usingIndex = false\nJavaRDD queryResult = RangeQuery.SpatialRangeQuery(spatialRDD, rangeQueryWindow, spatialPredicate, usingIndex)\n</code></pre> <pre><code>from sedona.core.geom.envelope import Envelope\nfrom sedona.core.spatialOperator import RangeQuery\n\nrange_query_window = Envelope(-90.01, -80.01, 30.01, 40.01)\nconsider_boundary_intersection = False  ## Only return gemeotries fully covered by the window\nusing_index = False\nquery_result = RangeQuery.SpatialRangeQuery(spatial_rdd, range_query_window, consider_boundary_intersection, using_index)\n</code></pre> <p>Note</p> <p>Sedona Python users: Please use RangeQueryRaw from the same module if you want to avoid jvm python serde while converting to Spatial DataFrame. It takes the same parameters as RangeQuery but returns reference to jvm rdd which can be converted to dataframe without python - jvm serde using Adapter.</p> <p>Example: <pre><code>from sedona.core.geom.envelope import Envelope\nfrom sedona.core.spatialOperator import RangeQueryRaw\nfrom sedona.utils.adapter import Adapter\n\nrange_query_window = Envelope(-90.01, -80.01, 30.01, 40.01)\nconsider_boundary_intersection = False  ## Only return gemeotries fully covered by the window\nusing_index = False\nquery_result = RangeQueryRaw.SpatialRangeQuery(spatial_rdd, range_query_window, consider_boundary_intersection, using_index)\ngdf = Adapter.toDf(query_result, spark, [\"col1\", ..., \"coln\"])\n</code></pre></p>"},{"location":"tutorial/rdd/#range-query-window","title":"Range query window","text":"<p>Besides the rectangle (Envelope) type range query window, Sedona range query window can be Point/Polygon/LineString.</p> <p>The code to create a point, linestring (4 vertices) and polygon (4 vertices) is as follows:</p> ScalaJavaPython <pre><code>val geometryFactory = new GeometryFactory()\nval pointObject = geometryFactory.createPoint(new Coordinate(-84.01, 34.01))\n\nval geometryFactory = new GeometryFactory()\nval coordinates = new Array[Coordinate](5)\ncoordinates(0) = new Coordinate(0,0)\ncoordinates(1) = new Coordinate(0,4)\ncoordinates(2) = new Coordinate(4,4)\ncoordinates(3) = new Coordinate(4,0)\ncoordinates(4) = coordinates(0) // The last coordinate is the same as the first coordinate in order to compose a closed ring\nval polygonObject = geometryFactory.createPolygon(coordinates)\n\nval geometryFactory = new GeometryFactory()\nval coordinates = new Array[Coordinate](4)\ncoordinates(0) = new Coordinate(0,0)\ncoordinates(1) = new Coordinate(0,4)\ncoordinates(2) = new Coordinate(4,4)\ncoordinates(3) = new Coordinate(4,0)\nval linestringObject = geometryFactory.createLineString(coordinates)\n</code></pre> <pre><code>GeometryFactory geometryFactory = new GeometryFactory()\nPoint pointObject = geometryFactory.createPoint(new Coordinate(-84.01, 34.01))\n\nGeometryFactory geometryFactory = new GeometryFactory()\nCoordinate[] coordinates = new Array[Coordinate](5)\ncoordinates(0) = new Coordinate(0,0)\ncoordinates(1) = new Coordinate(0,4)\ncoordinates(2) = new Coordinate(4,4)\ncoordinates(3) = new Coordinate(4,0)\ncoordinates(4) = coordinates(0) // The last coordinate is the same as the first coordinate in order to compose a closed ring\nPolygon polygonObject = geometryFactory.createPolygon(coordinates)\n\nGeometryFactory geometryFactory = new GeometryFactory()\nval coordinates = new Array[Coordinate](4)\ncoordinates(0) = new Coordinate(0,0)\ncoordinates(1) = new Coordinate(0,4)\ncoordinates(2) = new Coordinate(4,4)\ncoordinates(3) = new Coordinate(4,0)\nLineString linestringObject = geometryFactory.createLineString(coordinates)\n</code></pre> <p>A Shapely geometry can be used as a query window. To create shapely geometries, please follow Shapely official docs</p>"},{"location":"tutorial/rdd/#use-spatial-indexes","title":"Use spatial indexes","text":"<p>Sedona provides two types of spatial indexes, Quad-Tree and R-Tree. Once you specify an index type, Sedona will build a local tree index on each of the SpatialRDD partition.</p> <p>To utilize a spatial index in a spatial range query, use the following code:</p> ScalaJavaPython <pre><code>val rangeQueryWindow = new Envelope(-90.01, -80.01, 30.01, 40.01)\nval spatialPredicate = SpatialPredicate.COVERED_BY // Only return gemeotries fully covered by the window\n\nval buildOnSpatialPartitionedRDD = false // Set to TRUE only if run join query\nspatialRDD.buildIndex(IndexType.QUADTREE, buildOnSpatialPartitionedRDD)\n\nval usingIndex = true\nvar queryResult = RangeQuery.SpatialRangeQuery(spatialRDD, rangeQueryWindow, spatialPredicate, usingIndex)\n</code></pre> <pre><code>Envelope rangeQueryWindow = new Envelope(-90.01, -80.01, 30.01, 40.01)\nSpatialPredicate spatialPredicate = SpatialPredicate.COVERED_BY // Only return gemeotries fully covered by the window\n\nboolean buildOnSpatialPartitionedRDD = false // Set to TRUE only if run join query\nspatialRDD.buildIndex(IndexType.QUADTREE, buildOnSpatialPartitionedRDD)\n\nboolean usingIndex = true\nJavaRDD queryResult = RangeQuery.SpatialRangeQuery(spatialRDD, rangeQueryWindow, spatialPredicate, usingIndex)\n</code></pre> <pre><code>from sedona.core.geom.envelope import Envelope\nfrom sedona.core.enums import IndexType\nfrom sedona.core.spatialOperator import RangeQuery\n\nrange_query_window = Envelope(-90.01, -80.01, 30.01, 40.01)\nconsider_boundary_intersection = False ## Only return gemeotries fully covered by the window\n\nbuild_on_spatial_partitioned_rdd = False ## Set to TRUE only if run join query\nspatial_rdd.buildIndex(IndexType.QUADTREE, build_on_spatial_partitioned_rdd)\n\nusing_index = True\n\nquery_result = RangeQuery.SpatialRangeQuery(\n    spatial_rdd,\n    range_query_window,\n    consider_boundary_intersection,\n    using_index\n)\n</code></pre> <p>Tip</p> <p>Using an index might not be the best choice all the time because building index also takes time. A spatial index is very useful when your data is complex polygons and line strings.</p>"},{"location":"tutorial/rdd/#output-format","title":"Output format","text":"Scala/JavaPython <p>The output format of the spatial range query is another SpatialRDD.</p> <p>The output format of the spatial range query is another RDD which consists of GeoData objects.</p> <p>SpatialRangeQuery result can be used as RDD with map or other spark RDD functions. Also it can be used as Python objects when using collect method. Example:</p> <pre><code>query_result.map(lambda x: x.geom.length).collect()\n</code></pre> <pre><code>[\n 1.5900840000000045,\n 1.5906639999999896,\n 1.1110299999999995,\n 1.1096700000000084,\n 1.1415619999999933,\n 1.1386399999999952,\n 1.1415619999999933,\n 1.1418860000000137,\n 1.1392780000000045,\n ...\n]\n</code></pre> <p>Or transformed to GeoPandas GeoDataFrame</p> <pre><code>import geopandas as gpd\ngpd.GeoDataFrame(\n    query_result.map(lambda x: [x.geom, x.userData]).collect(),\n    columns=[\"geom\", \"user_data\"],\n    geometry=\"geom\"\n)\n</code></pre>"},{"location":"tutorial/rdd/#write-a-spatial-knn-query","title":"Write a Spatial KNN Query","text":"<p>A spatial K Nearnest Neighbor query takes as input a K, a query point and an SpatialRDD and finds the K geometries in the RDD which are the closest to he query point.</p> <p>Assume you now have an SpatialRDD (typed or generic). You can use the following code to issue an Spatial KNN Query on it.</p> ScalaJavaPython <pre><code>val geometryFactory = new GeometryFactory()\nval pointObject = geometryFactory.createPoint(new Coordinate(-84.01, 34.01))\nval K = 1000 // K Nearest Neighbors\nval usingIndex = false\nval result = KNNQuery.SpatialKnnQuery(objectRDD, pointObject, K, usingIndex)\n</code></pre> <pre><code>GeometryFactory geometryFactory = new GeometryFactory()\nPoint pointObject = geometryFactory.createPoint(new Coordinate(-84.01, 34.01))\nint K = 1000 // K Nearest Neighbors\nboolean usingIndex = false\nJavaRDD result = KNNQuery.SpatialKnnQuery(objectRDD, pointObject, K, usingIndex)\n</code></pre> <pre><code>from sedona.core.spatialOperator import KNNQuery\nfrom shapely.geometry import Point\n\npoint = Point(-84.01, 34.01)\nk = 1000 ## K Nearest Neighbors\nusing_index = False\nresult = KNNQuery.SpatialKnnQuery(object_rdd, point, k, using_index)\n</code></pre> <p>Note</p> <p>Spatial KNN query that returns 5 Nearest Neighbors is equal to the following statement in Spatial SQL <pre><code>SELECT ck.name, ck.rating, ST_Distance(ck.location, myLocation) AS distance\nFROM checkins ck\nORDER BY distance DESC\nLIMIT 5\n</code></pre></p>"},{"location":"tutorial/rdd/#query-center-geometry","title":"Query center geometry","text":"<p>Besides the Point type, Sedona KNN query center can be Polygon and LineString.</p> Scala/JavaPython <p>To learn how to create Polygon and LineString object, see Range query window.</p> <p>To create Polygon or Linestring object please follow Shapely official docs</p>"},{"location":"tutorial/rdd/#use-spatial-indexes_1","title":"Use spatial indexes","text":"<p>To utilize a spatial index in a spatial KNN query, use the following code:</p> ScalaJavaPython <pre><code>val geometryFactory = new GeometryFactory()\nval pointObject = geometryFactory.createPoint(new Coordinate(-84.01, 34.01))\nval K = 1000 // K Nearest Neighbors\n\n\nval buildOnSpatialPartitionedRDD = false // Set to TRUE only if run join query\nobjectRDD.buildIndex(IndexType.RTREE, buildOnSpatialPartitionedRDD)\n\nval usingIndex = true\nval result = KNNQuery.SpatialKnnQuery(objectRDD, pointObject, K, usingIndex)\n</code></pre> <pre><code>GeometryFactory geometryFactory = new GeometryFactory()\nPoint pointObject = geometryFactory.createPoint(new Coordinate(-84.01, 34.01))\nval K = 1000 // K Nearest Neighbors\n\n\nboolean buildOnSpatialPartitionedRDD = false // Set to TRUE only if run join query\nobjectRDD.buildIndex(IndexType.RTREE, buildOnSpatialPartitionedRDD)\n\nboolean usingIndex = true\nJavaRDD result = KNNQuery.SpatialKnnQuery(objectRDD, pointObject, K, usingIndex)\n</code></pre> <pre><code>from sedona.core.spatialOperator import KNNQuery\nfrom sedona.core.enums import IndexType\nfrom shapely.geometry import Point\n\npoint = Point(-84.01, 34.01)\nk = 5 ## K Nearest Neighbors\n\nbuild_on_spatial_partitioned_rdd = False ## Set to TRUE only if run join query\nspatial_rdd.buildIndex(IndexType.RTREE, build_on_spatial_partitioned_rdd)\n\nusing_index = True\nresult = KNNQuery.SpatialKnnQuery(spatial_rdd, point, k, using_index)\n</code></pre> <p>Warning</p> <p>Only R-Tree index supports Spatial KNN query</p>"},{"location":"tutorial/rdd/#output-format_1","title":"Output format","text":"Scala/JavaPython <p>The output format of the spatial KNN query is a list of geometries. The list has K geometry objects.</p> <p>The output format of the spatial KNN query is a list of GeoData objects. The list has K GeoData objects.</p> <p>Example: <pre><code>&gt;&gt; result\n\n[GeoData, GeoData, GeoData, GeoData, GeoData]\n</code></pre></p>"},{"location":"tutorial/rdd/#write-a-spatial-join-query","title":"Write a Spatial Join Query","text":"<p>A spatial join query takes as input two Spatial RDD A and B. For each geometry in A, finds the geometries (from B) covered/intersected by it. A and B can be any geometry type and are not necessary to have the same geometry type.</p> <p>Assume you now have two SpatialRDDs (typed or generic). You can use the following code to issue an Spatial Join Query on them.</p> ScalaJavaPython <pre><code>val spatialPredicate = SpatialPredicate.COVERED_BY // Only return gemeotries fully covered by each query window in queryWindowRDD\nval usingIndex = false\n\nobjectRDD.analyze()\n\nobjectRDD.spatialPartitioning(GridType.KDBTREE)\nqueryWindowRDD.spatialPartitioning(objectRDD.getPartitioner)\n\nval result = JoinQuery.SpatialJoinQuery(objectRDD, queryWindowRDD, usingIndex, spatialPredicate)\n</code></pre> <pre><code>SpatialPredicate spatialPredicate = SpatialPredicate.COVERED_BY // Only return gemeotries fully covered by each query window in queryWindowRDD\nval usingIndex = false\n\nobjectRDD.analyze()\n\nobjectRDD.spatialPartitioning(GridType.KDBTREE)\nqueryWindowRDD.spatialPartitioning(objectRDD.getPartitioner)\n\nJavaPairRDD result = JoinQuery.SpatialJoinQuery(objectRDD, queryWindowRDD, usingIndex, spatialPredicate)\n</code></pre> <pre><code>from sedona.core.enums import GridType\nfrom sedona.core.spatialOperator import JoinQuery\n\nconsider_boundary_intersection = False ## Only return geometries fully covered by each query window in queryWindowRDD\nusing_index = False\n\nobject_rdd.analyze()\n\nobject_rdd.spatialPartitioning(GridType.KDBTREE)\nquery_window_rdd.spatialPartitioning(object_rdd.getPartitioner())\n\nresult = JoinQuery.SpatialJoinQuery(object_rdd, query_window_rdd, using_index, consider_boundary_intersection)\n</code></pre> <p>Note</p> <p>Spatial join query is equal to the following query in Spatial SQL: <pre><code>SELECT superhero.name\nFROM city, superhero\nWHERE ST_Contains(city.geom, superhero.geom);\n</code></pre> Find the superheroes in each city</p>"},{"location":"tutorial/rdd/#use-spatial-partitioning","title":"Use spatial partitioning","text":"<p>Sedona spatial partitioning method can significantly speed up the join query. Three spatial partitioning methods are available: KDB-Tree, Quad-Tree and R-Tree. Two SpatialRDD must be partitioned by the same way.</p> <p>If you first partition SpatialRDD A, then you must use the partitioner of A to partition B.</p> Scala/JavaPython <pre><code>objectRDD.spatialPartitioning(GridType.KDBTREE)\nqueryWindowRDD.spatialPartitioning(objectRDD.getPartitioner)\n</code></pre> <pre><code>object_rdd.spatialPartitioning(GridType.KDBTREE)\nquery_window_rdd.spatialPartitioning(object_rdd.getPartitioner())\n</code></pre> <p>Or</p> Scala/JavaPython <pre><code>queryWindowRDD.spatialPartitioning(GridType.KDBTREE)\nobjectRDD.spatialPartitioning(queryWindowRDD.getPartitioner)\n</code></pre> <pre><code>query_window_rdd.spatialPartitioning(GridType.KDBTREE)\nobject_rdd.spatialPartitioning(query_window_rdd.getPartitioner())\n</code></pre>"},{"location":"tutorial/rdd/#use-spatial-indexes_2","title":"Use spatial indexes","text":"<p>To utilize a spatial index in a spatial join query, use the following code:</p> ScalaJavaPython <pre><code>objectRDD.spatialPartitioning(joinQueryPartitioningType)\nqueryWindowRDD.spatialPartitioning(objectRDD.getPartitioner)\n\nval buildOnSpatialPartitionedRDD = true // Set to TRUE only if run join query\nval usingIndex = true\nqueryWindowRDD.buildIndex(IndexType.QUADTREE, buildOnSpatialPartitionedRDD)\n\nval result = JoinQuery.SpatialJoinQueryFlat(objectRDD, queryWindowRDD, usingIndex, spatialPredicate)\n</code></pre> <pre><code>objectRDD.spatialPartitioning(joinQueryPartitioningType)\nqueryWindowRDD.spatialPartitioning(objectRDD.getPartitioner)\n\nboolean buildOnSpatialPartitionedRDD = true // Set to TRUE only if run join query\nboolean usingIndex = true\nqueryWindowRDD.buildIndex(IndexType.QUADTREE, buildOnSpatialPartitionedRDD)\n\nJavaPairRDD result = JoinQuery.SpatialJoinQueryFlat(objectRDD, queryWindowRDD, usingIndex, spatialPredicate)\n</code></pre> <pre><code>from sedona.core.enums import GridType\nfrom sedona.core.enums import IndexType\nfrom sedona.core.spatialOperator import JoinQuery\n\nobject_rdd.spatialPartitioning(GridType.KDBTREE)\nquery_window_rdd.spatialPartitioning(object_rdd.getPartitioner())\n\nbuild_on_spatial_partitioned_rdd = True ## Set to TRUE only if run join query\nusing_index = True\nquery_window_rdd.buildIndex(IndexType.QUADTREE, build_on_spatial_partitioned_rdd)\n\nresult = JoinQuery.SpatialJoinQueryFlat(object_rdd, query_window_rdd, using_index, True)\n</code></pre> <p>The index should be built on either one of two SpatialRDDs. In general, you should build it on the larger SpatialRDD.</p>"},{"location":"tutorial/rdd/#output-format_2","title":"Output format","text":"Scala/JavaPython <p>The output format of the spatial join query is a PairRDD. In this PairRDD, each object is a pair of two geometries. The left one is the geometry from objectRDD and the right one is the geometry from the queryWindowRDD.</p> <pre><code>Point,Polygon\nPoint,Polygon\nPoint,Polygon\nPolygon,Polygon\nLineString,LineString\nPolygon,LineString\n...\n</code></pre> <p>Each object on the left is covered/intersected by the object on the right.</p> <p>Result for this query is RDD which holds two GeoData objects within list of lists. Example: <pre><code>result.collect()\n</code></pre></p> <pre><code>[[GeoData, GeoData], [GeoData, GeoData] ...]\n</code></pre> <p>It is possible to do some RDD operation on result data ex. Getting polygon centroid. <pre><code>result.map(lambda x: x[0].geom.centroid).collect()\n</code></pre></p> <p>Note</p> <p>Sedona Python users: Please use JoinQueryRaw from the same module for methods</p> <ul> <li> <p>spatialJoin</p> </li> <li> <p>DistanceJoinQueryFlat</p> </li> <li> <p>SpatialJoinQueryFlat</p> </li> </ul> <p>For better performance while converting to dataframe with adapter. That approach allows to avoid costly serialization between Python and jvm and in result operating on python object instead of native geometries.</p> <p>Example: <pre><code>from sedona.core.SpatialRDD import CircleRDD\nfrom sedona.core.enums import GridType\nfrom sedona.core.spatialOperator import JoinQueryRaw\n\nobject_rdd.analyze()\n\ncircle_rdd = CircleRDD(object_rdd, 0.1) ## Create a CircleRDD using the given distance\ncircle_rdd.analyze()\n\ncircle_rdd.spatialPartitioning(GridType.KDBTREE)\nspatial_rdd.spatialPartitioning(circle_rdd.getPartitioner())\n\nconsider_boundary_intersection = False ## Only return gemeotries fully covered by each query window in queryWindowRDD\nusing_index = False\n\nresult = JoinQueryRaw.DistanceJoinQueryFlat(spatial_rdd, circle_rdd, using_index, consider_boundary_intersection)\n\ngdf = Adapter.toDf(result, [\"left_col1\", ..., \"lefcoln\"], [\"rightcol1\", ..., \"rightcol2\"], spark)\n</code></pre></p>"},{"location":"tutorial/rdd/#write-a-distance-join-query","title":"Write a Distance Join Query","text":"<p>Warning</p> <p>RDD distance joins are only reliable for points. For other geometry types, please use Spatial SQL.</p> <p>A distance join query takes as input two Spatial RDD A and B and a distance. For each geometry in A, finds the geometries (from B) are within the given distance to it. A and B can be any geometry type and are not necessary to have the same geometry type. The unit of the distance is explained here.</p> <p>If you don't want to transform your data and are ok with sacrificing the query accuracy, you can use an approximate degree value for distance. Please use this calculator.</p> <p>Assume you now have two SpatialRDDs (typed or generic). You can use the following code to issue an Distance Join Query on them.</p> ScalaJavaPython <pre><code>objectRddA.analyze()\n\nval circleRDD = new CircleRDD(objectRddA, 0.1) // Create a CircleRDD using the given distance\n\ncircleRDD.spatialPartitioning(GridType.KDBTREE)\nobjectRddB.spatialPartitioning(circleRDD.getPartitioner)\n\nval spatialPredicate = SpatialPredicate.COVERED_BY // Only return gemeotries fully covered by each query window in queryWindowRDD\nval usingIndex = false\n\nval result = JoinQuery.DistanceJoinQueryFlat(objectRddB, circleRDD, usingIndex, spatialPredicate)\n</code></pre> <pre><code>objectRddA.analyze()\n\nCircleRDD circleRDD = new CircleRDD(objectRddA, 0.1) // Create a CircleRDD using the given distance\n\ncircleRDD.spatialPartitioning(GridType.KDBTREE)\nobjectRddB.spatialPartitioning(circleRDD.getPartitioner)\n\nSpatialPredicate spatialPredicate = SpatialPredicate.COVERED_BY // Only return gemeotries fully covered by each query window in queryWindowRDD\nboolean usingIndex = false\n\nJavaPairRDD result = JoinQuery.DistanceJoinQueryFlat(objectRddB, circleRDD, usingIndex, spatialPredicate)\n</code></pre> <pre><code>from sedona.core.SpatialRDD import CircleRDD\nfrom sedona.core.enums import GridType\nfrom sedona.core.spatialOperator import JoinQuery\n\nobject_rdd.analyze()\n\ncircle_rdd = CircleRDD(object_rdd, 0.1) ## Create a CircleRDD using the given distance\ncircle_rdd.analyze()\n\ncircle_rdd.spatialPartitioning(GridType.KDBTREE)\nspatial_rdd.spatialPartitioning(circle_rdd.getPartitioner())\n\nconsider_boundary_intersection = False ## Only return gemeotries fully covered by each query window in queryWindowRDD\nusing_index = False\n\nresult = JoinQuery.DistanceJoinQueryFlat(spatial_rdd, circle_rdd, using_index, consider_boundary_intersection)\n</code></pre> <p>Distance join can only accept <code>COVERED_BY</code> and <code>INTERSECTS</code> as spatial predicates. The rest part of the join query is same as the spatial join query.</p> <p>The details of spatial partitioning in join query is here.</p> <p>The details of using spatial indexes in join query is here.</p> <p>The output format of the distance join query is here.</p> <p>Note</p> <p>Distance join query is equal to the following query in Spatial SQL: <pre><code>SELECT superhero.name\nFROM city, superhero\nWHERE ST_Distance(city.geom, superhero.geom) &lt;= 10;\n</code></pre> Find the superheroes within 10 miles of each city</p>"},{"location":"tutorial/rdd/#save-to-permanent-storage","title":"Save to permanent storage","text":"<p>You can always save an SpatialRDD back to some permanent storage such as HDFS and Amazon S3. You can save distributed SpatialRDD to WKT, GeoJSON and object files.</p> <p>Note</p> <p>Non-spatial attributes such as price, age and name will also be stored to permanent storage.</p>"},{"location":"tutorial/rdd/#save-an-spatialrdd-not-indexed","title":"Save an SpatialRDD (not indexed)","text":"<p>Typed SpatialRDD and generic SpatialRDD can be saved to permanent storage.</p>"},{"location":"tutorial/rdd/#save-to-distributed-wkt-text-file","title":"Save to distributed WKT text file","text":"<p>Use the following code to save an SpatialRDD as a distributed WKT text file:</p> <pre><code>objectRDD.rawSpatialRDD.saveAsTextFile(\"hdfs://PATH\")\nobjectRDD.saveAsWKT(\"hdfs://PATH\")\n</code></pre>"},{"location":"tutorial/rdd/#save-to-distributed-wkb-text-file","title":"Save to distributed WKB text file","text":"<p>Use the following code to save an SpatialRDD as a distributed WKB text file:</p> <pre><code>objectRDD.saveAsWKB(\"hdfs://PATH\")\n</code></pre>"},{"location":"tutorial/rdd/#save-to-distributed-geojson-text-file","title":"Save to distributed GeoJSON text file","text":"<p>Use the following code to save an SpatialRDD as a distributed GeoJSON text file:</p> <pre><code>objectRDD.saveAsGeoJSON(\"hdfs://PATH\")\n</code></pre>"},{"location":"tutorial/rdd/#save-to-distributed-object-file","title":"Save to distributed object file","text":"<p>Use the following code to save an SpatialRDD as a distributed object file:</p> Scala/JavaPython <pre><code>objectRDD.rawSpatialRDD.saveAsObjectFile(\"hdfs://PATH\")\n</code></pre> <pre><code>object_rdd.rawJvmSpatialRDD.saveAsObjectFile(\"hdfs://PATH\")\n</code></pre> <p>Note</p> <p>Each object in a distributed object file is a byte array (not human-readable). This byte array is the serialized format of a Geometry or a SpatialIndex.</p>"},{"location":"tutorial/rdd/#save-an-spatialrdd-indexed","title":"Save an SpatialRDD (indexed)","text":"<p>Indexed typed SpatialRDD and generic SpatialRDD can be saved to permanent storage. However, the indexed SpatialRDD has to be stored as a distributed object file.</p>"},{"location":"tutorial/rdd/#save-to-distributed-object-file_1","title":"Save to distributed object file","text":"<p>Use the following code to save an SpatialRDD as a distributed object file:</p> <pre><code>objectRDD.indexedRawRDD.saveAsObjectFile(\"hdfs://PATH\")\n</code></pre>"},{"location":"tutorial/rdd/#save-an-spatialrdd-spatialpartitioned-wo-indexed","title":"Save an SpatialRDD (spatialPartitioned W/O indexed)","text":"<p>A spatial partitioned RDD can be saved to permanent storage but Spark is not able to maintain the same RDD partition Id of the original RDD. This will lead to wrong join query results. We are working on some solutions. Stay tuned!</p>"},{"location":"tutorial/rdd/#reload-a-saved-spatialrdd","title":"Reload a saved SpatialRDD","text":"<p>You can easily reload an SpatialRDD that has been saved to a distributed object file.</p>"},{"location":"tutorial/rdd/#load-to-a-typed-spatialrdd","title":"Load to a typed SpatialRDD","text":"<p>Warning</p> <p>Typed SpatialRDD has been deprecated for a long time. We do NOT recommend it anymore.</p>"},{"location":"tutorial/rdd/#load-to-a-generic-spatialrdd","title":"Load to a generic SpatialRDD","text":"<p>Use the following code to reload the SpatialRDD:</p> ScalaJavaPython <pre><code>var savedRDD = new SpatialRDD[Geometry]\nsavedRDD.rawSpatialRDD = sc.objectFile[Geometry](\"hdfs://PATH\")\n</code></pre> <pre><code>SpatialRDD savedRDD = new SpatialRDD&lt;Geometry&gt;\nsavedRDD.rawSpatialRDD = sc.objectFile&lt;Geometry&gt;(\"hdfs://PATH\")\n</code></pre> <pre><code>saved_rdd = load_spatial_rdd_from_disc(sc, \"hdfs://PATH\", GeoType.GEOMETRY)\n</code></pre> <p>Use the following code to reload the indexed SpatialRDD:</p> ScalaJavaPython <pre><code>var savedRDD = new SpatialRDD[Geometry]\nsavedRDD.indexedRawRDD = sc.objectFile[SpatialIndex](\"hdfs://PATH\")\n</code></pre> <pre><code>SpatialRDD savedRDD = new SpatialRDD&lt;Geometry&gt;\nsavedRDD.indexedRawRDD = sc.objectFile&lt;SpatialIndex&gt;(\"hdfs://PATH\")\n</code></pre> <pre><code>saved_rdd = SpatialRDD()\nsaved_rdd.indexedRawRDD = load_spatial_index_rdd_from_disc(sc, \"hdfs://PATH\")\n</code></pre>"},{"location":"tutorial/sql-pure-sql/","title":"Pure SQL environment","text":"<p>Starting from Sedona v1.0.1, you can use Sedona in a pure Spark SQL environment. The example code is written in SQL.</p> <p>SedonaSQL supports SQL/MM Part3 Spatial SQL Standard. Detailed SedonaSQL APIs are available here: SedonaSQL API</p>"},{"location":"tutorial/sql-pure-sql/#initiate-session","title":"Initiate Session","text":"<p>Start <code>spark-sql</code> as following (replace <code>&lt;VERSION&gt;</code> with actual version like <code>1.5.1</code>):</p> <p>Run spark-sql with Apache Sedona</p> Spark 3.0 to 3.3 and Scala 2.12Spark 3.4+ and Scala 2.12 <pre><code>spark-sql --packages org.apache.sedona:sedona-spark-shaded-3.0_2.12:&lt;VERSION&gt;,org.datasyslab:geotools-wrapper:&lt;VERSION&gt;-28.2 \\\n--conf spark.serializer=org.apache.spark.serializer.KryoSerializer \\\n--conf spark.kryo.registrator=org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator \\\n--conf spark.sql.extensions=org.apache.sedona.viz.sql.SedonaVizExtensions,org.apache.sedona.sql.SedonaSqlExtensions\n</code></pre> <p><pre><code>spark-sql --packages org.apache.sedona:sedona-spark-shaded-3.4_2.12:&lt;VERSION&gt;,org.datasyslab:geotools-wrapper:&lt;VERSION&gt;-28.2 \\\n--conf spark.serializer=org.apache.spark.serializer.KryoSerializer \\\n--conf spark.kryo.registrator=org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator \\\n--conf spark.sql.extensions=org.apache.sedona.viz.sql.SedonaVizExtensions,org.apache.sedona.sql.SedonaSqlExtensions\n</code></pre> If you are using Spark versions higher than 3.4, please replace the <code>3.4</code> in artifact names with the corresponding major.minor version of Spark.</p> <p>This will register all Sedona types, functions and optimizations in SedonaSQL and SedonaViz.</p>"},{"location":"tutorial/sql-pure-sql/#load-data","title":"Load data","text":"<p>Let use data from <code>examples/sql</code>.  To load data from CSV file we need to execute two commands:</p> <p>Use the following code to load the data and create a raw DataFrame:</p> <pre><code>CREATE TABLE IF NOT EXISTS pointraw (_c0 string, _c1 string)\nUSING csv\nOPTIONS(header='false')\nLOCATION '&lt;some path&gt;/sedona/examples/sql/src/test/resources/testpoint.csv';\n\nCREATE TABLE IF NOT EXISTS polygonraw (_c0 string, _c1 string, _c2 string, _c3 string)\nUSING csv\nOPTIONS(header='false')\nLOCATION '&lt;some path&gt;/sedona/examples/sql/src/test/resources/testenvelope.csv';\n</code></pre>"},{"location":"tutorial/sql-pure-sql/#transform-the-data","title":"Transform the data","text":"<p>We need to transform our point and polygon data into respective types:</p> <pre><code>CREATE OR REPLACE TEMP VIEW pointdata AS\nSELECT ST_Point(cast(pointraw._c0 as Decimal(24,20)), cast(pointraw._c1 as Decimal(24,20))) AS pointshape\nFROM pointraw;\n\nCREATE OR REPLACE TEMP VIEW polygondata AS\nselect ST_PolygonFromEnvelope(cast(polygonraw._c0 as Decimal(24,20)),\ncast(polygonraw._c1 as Decimal(24,20)), cast(polygonraw._c2 as Decimal(24,20)),\ncast(polygonraw._c3 as Decimal(24,20))) AS polygonshape\nFROM polygonraw;\n</code></pre>"},{"location":"tutorial/sql-pure-sql/#work-with-data","title":"Work with data","text":"<p>For example, let join polygon and test data:</p> <pre><code>SELECT * from polygondata, pointdata\nWHERE ST_Contains(polygondata.polygonshape, pointdata.pointshape)\nAND ST_Contains(ST_PolygonFromEnvelope(1.0,101.0,501.0,601.0), polygondata.polygonshape)\nLIMIT 5;\n</code></pre>"},{"location":"tutorial/sql/","title":"Spatial SQL app","text":"<p>The page outlines the steps to manage spatial data using SedonaSQL.</p> <p>Note</p> <p>Since v<code>1.5.0</code>, Sedona assumes geographic coordinates to be in longitude/latitude order. If your data is lat/lon order, please use <code>ST_FlipCoordinates</code> to swap X and Y.</p> <p>SedonaSQL supports SQL/MM Part3 Spatial SQL Standard. It includes four kinds of SQL operators as follows. All these operators can be directly called through:</p> ScalaJavaPython <pre><code>var myDataFrame = sedona.sql(\"YOUR_SQL\")\nmyDataFrame.createOrReplaceTempView(\"spatialDf\")\n</code></pre> <pre><code>Dataset&lt;Row&gt; myDataFrame = sedona.sql(\"YOUR_SQL\")\nmyDataFrame.createOrReplaceTempView(\"spatialDf\")\n</code></pre> <pre><code>myDataFrame = sedona.sql(\"YOUR_SQL\")\nmyDataFrame.createOrReplaceTempView(\"spatialDf\")\n</code></pre> <p>Detailed SedonaSQL APIs are available here: SedonaSQL API. You can find example county data (i.e., <code>county_small.tsv</code>) in Sedona GitHub repo.</p>"},{"location":"tutorial/sql/#set-up-dependencies","title":"Set up dependencies","text":"Scala/JavaPython <ol> <li>Read Sedona Maven Central coordinates and add Sedona dependencies in build.sbt or pom.xml.</li> <li>Add Apache Spark core, Apache SparkSQL in build.sbt or pom.xml.</li> <li>Please see SQL example project</li> </ol> <ol> <li>Please read Quick start to install Sedona Python.</li> <li>This tutorial is based on Sedona SQL Jupyter Notebook example. You can interact with Sedona Python Jupyter notebook immediately on Binder. Click  to interact with Sedona Python Jupyter notebook immediately on Binder.</li> </ol>"},{"location":"tutorial/sql/#create-sedona-config","title":"Create Sedona config","text":"<p>Use the following code to create your Sedona config at the beginning. If you already have a SparkSession (usually named <code>spark</code>) created by Wherobots/AWS EMR/Databricks, please skip this step and can use <code>spark</code> directly.</p> <p>Sedona &gt;= 1.4.1</p> <p>You can add additional Spark runtime config to the config builder. For example, <code>SedonaContext.builder().config(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\")</code></p> ScalaJavaPython <p><pre><code>import org.apache.sedona.spark.SedonaContext\n\nval config = SedonaContext.builder()\n.master(\"local[*]\") // Delete this if run in cluster mode\n.appName(\"readTestScala\") // Change this to a proper name\n.getOrCreate()\n</code></pre> If you use SedonaViz together with SedonaSQL, please add the following line after <code>SedonaContext.builder()</code> to enable Sedona Kryo serializer: <pre><code>.config(\"spark.kryo.registrator\", classOf[SedonaVizKryoRegistrator].getName) // org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator\n</code></pre></p> <p><pre><code>import org.apache.sedona.spark.SedonaContext;\n\nSparkSession config = SedonaContext.builder()\n.master(\"local[*]\") // Delete this if run in cluster mode\n.appName(\"readTestScala\") // Change this to a proper name\n.getOrCreate()\n</code></pre> If you use SedonaViz together with SedonaSQL, please add the following line after <code>SedonaContext.builder()</code> to enable Sedona Kryo serializer: <pre><code>.config(\"spark.kryo.registrator\", SedonaVizKryoRegistrator.class.getName) // org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator\n</code></pre></p> <p><pre><code>from sedona.spark import *\n\nconfig = SedonaContext.builder() .\\\n    config('spark.jars.packages',\n           'org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.5.1,'\n           'org.datasyslab:geotools-wrapper:1.5.1-28.2'). \\\n    getOrCreate()\n</code></pre> If you are using Spark versions &gt;= 3.4, please replace the <code>3.0</code> in package name of sedona-spark-shaded with the corresponding major.minor version of Spark, such as <code>sedona-spark-shaded-3.4_2.12:1.5.1</code>.</p> <p>Sedona &lt; 1.4.1</p> <p>The following method has been deprecated since Sedona 1.4.1. Please use the method above to create your Sedona config.</p> ScalaJavaPython <p><pre><code>var sparkSession = SparkSession.builder()\n.master(\"local[*]\") // Delete this if run in cluster mode\n.appName(\"readTestScala\") // Change this to a proper name\n// Enable Sedona custom Kryo serializer\n.config(\"spark.serializer\", classOf[KryoSerializer].getName) // org.apache.spark.serializer.KryoSerializer\n.config(\"spark.kryo.registrator\", classOf[SedonaKryoRegistrator].getName)\n.getOrCreate() // org.apache.sedona.core.serde.SedonaKryoRegistrator\n</code></pre> If you use SedonaViz together with SedonaSQL, please use the following two lines to enable Sedona Kryo serializer instead: <pre><code>.config(\"spark.serializer\", classOf[KryoSerializer].getName) // org.apache.spark.serializer.KryoSerializer\n.config(\"spark.kryo.registrator\", classOf[SedonaVizKryoRegistrator].getName) // org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator\n</code></pre></p> <p><pre><code>SparkSession sparkSession = SparkSession.builder()\n.master(\"local[*]\") // Delete this if run in cluster mode\n.appName(\"readTestScala\") // Change this to a proper name\n// Enable Sedona custom Kryo serializer\n.config(\"spark.serializer\", KryoSerializer.class.getName) // org.apache.spark.serializer.KryoSerializer\n.config(\"spark.kryo.registrator\", SedonaKryoRegistrator.class.getName)\n.getOrCreate() // org.apache.sedona.core.serde.SedonaKryoRegistrator\n</code></pre> If you use SedonaViz together with SedonaSQL, please use the following two lines to enable Sedona Kryo serializer instead: <pre><code>.config(\"spark.serializer\", KryoSerializer.class.getName) // org.apache.spark.serializer.KryoSerializer\n.config(\"spark.kryo.registrator\", SedonaVizKryoRegistrator.class.getName) // org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator\n</code></pre></p> <p><pre><code>sparkSession = SparkSession. \\\n    builder. \\\n    appName('appName'). \\\n    config(\"spark.serializer\", KryoSerializer.getName). \\\n    config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName). \\\n    config('spark.jars.packages',\n           'org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.5.1,'\n           'org.datasyslab:geotools-wrapper:1.5.1-28.2'). \\\n    getOrCreate()\n</code></pre> If you are using Spark versions &gt;= 3.4, please replace the <code>3.0</code> in package name of sedona-spark-shaded with the corresponding major.minor version of Spark, such as <code>sedona-spark-shaded-3.4_2.12:1.5.1</code>.</p>"},{"location":"tutorial/sql/#initiate-sedonacontext","title":"Initiate SedonaContext","text":"<p>Add the following line after creating Sedona config. If you already have a SparkSession (usually named <code>spark</code>) created by Wherobots/AWS EMR/Databricks, please call <code>SedonaContext.create(spark)</code> instead.</p> <p>Sedona &gt;= 1.4.1</p> ScalaJavaPython <pre><code>import org.apache.sedona.spark.SedonaContext\n\nval sedona = SedonaContext.create(config)\n</code></pre> <pre><code>import org.apache.sedona.spark.SedonaContext;\n\nSparkSession sedona = SedonaContext.create(config)\n</code></pre> <pre><code>from sedona.spark import *\n\nsedona = SedonaContext.create(config)\n</code></pre> <p>Sedona &lt; 1.4.1</p> <p>The following method has been deprecated since Sedona 1.4.1. Please use the method above to create your SedonaContext.</p> ScalaJavaPython <pre><code>SedonaSQLRegistrator.registerAll(sparkSession)\n</code></pre> <pre><code>SedonaSQLRegistrator.registerAll(sparkSession)\n</code></pre> <pre><code>from sedona.register import SedonaRegistrator\n\nSedonaRegistrator.registerAll(spark)\n</code></pre> <p>You can also register everything by passing <code>--conf spark.sql.extensions=org.apache.sedona.sql.SedonaSqlExtensions</code> to <code>spark-submit</code> or <code>spark-shell</code>.</p>"},{"location":"tutorial/sql/#load-data-from-files","title":"Load data from files","text":"<p>Assume we have a WKT file, namely <code>usa-county.tsv</code>, at Path <code>/Download/usa-county.tsv</code> as follows:</p> <p><pre><code>POLYGON (..., ...)  Cuming County\nPOLYGON (..., ...)  Wahkiakum County\nPOLYGON (..., ...)  De Baca County\nPOLYGON (..., ...)  Lancaster County\n</code></pre> The file may have many other columns.</p> <p>Use the following code to load the data and create a raw DataFrame:</p> ScalaJavaPython <pre><code>var rawDf = sedona.read.format(\"csv\").option(\"delimiter\", \"\\t\").option(\"header\", \"false\").load(\"/Download/usa-county.tsv\")\nrawDf.createOrReplaceTempView(\"rawdf\")\nrawDf.show()\n</code></pre> <pre><code>Dataset&lt;Row&gt; rawDf = sedona.read.format(\"csv\").option(\"delimiter\", \"\\t\").option(\"header\", \"false\").load(\"/Download/usa-county.tsv\")\nrawDf.createOrReplaceTempView(\"rawdf\")\nrawDf.show()\n</code></pre> <pre><code>rawDf = sedona.read.format(\"csv\").option(\"delimiter\", \"\\t\").option(\"header\", \"false\").load(\"/Download/usa-county.tsv\")\nrawDf.createOrReplaceTempView(\"rawdf\")\nrawDf.show()\n</code></pre> <p>The output will be like this:</p> <pre><code>|                 _c0|_c1|_c2|     _c3|  _c4|        _c5|                 _c6|_c7|_c8|  _c9|_c10| _c11|_c12|_c13|      _c14|    _c15|       _c16|        _c17|\n+--------------------+---+---+--------+-----+-----------+--------------------+---+---+-----+----+-----+----+----+----------+--------+-----------+------------+\n|POLYGON ((-97.019...| 31|039|00835841|31039|     Cuming|       Cuming County| 06| H1|G4020|null| null|null|   A|1477895811|10447360|+41.9158651|-096.7885168|\n|POLYGON ((-123.43...| 53|069|01513275|53069|  Wahkiakum|    Wahkiakum County| 06| H1|G4020|null| null|null|   A| 682138871|61658258|+46.2946377|-123.4244583|\n|POLYGON ((-104.56...| 35|011|00933054|35011|    De Baca|      De Baca County| 06| H1|G4020|null| null|null|   A|6015539696|29159492|+34.3592729|-104.3686961|\n|POLYGON ((-96.910...| 31|109|00835876|31109|  Lancaster|    Lancaster County| 06| H1|G4020| 339|30700|null|   A|2169240202|22877180|+40.7835474|-096.6886584|\n</code></pre>"},{"location":"tutorial/sql/#create-a-geometry-type-column","title":"Create a Geometry type column","text":"<p>All geometrical operations in SedonaSQL are on Geometry type objects. Therefore, before any kind of queries, you need to create a Geometry type column on a DataFrame.</p> <pre><code>SELECT ST_GeomFromWKT(_c0) AS countyshape, _c1, _c2\n</code></pre> <p>You can select many other attributes to compose this <code>spatialdDf</code>. The output will be something like this:</p> <pre><code>|                 countyshape|_c1|_c2|     _c3|  _c4|        _c5|                 _c6|_c7|_c8|  _c9|_c10| _c11|_c12|_c13|      _c14|    _c15|       _c16|        _c17|\n+--------------------+---+---+--------+-----+-----------+--------------------+---+---+-----+----+-----+----+----+----------+--------+-----------+------------+\n|POLYGON ((-97.019...| 31|039|00835841|31039|     Cuming|       Cuming County| 06| H1|G4020|null| null|null|   A|1477895811|10447360|+41.9158651|-096.7885168|\n|POLYGON ((-123.43...| 53|069|01513275|53069|  Wahkiakum|    Wahkiakum County| 06| H1|G4020|null| null|null|   A| 682138871|61658258|+46.2946377|-123.4244583|\n|POLYGON ((-104.56...| 35|011|00933054|35011|    De Baca|      De Baca County| 06| H1|G4020|null| null|null|   A|6015539696|29159492|+34.3592729|-104.3686961|\n|POLYGON ((-96.910...| 31|109|00835876|31109|  Lancaster|    Lancaster County| 06| H1|G4020| 339|30700|null|   A|2169240202|22877180|+40.7835474|-096.6886584|\n</code></pre> <p>Although it looks same with the input, but actually the type of column countyshape has been changed to Geometry type.</p> <p>To verify this, use the following code to print the schema of the DataFrame:</p> <pre><code>spatialDf.printSchema()\n</code></pre> <p>The output will be like this:</p> <pre><code>root\n |-- countyshape: geometry (nullable = false)\n |-- _c1: string (nullable = true)\n |-- _c2: string (nullable = true)\n |-- _c3: string (nullable = true)\n |-- _c4: string (nullable = true)\n |-- _c5: string (nullable = true)\n |-- _c6: string (nullable = true)\n |-- _c7: string (nullable = true)\n</code></pre> <p>Note</p> <p>SedonaSQL provides lots of functions to create a Geometry column, please read SedonaSQL constructor API.</p>"},{"location":"tutorial/sql/#load-geojson-using-spark-json-data-source","title":"Load GeoJSON using Spark JSON Data Source","text":"<p>Spark SQL's built-in JSON data source supports reading GeoJSON data. To ensure proper parsing of the geometry property, we can define a schema with the geometry property set to type 'string'. This prevents Spark from interpreting the property and allows us to use the ST_GeomFromGeoJSON function for accurate geometry parsing.</p> ScalaJavaPython <pre><code>val schema = \"type string, crs string, totalFeatures long, features array&lt;struct&lt;type string, geometry string, properties map&lt;string, string&gt;&gt;&gt;\"\nsedona.read.schema(schema).json(geojson_path)\n.selectExpr(\"explode(features) as features\") // Explode the envelope to get one feature per row.\n.select(\"features.*\") // Unpack the features struct.\n.withColumn(\"geometry\", expr(\"ST_GeomFromGeoJSON(geometry)\")) // Convert the geometry string.\n.printSchema()\n</code></pre> <pre><code>String schema = \"type string, crs string, totalFeatures long, features array&lt;struct&lt;type string, geometry string, properties map&lt;string, string&gt;&gt;&gt;\";\nsedona.read.schema(schema).json(geojson_path)\n.selectExpr(\"explode(features) as features\") // Explode the envelope to get one feature per row.\n.select(\"features.*\") // Unpack the features struct.\n.withColumn(\"geometry\", expr(\"ST_GeomFromGeoJSON(geometry)\")) // Convert the geometry string.\n.printSchema();\n</code></pre> <pre><code>schema = \"type string, crs string, totalFeatures long, features array&lt;struct&lt;type string, geometry string, properties map&lt;string, string&gt;&gt;&gt;\";\n(sedona.read.json(geojson_path, schema=schema)\n    .selectExpr(\"explode(features) as features\") # Explode the envelope to get one feature per row.\n    .select(\"features.*\") # Unpack the features struct.\n    .withColumn(\"geometry\", f.expr(\"ST_GeomFromGeoJSON(geometry)\")) # Convert the geometry string.\n    .printSchema())\n</code></pre>"},{"location":"tutorial/sql/#load-shapefile-and-geojson-using-spatialrdd","title":"Load Shapefile and GeoJSON using SpatialRDD","text":"<p>Shapefile and GeoJSON can be loaded by SpatialRDD and converted to DataFrame using Adapter. Please read Load SpatialRDD and DataFrame &lt;-&gt; RDD.</p>"},{"location":"tutorial/sql/#load-geoparquet","title":"Load GeoParquet","text":"<p>Since v<code>1.3.0</code>, Sedona natively supports loading GeoParquet file. Sedona will infer geometry fields using the \"geo\" metadata in GeoParquet files.</p> Scala/JavaJavaPython <pre><code>val df = sedona.read.format(\"geoparquet\").load(geoparquetdatalocation1)\ndf.printSchema()\n</code></pre> <pre><code>Dataset&lt;Row&gt; df = sedona.read.format(\"geoparquet\").load(geoparquetdatalocation1)\ndf.printSchema()\n</code></pre> <pre><code>df = sedona.read.format(\"geoparquet\").load(geoparquetdatalocation1)\ndf.printSchema()\n</code></pre> <p>The output will be as follows:</p> <pre><code>root\n |-- pop_est: long (nullable = true)\n |-- continent: string (nullable = true)\n |-- name: string (nullable = true)\n |-- iso_a3: string (nullable = true)\n |-- gdp_md_est: double (nullable = true)\n |-- geometry: geometry (nullable = true)\n</code></pre> <p>Sedona supports spatial predicate push-down for GeoParquet files, please refer to the SedonaSQL query optimizer documentation for details.</p> <p>GeoParquet file reader can also be used to read legacy Parquet files written by Apache Sedona 1.3.1-incubating or earlier. Please refer to Reading Legacy Parquet Files for details.</p> <p>Warning</p> <p>GeoParquet file reader does not work on Databricks runtime when Photon is enabled. Please disable Photon when using GeoParquet file reader on Databricks runtime.</p>"},{"location":"tutorial/sql/#inspect-geoparquet-metadata","title":"Inspect GeoParquet metadata","text":"<p>Since v<code>1.5.1</code>, Sedona provides a Spark SQL data source <code>\"geoparquet.metadata\"</code> for inspecting GeoParquet metadata. The resulting dataframe contains the \"geo\" metadata for each input file.</p> Scala/JavaJavaPython <pre><code>val df = sedona.read.format(\"geoparquet.metadata\").load(geoparquetdatalocation1)\ndf.printSchema()\n</code></pre> <pre><code>Dataset&lt;Row&gt; df = sedona.read.format(\"geoparquet.metadata\").load(geoparquetdatalocation1)\ndf.printSchema()\n</code></pre> <pre><code>df = sedona.read.format(\"geoparquet.metadata\").load(geoparquetdatalocation1)\ndf.printSchema()\n</code></pre> <p>The output will be as follows:</p> <pre><code>root\n |-- path: string (nullable = true)\n |-- version: string (nullable = true)\n |-- primary_column: string (nullable = true)\n |-- columns: map (nullable = true)\n |    |-- key: string\n |    |-- value: struct (valueContainsNull = true)\n |    |    |-- encoding: string (nullable = true)\n |    |    |-- geometry_types: array (nullable = true)\n |    |    |    |-- element: string (containsNull = true)\n |    |    |-- bbox: array (nullable = true)\n |    |    |    |-- element: double (containsNull = true)\n |    |    |-- crs: string (nullable = true)\n</code></pre> <p>If the input Parquet file does not have GeoParquet metadata, the values of <code>version</code>, <code>primary_column</code> and <code>columns</code> fields of the resulting dataframe will be <code>null</code>.</p> <p>Note</p> <p><code>geoparquet.metadata</code> only supports reading GeoParquet specific metadata. Users can use G-Research/spark-extension to read comprehensive metadata of generic Parquet files.</p>"},{"location":"tutorial/sql/#load-data-from-jdbc-data-sources","title":"Load data from JDBC data sources","text":"<p>The 'query' option in Spark SQL's JDBC data source can be used to convert geometry columns to a format that Sedona can interpret. This should work for most spatial JDBC data sources. For Postgis there is no need to add a query to convert geometry types since it's already using EWKB as it's wire format.</p> ScalaJavaPython <pre><code>// For any JDBC data source, including Postgis.\nval df = sedona.read.format(\"jdbc\")\n// Other options.\n.option(\"query\", \"SELECT id, ST_AsBinary(geom) as geom FROM my_table\")\n.load()\n.withColumn(\"geom\", expr(\"ST_GeomFromWKB(geom)\"))\n\n// This is a simplified version that works for Postgis.\nval df = sedona.read.format(\"jdbc\")\n// Other options.\n.option(\"dbtable\", \"my_table\")\n.load()\n.withColumn(\"geom\", expr(\"ST_GeomFromWKB(geom)\"))\n</code></pre> <pre><code>// For any JDBC data source, including Postgis.\nDataset&lt;Row&gt; df = sedona.read().format(\"jdbc\")\n// Other options.\n.option(\"query\", \"SELECT id, ST_AsBinary(geom) as geom FROM my_table\")\n.load()\n.withColumn(\"geom\", expr(\"ST_GeomFromWKB(geom)\"))\n\n// This is a simplified version that works for Postgis.\nDataset&lt;Row&gt; df = sedona.read().format(\"jdbc\")\n// Other options.\n.option(\"dbtable\", \"my_table\")\n.load()\n.withColumn(\"geom\", expr(\"ST_GeomFromWKB(geom)\"))\n</code></pre> <pre><code># For any JDBC data source, including Postgis.\ndf = (sedona.read.format(\"jdbc\")\n    # Other options.\n    .option(\"query\", \"SELECT id, ST_AsBinary(geom) as geom FROM my_table\")\n    .load()\n    .withColumn(\"geom\", f.expr(\"ST_GeomFromWKB(geom)\")))\n\n# This is a simplified version that works for Postgis.\ndf = (sedona.read.format(\"jdbc\")\n    # Other options.\n    .option(\"dbtable\", \"my_table\")\n    .load()\n    .withColumn(\"geom\", f.expr(\"ST_GeomFromWKB(geom)\")))\n</code></pre>"},{"location":"tutorial/sql/#transform-the-coordinate-reference-system","title":"Transform the Coordinate Reference System","text":"<p>Sedona doesn't control the coordinate unit (degree-based or meter-based) of all geometries in a Geometry column. The unit of all related distances in SedonaSQL is same as the unit of all geometries in a Geometry column.</p> <p>By default, this function uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order. You can use ST_FlipCoordinates to swap X and Y.</p> <p>For more details, please read the <code>ST_Transform</code> section in Sedona API References.</p> <p>To convert Coordinate Reference System of the Geometry column created before, use the following code:</p> <pre><code>SELECT ST_Transform(countyshape, \"epsg:4326\", \"epsg:3857\") AS newcountyshape, _c1, _c2, _c3, _c4, _c5, _c6, _c7\nFROM spatialdf\n</code></pre> <p>The first EPSG code EPSG:4326 in <code>ST_Transform</code> is the source CRS of the geometries. It is WGS84, the most common degree-based CRS.</p> <p>The second EPSG code EPSG:3857 in <code>ST_Transform</code> is the target CRS of the geometries. It is the most common meter-based CRS.</p> <p>This <code>ST_Transform</code> transform the CRS of these geometries from EPSG:4326 to EPSG:3857. The details CRS information can be found on EPSG.io</p> <p>The coordinates of polygons have been changed. The output will be like this:</p> <pre><code>+--------------------+---+---+--------+-----+-----------+--------------------+---+\n|      newcountyshape|_c1|_c2|     _c3|  _c4|        _c5|                 _c6|_c7|\n+--------------------+---+---+--------+-----+-----------+--------------------+---+\n|POLYGON ((-108001...| 31|039|00835841|31039|     Cuming|       Cuming County| 06|\n|POLYGON ((-137408...| 53|069|01513275|53069|  Wahkiakum|    Wahkiakum County| 06|\n|POLYGON ((-116403...| 35|011|00933054|35011|    De Baca|      De Baca County| 06|\n|POLYGON ((-107880...| 31|109|00835876|31109|  Lancaster|    Lancaster County| 06|\n</code></pre>"},{"location":"tutorial/sql/#run-spatial-queries","title":"Run spatial queries","text":"<p>After creating a Geometry type column, you are able to run spatial queries.</p>"},{"location":"tutorial/sql/#range-query","title":"Range query","text":"<p>Use ST_Contains, ST_Intersects, ST_Within to run a range query over a single column.</p> <p>The following example finds all counties that are within the given polygon:</p> <pre><code>SELECT *\nFROM spatialdf\nWHERE ST_Contains (ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), newcountyshape)\n</code></pre> <p>Note</p> <p>Read SedonaSQL constructor API to learn how to create a Geometry type query window</p>"},{"location":"tutorial/sql/#knn-query","title":"KNN query","text":"<p>Use ST_Distance to calculate the distance and rank the distance.</p> <p>The following code returns the 5 nearest neighbor of the given polygon.</p> <pre><code>SELECT countyname, ST_Distance(ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), newcountyshape) AS distance\nFROM spatialdf\nORDER BY distance DESC\nLIMIT 5\n</code></pre>"},{"location":"tutorial/sql/#join-query","title":"Join query","text":"<p>The details of a join query is available here Join query.</p>"},{"location":"tutorial/sql/#other-queries","title":"Other queries","text":"<p>There are lots of other functions can be combined with these queries. Please read SedonaSQL functions and SedonaSQL aggregate functions.</p>"},{"location":"tutorial/sql/#visualize-query-results","title":"Visualize query results","text":"<p>Sedona provides <code>SedonaPyDeck</code> and <code>SedonaKepler</code> wrappers, both of which expose APIs to create interactive map visualizations from SedonaDataFrames in a Jupyter environment.</p> <p>Note</p> <p>Both SedonaPyDeck and SedonaKepler expect the default geometry order to be lon-lat. If your dataframe has geometries in the lat-lon order, please check out ST_FlipCoordinates</p> <p>Note</p> <p>Both SedonaPyDeck and SedonaKepler are designed to work with SedonaDataFrames containing only 1 geometry column. Passing dataframes with multiple geometry columns will cause errors.</p>"},{"location":"tutorial/sql/#sedonapydeck","title":"SedonaPyDeck","text":"<p>Spatial query results can be visualized in a Jupyter lab/notebook environment using SedonaPyDeck.</p> <p>SedonaPyDeck exposes APIs to create interactive map visualizations using pydeck based on deck.gl</p> <p>Note</p> <p>To use SedonaPyDeck, GeoPandas and PyDeck must be installed. We recommend the following installation commands: <pre><code>pip install 'pandas&lt;=1.3.5'\npip install 'geopandas&lt;=0.10.2'\npip install pydeck==0.8.0\n</code></pre></p> <p>The following tutorial showcases the various maps that can be created using SedonaPyDeck, the datasets used to create these maps are publicly available.</p> <p>Each API exposed by SedonaPyDeck offers customization via optional arguments, details on all possible arguments can be found in the API docs of SedonaPyDeck.</p>"},{"location":"tutorial/sql/#creating-a-choropleth-map-using-sedonapydeck","title":"Creating a Choropleth map using SedonaPyDeck","text":"<p>SedonaPyDeck exposes a <code>create_choropleth_map</code> API which can be used to visualize a choropleth map out of the passed SedonaDataFrame containing polygons with an observation:</p> <p>Example (referenced from example notebook available via binder):</p> <pre><code>SedonaPyDeck.create_choropleth_map(df=groupedresult, plot_col='AirportCount')\n</code></pre> <p>Note</p> <p><code>plot_col</code> is a required argument informing SedonaPyDeck of the column name used to render the choropleth effect.</p> <p></p> <p>The dataset used is available here and can also be found in the example notebook available here</p>"},{"location":"tutorial/sql/#creating-a-geometry-map-using-sedonapydeck","title":"Creating a Geometry map using SedonaPyDeck","text":"<p>SedonaPyDeck exposes a create_geometry_map API which can be used to visualize a passed SedonaDataFrame containing any type of geometries:</p> <p>Example (referenced from overture notebook available via binder):</p> <pre><code>SedonaPyDeck.create_geometry_map(df_building, elevation_col='height')\n</code></pre> <p></p> <p>Tip</p> <p><code>elevation_col</code> is an optional argument which can be used to render a 3D map. Pass the column with 'elevation' values for the geometries here.</p>"},{"location":"tutorial/sql/#creating-a-scatterplot-map-using-sedonapydeck","title":"Creating a Scatterplot map using SedonaPyDeck","text":"<p>SedonaPyDeck exposes a create_scatterplot_map API which can be used to visualize a scatterplot out of the passed SedonaDataFrame containing points:</p> <p>Example:</p> <pre><code>SedonaPyDeck.create_scatterplot_map(df=crimes_df)\n</code></pre> <p></p> <p>The dataset used here is the Chicago crimes dataset, available here</p>"},{"location":"tutorial/sql/#creating-a-heatmap-using-sedonapydeck","title":"Creating a heatmap using SedonaPyDeck","text":"<p>SedonaPyDeck exposes a create_heatmap API which can be used to visualize a heatmap out of the passed SedonaDataFrame containing points:</p> <p>Example: <pre><code>SedonaPyDeck.create_heatmap(df=crimes_df)\n</code></pre></p> <p></p> <p>The dataset used here is the Chicago crimes dataset, available here</p>"},{"location":"tutorial/sql/#sedonakepler","title":"SedonaKepler","text":"<p>Spatial query results can be visualized in a Jupyter lab/notebook environment using SedonaKepler.</p> <p>SedonaKepler exposes APIs to create interactive and customizable map visualizations using KeplerGl.</p> <p>Note</p> <p>To use SedonaKepler, GeoPandas and KeplerGL must be installed. We recommend the following installation commands: <pre><code>pip install 'pandas&lt;=1.3.5'\npip install 'geopandas&lt;=0.10.2'\npip install keplergl==0.3.2\n</code></pre></p> <p>This tutorial showcases how simple it is to instantly visualize geospatial data using SedonaKepler.</p> <p>Example (referenced from an example notebook via the binder):</p> <pre><code>SedonaKepler.create_map(df=groupedresult, name=\"AirportCount\")\n</code></pre> <p></p> <p>The dataset used is available here and can also be found in the example notebook available here</p> <p>Details on all the APIs available by SedonaKepler are listed in the SedonaKepler API docs</p>"},{"location":"tutorial/sql/#create-a-user-defined-function-udf","title":"Create a User-Defined Function (UDF)","text":"<p>User-Defined Functions (UDFs) are user-created procedures that can perform operations on a single row of information. To cover almost all use cases, we will showcase 4 types of UDFs for a better understanding of how to use geometry with UDFs. Sedona's serializer deserializes the SQL geometry type to JTS Geometry (Scala/Java) or Shapely Geometry (Python). You can implement any custom logic using the rich ecosystem around these two libraries.</p>"},{"location":"tutorial/sql/#geometry-to-primitive","title":"Geometry to primitive","text":"<p>This UDF example takes a geometry type input and returns a primitive type output:</p> ScalaJavaPython <pre><code>import org.locationtech.jts.geom.Geometry\nimport org.apache.spark.sql.types._\n\ndef lengthPoly(geom: Geometry): Double = {\ngeom.getLength\n}\n\nsedona.udf.register(\"udf_lengthPoly\", lengthPoly _)\n\ndf.selectExpr(\"udf_lengthPoly(geom)\").show()\n</code></pre> <pre><code>import org.apache.spark.sql.api.java.UDF1;\nimport org.apache.spark.sql.types.DataTypes;\n\n// using lambda function to register the UDF\nsparkSession.udf().register(\n\"udf_lengthPoly\",\n(UDF1&lt;Geometry, Double&gt;) Geometry::getLength,\nDataTypes.DoubleType);\n\ndf.selectExpr(\"udf_lengthPoly(geom)\").show()\n</code></pre> <pre><code>from sedona.sql.types import GeometryType\nfrom pyspark.sql.types import DoubleType\n\ndef lengthPoly(geom: GeometryType()):\n    return geom.length\n\nsedona.udf.register(\"udf_lengthPoly\", lengthPoly, DoubleType())\n\ndf.selectExpr(\"udf_lengthPoly(geom)\").show()\n</code></pre> <p>Output:</p> <pre><code>+--------------------+\n|udf_lengthPoly(geom)|\n+--------------------+\n|   3.414213562373095|\n+--------------------+\n</code></pre>"},{"location":"tutorial/sql/#geometry-to-geometry","title":"Geometry to Geometry","text":"<p>This UDF example takes a geometry type input and returns a geometry type output:</p> ScalaJavaPython <pre><code>import org.locationtech.jts.geom.Geometry\nimport org.apache.spark.sql.types._\n\ndef bufferFixed(geom: Geometry): Geometry = {\ngeom.buffer(5.5)\n}\n\nsedona.udf.register(\"udf_bufferFixed\", bufferFixed _)\n\ndf.selectExpr(\"udf_bufferFixed(geom)\").show()\n</code></pre> <pre><code>import org.apache.spark.sql.api.java.UDF1;\nimport org.apache.spark.sql.types.DataTypes;\n\n// using lambda function to register the UDF\nsparkSession.udf().register(\n\"udf_bufferFixed\",\n(UDF1&lt;Geometry, Geometry&gt;) geom -&gt;\ngeom.buffer(5.5),\nnew GeometryUDT());\n\ndf.selectExpr(\"udf_bufferFixed(geom)\").show()\n</code></pre> <pre><code>from sedona.sql.types import GeometryType\nfrom pyspark.sql.types import DoubleType\n\ndef bufferFixed(geom: GeometryType()):\n    return geom.buffer(5.5)\n\nsedona.udf.register(\"udf_bufferFixed\", bufferFixed, GeometryType())\n\ndf.selectExpr(\"udf_bufferFixed(geom)\").show()\n</code></pre> <p>Output:</p> <pre><code>+--------------------------------------------------+\n|                             udf_bufferFixed(geom)|\n+--------------------------------------------------+\n|POLYGON ((1 -4.5, -0.0729967710887076 -4.394319...|\n+--------------------------------------------------+\n</code></pre>"},{"location":"tutorial/sql/#geometry-primitive-to-geometry","title":"Geometry, primitive to geometry","text":"<p>This UDF example takes a geometry type input and a primitive type input and returns a geometry type output:</p> ScalaJavaPython <pre><code>import org.locationtech.jts.geom.Geometry\nimport org.apache.spark.sql.types._\n\ndef bufferIt(geom: Geometry, distance: Double): Geometry = {\ngeom.buffer(distance)\n}\n\nsedona.udf.register(\"udf_buffer\", bufferIt _)\n\ndf.selectExpr(\"udf_buffer(geom, distance)\").show()\n</code></pre> <pre><code>import org.apache.spark.sql.api.java.UDF2;\nimport org.apache.spark.sql.types.DataTypes;\n\n// using lambda function to register the UDF\nsparkSession.udf().register(\n\"udf_buffer\",\n(UDF2&lt;Geometry, Double, Geometry&gt;) Geometry::buffer,\nnew GeometryUDT());\n\ndf.selectExpr(\"udf_buffer(geom, distance)\").show()\n</code></pre> <pre><code>from sedona.sql.types import GeometryType\nfrom pyspark.sql.types import DoubleType\n\ndef bufferIt(geom: GeometryType(), distance: DoubleType()):\n    return geom.buffer(distance)\n\nsedona.udf.register(\"udf_buffer\", bufferIt, GeometryType())\n\ndf.selectExpr(\"udf_buffer(geom, distance)\").show()\n</code></pre> <p>Output:</p> <pre><code>+--------------------------------------------------+\n|                        udf_buffer(geom, distance)|\n+--------------------------------------------------+\n|POLYGON ((1 -9, -0.9509032201612866 -8.80785280...|\n+--------------------------------------------------+\n</code></pre>"},{"location":"tutorial/sql/#geometry-primitive-to-geometry-primitive","title":"Geometry, primitive to Geometry, primitive","text":"<p>This UDF example takes a geometry type input and a primitive type input and returns a geometry type and a primitive type output:</p> ScalaJavaPython <pre><code>import org.locationtech.jts.geom.Geometry\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.api.java.UDF2\n\nval schemaUDF = StructType(Array(\nStructField(\"buffed\", GeometryUDT),\nStructField(\"length\", DoubleType)\n))\n\nval udf_bufferLength = udf(\nnew UDF2[Geometry, Double, (Geometry, Double)] {\ndef call(geom: Geometry, distance: Double): (Geometry, Double) = {\nval buffed = geom.buffer(distance)\nval length = geom.getLength\n(buffed, length)\n}\n}, schemaUDF)\n\nsedona.udf.register(\"udf_bufferLength\", udf_bufferLength)\n\ndata.withColumn(\"bufferLength\", expr(\"udf_bufferLengths(geom, distance)\"))\n.select(\"geom\", \"distance\", \"bufferLength.*\")\n.show()\n</code></pre> <pre><code>import org.apache.spark.sql.api.java.UDF2;\nimport org.apache.spark.sql.types.DataTypes;\nimport org.apache.spark.sql.types.StructType;\nimport scala.Tuple2;\n\nStructType schemaUDF = new StructType()\n.add(\"buffedGeom\", new GeometryUDT())\n.add(\"length\", DataTypes.DoubleType);\n\n// using lambda function to register the UDF\nsparkSession.udf().register(\"udf_bufferLength\",\n(UDF2&lt;Geometry, Double, Tuple2&lt;Geometry, Double&gt;&gt;) (geom, distance) -&gt; {\nGeometry buffed = geom.buffer(distance);\nDouble length = buffed.getLength();\nreturn new Tuple2&lt;&gt;(buffed, length);\n},\nschemaUDF);\n\ndf.withColumn(\"bufferLength\", functions.expr(\"udf_bufferLength(geom, distance)\"))\n.select(\"geom\", \"distance\", \"bufferLength.*\")\n.show();\n</code></pre> <pre><code>from sedona.sql.types import GeometryType\nfrom pyspark.sql.types import *\n\nschemaUDF = StructType([\n    StructField(\"buffed\", GeometryType()),\n    StructField(\"length\", DoubleType())\n    ])\n\ndef bufferAndLength(geom: GeometryType(), distance: DoubleType()):\n    buffed = geom.buffer(distance)\n    length = buffed.length\n    return [buffed, length]\n\nsedona.udf.register(\"udf_bufferLength\", bufferAndLength, schemaUDF)\n\ndf.withColumn(\"bufferLength\", expr(\"udf_bufferLength(geom, buffer)\"))\n            .select(\"geom\", \"buffer\", \"bufferLength.*\")\n            .show()\n</code></pre> <p>Output:</p> <pre><code>+------------------------------+--------+--------------------------------------------------+-----------------+\n|                          geom|distance|                                        buffedGeom|           length|\n+------------------------------+--------+--------------------------------------------------+-----------------+\n|POLYGON ((1 1, 1 2, 2 1, 1 1))|    10.0|POLYGON ((1 -9, -0.9509032201612866 -8.80785280...|66.14518337329191|\n+------------------------------+--------+--------------------------------------------------+-----------------+\n</code></pre>"},{"location":"tutorial/sql/#save-to-permanent-storage","title":"Save to permanent storage","text":"<p>To save a Spatial DataFrame to some permanent storage such as Hive tables and HDFS, you can simply convert each geometry in the Geometry type column back to a plain String and save the plain DataFrame to wherever you want.</p> <p>Use the following code to convert the Geometry column in a DataFrame back to a WKT string column:</p> <pre><code>SELECT ST_AsText(countyshape)\nFROM polygondf\n</code></pre> <p>Note</p> <p>ST_AsGeoJSON is also available. We would like to invite you to contribute more functions</p>"},{"location":"tutorial/sql/#save-geoparquet","title":"Save GeoParquet","text":"<p>Since v<code>1.3.0</code>, Sedona natively supports writing GeoParquet file. GeoParquet can be saved as follows:</p> <pre><code>df.write.format(\"geoparquet\").save(geoparquetoutputlocation + \"/GeoParquet_File_Name.parquet\")\n</code></pre> <p>Since v<code>1.5.1</code>, Sedona supports writing GeoParquet files with custom GeoParquet spec version and crs. The default GeoParquet spec version is <code>1.0.0</code> and the default crs is <code>null</code>. You can specify the GeoParquet spec version and crs as follows:</p> <pre><code>val projjson = \"{...}\" // PROJJSON string for all geometry columns\ndf.write.format(\"geoparquet\")\n.option(\"geoparquet.version\", \"1.0.0\")\n.option(\"geoparquet.crs\", projjson)\n.save(geoparquetoutputlocation + \"/GeoParquet_File_Name.parquet\")\n</code></pre> <p>If you have multiple geometry columns written to the GeoParquet file, you can specify the CRS for each column. For example, <code>g0</code> and <code>g1</code> are two geometry columns in the DataFrame <code>df</code>, and you want to specify the CRS for each column as follows:</p> <pre><code>val projjson_g0 = \"{...}\" // PROJJSON string for g0\nval projjson_g1 = \"{...}\" // PROJJSON string for g1\ndf.write.format(\"geoparquet\")\n.option(\"geoparquet.version\", \"1.0.0\")\n.option(\"geoparquet.crs.g0\", projjson_g0)\n.option(\"geoparquet.crs.g1\", projjson_g1)\n.save(geoparquetoutputlocation + \"/GeoParquet_File_Name.parquet\")\n</code></pre> <p>The value of <code>geoparquet.crs</code> and <code>geoparquet.crs.&lt;column_name&gt;</code> can be one of the following:</p> <ul> <li><code>\"null\"</code>: Explicitly setting <code>crs</code> field to <code>null</code>. This is the default behavior.</li> <li><code>\"\"</code> (empty string): Omit the <code>crs</code> field. This implies that the CRS is OGC:CRS84 for CRS-aware implementations.</li> <li><code>\"{...}\"</code> (PROJJSON string): The <code>crs</code> field will be set as the PROJJSON object representing the Coordinate Reference System (CRS) of the geometry. You can find the PROJJSON string of a specific CRS from here: https://epsg.io/ (click the JSON option at the bottom of the page). You can also customize your PROJJSON string as needed.</li> </ul> <p>Please note that Sedona currently cannot set/get a projjson string to/from a CRS. Its geoparquet reader will ignore the projjson metadata and you will have to set your CRS via <code>ST_SetSRID</code> after reading the file. Its geoparquet writer will not leverage the SRID field of a geometry so you will have to always set the <code>geoparquet.crs</code> option manually when writing the file, if you want to write a meaningful CRS field.</p> <p>Due to the same reason, Sedona geoparquet reader and writer do NOT check the axis order (lon/lat or lat/lon) and assume they are handled by the users themselves when writing / reading the files. You can always use <code>ST_FlipCoordinates</code> to swap the axis order of your geometries.</p>"},{"location":"tutorial/sql/#sort-then-save-geoparquet","title":"Sort then Save GeoParquet","text":"<p>To maximize the performance of Sedona GeoParquet filter pushdown, we suggest that you sort the data by their geohash values (see ST_GeoHash) and then save as a GeoParquet file. An example is as follows:</p> <pre><code>SELECT col1, col2, geom, ST_GeoHash(geom, 5) as geohash\nFROM spatialDf\nORDER BY geohash\n</code></pre>"},{"location":"tutorial/sql/#save-to-postgis","title":"Save to Postgis","text":"<p>Unfortunately, the Spark SQL JDBC data source doesn't support creating geometry types in PostGIS using the 'createTableColumnTypes' option. Only the Spark built-in types are recognized. This means that you'll need to manage your PostGIS schema separately from Spark. One way to do this is to create the table with the correct geometry column before writing data to it with Spark. Alternatively, you can write your data to the table using Spark and then manually alter the column to be a geometry type afterward.</p> <p>Postgis uses EWKB to serialize geometries. If you convert your geometries to EWKB format in Sedona you don't have to do any additional conversion in Postgis.</p> <pre><code>my_postgis_db# create table my_table (id int8, geom geometry);\n\ndf.withColumn(\"geom\", expr(\"ST_AsEWKB(geom)\")\n    .write.format(\"jdbc\")\n    .option(\"truncate\",\"true\") // Don't let Spark recreate the table.\n    // Other options.\n    .save()\n\n// If you didn't create the table before writing you can change the type afterward.\nmy_postgis_db# alter table my_table alter column geom type geometry;\n</code></pre>"},{"location":"tutorial/sql/#convert-between-dataframe-and-spatialrdd","title":"Convert between DataFrame and SpatialRDD","text":""},{"location":"tutorial/sql/#dataframe-to-spatialrdd","title":"DataFrame to SpatialRDD","text":"<p>Use SedonaSQL DataFrame-RDD Adapter to convert a DataFrame to an SpatialRDD. Please read Adapter Scaladoc</p> ScalaJavaPython <pre><code>var spatialRDD = Adapter.toSpatialRdd(spatialDf, \"usacounty\")\n</code></pre> <pre><code>SpatialRDD spatialRDD = Adapter.toSpatialRdd(spatialDf, \"usacounty\")\n</code></pre> <pre><code>from sedona.utils.adapter import Adapter\n\nspatialRDD = Adapter.toSpatialRdd(spatialDf, \"usacounty\")\n</code></pre> <p>\"usacounty\" is the name of the geometry column</p> <p>Warning</p> <p>Only one Geometry type column is allowed per DataFrame.</p>"},{"location":"tutorial/sql/#spatialrdd-to-dataframe","title":"SpatialRDD to DataFrame","text":"<p>Use SedonaSQL DataFrame-RDD Adapter to convert a DataFrame to an SpatialRDD. Please read Adapter Scaladoc</p> ScalaJavaPython <pre><code>var spatialDf = Adapter.toDf(spatialRDD, sedona)\n</code></pre> <pre><code>Dataset&lt;Row&gt; spatialDf = Adapter.toDf(spatialRDD, sedona)\n</code></pre> <pre><code>from sedona.utils.adapter import Adapter\n\nspatialDf = Adapter.toDf(spatialRDD, sedona)\n</code></pre> <p>All other attributes such as price and age will be also brought to the DataFrame as long as you specify carryOtherAttributes (see Read other attributes in an SpatialRDD).</p> <p>You may also manually specify a schema for the resulting DataFrame in case you require different column names or data types. Note that string schemas and not all data types are supported\u2014please check the Adapter Scaladoc to confirm what is supported for your use case. At least one column for the user data must be provided.</p> Scala <pre><code>val schema = StructType(Array(\nStructField(\"county\", GeometryUDT, nullable = true),\nStructField(\"name\", StringType, nullable = true),\nStructField(\"price\", DoubleType, nullable = true),\nStructField(\"age\", IntegerType, nullable = true)\n))\nval spatialDf = Adapter.toDf(spatialRDD, schema, sedona)\n</code></pre>"},{"location":"tutorial/sql/#spatialpairrdd-to-dataframe","title":"SpatialPairRDD to DataFrame","text":"<p>PairRDD is the result of a spatial join query or distance join query. SedonaSQL DataFrame-RDD Adapter can convert the result to a DataFrame. But you need to provide the name of other attributes.</p> ScalaJavaPython <pre><code>var joinResultDf = Adapter.toDf(joinResultPairRDD, Seq(\"left_attribute1\", \"left_attribute2\"), Seq(\"right_attribute1\", \"right_attribute2\"), sedona)\n</code></pre> <pre><code>import scala.collection.JavaConverters;\n\nList leftFields = new ArrayList&lt;&gt;(Arrays.asList(\"c1\", \"c2\", \"c3\"));\nList rightFields = new ArrayList&lt;&gt;(Arrays.asList(\"c4\", \"c5\", \"c6\"));\nDataset joinResultDf = Adapter.toDf(joinResultPairRDD, JavaConverters.asScalaBuffer(leftFields).toSeq(), JavaConverters.asScalaBuffer(rightFields).toSeq(), sedona);\n</code></pre> <pre><code>from sedona.utils.adapter import Adapter\n\njoinResultDf = Adapter.toDf(jvm_sedona_rdd, [\"poi_from_id\", \"poi_from_name\"], [\"poi_to_id\", \"poi_to_name\"], spark))\n</code></pre> <p>or you can use the attribute names directly from the input RDD</p> ScalaJavaPython <pre><code>import scala.collection.JavaConversions._\nvar joinResultDf = Adapter.toDf(joinResultPairRDD, leftRdd.fieldNames, rightRdd.fieldNames, sedona)\n</code></pre> <pre><code>import scala.collection.JavaConverters;\nDataset joinResultDf = Adapter.toDf(joinResultPairRDD, JavaConverters.asScalaBuffer(leftRdd.fieldNames).toSeq(), JavaConverters.asScalaBuffer(rightRdd.fieldNames).toSeq(), sedona);\n</code></pre> <pre><code>from sedona.utils.adapter import Adapter\n\njoinResultDf = Adapter.toDf(result_pair_rdd, leftRdd.fieldNames, rightRdd.fieldNames, spark)\n</code></pre> <p>All other attributes such as price and age will be also brought to the DataFrame as long as you specify carryOtherAttributes (see Read other attributes in an SpatialRDD).</p> <p>You may also manually specify a schema for the resulting DataFrame in case you require different column names or data types. Note that string schemas and not all data types are supported\u2014please check the Adapter Scaladoc to confirm what is supported for your use case. Columns for the left and right user data must be provided.</p> Scala <pre><code>val schema = StructType(Array(\nStructField(\"leftGeometry\", GeometryUDT, nullable = true),\nStructField(\"name\", StringType, nullable = true),\nStructField(\"price\", DoubleType, nullable = true),\nStructField(\"age\", IntegerType, nullable = true),\nStructField(\"rightGeometry\", GeometryUDT, nullable = true),\nStructField(\"category\", StringType, nullable = true)\n))\nval joinResultDf = Adapter.toDf(joinResultPairRDD, schema, sedona)\n</code></pre>"},{"location":"tutorial/storing-blobs-in-parquet/","title":"Storing large raster geometries in Parquet files","text":"<p>Warning</p> <p>Always convert the raster geometries to a well known format with the RS_AsXXX functions before saving them. It is possible to save the raw bytes of the raster geometries, but they will be stored in an internal Sedona format that is not guaranteed to be stable across versions.</p> <p>The default settings in Spark are not well suited for storing large binaries like raster geometries. It is very much worth the time to tune and benchmark your settings. Writing large binaries with the default settings will result in poorly structured Parquet files that are very expensive to read. Some basic tuning can increase the read performance by several magnitudes.</p>"},{"location":"tutorial/storing-blobs-in-parquet/#background","title":"Background","text":"<p>Parquet files are divided into one or several row groups. Each column in a row group is stored in a column chunk. Each column chunk is further divided into pages. A page is conceptually an indivisible unit in terms of compression and encoding. The default size for a page is 1 MB. Data is buffered until the page is full and then written to disk. The frequency of checks of the page size limit will be between <code>parquet.page.size.row.check.min</code> and <code>parquet.page.size.row.check.max</code> (default between 100 and 10000 rows).</p> <p>If you write 5 MB image files to Parquet with the default setting the first page size check will happen after 100 rows. You will end up with pages of 500 MB instead of 1 MB. Reading such a file will require a lot of memory and will be slow.</p>"},{"location":"tutorial/storing-blobs-in-parquet/#reading-poorly-structured-parquet-files","title":"Reading poorly structured Parquet files","text":"<p>Especially snappy compressed files are sensitive to oversized pages. More performant options are no compression or zstd compression. You can set <code>spark.buffer.size</code> to a value larger than the default of 64k to improve read performance. Increasing <code>spark.buffer.size</code> might add an io penalty for other columns in the Parquet file.</p>"},{"location":"tutorial/storing-blobs-in-parquet/#writing-better-structured-parquet-files-for-blobs","title":"Writing better structured Parquet files for blobs","text":"<p>Ideally you want to write Parquet files with a sane page size to get better and more consistent read performance across different clients. Since version 1.12.0 of parquet-hadoop, bundled with Spark 3.2, you can add Hadoop properties for controlling page size checks. Better values for writing blobs are:</p> <pre><code>spark.sql.parquet.compression.codec=zstd\nspark.hadoop.parquet.page.size.row.check.min=2\nspark.hadoop.parquet.page.size.row.check.max=10\n</code></pre> <p>Zstd performs better than snappy in general. Even more so for large pages. The first page size check will happen after 2 rows. If the page is not full after 2 rows the next check will happen after another 2-10 rows, depending on the size of the two rows already written.</p> <p>Spark will set Hadoop properties from Spark properties prefixed with \"spark.hadoop.\". For a full list of Parquet Hadoop properties see: https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/README.md</p>"},{"location":"tutorial/viz-gallery/","title":"Gallery","text":""},{"location":"tutorial/viz/","title":"Scala/Java","text":"<p>The page outlines the steps to visualize spatial data using SedonaViz. The example code is written in Scala but also works for Java.</p> <p>SedonaViz provides native support for general cartographic design by extending Sedona to process large-scale spatial data. It can visualize Spatial RDD and Spatial Queries and render super high resolution image in parallel.</p> <p>SedonaViz offers Map Visualization SQL. This gives users a more flexible way to design beautiful map visualization effects including scatter plots and heat maps. SedonaViz RDD API is also available.</p> <p>Note</p> <p>All SedonaViz SQL/DataFrame APIs are explained in SedonaViz API. Please see Viz example project</p>"},{"location":"tutorial/viz/#why-scalable-map-visualization","title":"Why scalable map visualization?","text":"<p>Data visualization allows users to summarize, analyze and reason about data. Guaranteeing detailed and accurate geospatial map visualization (e.g., at multiple zoom levels) requires extremely high-resolution maps. Classic visualization solutions such as Google Maps, MapBox and ArcGIS suffer from limited computation resources and hence take a tremendous amount of time to generate maps for large-scale geospatial data. In big spatial data scenarios, these tools just crash or run forever.</p> <p>SedonaViz encapsulates the main steps of map visualization process, e.g., pixelize, aggregate, and render, into a set of massively parallelized GeoViz operators and the user can assemble any customized styles.</p>"},{"location":"tutorial/viz/#visualize-spatialrdd","title":"Visualize SpatialRDD","text":"<p>This tutorial mainly focuses on explaining SQL/DataFrame API. SedonaViz RDD example can be found in Please see Viz example project</p>"},{"location":"tutorial/viz/#set-up-dependencies","title":"Set up dependencies","text":"<ol> <li>Read Sedona Maven Central coordinates</li> <li>Add Apache Spark core, Apache SparkSQL, Sedona-core, Sedona-SQL, Sedona-Viz</li> </ol>"},{"location":"tutorial/viz/#create-sedona-config","title":"Create Sedona config","text":"<p>Use the following code to create your Sedona config at the beginning. If you already have a SparkSession (usually named <code>spark</code>) created by Wherobots/AWS EMR/Databricks, please skip this step and can use <code>spark</code> directly.</p> <p>Sedona &gt;= 1.4.1=</p> <pre><code>val config = SedonaContext.builder()\n.config(\"spark.kryo.registrator\", classOf[SedonaVizKryoRegistrator].getName) // org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator\n.master(\"local[*]\") // Delete this if run in cluster mode\n.appName(\"Sedona Viz\") // Change this to a proper name\n.getOrCreate()\n</code></pre> <p>Sedona &lt;1.4.1</p> <p>The following method has been deprecated since Sedona 1.4.1. Please use the method above to create your Sedona config.</p> <pre><code>var sparkSession = SparkSession.builder()\n.master(\"local[*]\") // Delete this if run in cluster mode\n.appName(\"Sedona Viz\") // Change this to a proper name\n// Enable Sedona custom Kryo serializer\n.config(\"spark.serializer\", classOf[KryoSerializer].getName) // org.apache.spark.serializer.KryoSerializer\n.config(\"spark.kryo.registrator\", classOf[SedonaVizKryoRegistrator].getName) // org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator\n.getOrCreate()\n</code></pre>"},{"location":"tutorial/viz/#initiate-sedonacontext","title":"Initiate SedonaContext","text":"<p>Add the following line after creating Sedona config. If you already have a SparkSession (usually named <code>spark</code>) created by Wherobots/AWS EMR/Databricks, please call <code>SedonaContext.create(spark)</code> instead.</p> <p>Sedona &gt;= 1.4.1=</p> <pre><code>val sedona = SedonaContext.create(config)\nSedonaVizRegistrator.registerAll(sedona)\n</code></pre> <p>Sedona &lt;1.4.1</p> <p>The following method has been deprecated since Sedona 1.4.1. Please use the method above to create your SedonaContext.</p> <pre><code>SedonaSQLRegistrator.registerAll(sparkSession)\nSedonaVizRegistrator.registerAll(sparkSession)\n</code></pre> <p>You can also register everything by passing <code>--conf spark.sql.extensions=org.apache.sedona.viz.sql.SedonaVizExtensions,org.apache.sedona.sql.SedonaSqlExtensions</code> to <code>spark-submit</code> or <code>spark-shell</code>.</p>"},{"location":"tutorial/viz/#create-spatial-dataframe","title":"Create Spatial DataFrame","text":"<p>There is a DataFrame as follows:</p> <pre><code>+----------+---------+\n|       _c0|      _c1|\n+----------+---------+\n|-88.331492|32.324142|\n|-88.175933|32.360763|\n|-88.388954|32.357073|\n|-88.221102| 32.35078|\n</code></pre> <p>You first need to create a Geometry type column.</p> <pre><code>CREATE OR REPLACE TEMP VIEW pointtable AS\nSELECT ST_Point(cast(pointtable._c0 as Decimal(24,20)),cast(pointtable._c1 as Decimal(24,20))) as shape\nFROM pointtable\n</code></pre> <p>As you know, Sedona provides many different methods to load various spatial data formats. Please read Write an Spatial DataFrame application.</p>"},{"location":"tutorial/viz/#generate-a-single-image","title":"Generate a single image","text":"<p>In most cases, you just want to see a single image out of your spatial dataset.</p>"},{"location":"tutorial/viz/#pixelize-spatial-objects","title":"Pixelize spatial objects","text":"<p>To put spatial objects on a map image, you first need to convert them to pixels.</p> <p>First, compute the spatial boundary of this column.</p> <pre><code>CREATE OR REPLACE TEMP VIEW boundtable AS\nSELECT ST_Envelope_Aggr(shape) as bound FROM pointtable\n</code></pre> <p>Then use ST_Pixelize to convert them to pixels.</p> <p>This example is for Sedona before v1.0.1. ST_Pixelize extends Generator so it can directly flatten the array without the explode function.</p> <pre><code>CREATE OR REPLACE TEMP VIEW pixels AS\nSELECT pixel, shape FROM pointtable\nLATERAL VIEW ST_Pixelize(ST_Transform(shape, 'epsg:4326','epsg:3857'), 256, 256, (SELECT ST_Transform(bound, 'epsg:4326','epsg:3857') FROM boundtable)) AS pixel\n</code></pre> <p>This example is for Sedona on and after v1.0.1. ST_Pixelize returns an array of pixels. You need to use explode to flatten it.</p> <pre><code>CREATE OR REPLACE TEMP VIEW pixels AS\nSELECT pixel, shape FROM pointtable\nLATERAL VIEW explode(ST_Pixelize(ST_Transform(shape, 'epsg:4326','epsg:3857'), 256, 256, (SELECT ST_Transform(bound, 'epsg:4326','epsg:3857') FROM boundtable))) AS pixel\n</code></pre> <p>This will give you a 256*256 resolution image after you run ST_Render at the end of this tutorial.</p> <p>Warning</p> <p>We highly suggest that you should use ST_Transform to transform coordinates to a visualization-specific coordinate system such as epsg:3857. Otherwise you map may look distorted.</p>"},{"location":"tutorial/viz/#aggregate-pixels","title":"Aggregate pixels","text":"<p>Many objects may be pixelized to the same pixel locations. You now need to aggregate them based on either their spatial aggregation or spatial observations such as temperature or humidity.</p> <pre><code>CREATE OR REPLACE TEMP VIEW pixelaggregates AS\nSELECT pixel, count(*) as weight\nFROM pixels\nGROUP BY pixel\n</code></pre> <p>The weight indicates the degree of spatial aggregation or spatial observations. Later on, it will determine the color of this pixel.</p>"},{"location":"tutorial/viz/#colorize-pixels","title":"Colorize pixels","text":"<p>Run the following command to assign colors for pixels based on their weights.</p> <pre><code>CREATE OR REPLACE TEMP VIEW pixelaggregates AS\nSELECT pixel, ST_Colorize(weight, (SELECT max(weight) FROM pixelaggregates)) as color\nFROM pixelaggregates\n</code></pre> <p>Please read ST_Colorize for a detailed API description.</p>"},{"location":"tutorial/viz/#render-the-image","title":"Render the image","text":"<p>Use ST_Render to plot all pixels on a single image.</p> <pre><code>CREATE OR REPLACE TEMP VIEW images AS\nSELECT ST_Render(pixel, color) AS image, (SELECT ST_AsText(bound) FROM boundtable) AS boundary\nFROM pixelaggregates\n</code></pre> <p>This DataFrame will contain a Image type column which has only one image.</p>"},{"location":"tutorial/viz/#store-the-image-on-disk","title":"Store the image on disk","text":"<p>Fetch the image from the previous DataFrame</p> <pre><code>var image = sedona.table(\"images\").take(1)(0)(0).asInstanceOf[ImageSerializableWrapper].getImage\n</code></pre> <p>Use Sedona Viz ImageGenerator to store this image on disk.</p> <pre><code>var imageGenerator = new ImageGenerator\nimageGenerator.SaveRasterImageAsLocalFile(image, System.getProperty(\"user.dir\")+\"/target/points\", ImageType.PNG)\n</code></pre>"},{"location":"tutorial/viz/#generate-map-tiles","title":"Generate map tiles","text":"<p>If you are a map professional, you may need to generate map tiles for different zoom levels and eventually create the map tile layer.</p>"},{"location":"tutorial/viz/#pixelization-and-pixel-aggregation","title":"Pixelization and pixel aggregation","text":"<p>Please first do pixelization and pixel aggregation using the same commands in single image generation. In ST_Pixelize, you need specify a very high resolution, such as 1000*1000. Note that, each dimension should be divisible by 2^zoom-level</p>"},{"location":"tutorial/viz/#create-tile-name","title":"Create tile name","text":"<p>Run the following command to compute the tile name for every pixels</p> <pre><code>CREATE OR REPLACE TEMP VIEW pixelaggregates AS\nSELECT pixel, weight, ST_TileName(pixel, 3) AS pid\nFROM pixelaggregates\n</code></pre> <p>\"3\" is the zoom level for these map tiles.</p>"},{"location":"tutorial/viz/#colorize-pixels_1","title":"Colorize pixels","text":"<p>Use the same command explained in single image generation to assign colors.</p>"},{"location":"tutorial/viz/#render-map-tiles","title":"Render map tiles","text":"<p>You now need to group pixels by tiles and then render map tile images in parallel.</p> <pre><code>CREATE OR REPLACE TEMP VIEW images AS\nSELECT ST_Render(pixel, color, 3) AS image\nFROM pixelaggregates\nGROUP BY pid\n</code></pre> <p>\"3\" is the zoom level for these map tiles.</p>"},{"location":"tutorial/viz/#store-map-tiles-on-disk","title":"Store map tiles on disk","text":"<p>You can use the same commands in single image generation to fetch all map tiles and store them one by one.</p>"},{"location":"tutorial/zeppelin/","title":"Use Apache Zeppelin","text":"<p>Sedona provides a Helium visualization plugin tailored for Apache Zeppelin. This finally bridges the gap between Sedona and Zeppelin.  Please read Install Sedona-Zeppelin to learn how to install this plugin in Zeppelin.</p> <p>Sedona-Zeppelin equips two approaches to visualize spatial data in Zeppelin. The first approach uses Zeppelin to plot all spatial objects on the map. The second one leverages SedonaViz to generate map images and overlay them on maps.</p>"},{"location":"tutorial/zeppelin/#small-scale-without-sedonaviz","title":"Small-scale without SedonaViz","text":"<p>Danger</p> <p>Zeppelin is just a front-end visualization framework. This approach is not scalable and will fail at large-scale geospatial data. Please scroll down to read SedonaViz solution.</p> <p>You can use Apache Zeppelin to plot a small number of spatial objects, such as 1000 points. Assume you already have a Spatial DataFrame, you need to convert the geometry column to WKT string column use the following command in your Zeppelin Spark notebook Scala paragraph:</p> <pre><code>spark.sql(\n\"\"\"\n    |CREATE OR REPLACE TEMP VIEW wktpoint AS\n    |SELECT ST_AsText(shape) as geom\n    |FROM pointtable\n  \"\"\".stripMargin)\n</code></pre> <p>Then create an SQL paragraph to fetch the data <pre><code>%sql\nSELECT *\nFROM wktpoint\n</code></pre></p> <p>Select the geometry column to visualize:</p> <p></p>"},{"location":"tutorial/zeppelin/#large-scale-with-sedonaviz","title":"Large-scale with SedonaViz","text":"<p>SedonaViz is a distributed visualization system that allows you to visualize big spatial data at scale. Please read How to use SedonaViz.</p> <p>You can use Sedona-Zeppelin to ask Zeppelin to overlay SedonaViz images on a map background. This way, you can easily visualize 1 billion spatial objects or more (depends on your cluster size).</p> <p>First, encode images of SedonaViz DataFrame in Zeppelin Spark notebook Scala paragraph,</p> <pre><code>spark.sql(\n  \"\"\"\n    |CREATE OR REPLACE TEMP VIEW images AS\n    |SELECT ST_EncodeImage(image) AS image, (SELECT ST_AsText(bound) FROM boundtable) AS boundary\n    |FROM images\n  \"\"\".stripMargin)\n</code></pre> <p>Then create an SQL paragraph to fetch the data <pre><code>%sql\nSELECT *, 'I am the map center!'\nFROM images\n</code></pre></p> <p>Select the image and its geospatial boundary:</p> <p></p>"},{"location":"tutorial/zeppelin/#zeppelin-spark-notebook-demo","title":"Zeppelin Spark notebook demo","text":"<p>We provide a full Zeppelin Spark notebook which demonstrates al functions. Please download Sedona-Zeppelin notebook template and test data - arealm.csv.</p> <p>You need to use Zeppelin to import this notebook JSON file and modify the input data path in the notebook.</p>"},{"location":"tutorial/flink/sql/","title":"Spatial SQL app","text":"<p>The page outlines the steps to manage spatial data using SedonaSQL. The example code is written in Java but also works for Scala.</p> <p>SedonaSQL supports SQL/MM Part3 Spatial SQL Standard. It includes four kinds of SQL operators as follows. All these operators can be directly called through: <pre><code>Table myTable = tableEnv.sqlQuery(\"YOUR_SQL\")\n</code></pre></p> <p>Detailed SedonaSQL APIs are available here: SedonaSQL API</p>"},{"location":"tutorial/flink/sql/#set-up-dependencies","title":"Set up dependencies","text":"<ol> <li>Read Sedona Maven Central coordinates</li> <li>Add Sedona dependencies in build.sbt or pom.xml.</li> <li>Add Flink dependencies in build.sbt or pom.xml.</li> <li>Please see SQL example project</li> </ol>"},{"location":"tutorial/flink/sql/#initiate-stream-environment","title":"Initiate Stream Environment","text":"<p>Use the following code to initiate your <code>StreamExecutionEnvironment</code> at the beginning: <pre><code>StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\nEnvironmentSettings settings = EnvironmentSettings.newInstance().inStreamingMode().build();\nStreamTableEnvironment tableEnv = StreamTableEnvironment.create(env, settings);\n</code></pre></p>"},{"location":"tutorial/flink/sql/#initiate-sedonacontext","title":"Initiate SedonaContext","text":"<p>Add the following line after your <code>StreamExecutionEnvironment</code> and <code>StreamTableEnvironment</code> declaration</p> <p>Sedona &gt;= 1.4.1</p> <pre><code>StreamTableEnvironment sedona = SedonaContext.create(env, tableEnv);\n</code></pre> <p>Sedona &lt;1.4.1</p> <p>The following method has been deprecated since Sedona 1.4.1. Please use the method above to create your SedonaContext.</p> <pre><code>SedonaFlinkRegistrator.registerType(env);\nSedonaFlinkRegistrator.registerFunc(tableEnv);\n</code></pre> <p>Warning</p> <p>Sedona has a suite of well-written geometry and index serializers. Forgetting to enable these serializers will lead to high memory consumption.</p> <p>This function will register Sedona User Defined Type and User Defined Function</p>"},{"location":"tutorial/flink/sql/#create-a-geometry-type-column","title":"Create a Geometry type column","text":"<p>All geometrical operations in SedonaSQL are on Geometry type objects. Therefore, before any kind of queries, you need to create a Geometry type column on a DataFrame.</p> <p>Assume you have a Flink Table <code>tbl</code> like this:</p> <pre><code>+----+--------------------------------+--------------------------------+\n| op |                   geom_polygon |                   name_polygon |\n+----+--------------------------------+--------------------------------+\n| +I | POLYGON ((-0.5 -0.5, -0.5 0... |                       polygon0 |\n| +I | POLYGON ((0.5 0.5, 0.5 1.5,... |                       polygon1 |\n| +I | POLYGON ((1.5 1.5, 1.5 2.5,... |                       polygon2 |\n| +I | POLYGON ((2.5 2.5, 2.5 3.5,... |                       polygon3 |\n| +I | POLYGON ((3.5 3.5, 3.5 4.5,... |                       polygon4 |\n| +I | POLYGON ((4.5 4.5, 4.5 5.5,... |                       polygon5 |\n| +I | POLYGON ((5.5 5.5, 5.5 6.5,... |                       polygon6 |\n| +I | POLYGON ((6.5 6.5, 6.5 7.5,... |                       polygon7 |\n| +I | POLYGON ((7.5 7.5, 7.5 8.5,... |                       polygon8 |\n| +I | POLYGON ((8.5 8.5, 8.5 9.5,... |                       polygon9 |\n+----+--------------------------------+--------------------------------+\n10 rows in set\n</code></pre> <p>You can create a Table with a Geometry type column as follows:</p> <pre><code>sedona.createTemporaryView(\"myTable\", tbl)\nTable geomTbl = sedona.sqlQuery(\"SELECT ST_GeomFromWKT(geom_polygon) as geom_polygon, name_polygon FROM myTable\")\ngeomTbl.execute().print()\n</code></pre> <p>The output will be:</p> <pre><code>+----+--------------------------------+--------------------------------+\n| op |                   geom_polygon |                   name_polygon |\n+----+--------------------------------+--------------------------------+\n| +I | POLYGON ((-0.5 -0.5, -0.5 0... |                       polygon0 |\n| +I | POLYGON ((0.5 0.5, 0.5 1.5,... |                       polygon1 |\n| +I | POLYGON ((1.5 1.5, 1.5 2.5,... |                       polygon2 |\n| +I | POLYGON ((2.5 2.5, 2.5 3.5,... |                       polygon3 |\n| +I | POLYGON ((3.5 3.5, 3.5 4.5,... |                       polygon4 |\n| +I | POLYGON ((4.5 4.5, 4.5 5.5,... |                       polygon5 |\n| +I | POLYGON ((5.5 5.5, 5.5 6.5,... |                       polygon6 |\n| +I | POLYGON ((6.5 6.5, 6.5 7.5,... |                       polygon7 |\n| +I | POLYGON ((7.5 7.5, 7.5 8.5,... |                       polygon8 |\n| +I | POLYGON ((8.5 8.5, 8.5 9.5,... |                       polygon9 |\n+----+--------------------------------+--------------------------------+\n10 rows in set\n</code></pre> <p>Although it looks same with the input, actually the type of column geom_polygon has been changed to Geometry type.</p> <p>To verify this, use the following code to print the schema of the DataFrame:</p> <pre><code>geomTbl.printSchema()\n</code></pre> <p>The output will be like this:</p> <pre><code>(\n  `geom_polygon` RAW('org.locationtech.jts.geom.Geometry', '...'),\n  `name_polygon` STRING\n)\n</code></pre> <p>Note</p> <p>SedonaSQL provides lots of functions to create a Geometry column, please read SedonaSQL constructor API.</p>"},{"location":"tutorial/flink/sql/#transform-the-coordinate-reference-system","title":"Transform the Coordinate Reference System","text":"<p>Sedona doesn't control the coordinate unit (degree-based or meter-based) of all geometries in a Geometry column. The unit of all related distances in SedonaSQL is same as the unit of all geometries in a Geometry column.</p> <p>To convert Coordinate Reference System of the Geometry column created before, use the following code:</p> <pre><code>Table geomTbl3857 = sedona.sqlQuery(\"SELECT ST_Transform(countyshape, \"epsg:4326\", \"epsg:3857\") AS geom_polygon, name_polygon FROM myTable\")\ngeomTbl3857.execute().print()\n</code></pre> <p>The first EPSG code EPSG:4326 in <code>ST_Transform</code> is the source CRS of the geometries. It is WGS84, the most common degree-based CRS.</p> <p>The second EPSG code EPSG:3857 in <code>ST_Transform</code> is the target CRS of the geometries. It is the most common meter-based CRS.</p> <p>This <code>ST_Transform</code> transform the CRS of these geometries from EPSG:4326 to EPSG:3857. The details CRS information can be found on EPSG.io</p> <p>Note</p> <p>Read SedonaSQL ST_Transform API to learn different spatial query predicates.</p> <p>For example, a Table that has coordinates in the US will become like this.</p> <p>Before the transformation: <pre><code>+----+--------------------------------+--------------------------------+\n| op |                     geom_point |                     name_point |\n+----+--------------------------------+--------------------------------+\n| +I |                POINT (32 -118) |                          point |\n| +I |                POINT (33 -117) |                          point |\n| +I |                POINT (34 -116) |                          point |\n| +I |                POINT (35 -115) |                          point |\n| +I |                POINT (36 -114) |                          point |\n| +I |                POINT (37 -113) |                          point |\n| +I |                POINT (38 -112) |                          point |\n| +I |                POINT (39 -111) |                          point |\n| +I |                POINT (40 -110) |                          point |\n| +I |                POINT (41 -109) |                          point |\n+----+--------------------------------+--------------------------------+\n</code></pre></p> <p>After the transformation:</p> <pre><code>+----+--------------------------------+--------------------------------+\n| op |                            _c0 |                     name_point |\n+----+--------------------------------+--------------------------------+\n| +I | POINT (-13135699.91360628 3... |                          point |\n| +I | POINT (-13024380.422813008 ... |                          point |\n| +I | POINT (-12913060.932019735 ... |                          point |\n| +I | POINT (-12801741.44122646 4... |                          point |\n| +I | POINT (-12690421.950433187 ... |                          point |\n| +I | POINT (-12579102.459639912 ... |                          point |\n| +I | POINT (-12467782.96884664 4... |                          point |\n| +I | POINT (-12356463.478053367 ... |                          point |\n| +I | POINT (-12245143.987260092 ... |                          point |\n| +I | POINT (-12133824.496466817 ... |                          point |\n+----+--------------------------------+--------------------------------+\n</code></pre> <p>After creating a Geometry type column, you are able to run spatial queries.</p>"},{"location":"tutorial/flink/sql/#range-query","title":"Range query","text":"<p>Use ST_Contains, ST_Intersects and so on to run a range query over a single column.</p> <p>The following example finds all counties that are within the given polygon:</p> <pre><code>geomTable = sedona.sqlQuery(\n\"\n    SELECT *\n    FROM spatialdf\n    WHERE ST_Contains (ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), newcountyshape)\n  \")\ngeomTable.execute().print()\n</code></pre> <p>Note</p> <p>Read SedonaSQL Predicate API to learn different spatial query predicates.</p>"},{"location":"tutorial/flink/sql/#knn-query","title":"KNN query","text":"<p>Use ST_Distance to calculate the distance and rank the distance.</p> <p>The following code returns the 5 nearest neighbor of the given polygon.</p> <pre><code>geomTable = sedona.sqlQuery(\n\"\n    SELECT countyname, ST_Distance(ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), newcountyshape) AS distance\n    FROM geomTable\n    ORDER BY distance DESC\n    LIMIT 5\n  \")\ngeomTable.execute().print()\n</code></pre>"},{"location":"tutorial/flink/sql/#join-query","title":"Join query","text":"<p>This equi-join leverages Flink's internal equi-join algorithm. You can opt to skip the Sedona refinement step  by sacrificing query accuracy. A running example is in SQL example project.</p> <p>Please use the following steps:</p>"},{"location":"tutorial/flink/sql/#1-generate-s2-ids-for-both-tables","title":"1. Generate S2 ids for both tables","text":"<p>Use ST_S2CellIds to generate cell IDs. Each geometry may produce one or more IDs.</p> <pre><code>SELECT id, geom, name, ST_S2CellIDs(geom, 15) as idarray\nFROM lefts\n</code></pre> <pre><code>SELECT id, geom, name, ST_S2CellIDs(geom, 15) as idarray\nFROM rights\n</code></pre>"},{"location":"tutorial/flink/sql/#2-explode-id-array","title":"2. Explode id array","text":"<p>The produced S2 ids are arrays of integers. We need to explode these Ids to multiple rows so later we can join two tables by ids.</p> <pre><code>SELECT id, geom, name, cellId\nFROM lefts CROSS JOIN UNNEST(lefts.idarray) AS tmpTbl1(cellId)\n</code></pre> <pre><code>SELECT id, geom, name, cellId\nFROM rights CROSS JOIN UNNEST(rights.idarray) AS tmpTbl2(cellId)\n</code></pre>"},{"location":"tutorial/flink/sql/#3-perform-equi-join","title":"3. Perform equi-join","text":"<p>Join the two tables by their S2 cellId</p> <pre><code>SELECT lcs.id as lcs_id, lcs.geom as lcs_geom, lcs.name as lcs_name, rcs.id as rcs_id, rcs.geom as rcs_geom, rcs.name as rcs_name\nFROM lcs JOIN rcs ON lcs.cellId = rcs.cellId\n</code></pre>"},{"location":"tutorial/flink/sql/#4-optional-refine-the-result","title":"4. Optional: Refine the result","text":"<p>Due to the nature of S2 Cellid, the equi-join results might have a few false-positives depending on the S2 level you choose. A smaller level indicates bigger cells, less exploded rows, but more false positives.</p> <p>To ensure the correctness, you can use one of the Spatial Predicates to filter out them. Use this query as the query in Step 3.</p> <pre><code>SELECT lcs.id as lcs_id, lcs.geom as lcs_geom, lcs.name as lcs_name, rcs.id as rcs_id, rcs.geom as rcs_geom, rcs.name as rcs_name\nFROM lcs, rcs\nWHERE lcs.cellId = rcs.cellId AND ST_Contains(lcs.geom, rcs.geom)\n</code></pre> <p>As you see, compared to the query in Step 2, we added one more filter, which is <code>ST_Contains</code>, to remove false positives. You can also use <code>ST_Intersects</code> and so on.</p> <p>Tip</p> <p>You can skip this step if you don't need 100% accuracy and want faster query speed.</p>"},{"location":"tutorial/flink/sql/#5-optional-de-duplcate","title":"5. Optional: De-duplcate","text":"<p>Due to the explode function used when we generate S2 Cell Ids, the resulting DataFrame may have several duplicate  matches. You can remove them by performing a GroupBy query. <pre><code>SELECT lcs_id, rcs_id, FIRST_VALUE(lcs_geom), FIRST_VALUE(lcs_name), first(rcs_geom), first(rcs_name)\nFROM joinresult\nGROUP BY (lcs_id, rcs_id)\n</code></pre> <p>The <code>FIRST_VALUE</code> function is to take the first value from a number of duplicate values.</p> <p>If you don't have a unique id for each geometry, you can also group by geometry itself. See below:</p> <pre><code>SELECT lcs_geom, rcs_geom, first(lcs_name), first(rcs_name)\nFROM joinresult\nGROUP BY (lcs_geom, rcs_geom)\n</code></pre> <p>Note</p> <p>If you are doing point-in-polygon join, this is not a problem and you can safely discard this issue. This issue only happens when you do polygon-polygon, polygon-linestring, linestring-linestring join.</p>"},{"location":"tutorial/flink/sql/#s2-for-distance-join","title":"S2 for distance join","text":"<p>This also works for distance join. You first need to use <code>ST_Buffer(geometry, distance)</code> to wrap one of your original geometry column. If your original geometry column contains points, this <code>ST_Buffer</code> will make them become circles with a radius of <code>distance</code>.</p> <p>For example. run this query first on the left table before Step 1.</p> <pre><code>SELECT id, ST_Buffer(geom, DISTANCE), name\nFROM lefts\n</code></pre> <p>Since the coordinates are in the longitude and latitude system, so the unit of <code>distance</code> should be degree instead of meter or mile. You will have to estimate the corresponding degrees based on your meter values. Please use this calculator.</p>"},{"location":"tutorial/flink/sql/#convert-spatial-table-to-spatial-datastream","title":"Convert Spatial Table to Spatial DataStream","text":""},{"location":"tutorial/flink/sql/#get-datastream","title":"Get DataStream","text":"<p>Use TableEnv's toDataStream function</p> <pre><code>DataStream&lt;Row&gt; geomStream = sedona.toDataStream(geomTable)\n</code></pre>"},{"location":"tutorial/flink/sql/#retrieve-geometries","title":"Retrieve Geometries","text":"<p>Then get the Geometry from each Row object using Map</p> <pre><code>import org.locationtech.jts.geom.Geometry;\n\nDataStream&lt;Geometry&gt; geometries = geomStream.map(new MapFunction&lt;Row, Geometry&gt;() {\n@Override\npublic Geometry map(Row value) throws Exception {\nreturn (Geometry) value.getField(0);\n}\n});\ngeometries.print();\n</code></pre> <p>The output will be</p> <pre><code>14&gt; POLYGON ((1.5 1.5, 1.5 2.5, 2.5 2.5, 2.5 1.5, 1.5 1.5))\n2&gt; POLYGON ((5.5 5.5, 5.5 6.5, 6.5 6.5, 6.5 5.5, 5.5 5.5))\n5&gt; POLYGON ((8.5 8.5, 8.5 9.5, 9.5 9.5, 9.5 8.5, 8.5 8.5))\n16&gt; POLYGON ((3.5 3.5, 3.5 4.5, 4.5 4.5, 4.5 3.5, 3.5 3.5))\n12&gt; POLYGON ((-0.5 -0.5, -0.5 0.5, 0.5 0.5, 0.5 -0.5, -0.5 -0.5))\n13&gt; POLYGON ((0.5 0.5, 0.5 1.5, 1.5 1.5, 1.5 0.5, 0.5 0.5))\n15&gt; POLYGON ((2.5 2.5, 2.5 3.5, 3.5 3.5, 3.5 2.5, 2.5 2.5))\n3&gt; POLYGON ((6.5 6.5, 6.5 7.5, 7.5 7.5, 7.5 6.5, 6.5 6.5))\n1&gt; POLYGON ((4.5 4.5, 4.5 5.5, 5.5 5.5, 5.5 4.5, 4.5 4.5))\n4&gt; POLYGON ((7.5 7.5, 7.5 8.5, 8.5 8.5, 8.5 7.5, 7.5 7.5))\n</code></pre>"},{"location":"tutorial/flink/sql/#store-non-spatial-attributes-in-geometries","title":"Store non-spatial attributes in Geometries","text":"<p>You can concatenate other non-spatial attributes and store them in Geometry's <code>userData</code> field so you can recover them later on. <code>userData</code> field can be any object type.</p> <pre><code>import org.locationtech.jts.geom.Geometry;\n\nDataStream&lt;Geometry&gt; geometries = geomStream.map(new MapFunction&lt;Row, Geometry&gt;() {\n@Override\npublic Geometry map(Row value) throws Exception {\nGeometry geom = (Geometry) value.getField(0);\ngeom.setUserData(value.getField(1));\nreturn geom;\n}\n});\ngeometries.print();\n</code></pre> <p>The <code>print</code> command will not print out <code>userData</code> field. But you can get it this way:</p> <pre><code>import org.locationtech.jts.geom.Geometry;\n\ngeometries.map(new MapFunction&lt;Geometry, String&gt;() {\n@Override\npublic String map(Geometry value) throws Exception\n{\nreturn (String) value.getUserData();\n}\n}).print();\n</code></pre> <p>The output will be</p> <pre><code>13&gt; polygon9\n6&gt; polygon2\n10&gt; polygon6\n11&gt; polygon7\n5&gt; polygon1\n12&gt; polygon8\n8&gt; polygon4\n4&gt; polygon0\n7&gt; polygon3\n9&gt; polygon5\n</code></pre>"},{"location":"tutorial/flink/sql/#convert-spatial-datastream-to-spatial-table","title":"Convert Spatial DataStream to Spatial Table","text":""},{"location":"tutorial/flink/sql/#create-geometries-using-sedona-formatutils","title":"Create Geometries using Sedona FormatUtils","text":"<ul> <li>Create a Geometry from a WKT string</li> </ul> <pre><code>import org.apache.sedona.common.utils.FormatUtils;\nimport org.locationtech.jts.geom.Geometry;\n\nDataStream&lt;Geometry&gt; geometries = text.map(new MapFunction&lt;String, Geometry&gt;() {\n@Override\npublic Geometry map(String value) throws Exception\n{\nFormatUtils formatUtils = new FormatUtils(FileDataSplitter.WKT, false);\nreturn formatUtils.readGeometry(value);\n}\n})\n</code></pre> <ul> <li>Create a Point from a String <code>1.1, 2.2</code>. Use <code>,</code> as the delimiter.</li> </ul> <pre><code>import org.apache.sedona.common.utils.FormatUtils;\nimport org.locationtech.jts.geom.Geometry;\n\nDataStream&lt;Geometry&gt; geometries = text.map(new MapFunction&lt;String, Geometry&gt;() {\n@Override\npublic Geometry map(String value) throws Exception\n{\nFormatUtils&lt;Geometry&gt; formatUtils = new FormatUtils(\",\", false, GeometryType.POINT);\nreturn formatUtils.readGeometry(value);\n}\n})\n</code></pre> <ul> <li>Create a Polygon from a String <code>1.1, 1.1, 10.1, 10.1</code>. This is a rectangle with (1.1, 1.1) and (10.1, 10.1) as their min/max corners.</li> </ul> <pre><code>import org.apache.sedona.common.utils.FormatUtils;\nimport org.locationtech.jts.geom.GeometryFactory;\nimport org.locationtech.jts.geom.Geometry;\n\nDataStream&lt;Geometry&gt; geometries = text.map(new MapFunction&lt;String, Geometry&gt;() {\n@Override\npublic Geometry map(String value) throws Exception\n{\n// Write some code to get four double type values: minX, minY, maxX, maxY\n...\nCoordinate[] coordinates = new Coordinate[5];\ncoordinates[0] = new Coordinate(minX, minY);\ncoordinates[1] = new Coordinate(minX, maxY);\ncoordinates[2] = new Coordinate(maxX, maxY);\ncoordinates[3] = new Coordinate(maxX, minY);\ncoordinates[4] = coordinates[0];\nGeometryFactory geometryFactory = new GeometryFactory();\nreturn geometryFactory.createPolygon(coordinates);\n}\n})\n</code></pre>"},{"location":"tutorial/flink/sql/#create-row-objects","title":"Create Row objects","text":"<p>Put a geometry in a Flink Row to a <code>geomStream</code>. Note that you can put other attributes in Row as well. This example uses a constant value <code>myName</code> for all geometries.</p> <pre><code>import org.apache.sedona.common.utils.FormatUtils;\nimport org.locationtech.jts.geom.Geometry;\nimport org.apache.flink.types.Row;\n\nDataStream&lt;Row&gt; geomStream = text.map(new MapFunction&lt;String, Row&gt;() {\n@Override\npublic Row map(String value) throws Exception\n{\nFormatUtils formatUtils = new FormatUtils(FileDataSplitter.WKT, false);\nreturn Row.of(formatUtils.readGeometry(value), \"myName\");\n}\n})\n</code></pre>"},{"location":"tutorial/flink/sql/#get-spatial-table","title":"Get Spatial Table","text":"<p>Use TableEnv's fromDataStream function, with two column names <code>geom</code> and <code>geom_name</code>. <pre><code>Table geomTable = sedona.fromDataStream(geomStream, \"geom\", \"geom_name\")\n</code></pre></p>"},{"location":"tutorial/snowflake/sql/","title":"Spatial SQL app","text":"<p>After the installation done, you can start using Sedona functions. Please log in to Snowflake again using the user that has the privilege to access the database.</p> <p>Note</p> <p>Please always keep the schema name <code>SEDONA</code> (e.g., <code>SEDONA.ST_GeomFromWKT</code>) when you use Sedona functions to avoid conflicting with Snowflake's built-in functions.</p>"},{"location":"tutorial/snowflake/sql/#create-a-sample-table","title":"Create a sample table","text":"<p>Let's create a <code>city_tbl</code> that contains the locations and names of cities. Each location is a WKT string.</p> <pre><code>CREATE TABLE city_tbl (wkt STRING, city_name STRING);\nINSERT INTO city_tbl(wkt, city_name) VALUES ('POINT (-122.33 47.61)', 'Seattle');\nINSERT INTO city_tbl(wkt, city_name) VALUES ('POINT (-122.42 37.76)', 'San Francisco');\n</code></pre> <p>Then we can show the content of this table:</p> <pre><code>SELECT *\nFROM city_tbl;\n</code></pre> <p>Output: <pre><code>WKT CITY_NAME\nPOINT (-122.33 47.61)   Seattle\nPOINT (-122.42 37.76)   San Francisco\n</code></pre></p>"},{"location":"tutorial/snowflake/sql/#create-a-geometry-column","title":"Create a Geometry column","text":"<p>All geometrical operations in SedonaSQL are on Geometry type objects. Therefore, before any kind of queries, you need to create a Geometry type column on the table.</p> <pre><code>CREATE TABLE city_tbl_geom AS\nSELECT Sedona.ST_GeomFromWKT(wkt) AS geom, city_name\nFROM city_tbl\n</code></pre> <p>The <code>geom</code> column Table <code>city_tbl_geom</code> is now in a <code>Binary</code> type and data in this column is in a format that can be understood by Sedona. The output of this query will show geometries in WKB binary format like this:</p> <pre><code>GEOM CITY_NAME\n010100000085eb51b81e955ec0ae47e17a14ce4740  Seattle\n01010000007b14ae47e19a5ec0e17a14ae47e14240  San Francisco\n</code></pre> <p>To view the content of this column in a human-readable format, you can use <code>ST_AsText</code>. For example,</p> <pre><code>SELECT Sedona.ST_AsText(geom), city_name\nFROM city_tbl_geom\n</code></pre> <p>Note</p> <p>SedonaSQL provides lots of functions to create a Geometry column, please read SedonaSQL API.</p>"},{"location":"tutorial/snowflake/sql/#check-the-lonlat-order","title":"Check the lon/lat order","text":"<p>In SedonaSnow <code>v1.4.1</code> and before, we use lat/lon order in the following functions:</p> <ul> <li>ST_Transform</li> <li>ST_DistanceSphere</li> <li>ST_DistanceSpheroid</li> </ul> <p>We use <code>lon/lat</code> order in the following functions:</p> <ul> <li>ST_GeomFromGeoHash</li> <li>ST_GeoHash</li> <li>ST_S2CellIDs</li> </ul> <p>In Sedona <code>v1.5.0</code> and above, all functions will be fixed to lon/lat order.</p> <p>If your original data is not in the order you want, you need to flip the coordinate using <code>ST_FlipCoordinates(geom: Geometry)</code>.</p> <p>The sample data used above is in lon/lat order, we can flip the coordinates as follows:</p> <pre><code>CREATE OR REPLACE TABLE city_tbl_geom AS\nSELECT Sedona.ST_FlipCoordinates(geom) AS geom, city_name\nFROM city_tbl_geom\n</code></pre> <p>If we show the content of this table, it is now in lat/lon order:</p> <pre><code>SELECT Sedona.ST_AsText(geom), city_name\nFROM city_tbl_geom\n</code></pre> <p>Output:</p> <pre><code>GEOM    CITY_NAME\nPOINT (47.61 -122.33)   Seattle\nPOINT (37.76 -122.42)   San Francisco\n</code></pre>"},{"location":"tutorial/snowflake/sql/#save-as-an-ordinary-column","title":"Save as an ordinary column","text":"<p>To save a table to some permanent storage, you can simply convert each geometry in the Geometry type column back to a plain String and save it anywhere you want.</p> <p>Use the following code to convert the Geometry column in a table back to a WKT string column:</p> <pre><code>SELECT ST_AsText(geom)\nFROM city_tbl_geom\n</code></pre> <p>Note</p> <p>SedonaSQL provides lots of functions to save the Geometry column, please read SedonaSQL API.</p>"},{"location":"tutorial/snowflake/sql/#transform-the-coordinate-reference-system","title":"Transform the Coordinate Reference System","text":"<p>Sedona doesn't control the coordinate unit (degree-based or meter-based) of all geometries in a Geometry column. The unit of all related distances in SedonaSQL is same as the unit of all geometries in a Geometry column.</p> <p>To convert Coordinate Reference System of the Geometry column created before, use <code>ST_Transform (A:geometry, SourceCRS:string, TargetCRS:string</code></p> <p>The first EPSG code EPSG:4326 in <code>ST_Transform</code> is the source CRS of the geometries. It is WGS84, the most common degree-based CRS.</p> <p>The second EPSG code EPSG:3857 in <code>ST_Transform</code> is the target CRS of the geometries. It is the most common meter-based CRS.</p> <p>This <code>ST_Transform</code> transform the CRS of these geometries from EPSG:4326 to EPSG:3857. The details CRS information can be found on EPSG.io.</p> <p>Note</p> <p>This function follows lon/order in 1.5.0+ and lat/lon order in 1.4.1 and before. You can use <code>ST_FlipCoordinates</code> to swap X and Y.</p> <p>We can transform our sample data as follows</p> <pre><code>SELECT Sedona.ST_AsText(Sedona.ST_Transform(geom, 'epsg:4326', 'epsg:3857')), city_name\nFROM city_tbl_geom\n</code></pre> <p>The output will be like this:</p> <pre><code>POINT (6042216.250411431 -13617713.308741156)  Seattle\nPOINT (4545577.120361927 -13627732.06291255)  San Francisco\n</code></pre> <p><code>ST_Transform</code> also supports the CRS string in OGC WKT format. For example, the following query generates the same output but with a OGC WKT CRS string.</p> <pre><code>SELECT Sedona.ST_AsText(Sedona.ST_Transform(geom, 'epsg:4326', 'PROJCS[\"WGS 84 / Pseudo-Mercator\",\n     GEOGCS[\"WGS 84\",\n         DATUM[\"WGS_1984\",\n             SPHEROID[\"WGS 84\",6378137,298.257223563,\n                 AUTHORITY[\"EPSG\",\"7030\"]],\n             AUTHORITY[\"EPSG\",\"6326\"]],\n         PRIMEM[\"Greenwich\",0,\n             AUTHORITY[\"EPSG\",\"8901\"]],\n         UNIT[\"degree\",0.0174532925199433,\n             AUTHORITY[\"EPSG\",\"9122\"]],\n         AUTHORITY[\"EPSG\",\"4326\"]],\n     PROJECTION[\"Mercator_1SP\"],\n     PARAMETER[\"central_meridian\",0],\n     PARAMETER[\"scale_factor\",1],\n     PARAMETER[\"false_easting\",0],\n     PARAMETER[\"false_northing\",0],\n     UNIT[\"metre\",1,\n         AUTHORITY[\"EPSG\",\"9001\"]],\n     AXIS[\"Easting\",EAST],\n     AXIS[\"Northing\",NORTH],\n     EXTENSION[\"PROJ4\",\"+proj=merc +a=6378137 +b=6378137 +lat_ts=0 +lon_0=0 +x_0=0 +y_0=0 +k=1 +units=m +nadgrids=@null +wktext +no_defs\"],\n     AUTHORITY[\"EPSG\",\"3857\"]]')), city_name\nFROM city_tbl_geom\n</code></pre>"},{"location":"tutorial/snowflake/sql/#range-query","title":"Range query","text":"<p>Use ST_Contains, ST_Intersects, ST_Within to run a range query over a single column.</p> <p>The following example finds all geometries that are within the given polygon:</p> <pre><code>SELECT *\nFROM city_tbl_geom\nWHERE Sedona.ST_Contains(Sedona.ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), geom)\n</code></pre> <p>Note</p> <p>Read SedonaSQL API to learn how to create a Geometry type query window.</p>"},{"location":"tutorial/snowflake/sql/#knn-query","title":"KNN query","text":"<p>Use ST_Distance, ST_DistanceSphere, ST_DistanceSpheroid to calculate the distance and rank the distance.</p> <p>The following code returns the 5 nearest neighbor of the given point.</p> <pre><code>SELECT geom, ST_Distance(Sedona.ST_Point(1.0, 1.0), geom) AS distance\nFROM city_tbl_geom\nORDER BY distance DESC\nLIMIT 5\n</code></pre>"},{"location":"tutorial/snowflake/sql/#range-join-query","title":"Range join query","text":"<p>Warning</p> <p>Sedona range join in Snowflake does not trigger Sedona's optimized spatial join algorithm while Sedona Spark does. It uses Snowflake's default Cartesian join which is very slow. Therefore, it is recommended to use Sedona's S2-based join or Snowflake's native ST functions + native <code>Geography</code> type to do range join, which will trigger Snowflake's <code>GeoJoin</code> algorithm.</p> <p>Introduction: Find geometries from A and geometries from B such that each geometry pair satisfies a certain predicate.</p> <p>Example:</p> <pre><code>SELECT *\nFROM polygondf, pointdf\nWHERE ST_Contains(polygondf.polygonshape,pointdf.pointshape)\n</code></pre> <pre><code>SELECT *\nFROM polygondf, pointdf\nWHERE ST_Intersects(polygondf.polygonshape,pointdf.pointshape)\n</code></pre> <pre><code>SELECT *\nFROM pointdf, polygondf\nWHERE ST_Within(pointdf.pointshape, polygondf.polygonshape)\n</code></pre>"},{"location":"tutorial/snowflake/sql/#distance-join","title":"Distance join","text":"<p>Warning</p> <p>Sedona distance join in Snowflake does not trigger Sedona's optimized spatial join algorithm while Sedona Spark does. It uses Snowflake's default Cartesian join which is very slow. Therefore, it is recommended to use Sedona's S2-based join or Snowflake's native ST functions to do range join, which will trigger Snowflake's <code>GeoJoin</code> algorithm.</p> <p>Introduction: Find geometries from A and geometries from B such that the distance of each geometry pair is less or equal than a certain distance. It supports the planar Euclidean distance calculators <code>ST_Distance</code>, <code>ST_HausdorffDistance</code>, <code>ST_FrechetDistance</code> and the meter-based geodesic distance calculators <code>ST_DistanceSpheroid</code> and <code>ST_DistanceSphere</code>.</p> <p>Example for planar Euclidean distance:</p> <pre><code>SELECT *\nFROM pointdf1, pointdf2\nWHERE ST_Distance(pointdf1.pointshape1,pointdf2.pointshape2) &lt; 2\n</code></pre> <pre><code>SELECT *\nFROM pointDf, polygonDF\nWHERE ST_HausdorffDistance(pointDf.pointshape, polygonDf.polygonshape, 0.3) &lt; 2\n</code></pre> <pre><code>SELECT *\nFROM pointDf, polygonDF\nWHERE ST_FrechetDistance(pointDf.pointshape, polygonDf.polygonshape) &lt; 2\n</code></pre> <p>Warning</p> <p>If you use planar euclidean distance functions like <code>ST_Distance</code>, <code>ST_HausdorffDistance</code> or <code>ST_FrechetDistance</code> as the predicate, Sedona doesn't control the distance's unit (degree or meter). It is same with the geometry. If your coordinates are in the longitude and latitude system, the unit of <code>distance</code> should be degree instead of meter or mile. To change the geometry's unit, please either transform the coordinate reference system to a meter-based system. See ST_Transform. If you don't want to transform your data, please consider using <code>ST_DistanceSpheroid</code> or <code>ST_DistanceSphere</code>.</p> <pre><code>SELECT *\nFROM pointdf1, pointdf2\nWHERE ST_DistanceSpheroid(pointdf1.pointshape1,pointdf2.pointshape2) &lt; 2\n</code></pre>"},{"location":"tutorial/snowflake/sql/#google-s2-based-approximate-equi-join","title":"Google S2 based approximate equi-join","text":"<p>You can use Sedona built-in Google S2 functions to perform an approximate equi-join. This algorithm leverages Snowflake's internal equi-join algorithm and might be performant given that you can opt to skip the refinement step  by sacrificing query accuracy.</p> <p>Please use the following steps:</p>"},{"location":"tutorial/snowflake/sql/#1-generate-s2-ids-for-both-tables","title":"1. Generate S2 ids for both tables","text":"<p>Use ST_S2CellIds to generate cell IDs. Each geometry may produce one or more IDs.</p> <pre><code>SELECT * FROM lefts, TABLE(FLATTEN(ST_S2CellIDs(lefts.geom, 15))) s1\n</code></pre> <pre><code>SELECT * FROM rights, TABLE(FLATTEN(ST_S2CellIDs(rights.geom, 15))) s2\n</code></pre>"},{"location":"tutorial/snowflake/sql/#2-perform-equi-join","title":"2. Perform equi-join","text":"<p>Join the two tables by their S2 cellId</p> <pre><code>SELECT lcs.id as lcs_id, lcs.geom as lcs_geom, lcs.name as lcs_name, rcs.id as rcs_id, rcs.geom as rcs_geom, rcs.name as rcs_name\nFROM lcs JOIN rcs ON lcs.cellId = rcs.cellId\n</code></pre>"},{"location":"tutorial/snowflake/sql/#3-optional-refine-the-result","title":"3. Optional: Refine the result","text":"<p>Due to the nature of S2 Cellid, the equi-join results might have a few false-positives depending on the S2 level you choose. A smaller level indicates bigger cells, less exploded rows, but more false positives.</p> <p>To ensure the correctness, you can use one of the Spatial Predicates to filter out them. Use this query instead of the query in Step 2.</p> <pre><code>SELECT lcs.id as lcs_id, lcs.geom as lcs_geom, lcs.name as lcs_name, rcs.id as rcs_id, rcs.geom as rcs_geom, rcs.name as rcs_name\nFROM lcs, rcs\nWHERE lcs.cellId = rcs.cellId AND ST_Contains(lcs.geom, rcs.geom)\n</code></pre> <p>As you see, compared to the query in Step 2, we added one more filter, which is <code>ST_Contains</code>, to remove false positives. You can also use <code>ST_Intersects</code> and so on.</p> <p>Tip</p> <p>You can skip this step if you don't need 100% accuracy and want faster query speed.</p>"},{"location":"tutorial/snowflake/sql/#4-optional-de-duplicate","title":"4. Optional: De-duplicate","text":"<p>Due to the <code>Flatten</code> function used when we generate S2 Cell Ids, the resulting DataFrame may have several duplicate  matches. You can remove them by performing a GroupBy query. <pre><code>SELECT lcs_id, rcs_id, ANY_VALUE(lcs_geom), ANY_VALUE(lcs_name), ANY_VALUE(rcs_geom), ANY_VALUE(rcs_name)\nFROM joinresult\nGROUP BY (lcs_id, rcs_id)\n</code></pre> <p>The <code>ANY_VALUE</code> function is to take the first value from a number of duplicate values.</p> <p>If you don't have a unique id for each geometry, you can also group by geometry itself. See below:</p> <pre><code>SELECT lcs_geom, rcs_geom, ANY_VALUE(lcs_name), ANY_VALUE(rcs_name)\nFROM joinresult\nGROUP BY (lcs_geom, rcs_geom)\n</code></pre> <p>Note</p> <p>If you are doing point-in-polygon join, this is not a problem and you can safely discard this issue. This issue only happens when you do polygon-polygon, polygon-linestring, linestring-linestring join.</p>"},{"location":"tutorial/snowflake/sql/#s2-for-distance-join","title":"S2 for distance join","text":"<p>This also works for distance join. You first need to use <code>ST_Buffer(geometry, distance)</code> to wrap one of your original geometry column. If your original geometry column contains points, this <code>ST_Buffer</code> will make them become circles with a radius of <code>distance</code>.</p> <p>Since the coordinates are in the longitude and latitude system, so the unit of <code>distance</code> should be degree instead of meter or mile. You can get an approximation by performing <code>METER_DISTANCE/111000.0</code>, then filter out false-positives.  Note that this might lead to inaccurate results if your data is close to the poles or antimeridian.</p> <p>In a nutshell, run this query first on the left table before Step 1. Please replace <code>METER_DISTANCE</code> with a meter distance. In Step 1, generate S2 IDs based on the <code>buffered_geom</code> column. Then run Step 2, 3, 4 on the original <code>geom</code> column.</p> <pre><code>SELECT id, geom, ST_Buffer(geom, METER_DISTANCE/111000.0) as buffered_geom, name\nFROM lefts\n</code></pre>"},{"location":"tutorial/snowflake/sql/#functions-that-are-only-available-in-sedona","title":"Functions that are only available in Sedona","text":"<p>Sedona implements over 200 geospatial vector and raster functions, which are much more than what Snowflake native functions offer. For example:</p> <ul> <li>ST_3DDistance</li> <li>ST_Force2D</li> <li>ST_GeometryN</li> <li>ST_MakeValid</li> <li>ST_Multi</li> <li>ST_NumGeometries</li> <li>ST_ReducePrecision</li> <li>ST_SubdivdeExplode</li> </ul> <p>You can click the links above to learn more about these functions. More functions can be found in SedonaSQL API.</p>"},{"location":"tutorial/snowflake/sql/#interoperate-with-snowflake-native-functions","title":"Interoperate with Snowflake native functions","text":"<p>Sedona can interoperate with Snowflake native functions seamlessly. There are two ways to do this:</p> <ul> <li>Use Sedona functions to create a Geometry column, then use Snowflake native functions and Sedona functions to query the Geometry column.</li> <li>Use Snowflake native functions to create a Geometry column, then use Snowflake native functions and Sedona functions to query the Geometry column.</li> </ul> <p>Now we will show you how to do this.</p>"},{"location":"tutorial/snowflake/sql/#geometries-created-by-sedona-geometry-constructors","title":"Geometries created by Sedona Geometry constructors","text":"<p>In this case, Sedona uses EWKB type as the input/output type for geometry. If you have datasets of built-in Snowflake GEOMETRY/GEOGRAPHY type, you can easily transform them into EWKB through this function.</p>"},{"location":"tutorial/snowflake/sql/#from-snowflake-native-functions-to-sedona-functions","title":"From Snowflake native functions to Sedona functions","text":"<p>In this example, <code>SEDONA.ST_X</code> is a Sedona function, <code>ST_GeommetryFromWkt</code> and <code>ST_AsEWKB</code> are Snowflake native functions.</p> <pre><code>SELECT SEDONA.ST_X(ST_AsEWKB(ST_GeommetryFromWkt('POLYGON((0 0, 0 1, 1 1, 1 0, 0 0))'))) FROM {{ geometry_table }};\n</code></pre>"},{"location":"tutorial/snowflake/sql/#from-sedona-functions-to-snowflake-native-functions","title":"From Sedona functions to Snowflake native functions","text":"<p>In this example, <code>SEDONA.ST_GeomFromText</code> is a Sedona function, <code>ST_AREA</code> and <code>to_geometry</code> are Snowflake native functions.</p> <pre><code>SELECT ST_AREA(to_geometry(SEDONA.ST_GeomFromText('POLYGON((0 0, 0 1, 1 1, 1 0, 0 0))')));\n</code></pre>"},{"location":"tutorial/snowflake/sql/#pros","title":"Pros:","text":"<p>Sedona geometry constructors are more powerful than Snowflake native functions. It has the following advantages:</p> <ul> <li>Sedona offers more constructors especially for 3D (XYZ) geometries, but Snowflake native functions don't.</li> <li>WKB serialization is more efficient. If you need to use multiple Sedona functions, it is more efficient to use this method, which might bring in 2X performance improvement.</li> <li>SRID information of geometries is preserved. The method below will lose SRID information.</li> </ul>"},{"location":"tutorial/snowflake/sql/#geometries-created-by-snowflake-geometry-constructors","title":"Geometries created by Snowflake Geometry constructors","text":"<p>In this case, Sedona uses Snowflake native GEOMETRY/GEOGRAPHY type as the input/output type for geometry. The serialization format is GeoJSON string.</p> <pre><code>SELECT ST_AREA(SEDONA.ST_Buffer(ST_GeometryFromWkt('POLYGON((0 0, 0 1, 1 1, 1 0, 0 0))'), 1));\n</code></pre> <p>In this example, <code>SEDONA.ST_Buffer</code> is a Sedona function, <code>ST_GeommetryFromWkt</code> and <code>ST_AREA</code> are Snowflake native functions.</p> <p>As you can see, you can use Sedona functions and Snowflake native functions together without explicit conversion.</p>"},{"location":"tutorial/snowflake/sql/#pros_1","title":"Pros:","text":"<ul> <li>You don't need to convert the geometry type, which is more convenient.</li> </ul> <p>Note that: Snowflake natively serializes Geometry type data to GeoJSON String and sends to UDF as input. GeoJSON spec does not include SRID. So the SRID information will be lost if you mix-match Snowflake functions and Sedona functions directly without using <code>WKB</code>.</p> <p>In the example below, the SRID=4326 information is lost.</p> <pre><code>SELECT ST_AsEWKT(SEDONA.ST_SetSRID(ST_GeometryFromWKT('POINT(1 2)'), 4326))\n</code></pre> <p>Output: <pre><code>SRID=0;POINT(1 2)\n</code></pre></p>"},{"location":"tutorial/snowflake/sql/#known-issues","title":"Known issues","text":"<ol> <li>Sedona Snowflake doesn't support <code>M</code> dimension due to the limitation of WKB serialization. Sedona Spark and Sedona Flink support XYZM because it uses our in-house serialization format. Although Sedona Snowflake has functions related to <code>M</code> dimension, all <code>M</code> values will be ignored.</li> <li>Sedona H3 functions are not supported because Snowflake does not allow embedded C code in UDF.</li> <li>All User Defined Table Functions only work with geometries created by Sedona constructors due to Snowflake current limitation <code>Data type GEOMETRY is not supported in non-SQL UDTF return type</code>. This includes:</li> <li>ST_MinimumBoundingRadius</li> <li>ST_Intersection_Aggr</li> <li>ST_SubDivideExplode</li> <li>ST_Envelope_Aggr</li> <li>ST_Union_Aggr</li> <li>ST_Collect</li> <li>ST_Dump</li> <li>Only Sedona ST functions are available in Snowflake. Raster functions (RS functions) are not available in Snowflake yet.</li> </ol>"},{"location":"usecases/airport-country/","title":"Spatially aggregate airports per country","text":""},{"location":"usecases/foot-traffic/","title":"Match foot traffic to Seattle coffee shops","text":""},{"location":"usecases/raster/","title":"Raster image manipulation","text":""}]}