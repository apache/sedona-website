{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#06252023-sedona-141-released-it-adds-geodesic-geography-functions-more-raster-functions-and-support-spark-34","title":"06/25/2023: Sedona 1.4.1 released. It adds geodesic / geography functions, more raster functions and support Spark 3.4.","text":""},{"location":"#03192023-sedona-140-released-it-provides-geoparquet-filter-pushdown-10x-less-memory-footprint-faster-serialization-3x-speed-s2-based-fast-approximate-join-and-enhanced-r-language-support","title":"03/19/2023: Sedona 1.4.0 released. It provides GeoParquet filter pushdown (10X less memory footprint), faster serialization (3X speed), S2-based fast approximate join and enhanced R language support","text":""},{"location":"#012023-apache-sedona-graduated-to-an-apache-top-level-project","title":"01/2023: Apache Sedona graduated to an Apache Top Level Project!","text":""},{"location":"#12232022-sedona-131-incubating-is-released-it-adds-native-support-of-geoparquet-dataframe-style-api-scala-213-python-310-spatial-aggregation-on-flink-please-check-sedona-release-notes","title":"12/23/2022: Sedona 1.3.1-incubating is released. It adds native support of GeoParquet, DataFrame style API, Scala 2.13, Python 3.10, spatial aggregation on Flink. Please check Sedona release notes.","text":""},{"location":"#08302022-sedona-121-incubating-is-released-it-supports-spark-24-33-and-flink-112","title":"08/30/2022: Sedona 1.2.1-incubating is released. It supports Spark 2.4 - 3.3. and Flink 1.12+.","text":""},{"location":"#04162022-sedona-120-incubating-is-released-sedona-now-supports-geospatial-stream-processing-in-apache-flink","title":"04/16/2022: Sedona 1.2.0-incubating is released. Sedona now supports geospatial stream processing in Apache Flink.","text":""},{"location":"download/","title":"Download","text":""},{"location":"download/#github-repository","title":"GitHub repository","text":"<p>Latest source code: GitHub repository</p> <p>Old GeoSpark releases: GitHub releases</p> <p>Automatically generated binary JARs (per each Master branch commit): GitHub Action</p>"},{"location":"download/#verify-the-integrity","title":"Verify the integrity","text":"<p>Public keys</p> <p>Instructions</p>"},{"location":"download/#versions","title":"Versions","text":""},{"location":"download/#141","title":"1.4.1","text":"Download from ASF Checksum Signature Source code src sha512 asc Binary bin sha512 asc"},{"location":"download/#140","title":"1.4.0","text":"Download from ASF Checksum Signature Source code src sha512 asc Binary bin sha512 asc"},{"location":"download/#131-incubating","title":"1.3.1-incubating","text":"Download from ASF Checksum Signature Source code src sha512 asc Binary bin sha512 asc"},{"location":"download/#past-releases","title":"Past releases","text":"<p>Past Sedona releases are archived and can be found here: Apache archive (on and after 1.2.1-incubating) and Apache Incubator archive (before v1.2.1-incubating)</p>"},{"location":"download/#security","title":"Security","text":"<p>For security issues, please refer to https://www.apache.org/security/</p>"},{"location":"api/java-api/","title":"Scala/Java doc","text":"<p>Please read Javadoc</p> <p>Note: Scala can call Java APIs seamlessly. That means Scala users use the same APIs with Java users</p>"},{"location":"api/python-api/","title":"Python api","text":"<p>Will be available soon.</p>"},{"location":"api/flink/Aggregator/","title":"Aggregator","text":""},{"location":"api/flink/Aggregator/#st_envelope_aggr","title":"ST_Envelope_Aggr","text":"<p>Introduction: Return the entire envelope boundary of all geometries in A</p> <p>Format: <code>ST_Envelope_Aggr (A:geometryColumn)</code></p> <p>Since: <code>v1.3.0</code></p> <p>SQL example: <pre><code>SELECT ST_Envelope_Aggr(pointdf.arealandmark)\nFROM pointdf\n</code></pre></p>"},{"location":"api/flink/Aggregator/#st_union_aggr","title":"ST_Union_Aggr","text":"<p>Introduction: Return the polygon union of all polygons in A. All inputs must be polygons.</p> <p>Format: <code>ST_Union_Aggr (A:geometryColumn)</code></p> <p>Since: <code>v1.3.0</code></p> <p>SQL example: <pre><code>SELECT ST_Union_Aggr(polygondf.polygonshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/flink/Constructor/","title":"Constructor","text":""},{"location":"api/flink/Constructor/#st_geomfromgeohash","title":"ST_GeomFromGeoHash","text":"<p>Introduction: Create Geometry from geohash string and optional precision</p> <p>Format: <code>ST_GeomFromGeoHash(geohash: string, precision: int)</code></p> <p>Since: <code>v1.2.1</code></p> <p>SQL example: <pre><code>SELECT ST_GeomFromGeoHash('s00twy01mt', 4) AS geom\n</code></pre></p>"},{"location":"api/flink/Constructor/#st_geomfromgeojson","title":"ST_GeomFromGeoJSON","text":"<p>Introduction: Construct a Geometry from GeoJson</p> <p>Format: <code>ST_GeomFromGeoJSON (GeoJson:string)</code></p> <p>Since: <code>v1.2.0</code></p> <p>SQL example: <pre><code>SELECT ST_GeomFromGeoJSON(polygontable._c0) AS polygonshape\nFROM polygontable\n</code></pre></p>"},{"location":"api/flink/Constructor/#st_geomfromgml","title":"ST_GeomFromGML","text":"<p>Introduction: Construct a Geometry from GML.</p> <p>Format: <code>ST_GeomFromGML (gml:string)</code></p> <p>Since: <code>v1.3.0</code></p> <p>SQL example: <pre><code>SELECT ST_GeomFromGML('&lt;gml:LineString srsName=\"EPSG:4269\"&gt;&lt;gml:coordinates&gt;-71.16028,42.258729 -71.160837,42.259112 -71.161143,42.25932&lt;/gml:coordinates&gt;&lt;/gml:LineString&gt;') AS geometry\n</code></pre></p>"},{"location":"api/flink/Constructor/#st_geomfromkml","title":"ST_GeomFromKML","text":"<p>Introduction: Construct a Geometry from KML.</p> <p>Format: <code>ST_GeomFromKML (kml:string)</code></p> <p>Since: <code>v1.3.0</code></p> <p>SQL example: <pre><code>SELECT ST_GeomFromKML('&lt;LineString&gt;&lt;coordinates&gt;-71.1663,42.2614 -71.1667,42.2616&lt;/coordinates&gt;&lt;/LineString&gt;') AS geometry\n</code></pre></p>"},{"location":"api/flink/Constructor/#st_geomfromtext","title":"ST_GeomFromText","text":"<p>Introduction: Construct a Geometry from Wkt. Alias of  ST_GeomFromWKT</p> <p>Format: <code>ST_GeomFromText (Wkt:string)</code></p> <p>Since: <code>v1.2.1</code></p> <p>SQL example: <pre><code>SELECT ST_GeomFromText('POINT(40.7128 -74.0060)') AS geometry\n</code></pre></p>"},{"location":"api/flink/Constructor/#st_geomfromwkb","title":"ST_GeomFromWKB","text":"<p>Introduction: Construct a Geometry from WKB string or Binary</p> <p>Format: <code>ST_GeomFromWKB (Wkb:string)</code> <code>ST_GeomFromWKB (Wkb:binary)</code></p> <p>Since: <code>v1.2.0</code></p> <p>SQL example: <pre><code>SELECT ST_GeomFromWKB(polygontable._c0) AS polygonshape\nFROM polygontable\n</code></pre></p> <p>Format: <code>ST_GeomFromWKB (Wkb:bytes)</code></p> <p>Since: <code>v1.2.1</code></p> <p>SQL example: <pre><code>SELECT ST_GeomFromWKB(polygontable._c0) AS polygonshape\nFROM polygontable\n</code></pre></p>"},{"location":"api/flink/Constructor/#st_geomfromwkt","title":"ST_GeomFromWKT","text":"<p>Introduction: Construct a Geometry from Wkt</p> <p>Format: <code>ST_GeomFromWKT (Wkt:string)</code></p> <p>Since: <code>v1.2.0</code></p> <p>SQL example: <pre><code>SELECT ST_GeomFromWKT('POINT(40.7128 -74.0060)') AS geometry\n</code></pre></p>"},{"location":"api/flink/Constructor/#st_linefromtext","title":"ST_LineFromText","text":"<p>Introduction: Construct a LineString from Text, delimited by Delimiter (Optional)</p> <p>Format: <code>ST_LineFromText (Text:string, Delimiter:char)</code></p> <p>Since: <code>v1.2.1</code></p> <p>SQL example: <pre><code>SELECT ST_LineFromText('Linestring(1 2, 3 4)') AS line\n</code></pre></p>"},{"location":"api/flink/Constructor/#st_linestringfromtext","title":"ST_LineStringFromText","text":"<p>Introduction: Construct a LineString from Text, delimited by Delimiter (Optional). Alias of  ST_LineFromText</p> <p>Format: <code>ST_LineStringFromText (Text:string, Delimiter:char)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Spark SQL example: <pre><code>SELECT ST_LineStringFromText('Linestring(1 2, 3 4)') AS line\n</code></pre></p>"},{"location":"api/flink/Constructor/#st_mlinefromtext","title":"ST_MLineFromText","text":"<p>Introduction: Construct a MultiLineString from Text and Optional SRID</p> <p>Format: <code>ST_MLineFromText (Text:string, Srid: int)</code></p> <p>Since: <code>1.3.1</code></p> <p>SQL example: <pre><code>SELECT ST_MLineFromText('MULTILINESTRING((1 2, 3 4), (4 5, 6 7))') AS multiLine\nSELECT ST_MLineFromText('MULTILINESTRING((1 2, 3 4), (4 5, 6 7))', 4269) AS multiLine\n</code></pre></p>"},{"location":"api/flink/Constructor/#st_mpolyfromtext","title":"ST_MPolyFromText","text":"<p>Introduction: Construct a MultiPolygon from Text and Optional SRID</p> <p>Format: <code>ST_MPolyFromText (Text:string, Srid: int)</code></p> <p>Since: <code>1.3.1</code></p> <p>SQL example: <pre><code>SELECT ST_MPolyFromText('MULTIPOLYGON(((-70.916 42.1002,-70.9468 42.0946,-70.9765 42.0872 )))') AS multiPolygon\nSELECT ST_MPolyFromText('MULTIPOLYGON(((-70.916 42.1002,-70.9468 42.0946,-70.9765 42.0872 )))', 4269) AS multiPolygon\n</code></pre></p>"},{"location":"api/flink/Constructor/#st_point","title":"ST_Point","text":"<p>Introduction: Construct a Point from X and Y</p> <p>Format: <code>ST_Point (X:decimal, Y:decimal)</code></p> <p>Since: <code>v1.2.1</code></p> <p>SQL example: <pre><code>SELECT ST_Point(x, y) AS pointshape\nFROM pointtable\n</code></pre></p>"},{"location":"api/flink/Constructor/#st_pointfromtext","title":"ST_PointFromText","text":"<p>Introduction: Construct a Point from Text, delimited by Delimiter</p> <p>Format: <code>ST_PointFromText (Text:string, Delimiter:char)</code></p> <p>Since: <code>v1.2.0</code></p> <p>SQL example: <pre><code>SELECT ST_PointFromText('40.7128,-74.0060', ',') AS pointshape\n</code></pre></p>"},{"location":"api/flink/Constructor/#st_polygonfromenvelope","title":"ST_PolygonFromEnvelope","text":"<p>Introduction: Construct a Polygon from MinX, MinY, MaxX, MaxY.</p> <p>Format: <code>ST_PolygonFromEnvelope (MinX:decimal, MinY:decimal, MaxX:decimal, MaxY:decimal)</code></p> <p>Since: <code>v1.2.0</code></p> <p>SQL example: <pre><code>SELECT *\nFROM pointdf\nWHERE ST_Contains(ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), pointdf.pointshape)\n</code></pre></p>"},{"location":"api/flink/Constructor/#st_polygonfromtext","title":"ST_PolygonFromText","text":"<p>Introduction: Construct a Polygon from Text, delimited by Delimiter. Path must be closed</p> <p>Format: <code>ST_PolygonFromText (Text:string, Delimiter:char)</code></p> <p>Since: <code>v1.2.0</code></p> <p>SQL example: <pre><code>SELECT ST_PolygonFromText('-74.0428197,40.6867969,-74.0421975,40.6921336,-74.0508020,40.6912794,-74.0428197,40.6867969', ',') AS polygonshape\n</code></pre></p>"},{"location":"api/flink/Function/","title":"Function","text":""},{"location":"api/flink/Function/#st_3ddistance","title":"ST_3DDistance","text":"<p>Introduction: Return the 3-dimensional minimum cartesian distance between A and B</p> <p>Format: <code>ST_3DDistance (A:geometry, B:geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_3DDistance(polygondf.countyshape, polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/flink/Function/#st_addpoint","title":"ST_AddPoint","text":"<p>Introduction: Return Linestring with additional point at the given index, if position is not available the point will be added at the end of line.</p> <p>Format: <code>ST_AddPoint(geom: geometry, point: geometry, position: integer)</code></p> <p>Format: <code>ST_AddPoint(geom: geometry, point: geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example: <pre><code>SELECT ST_AddPoint(ST_GeomFromText(\"LINESTRING(0 0, 1 1, 1 0)\"), ST_GeomFromText(\"Point(21 52)\"), 1)\n\nSELECT ST_AddPoint(ST_GeomFromText(\"Linestring(0 0, 1 1, 1 0)\"), ST_GeomFromText(\"Point(21 52)\"))\n</code></pre></p> <p>Output: <pre><code>LINESTRING(0 0, 21 52, 1 1, 1 0)\nLINESTRING(0 0, 1 1, 1 0, 21 52)\n</code></pre></p>"},{"location":"api/flink/Function/#st_area","title":"ST_Area","text":"<p>Introduction: Return the area of A</p> <p>Format: <code>ST_Area (A:geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Area(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/flink/Function/#st_areaspheroid","title":"ST_AreaSpheroid","text":"<p>Introduction: Return the geodesic area of A using WGS84 spheroid. Unit is meter. Works better for large geometries (country level) compared to <code>ST_Area</code> + <code>ST_Transform</code>. It is equivalent to PostGIS <code>ST_Area(geography, use_spheroid=true)</code> function and produces nearly identical results.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lat/lon order. You can use ST_FlipCoordinates to swap lat and lon.</p> <p>Format: <code>ST_AreaSpheroid (A:geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Example:</p> <pre><code>SELECT ST_AreaSpheroid(ST_GeomFromWKT('Polygon ((35 34, 30 28, 34 25, 35 34))'))\n</code></pre> <p>Output: <code>201824850811.76245</code></p>"},{"location":"api/flink/Function/#st_asbinary","title":"ST_AsBinary","text":"<p>Introduction: Return the Well-Known Binary representation of a geometry</p> <p>Format: <code>ST_AsBinary (A:geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsBinary(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/flink/Function/#st_asewkb","title":"ST_AsEWKB","text":"<p>Introduction: Return the Extended Well-Known Binary representation of a geometry. EWKB is an extended version of WKB which includes the SRID of the geometry. The format originated in PostGIS but is supported by many GIS tools. If the geometry is lacking SRID a WKB format is produced.</p> <p>Format: <code>ST_AsEWKB (A:geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsEWKB(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/flink/Function/#st_asewkt","title":"ST_AsEWKT","text":"<p>Introduction: Return the Extended Well-Known Text representation of a geometry. EWKT is an extended version of WKT which includes the SRID of the geometry. The format originated in PostGIS but is supported by many GIS tools. If the geometry is lacking SRID a WKT format is produced. See ST_SetSRID</p> <p>Format: <code>ST_AsEWKT (A:geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Spark SQL example: <pre><code>SELECT ST_AsEWKT(polygondf.countyshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/flink/Function/#st_asgeojson","title":"ST_AsGeoJSON","text":"<p>Introduction: Return the GeoJSON string representation of a geometry</p> <p>Format: <code>ST_AsGeoJSON (A:geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_AsGeoJSON(polygondf.countyshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/flink/Function/#st_asgml","title":"ST_AsGML","text":"<p>Introduction: Return the GML string representation of a geometry</p> <p>Format: <code>ST_AsGML (A:geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_AsGML(polygondf.countyshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/flink/Function/#st_askml","title":"ST_AsKML","text":"<p>Introduction: Return the KML string representation of a geometry</p> <p>Format: <code>ST_AsKML (A:geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_AsKML(polygondf.countyshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/flink/Function/#st_astext","title":"ST_AsText","text":"<p>Introduction: Return the Well-Known Text string representation of a geometry</p> <p>Format: <code>ST_AsText (A:geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/flink/Function/#st_azimuth","title":"ST_Azimuth","text":"<p>Introduction: Returns Azimuth for two given points in radians null otherwise.</p> <p>Format: <code>ST_Azimuth(pointA: Point, pointB: Point)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Azimuth(ST_POINT(0.0, 25.0), ST_POINT(0.0, 0.0))\n</code></pre> <p>Output: <code>3.141592653589793</code></p>"},{"location":"api/flink/Function/#st_boundary","title":"ST_Boundary","text":"<p>Introduction: Returns the closure of the combinatorial boundary of this Geometry.</p> <p>Format: <code>ST_Boundary(geom: geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example: <pre><code>SELECT ST_Boundary(ST_GeomFromText('POLYGON ((1 1, 0 0, -1 1, 1 1))'))\n</code></pre></p> <p>Output: <code>LINEARRING (1 1, 0 0, -1 1, 1 1)</code></p>"},{"location":"api/flink/Function/#st_buffer","title":"ST_Buffer","text":"<p>Introduction: Returns a geometry/geography that represents all points whose distance from this Geometry/geography is less than or equal to distance.</p> <p>Format: <code>ST_Buffer (A:geometry, buffer: Double)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_Buffer(polygondf.countyshape, 1)\nFROM polygondf\n</code></pre></p>"},{"location":"api/flink/Function/#st_buildarea","title":"ST_BuildArea","text":"<p>Introduction: Returns the areal geometry formed by the constituent linework of the input geometry.</p> <p>Format: <code>ST_BuildArea (A:geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_BuildArea(ST_Collect(smallDf, bigDf)) AS geom\nFROM smallDf, bigDf\n</code></pre> <p>Input: <code>MULTILINESTRING((0 0, 10 0, 10 10, 0 10, 0 0),(10 10, 20 10, 20 20, 10 20, 10 10))</code></p> <p>Output: <code>MULTIPOLYGON(((0 0,0 10,10 10,10 0,0 0)),((10 10,10 20,20 20,20 10,10 10)))</code></p>"},{"location":"api/flink/Function/#st_concavehull","title":"ST_ConcaveHull","text":"<p>Introduction: Return the Concave Hull of polgyon A, with alpha set to pctConvex[0, 1] in the Delaunay Triangulation method, the concave hull will not contain a hole unless allowHoles is set to true</p> <p>Format: <code>ST_ConcaveHull (A:geometry, pctConvex:float)</code></p> <p>Format: <code>ST_ConcaveHull (A:geometry, pctConvex:float, allowHoles:Boolean)</code></p> <p>Since: <code>v1.4.0</code></p> <p>Example:</p> <pre><code>SELECT ST_ConcaveHull(polygondf.countyshape, pctConvex)`\nFROM polygondf\n</code></pre> <p>Input: <code>Polygon ((0 0, 1 2, 2 2, 3 2, 5 0, 4 0, 3 1, 2 1, 1 0, 0 0))</code></p> <p>Output: <code>POLYGON ((1 2, 2 2, 3 2, 5 0, 4 0, 1 0, 0 0, 1 2))</code></p>"},{"location":"api/flink/Function/#st_distance","title":"ST_Distance","text":"<p>Introduction: Return the Euclidean distance between A and B</p> <p>Format: <code>ST_Distance (A:geometry, B:geometry)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_Distance(polygondf.countyshape, polygondf.countyshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/flink/Function/#st_distancesphere","title":"ST_DistanceSphere","text":"<p>Introduction: Return the haversine / great-circle distance of A using a given earth radius (default radius: 6371008.0). Unit is meter. Works better for large geometries (country level) compared to <code>ST_Distance</code> + <code>ST_Transform</code>. It is equivalent to PostGIS <code>ST_Distance(geography, use_spheroid=false)</code> and <code>ST_DistanceSphere</code> function and produces nearly identical results. It provides faster but less accurate result compared to <code>ST_DistanceSpheroid</code>.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lat/lon order. You can use ST_FlipCoordinates to swap lat and lon. For non-point data, we first take the centroids of both geometries and then compute the distance.</p> <p>Format: <code>ST_DistanceSphere (A:geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Example 1:</p> <pre><code>SELECT ST_DistanceSphere(ST_GeomFromWKT('POINT (51.3168 -0.56)'), ST_GeomFromWKT('POINT (55.9533 -3.1883)'))\n</code></pre> <p>Output: <code>543796.9506134904</code></p> <p>Example 2:</p> <pre><code>SELECT ST_DistanceSphere(ST_GeomFromWKT('POINT (51.3168 -0.56)'), ST_GeomFromWKT('POINT (55.9533 -3.1883)'), 6378137.0)\n</code></pre> <p>Output: <code>544405.4459192449</code></p>"},{"location":"api/flink/Function/#st_distancespheroid","title":"ST_DistanceSpheroid","text":"<p>Introduction: Return the geodesic distance of A using WGS84 spheroid. Unit is meter. Works better for large geometries (country level) compared to <code>ST_Distance</code> + <code>ST_Transform</code>. It is equivalent to PostGIS <code>ST_Distance(geography, use_spheroid=true)</code> and <code>ST_DistanceSpheroid</code> function and produces nearly identical results. It provides slower but more accurate result compared to <code>ST_DistanceSphere</code>.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lat/lon order. You can use ST_FlipCoordinates to swap lat and lon. For non-point data, we first take the centroids of both geometries and then compute the distance.</p> <p>Format: <code>ST_DistanceSpheroid (A:geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Example: <pre><code>SELECT ST_DistanceSpheroid(ST_GeomFromWKT('POINT (51.3168 -0.56)'), ST_GeomFromWKT('POINT (55.9533 -3.1883)'))\n</code></pre></p> <p>Output: <code>544430.9411996207</code></p>"},{"location":"api/flink/Function/#st_envelope","title":"ST_Envelope","text":"<p>Introduction: Return the envelop boundary of A</p> <p>Format: <code>ST_Envelope (A:geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Envelope(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/flink/Function/#st_exteriorring","title":"ST_ExteriorRing","text":"<p>Introduction: Returns a LINESTRING representing the exterior ring (shell) of a POLYGON. Returns NULL if the geometry is not a polygon.</p> <p>Format: <code>ST_ExteriorRing(A:geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Examples:</p> <pre><code>SELECT ST_ExteriorRing(df.geometry)\nFROM df\n</code></pre> <p>Input: <code>POLYGON ((0 0, 1 1, 2 1, 0 1, 1 -1, 0 0))</code></p> <p>Output: <code>LINESTRING (0 0, 1 1, 2 1, 0 1, 1 -1, 0 0)</code></p>"},{"location":"api/flink/Function/#st_flipcoordinates","title":"ST_FlipCoordinates","text":"<p>Introduction: Returns a version of the given geometry with X and Y axis flipped.</p> <p>Format: <code>ST_FlipCoordinates(A:geometry)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_FlipCoordinates(df.geometry)\nFROM df\n</code></pre></p> <p>Input: <code>POINT (1 2)</code></p> <p>Output: <code>POINT (2 1)</code></p>"},{"location":"api/flink/Function/#st_force_2d","title":"ST_Force_2D","text":"<p>Introduction: Forces the geometries into a \"2-dimensional mode\" so that all output representations will only have the X and Y coordinates</p> <p>Format: <code>ST_Force_2D (A:geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_Force_2D(df.geometry) AS geom\nFROM df\n</code></pre> <p>Input: <code>POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))</code></p> <p>Output: <code>POLYGON((0 0,0 5,5 0,0 0),(1 1,3 1,1 3,1 1))</code></p>"},{"location":"api/flink/Function/#st_force3d","title":"ST_Force3D","text":"<p>Introduction: Forces the geometry into a 3-dimensional model so that all output representations will have X, Y and Z coordinates. An optionally given zValue is tacked onto the geometry if the geometry is 2-dimensional. Default value of zValue is 0.0 If the given geometry is 3-dimensional, no change is performed on it. If the given geometry is empty, no change is performed on it.</p> <p>Note</p> <p>Example output is after calling ST_AsText() on returned geometry, which adds Z for in the WKT for 3D geometries</p> <p>Format: <code>ST_Force3D(geometry, zValue)</code></p> <p>Since: <code>1.4.1</code></p> <p>Example: </p> <pre><code>SELECT ST_Force3D(df.geometry) AS geom\nfrom df\n</code></pre> <p>Input: <code>LINESTRING(0 1, 1 2, 2 1)</code></p> <p>Output: <code>LINESTRING Z(0 1 0, 1 2 0, 2 1 0)</code></p> <p>Input: <code>POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))</code></p> <p>Output: <code>POLYGON Z((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))</code></p> <pre><code>SELECT ST_Force3D(df.geometry, 2.3) AS geom\nfrom df\n</code></pre> <p>Input: <code>LINESTRING(0 1, 1 2, 2 1)</code></p> <p>Output: <code>LINESTRING Z(0 1 2.3, 1 2 2.3, 2 1 2.3)</code></p> <p>Input: <code>POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))</code></p> <p>Output: <code>POLYGON Z((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))</code></p> <p>Input: <code>LINESTRING EMPTY</code></p> <p>Output: <code>LINESTRING EMPTY</code></p>"},{"location":"api/flink/Function/#st_geohash","title":"ST_GeoHash","text":"<p>Introduction: Returns GeoHash of the geometry with given precision</p> <p>Format: <code>ST_GeoHash(geom: geometry, precision: int)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example: </p> <p>Query:</p> <pre><code>SELECT ST_GeoHash(ST_GeomFromText('POINT(21.427834 52.042576573)'), 5) AS geohash\n</code></pre> <p>Result:</p> <pre><code>+-----------------------------+\n|geohash                      |\n+-----------------------------+\n|u3r0p                        |\n+-----------------------------+\n</code></pre>"},{"location":"api/flink/Function/#st_geometricmedian","title":"ST_GeometricMedian","text":"<p>Introduction: Computes the approximate geometric median of a MultiPoint geometry using the Weiszfeld algorithm. The geometric median provides a centrality measure that is less sensitive to outlier points than the centroid.</p> <p>The algorithm will iterate until the distance change between successive iterations is less than the supplied <code>tolerance</code> parameter. If this condition has not been met after <code>maxIter</code> iterations, the function will produce an error and exit, unless <code>failIfNotConverged</code> is set to <code>false</code>.</p> <p>If a <code>tolerance</code> value is not provided, a default <code>tolerance</code> value is <code>1e-6</code>.</p> <p>Format: <code>ST_GeometricMedian(geom: geometry, tolerance: float, maxIter: integer, failIfNotConverged: boolean)</code></p> <p>Format: <code>ST_GeometricMedian(geom: geometry, tolerance: float, maxIter: integer)</code></p> <p>Format: <code>ST_GeometricMedian(geom: geometry, tolerance: float)</code></p> <p>Format: <code>ST_GeometricMedian(geom: geometry)</code></p> <p>Default parameters: <code>tolerance: 1e-6, maxIter: 1000, failIfNotConverged: false</code></p> <p>Since: <code>1.4.1</code></p> <p>Example: <pre><code>SELECT ST_GeometricMedian(ST_GeomFromWKT('MULTIPOINT((0 0), (1 1), (2 2), (200 200))'))\n</code></pre></p> <p>Output: <pre><code>POINT (1.9761550281255005 1.9761550281255005)\n</code></pre></p>"},{"location":"api/flink/Function/#st_geometryn","title":"ST_GeometryN","text":"<p>Introduction: Return the 0-based Nth geometry if the geometry is a GEOMETRYCOLLECTION, (MULTI)POINT, (MULTI)LINESTRING, MULTICURVE or (MULTI)POLYGON. Otherwise, return null</p> <p>Format: <code>ST_GeometryN(geom: geometry, n: Int)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example: <pre><code>SELECT ST_GeometryN(ST_GeomFromText('MULTIPOINT((1 2), (3 4), (5 6), (8 9))'), 1)\n</code></pre></p> <p>Output: <code>POINT (3 4)</code></p>"},{"location":"api/flink/Function/#st_interiorringn","title":"ST_InteriorRingN","text":"<p>Introduction: Returns the Nth interior linestring ring of the polygon geometry. Returns NULL if the geometry is not a polygon or the given N is out of range</p> <p>Format: <code>ST_InteriorRingN(geom: geometry, n: Int)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example: <pre><code>SELECT ST_InteriorRingN(ST_GeomFromText('POLYGON((0 0, 0 5, 5 5, 5 0, 0 0), (1 1, 2 1, 2 2, 1 2, 1 1), (1 3, 2 3, 2 4, 1 4, 1 3), (3 3, 4 3, 4 4, 3 4, 3 3))'), 0)\n</code></pre></p> <p>Output: <code>LINEARRING (1 1, 2 1, 2 2, 1 2, 1 1)</code></p>"},{"location":"api/flink/Function/#st_isclosed","title":"ST_IsClosed","text":"<p>Introduction: RETURNS true if the LINESTRING start and end point are the same.</p> <p>Format: <code>ST_IsClosed(geom: geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_IsClosed(ST_GeomFromText('LINESTRING(0 0, 1 1, 1 0)'))\n</code></pre>"},{"location":"api/flink/Function/#st_isempty","title":"ST_IsEmpty","text":"<p>Introduction: Test if a geometry is empty geometry</p> <p>Format: <code>ST_IsEmpty (A:geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Spark SQL example:</p> <pre><code>SELECT ST_IsEmpty(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/flink/Function/#st_isring","title":"ST_IsRing","text":"<p>Introduction: RETURN true if LINESTRING is ST_IsClosed and ST_IsSimple.</p> <p>Format: <code>ST_IsRing(geom: geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_IsRing(ST_GeomFromText(\"LINESTRING(0 0, 0 1, 1 1, 1 0, 0 0)\"))\n</code></pre> <p>Output: <code>true</code></p>"},{"location":"api/flink/Function/#st_issimple","title":"ST_IsSimple","text":"<p>Introduction: Test if geometry's only self-intersections are at boundary points.</p> <p>Format: <code>ST_IsSimple (A:geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_IsSimple(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/flink/Function/#st_isvalid","title":"ST_IsValid","text":"<p>Introduction: Test if a geometry is well formed</p> <p>Format: <code>ST_IsValid (A:geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_IsValid(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/flink/Function/#st_length","title":"ST_Length","text":"<p>Introduction: Return the perimeter of A</p> <p>Format: ST_Length (A:geometry)</p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Length(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/flink/Function/#st_lengthspheroid","title":"ST_LengthSpheroid","text":"<p>Introduction: Return the geodesic perimeter of A using WGS84 spheroid. Unit is meter. Works better for large geometries (country level) compared to <code>ST_Length</code> + <code>ST_Transform</code>. It is equivalent to PostGIS <code>ST_Length(geography, use_spheroid=true)</code> and <code>ST_LengthSpheroid</code> function and produces nearly identical results.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lat/lon order. You can use ST_FlipCoordinates to swap lat and lon.</p> <p>Format: <code>ST_LengthSpheroid (A:geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Example: <pre><code>SELECT ST_LengthSpheroid(ST_GeomFromWKT('Polygon ((0 0, 0 90, 0 0))'))\n</code></pre></p> <p>Output: <code>20037508.342789244</code></p>"},{"location":"api/flink/Function/#st_linefrommultipoint","title":"ST_LineFromMultiPoint","text":"<p>Introduction: Creates a LineString from a MultiPoint geometry.</p> <p>Format: <code>ST_LineFromMultiPoint (A:geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_LineFromMultiPoint(df.geometry) AS geom\nFROM df\n</code></pre> <p>Input: <code>MULTIPOINT((10 40), (40 30), (20 20), (30 10))</code></p> <p>Output: <code>LINESTRING (10 40, 40 30, 20 20, 30 10)</code></p>"},{"location":"api/flink/Function/#st_normalize","title":"ST_Normalize","text":"<p>Introduction: Returns the input geometry in its normalized form.</p> <p>Format</p> <p><code>ST_Normalize(geom: geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsEWKT(ST_Normalize(ST_GeomFromWKT('POLYGON((0 1, 1 1, 1 0, 0 0, 0 1))'))) AS geom\n</code></pre> <p>Result:</p> <pre><code>+-----------------------------------+\n|geom                               |\n+-----------------------------------+\n|POLYGON ((0 0, 0 1, 1 1, 1 0, 0 0))|\n+-----------------------------------+\n</code></pre>"},{"location":"api/flink/Function/#st_npoints","title":"ST_NPoints","text":"<p>Introduction: Returns the number of points of the geometry</p> <p>Since: <code>v1.3.0</code></p> <p>Format: <code>ST_NPoints (A:geometry)</code></p> <p>Example: <pre><code>SELECT ST_NPoints(polygondf.countyshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/flink/Function/#st_ndims","title":"ST_NDims","text":"<p>Introduction: Returns the coordinate dimension of the geometry.</p> <p>Format: <code>ST_NDims(geom: geometry)</code></p> <p>Since: <code>v1.3.1</code></p> <p>Spark SQL example with z co-rodinate:</p> <pre><code>SELECT ST_NDims(ST_GeomFromEWKT('POINT(1 1 2)'))\n</code></pre> <p>Output: <code>3</code></p> <p>Example with x,y coordinate:</p> <pre><code>SELECT ST_NDims(ST_GeomFromText('POINT(1 1)'))\n</code></pre> <p>Output: <code>2</code></p>"},{"location":"api/flink/Function/#st_nrings","title":"ST_NRings","text":"<p>Introduction: Returns the number of rings in a Polygon or MultiPolygon. Contrary to ST_NumInteriorRings,  this function also takes into account the number of  exterior rings.</p> <p>This function returns 0 for an empty Polygon or MultiPolygon. If the geometry is not a Polygon or MultiPolygon, an IllegalArgument Exception is thrown.</p> <p>Format: <code>ST_NRings(geom: geometry)</code></p> <p>Since: <code>1.4.1</code></p> <p>Examples:</p> <p>Input: <code>POLYGON ((1 0, 1 1, 2 1, 2 0, 1 0))</code></p> <p>Output: <code>1</code></p> <p>Input: <code>'MULTIPOLYGON (((1 0, 1 6, 6 6, 6 0, 1 0), (2 1, 2 2, 3 2, 3 1, 2 1)), ((10 0, 10 6, 16 6, 16 0, 10 0), (12 1, 12 2, 13 2, 13 1, 12 1)))'</code></p> <p>Output: <code>4</code></p> <p>Input: <code>'POLYGON EMPTY'</code></p> <p>Output: <code>0</code></p> <p>Input: <code>'LINESTRING (1 0, 1 1, 2 1)'</code></p> <p>Output: <code>Unsupported geometry type: LineString, only Polygon or MultiPolygon geometries are supported.</code></p>"},{"location":"api/flink/Function/#st_numgeometries","title":"ST_NumGeometries","text":"<p>Introduction: Returns the number of Geometries. If geometry is a GEOMETRYCOLLECTION (or MULTI*) return the number of geometries, for single geometries will return 1.</p> <p>Format: <code>ST_NumGeometries (A:geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example: <pre><code>SELECT ST_NumGeometries(df.geometry)\nFROM df\n</code></pre></p>"},{"location":"api/flink/Function/#st_numinteriorrings","title":"ST_NumInteriorRings","text":"<p>Introduction: Returns number of interior rings of polygon geometries.</p> <p>Format: <code>ST_NumInteriorRings(geom: geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example: <pre><code>SELECT ST_NumInteriorRings(ST_GeomFromText('POLYGON ((0 0, 0 5, 5 5, 5 0, 0 0), (1 1, 2 1, 2 2, 1 2, 1 1))'))\n</code></pre></p> <p>Output: <code>1</code></p>"},{"location":"api/flink/Function/#st_numpoints","title":"ST_NumPoints","text":"<p>Introduction: Returns number of points in a LineString.</p> <p>Note</p> <p>If any other geometry is provided as an argument, an IllegalArgumentException is thrown.  Example:  <code>SELECT ST_NumPoints(ST_GeomFromWKT('MULTIPOINT ((0 0), (1 1), (0 1), (2 2))'))</code></p> <p>Output: <code>IllegalArgumentException: Unsupported geometry type: MultiPoint, only LineString geometry is supported.</code></p> <p>Format: <code>ST_NumPoints(geom: geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Example: <pre><code>SELECT ST_NumPoints(ST_GeomFromText('LINESTRING(1 2, 1 3)'))\n</code></pre></p> <p>Output: <code>2</code></p>"},{"location":"api/flink/Function/#st_pointn","title":"ST_PointN","text":"<p>Introduction: Return the Nth point in a single linestring or circular linestring in the geometry. Negative values are counted backwards from the end of the LineString, so that -1 is the last point. Returns NULL if there is no linestring in the geometry.</p> <p>Format: <code>ST_PointN(A:geometry, B:integer)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Examples:</p> <pre><code>SELECT ST_PointN(df.geometry, 2)\nFROM df\n</code></pre> <p>Input: <code>LINESTRING(0 0, 1 2, 2 4, 3 6), 2</code></p> <p>Output: <code>POINT (1 2)</code></p> <p>Input: <code>LINESTRING(0 0, 1 2, 2 4, 3 6), -2</code></p> <p>Output: <code>POINT (2 4)</code></p> <p>Input: <code>CIRCULARSTRING(1 1, 1 2, 2 4, 3 6, 1 2, 1 1), -1</code></p> <p>Output: <code>POINT (1 1)</code></p>"},{"location":"api/flink/Function/#st_pointonsurface","title":"ST_PointOnSurface","text":"<p>Introduction: Returns a POINT guaranteed to lie on the surface.</p> <p>Format: <code>ST_PointOnSurface(A:geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Examples:</p> <pre><code>SELECT ST_PointOnSurface(df.geometry)\nFROM df\n</code></pre> <ol> <li> <p>Input: <code>POINT (0 5)</code></p> <p>Output: <code>POINT (0 5)</code></p> </li> <li> <p>Input: <code>LINESTRING(0 5, 0 10)</code></p> <p>Output: <code>POINT (0 5)</code></p> </li> <li> <p>Input: <code>POLYGON((0 0, 0 5, 5 5, 5 0, 0 0))</code></p> <p>Output: <code>POINT (2.5 2.5)</code></p> </li> <li> <p>Input: <code>LINESTRING(0 5 1, 0 0 1, 0 10 2)</code></p> <p>Output: <code>POINT Z(0 0 1)</code></p> </li> </ol>"},{"location":"api/flink/Function/#st_reverse","title":"ST_Reverse","text":"<p>Introduction: Return the geometry with vertex order reversed</p> <p>Format: <code>ST_Reverse (A:geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_Reverse(df.geometry) AS geom\nFROM df\n</code></pre> <p>Input: <code>POLYGON ((-0.5 -0.5, -0.5 0.5, 0.5 0.5, 0.5 -0.5, -0.5 -0.5))</code></p> <p>Output: <code>POLYGON ((-0.5 -0.5, 0.5 -0.5, 0.5 0.5, -0.5 0.5, -0.5 -0.5))</code></p>"},{"location":"api/flink/Function/#st_removepoint","title":"ST_RemovePoint","text":"<p>Introduction: Return Linestring with removed point at given index, position can be omitted and then last one will be removed.</p> <p>Format: <code>ST_RemovePoint(geom: geometry, position: integer)</code></p> <p>Format: <code>ST_RemovePoint(geom: geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example: <pre><code>SELECT ST_RemovePoint(ST_GeomFromText(\"LINESTRING(0 0, 1 1, 1 0)\"), 1)\n</code></pre></p> <p>Output: <code>LINESTRING(0 0, 1 0)</code></p>"},{"location":"api/flink/Function/#st_s2cellids","title":"ST_S2CellIDs","text":"<p>Introduction: Cover the geometry with Google S2 Cells, return the corresponding cell IDs with the given level. The level indicates the size of cells. With a bigger level, the cells will be smaller, the coverage will be more accurate, but the result size will be exponentially increasing.</p> <p>Format: <code>ST_S2CellIDs(geom: geometry, level: Int)</code></p> <p>Since: <code>v1.4.0</code></p> <p>Example: <pre><code>SELECT ST_S2CellIDs(ST_GeomFromText('LINESTRING(1 3 4, 5 6 7)'), 6)\n</code></pre></p> <p>Output: <pre><code>[1159395429071192064, 1159958379024613376, 1160521328978034688, 1161084278931456000, 1170091478186196992, 1170654428139618304]\n</code></pre></p>"},{"location":"api/flink/Function/#st_setpoint","title":"ST_SetPoint","text":"<p>Introduction: Replace Nth point of linestring with given point. Index is 0-based. Negative index are counted backwards, e.g., -1 is last point.</p> <p>Format: <code>ST_SetPoint (linestring: geometry, index: integer, point: geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_SetPoint(ST_GeomFromText('LINESTRING (0 0, 0 1, 1 1)'), 2, ST_GeomFromText('POINT (1 0)')) AS geom\n</code></pre> <p>Result:</p> <pre><code>+--------------------------------+\n|                           geom |\n+--------------------------------+\n|     LINESTRING (0 0, 0 1, 1 0) |\n+--------------------------------+\n</code></pre>"},{"location":"api/flink/Function/#st_setsrid","title":"ST_SetSRID","text":"<p>Introduction: Sets the spatial reference system identifier (SRID) of the geometry.</p> <p>Format: <code>ST_SetSRID (A:geometry, srid: integer)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example: <pre><code>SELECT ST_SetSRID(polygondf.countyshape, 3021)\nFROM polygondf\n</code></pre></p>"},{"location":"api/flink/Function/#st_srid","title":"ST_SRID","text":"<p>Introduction: Return the spatial reference system identifier (SRID) of the geometry.</p> <p>Format: <code>ST_SRID (A:geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example: <pre><code>SELECT ST_SRID(polygondf.countyshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/flink/Function/#st_transform","title":"ST_Transform","text":"<p>Introduction:</p> <p>Transform the Spatial Reference System / Coordinate Reference System of A, from SourceCRS to TargetCRS For SourceCRS and TargetCRS, WKT format is also available since v1.3.1.</p> <p>Note</p> <p>By default, this function uses lat/lon order. You can use ST_FlipCoordinates to swap X and Y.</p> <p>Note</p> <p>If ST_Transform throws an Exception called \"Bursa wolf parameters required\", you need to disable the error notification in ST_Transform. You can append a boolean value at the end.</p> <p>Format: <code>ST_Transform (A:geometry, SourceCRS:string, TargetCRS:string ,[Optional] DisableError)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example (simple): <pre><code>SELECT ST_Transform(polygondf.countyshape, 'epsg:4326','epsg:3857') FROM polygondf\n</code></pre></p> <p>Example (with optional parameters): <pre><code>SELECT ST_Transform(polygondf.countyshape, 'epsg:4326','epsg:3857', false)\nFROM polygondf\n</code></pre></p> <p>Note</p> <p>The detailed EPSG information can be searched on EPSG.io.</p>"},{"location":"api/flink/Function/#st_translate","title":"ST_Translate","text":"<p>Introduction: Returns the input geometry with its X, Y and Z coordinates (if present in the geometry) translated by deltaX, deltaY and deltaZ (if specified)</p> <p>If the geometry is 2D, and a deltaZ parameter is specified, no change is done to the Z coordinate of the geometry and the resultant geometry is also 2D.</p> <p>If the geometry is empty, no change is done to it.</p> <p>If the given geometry contains sub-geometries (GEOMETRY COLLECTION, MULTI POLYGON/LINE/POINT), all underlying geometries are individually translated.</p> <p>Format: <code>ST_Translate(geometry: geometry, deltaX: deltaX, deltaY: deltaY, deltaZ: deltaZ)</code></p> <p>Since: <code>1.4.1</code></p> <p>Example: </p> <p>Input: <code>ST_Translate(GEOMETRYCOLLECTION(MULTIPOLYGON (((1 0, 1 1, 2 1, 2 0, 1 0)), ((1 2, 3 4, 3 5, 1 2))), POINT(1, 1, 1), LINESTRING EMPTY), 2, 2, 3)</code></p> <p>Output: <code>GEOMETRYCOLLECTION(MULTIPOLYGON (((3 2, 3 3, 4 3, 4 2, 3 2)), ((3 4, 5 6, 5 7, 3 4))), POINT(3, 3, 4), LINESTRING EMPTY)</code></p> <p>Input: <code>ST_Translate(POINT(1, 3, 2), 1, 2)</code></p> <p>Output: <code>POINT(2, 5, 2)</code></p>"},{"location":"api/flink/Function/#st_x","title":"ST_X","text":"<p>Introduction: Returns X Coordinate of given Point, null otherwise.</p> <p>Format: <code>ST_X(pointA: Point)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example: <pre><code>SELECT ST_X(ST_POINT(0.0 25.0))\n</code></pre></p> <p>Output: <code>0.0</code></p>"},{"location":"api/flink/Function/#st_xmax","title":"ST_XMax","text":"<p>Introduction: Returns the maximum X coordinate of a geometry</p> <p>Format: <code>ST_XMax (A:geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_XMax(df.geometry) AS xmax\nFROM df\n</code></pre> <p>Input: <code>POLYGON ((-1 -11, 0 10, 1 11, 2 12, -1 -11))</code></p> <p>Output: <code>2</code></p>"},{"location":"api/flink/Function/#st_xmin","title":"ST_XMin","text":"<p>Introduction: Returns the minimum X coordinate of a geometry</p> <p>Format: <code>ST_XMin (A:geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_XMin(df.geometry) AS xmin\nFROM df\n</code></pre> <p>Input: <code>POLYGON ((-1 -11, 0 10, 1 11, 2 12, -1 -11))</code></p> <p>Output: <code>-1</code></p>"},{"location":"api/flink/Function/#st_y","title":"ST_Y","text":"<p>Introduction: Returns Y Coordinate of given Point, null otherwise.</p> <p>Format: <code>ST_Y(pointA: Point)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example: <pre><code>SELECT ST_Y(ST_POINT(0.0 25.0))\n</code></pre></p> <p>Output: <code>25.0</code></p>"},{"location":"api/flink/Function/#st_ymax","title":"ST_YMax","text":"<p>Introduction: Return the minimum Y coordinate of A</p> <p>Format: <code>ST_YMax (A:geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Spark SQL example: <pre><code>SELECT ST_YMax(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre></p> <p>Output : 2</p>"},{"location":"api/flink/Function/#st_ymin","title":"ST_YMin","text":"<p>Introduction: Return the minimum Y coordinate of A</p> <p>Format: <code>ST_Y_Min (A:geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Spark SQL example: <pre><code>SELECT ST_YMin(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre></p> <p>Output : 0</p>"},{"location":"api/flink/Function/#st_z","title":"ST_Z","text":"<p>Introduction: Returns Z Coordinate of given Point, null otherwise.</p> <p>Format: <code>ST_Z(pointA: Point)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example: <pre><code>SELECT ST_Z(ST_POINT(0.0 25.0 11.0))\n</code></pre></p> <p>Output: <code>11.0</code></p>"},{"location":"api/flink/Function/#st_zmax","title":"ST_ZMax","text":"<p>Introduction: Returns Z maxima of the given geometry or null if there is no Z coordinate.</p> <p>Format: <code>ST_ZMax(geom: geometry)</code></p> <p>Since: <code>v1.3.1</code></p> <p>Spark SQL example: <pre><code>SELECT ST_ZMax(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre></p> <p>Output: <code>1.0</code></p>"},{"location":"api/flink/Function/#st_zmin","title":"ST_ZMin","text":"<p>Introduction: Returns Z minima of the given geometry or null if there is no Z coordinate.</p> <p>Format: <code>ST_ZMin(geom: geometry)</code></p> <p>Since: <code>v1.3.1</code></p> <p>Spark SQL example: <pre><code>SELECT ST_ZMin(ST_GeomFromText('LINESTRING(1 3 4, 5 6 7)'))\n</code></pre></p> <p>Output: <code>4.0</code></p>"},{"location":"api/flink/Overview/","title":"Introduction","text":"<p>SedonaSQL supports SQL/MM Part3 Spatial SQL Standard. Please read the programming guide: Sedona with Flink SQL app.</p> <p>Sedona includes SQL operators as follows.</p> <ul> <li>Constructor: Construct a Geometry given an input string or coordinates<ul> <li>Example: ST_GeomFromWKT (string). Create a Geometry from a WKT String.</li> </ul> </li> <li>Function: Execute a function on the given column or columns<ul> <li>Example: ST_Distance (A, B). Given two Geometry A and B, return the Euclidean distance of A and B.</li> </ul> </li> <li>Aggregator: Return a single aggregated value on the given column<ul> <li>Example: ST_Envelope_Aggr (Geometry column). Given a Geometry column, calculate the entire envelope boundary of this column.</li> </ul> </li> <li>Predicate: Execute a logic judgement on the given columns and return true or false<ul> <li>Example: ST_Contains (A, B). Check if A fully contains B. Return \"True\" if yes, else return \"False\".</li> </ul> </li> </ul>"},{"location":"api/flink/Predicate/","title":"Predicate","text":""},{"location":"api/flink/Predicate/#st_contains","title":"ST_Contains","text":"<p>Introduction: Return true if A fully contains B</p> <p>Format: <code>ST_Contains (A:geometry, B:geometry)</code></p> <p>Since: <code>v1.2.0</code></p> <p>SQL example: <pre><code>SELECT * FROM pointdf WHERE ST_Contains(ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), pointdf.arealandmark)\n</code></pre></p>"},{"location":"api/flink/Predicate/#st_disjoint","title":"ST_Disjoint","text":"<p>Introduction: Return true if A and B are disjoint</p> <p>Format: <code>ST_Disjoint (A:geometry, B:geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Spark SQL example: <pre><code>SELECT *\nFROM pointdf WHERE ST_Disjoinnt(ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), pointdf.arealandmark)\n</code></pre></p>"},{"location":"api/flink/Predicate/#st_intersects","title":"ST_Intersects","text":"<p>Introduction: Return true if A intersects B</p> <p>Format: <code>ST_Intersects (A:geometry, B:geometry)</code></p> <p>Since: <code>v1.2.0</code></p> <p>SQL example: <pre><code>SELECT * FROM pointdf WHERE ST_Intersects(ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), pointdf.arealandmark)\n</code></pre></p>"},{"location":"api/flink/Predicate/#st_within","title":"ST_Within","text":"<p>Introduction: Return true if A is within B</p> <p>Format: <code>ST_Within (A:geometry, B:geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>SQL example: <pre><code>SELECT * FROM pointdf WHERE ST_Within(pointdf.arealandmark, ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0))\n</code></pre></p>"},{"location":"api/flink/Predicate/#st_orderingequals","title":"ST_OrderingEquals","text":"<p>Introduction: Returns true if the geometries are equal and the coordinates are in the same order</p> <p>Format: <code>ST_OrderingEquals(A: geometry, B: geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>SQL example 1: <pre><code>SELECT ST_OrderingEquals(ST_GeomFromWKT('POLYGON((2 0, 0 2, -2 0, 2 0))'), ST_GeomFromWKT('POLYGON((2 0, 0 2, -2 0, 2 0))'))\n</code></pre></p> <p>Output: <code>true</code></p> <p>SQL example 2: <pre><code>SELECT ST_OrderingEquals(ST_GeomFromWKT('POLYGON((2 0, 0 2, -2 0, 2 0))'), ST_GeomFromWKT('POLYGON((0 2, -2 0, 2 0, 0 2))'))\n</code></pre></p> <p>Output: <code>false</code></p>"},{"location":"api/flink/Predicate/#st_covers","title":"ST_Covers","text":"<p>Introduction: Return true if A covers B</p> <p>Format: <code>ST_Covers (A:geometry, B:geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>SQL example: <pre><code>SELECT * FROM pointdf WHERE ST_Covers(ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), pointdf.arealandmark)\n</code></pre></p>"},{"location":"api/flink/Predicate/#st_coveredby","title":"ST_CoveredBy","text":"<p>Introduction: Return true if A is covered by B</p> <p>Format: <code>ST_CoveredBy (A:geometry, B:geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>SQL example: <pre><code>SELECT * FROM pointdf WHERE ST_CoveredBy(pointdf.arealandmark, ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0))\n</code></pre></p>"},{"location":"api/sql/AggregateFunction/","title":"Aggregate function","text":""},{"location":"api/sql/AggregateFunction/#st_envelope_aggr","title":"ST_Envelope_Aggr","text":"<p>Introduction: Return the entire envelope boundary of all geometries in A</p> <p>Format: <code>ST_Envelope_Aggr (A:geometryColumn)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_Envelope_Aggr(pointdf.arealandmark)\nFROM pointdf\n</code></pre></p>"},{"location":"api/sql/AggregateFunction/#st_intersection_aggr","title":"ST_Intersection_Aggr","text":"<p>Introduction: Return the polygon intersection of all polygons in A</p> <p>Format: <code>ST_Intersection_Aggr (A:geometryColumn)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_Intersection_Aggr(polygondf.polygonshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/sql/AggregateFunction/#st_union_aggr","title":"ST_Union_Aggr","text":"<p>Introduction: Return the polygon union of all polygons in A</p> <p>Format: <code>ST_Union_Aggr (A:geometryColumn)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_Union_Aggr(polygondf.polygonshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/sql/Constructor/","title":"Constructor","text":""},{"location":"api/sql/Constructor/#read-esri-shapefile","title":"Read ESRI Shapefile","text":"<p>Introduction: Construct a DataFrame from a Shapefile</p> <p>Since: <code>v1.0.0</code></p> <p>SparkSQL example:</p> <pre><code>var spatialRDD = new SpatialRDD[Geometry]\nspatialRDD.rawSpatialRDD = ShapefileReader.readToGeometryRDD(sparkSession.sparkContext, shapefileInputLocation)\nvar rawSpatialDf = Adapter.toDf(spatialRDD,sparkSession)\nrawSpatialDf.createOrReplaceTempView(\"rawSpatialDf\")\nvar spatialDf = sparkSession.sql(\"\"\"\n          | ST_GeomFromWKT(rddshape), _c1, _c2\n          | FROM rawSpatialDf\n        \"\"\".stripMargin)\nspatialDf.show()\nspatialDf.printSchema()\n</code></pre> <p>Note</p> <p>The file extensions of .shp, .shx, .dbf must be in lowercase. Assume you have a shape file called myShapefile, the file structure should be like this: <pre><code>- shapefile1\n- shapefile2\n- myshapefile\n- myshapefile.shp\n- myshapefile.shx\n- myshapefile.dbf\n- myshapefile...\n- ...\n</code></pre></p> <p>Warning</p> <p>Please make sure you use ST_GeomFromWKT to create Geometry type column otherwise that column cannot be used in SedonaSQL.</p> <p>If the file you are reading contains non-ASCII characters you'll need to explicitly set the encoding via <code>sedona.global.charset</code> system property before the call to <code>ShapefileReader.readToGeometryRDD</code>.</p> <p>Example:</p> <pre><code>System.setProperty(\"sedona.global.charset\", \"utf8\")\n</code></pre>"},{"location":"api/sql/Constructor/#st_geomfromgeohash","title":"ST_GeomFromGeoHash","text":"<p>Introduction: Create Geometry from geohash string and optional precision</p> <p>Format: <code>ST_GeomFromGeoHash(geohash: string, precision: int)</code></p> <p>Since: <code>v1.1.1</code></p> <p>Spark SQL example: <pre><code>SELECT ST_GeomFromGeoHash('s00twy01mt', 4) AS geom\n</code></pre></p> <p>result:</p> <pre><code>+--------------------------------------------------------------------------------------------------------------------+\n|geom                                                                                                                |\n+--------------------------------------------------------------------------------------------------------------------+\n|POLYGON ((0.703125 0.87890625, 0.703125 1.0546875, 1.0546875 1.0546875, 1.0546875 0.87890625, 0.703125 0.87890625)) |\n+--------------------------------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"api/sql/Constructor/#st_geomfromgeojson","title":"ST_GeomFromGeoJSON","text":"<p>Introduction: Construct a Geometry from GeoJson</p> <p>Format: <code>ST_GeomFromGeoJSON (GeoJson:string)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>var polygonJsonDf = sparkSession.read.format(\"csv\").option(\"delimiter\",\"\\t\").option(\"header\",\"false\").load(geoJsonGeomInputLocation)\npolygonJsonDf.createOrReplaceTempView(\"polygontable\")\npolygonJsonDf.show()\nvar polygonDf = sparkSession.sql(\n\"\"\"\n          | SELECT ST_GeomFromGeoJSON(polygontable._c0) AS countyshape\n          | FROM polygontable\n        \"\"\".stripMargin)\npolygonDf.show()\n</code></pre></p> <p>Warning</p> <p>The way that SedonaSQL reads GeoJSON is different from that in SparkSQL</p>"},{"location":"api/sql/Constructor/#st_geomfromgml","title":"ST_GeomFromGML","text":"<p>Introduction: Construct a Geometry from GML.</p> <p>Format: <code>ST_GeomFromGML (gml:string)</code></p> <p>Since: <code>v1.3.0</code></p> <p>SQL example: <pre><code>SELECT ST_GeomFromGML('&lt;gml:LineString srsName=\"EPSG:4269\"&gt;&lt;gml:coordinates&gt;-71.16028,42.258729 -71.160837,42.259112 -71.161143,42.25932&lt;/gml:coordinates&gt;&lt;/gml:LineString&gt;') AS geometry\n</code></pre></p>"},{"location":"api/sql/Constructor/#st_geomfromkml","title":"ST_GeomFromKML","text":"<p>Introduction: Construct a Geometry from KML.</p> <p>Format: <code>ST_GeomFromKML (kml:string)</code></p> <p>Since: <code>v1.3.0</code></p> <p>SQL example: <pre><code>SELECT ST_GeomFromKML('&lt;LineString&gt;&lt;coordinates&gt;-71.1663,42.2614 -71.1667,42.2616&lt;/coordinates&gt;&lt;/LineString&gt;') AS geometry\n</code></pre></p>"},{"location":"api/sql/Constructor/#st_geomfromtext","title":"ST_GeomFromText","text":"<p>Introduction: Construct a Geometry from Wkt. If srid is not set, it defaults to 0 (unknown). Alias of ST_GeomFromWKT</p> <p>Format: <code>ST_GeomFromText (Wkt:string)</code> <code>ST_GeomFromText (Wkt:string, srid:integer)</code></p> <p>Since: <code>v1.0.0</code></p> <p>The optional srid parameter was added in <code>v1.3.1</code></p> <p>Spark SQL example: <pre><code>SELECT ST_GeomFromText('POINT(40.7128 -74.0060)') AS geometry\n</code></pre></p>"},{"location":"api/sql/Constructor/#st_geomfromwkb","title":"ST_GeomFromWKB","text":"<p>Introduction: Construct a Geometry from WKB string or Binary</p> <p>Format: <code>ST_GeomFromWKB (Wkb:string)</code> <code>ST_GeomFromWKB (Wkb:binary)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_GeomFromWKB(polygontable._c0) AS polygonshape\nFROM polygontable\n</code></pre></p>"},{"location":"api/sql/Constructor/#st_geomfromwkt","title":"ST_GeomFromWKT","text":"<p>Introduction: Construct a Geometry from Wkt. If srid is not set, it defaults to 0 (unknown).</p> <p>Format: <code>ST_GeomFromWKT (Wkt:string)</code> <code>ST_GeomFromWKT (Wkt:string, srid:integer)</code></p> <p>Since: <code>v1.0.0</code></p> <p>The optional srid parameter was added in <code>v1.3.1</code></p> <p>Spark SQL example: <pre><code>SELECT ST_GeomFromWKT(polygontable._c0) AS polygonshape\nFROM polygontable\n</code></pre></p> <pre><code>SELECT ST_GeomFromWKT('POINT(40.7128 -74.0060)') AS geometry\n</code></pre>"},{"location":"api/sql/Constructor/#st_linefromtext","title":"ST_LineFromText","text":"<p>Introduction: Construct a Line from Wkt text</p> <p>Format: <code>ST_LineFromText (Wkt:string)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Spark SQL example: <pre><code>SELECT ST_LineFromText(linetable._c0) AS lineshape\nFROM linetable\n</code></pre></p> <pre><code>SELECT ST_LineFromText('Linestring(1 2, 3 4)') AS line\n</code></pre>"},{"location":"api/sql/Constructor/#st_linestringfromtext","title":"ST_LineStringFromText","text":"<p>Introduction: Construct a LineString from Text, delimited by Delimiter</p> <p>Format: <code>ST_LineStringFromText (Text:string, Delimiter:char)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_LineStringFromText(linestringtable._c0,',') AS linestringshape\nFROM linestringtable\n</code></pre></p> <pre><code>SELECT ST_LineStringFromText('-74.0428197,40.6867969,-74.0421975,40.6921336,-74.0508020,40.6912794', ',') AS linestringshape\n</code></pre>"},{"location":"api/sql/Constructor/#st_mlinefromtext","title":"ST_MLineFromText","text":"<p>Introduction: Construct a MultiLineString from Wkt. If srid is not set, it defaults to 0 (unknown).</p> <p>Format: <code>ST_MLineFromText (Wkt:string)</code> <code>ST_MLineFromText (Wkt:string, srid:integer)</code></p> <p>Since: <code>v1.3.1</code></p> <p>Spark SQL example: <pre><code>SELECT ST_MLineFromText('MULTILINESTRING((1 2, 3 4), (4 5, 6 7))') AS multiLine;\nSELECT ST_MLineFromText('MULTILINESTRING((1 2, 3 4), (4 5, 6 7))',4269) AS multiLine;\n</code></pre></p>"},{"location":"api/sql/Constructor/#st_mpolyfromtext","title":"ST_MPolyFromText","text":"<p>Introduction: Construct a MultiPolygon from Wkt. If srid is not set, it defaults to 0 (unknown).</p> <p>Format: <code>ST_MPolyFromText (Wkt:string)</code> <code>ST_MPolyFromText (Wkt:string, srid:integer)</code></p> <p>Since: <code>v1.3.1</code></p> <p>Spark SQL example: <pre><code>SELECT ST_MPolyFromText('MULTIPOLYGON(((-70.916 42.1002,-70.9468 42.0946,-70.9765 42.0872 )))') AS multiPolygon\nSELECT ST_MPolyFromText('MULTIPOLYGON(((-70.916 42.1002,-70.9468 42.0946,-70.9765 42.0872 )))',4269) AS multiPolygon\n</code></pre></p>"},{"location":"api/sql/Constructor/#st_point","title":"ST_Point","text":"<p>Introduction: Construct a Point from X and Y</p> <p>Format: <code>ST_Point (X:decimal, Y:decimal)</code></p> <p>Since: <code>v1.0.0</code></p> <p>In <code>v1.4.0</code> an optional Z parameter was removed to be more consistent with other spatial SQL implementations. If you are upgrading from an older version of Sedona - please use ST_PointZ to create 3D points.</p> <p>Spark SQL example: <pre><code>SELECT ST_Point(CAST(pointtable._c0 AS Decimal(24,20)), CAST(pointtable._c1 AS Decimal(24,20))) AS pointshape\nFROM pointtable\n</code></pre></p>"},{"location":"api/sql/Constructor/#st_pointz","title":"ST_PointZ","text":"<p>Introduction: Construct a Point from X, Y and Z and an optional srid. If srid is not set, it defaults to 0 (unknown).</p> <p>Format: <code>ST_PointZ (X:decimal, Y:decimal, Z:decimal)</code> Format: <code>ST_PointZ (X:decimal, Y:decimal, Z:decimal, srid:integer)</code></p> <p>Since: <code>v1.4.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_PointZ(1.0, 2.0, 3.0) AS pointshape\n</code></pre></p>"},{"location":"api/sql/Constructor/#st_pointfromtext","title":"ST_PointFromText","text":"<p>Introduction: Construct a Point from Text, delimited by Delimiter</p> <p>Format: <code>ST_PointFromText (Text:string, Delimiter:char)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_PointFromText(pointtable._c0,',') AS pointshape\nFROM pointtable\n</code></pre></p> <pre><code>SELECT ST_PointFromText('40.7128,-74.0060', ',') AS pointshape\n</code></pre>"},{"location":"api/sql/Constructor/#st_polygonfromenvelope","title":"ST_PolygonFromEnvelope","text":"<p>Introduction: Construct a Polygon from MinX, MinY, MaxX, MaxY.</p> <p>Format: <code>ST_PolygonFromEnvelope (MinX:decimal, MinY:decimal, MaxX:decimal, MaxY:decimal)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT *\nFROM pointdf\nWHERE ST_Contains(ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), pointdf.pointshape)\n</code></pre></p>"},{"location":"api/sql/Constructor/#st_polygonfromtext","title":"ST_PolygonFromText","text":"<p>Introduction: Construct a Polygon from Text, delimited by Delimiter. Path must be closed</p> <p>Format: <code>ST_PolygonFromText (Text:string, Delimiter:char)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_PolygonFromText(polygontable._c0,',') AS polygonshape\nFROM polygontable\n</code></pre></p> <pre><code>SELECT ST_PolygonFromText('-74.0428197,40.6867969,-74.0421975,40.6921336,-74.0508020,40.6912794,-74.0428197,40.6867969', ',') AS polygonshape\n</code></pre>"},{"location":"api/sql/DataFrameAPI/","title":"DataFrame Style functions","text":"<p>Sedona SQL functions can be used in a DataFrame style API similar to Spark functions.</p> <p>The following objects contain the exposed functions: <code>org.apache.spark.sql.sedona_sql.expressions.st_functions</code>, <code>org.apache.spark.sql.sedona_sql.expressions.st_constructors</code>, <code>org.apache.spark.sql.sedona_sql.expressions.st_predicates</code>, and <code>org.apache.spark.sql.sedona_sql.expressions.st_aggregates</code>.</p> <p>Every functions can take all <code>Column</code> arguments. Additionally, overloaded forms can commonly take a mix of <code>String</code> and other Scala types (such as <code>Double</code>) as arguments.</p> <p>In general the following rules apply (although check the documentation of specific functions for any exceptions):</p> ScalaPython <ol> <li>Every function returns a <code>Column</code> so that it can be used interchangeably with Spark functions as well as <code>DataFrame</code> methods such as <code>DataFrame.select</code> or <code>DataFrame.join</code>.</li> <li>Every function has a form that takes all <code>Column</code> arguments. These are the most versatile of the forms.</li> <li>Most functions have a form that takes a mix of <code>String</code> arguments with other Scala types.</li> </ol> <ol> <li><code>Column</code> type arguments are passed straight through and are always accepted.</li> <li><code>str</code> type arguments are always assumed to be names of columns and are wrapped in a <code>Column</code> to support that. If an actual string literal needs to be passed then it will need to be wrapped in a <code>Column</code> using <code>pyspark.sql.functions.lit</code>.</li> <li>Any other types of arguments are checked on a per function basis. Generally, arguments that could reasonably support a python native type are accepted and passed through.   4. Shapely <code>Geometry</code> objects are not currently accepted in any of the functions.</li> </ol> <p>The exact mixture of argument types allowed is function specific. However, in these instances, all <code>String</code> arguments are assumed to be the names of columns and will be wrapped in a <code>Column</code> automatically. Non-<code>String</code> arguments are assumed to be literals that are passed to the sedona function. If you need to pass a <code>String</code> literal then you should use the all <code>Column</code> form of the sedona function and wrap the <code>String</code> literal in a <code>Column</code> with the <code>lit</code> Spark function.</p> <p>A short example of using this API (uses the <code>array_min</code> and <code>array_max</code> Spark functions):</p> ScalaPython <pre><code>val values_df = spark.sql(\"SELECT array(0.0, 1.0, 2.0) AS values\")\nval min_value = array_min(\"values\")\nval max_value = array_max(\"values\")\nval point_df = values_df.select(ST_Point(min_value, max_value).as(\"point\"))\n</code></pre> <pre><code>from pyspark.sql import functions as f\n\nfrom sedona.sql import st_constructors as stc\n\ndf = spark.sql(\"SELECT array(0.0, 1.0, 2.0) AS values\")\n\nmin_value = f.array_min(\"values\")\nmax_value = f.array_max(\"values\")\n\ndf = df.select(stc.ST_Point(min_value, max_value).alias(\"point\"))\n</code></pre> <p>The above code will generate the following dataframe: <pre><code>+-----------+\n|point      |\n+-----------+\n|POINT (0 2)|\n+-----------+\n</code></pre></p> <p>Some functions will take native python values and infer them as literals. For example:</p> <pre><code>df = df.select(stc.ST_Point(1.0, 3.0).alias(\"point\"))\n</code></pre> <p>This will generate a dataframe with a constant point in a column: <pre><code>+-----------+\n|point      |\n+-----------+\n|POINT (1 3)|\n+-----------+\n</code></pre></p>"},{"location":"api/sql/Function/","title":"Function","text":""},{"location":"api/sql/Function/#st_3ddistance","title":"ST_3DDistance","text":"<p>Introduction: Return the 3-dimensional minimum cartesian distance between A and B</p> <p>Format: <code>ST_3DDistance (A:geometry, B:geometry)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_3DDistance(polygondf.countyshape, polygondf.countyshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/sql/Function/#st_addpoint","title":"ST_AddPoint","text":"<p>Introduction: RETURN Linestring with additional point at the given index, if position is not available the point will be added at the end of line.</p> <p>Format: <code>ST_AddPoint(geom: geometry, point: geometry, position: integer)</code></p> <p>Format: <code>ST_AddPoint(geom: geometry, point: geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_AddPoint(ST_GeomFromText(\"LINESTRING(0 0, 1 1, 1 0)\"), ST_GeomFromText(\"Point(21 52)\"), 1)\n\nSELECT ST_AddPoint(ST_GeomFromText(\"Linestring(0 0, 1 1, 1 0)\"), ST_GeomFromText(\"Point(21 52)\"))\n</code></pre></p> <p>Output: <pre><code>LINESTRING(0 0, 21 52, 1 1, 1 0)\nLINESTRING(0 0, 1 1, 1 0, 21 52)\n</code></pre></p>"},{"location":"api/sql/Function/#st_area","title":"ST_Area","text":"<p>Introduction: Return the area of A</p> <p>Format: <code>ST_Area (A:geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_Area(polygondf.countyshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/sql/Function/#st_areaspheroid","title":"ST_AreaSpheroid","text":"<p>Introduction: Return the geodesic area of A using WGS84 spheroid. Unit is square meter. Works better for large geometries (country level) compared to <code>ST_Area</code> + <code>ST_Transform</code>. It is equivalent to PostGIS <code>ST_Area(geography, use_spheroid=true)</code> function and produces nearly identical results.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lat/lon order. You can use ST_FlipCoordinates to swap lat and lon.</p> <p>Format: <code>ST_AreaSpheroid (A:geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Spark SQL example:</p> <pre><code>SELECT ST_AreaSpheroid(ST_GeomFromWKT('Polygon ((35 34, 30 28, 34 25, 35 34))'))\n</code></pre> <p>Output: <code>201824850811.76245</code></p>"},{"location":"api/sql/Function/#st_asbinary","title":"ST_AsBinary","text":"<p>Introduction: Return the Well-Known Binary representation of a geometry</p> <p>Format: <code>ST_AsBinary (A:geometry)</code></p> <p>Since: <code>v1.1.1</code></p> <p>Spark SQL example: <pre><code>SELECT ST_AsBinary(polygondf.countyshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/sql/Function/#st_asewkb","title":"ST_AsEWKB","text":"<p>Introduction: Return the Extended Well-Known Binary representation of a geometry. EWKB is an extended version of WKB which includes the SRID of the geometry. The format originated in PostGIS but is supported by many GIS tools. If the geometry is lacking SRID a WKB format is produced. Se ST_SetSRID</p> <p>Format: <code>ST_AsEWKB (A:geometry)</code></p> <p>Since: <code>v1.1.1</code></p> <p>Spark SQL example: <pre><code>SELECT ST_AsEWKB(polygondf.countyshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/sql/Function/#st_asewkt","title":"ST_AsEWKT","text":"<p>Introduction: Return the Extended Well-Known Text representation of a geometry. EWKT is an extended version of WKT which includes the SRID of the geometry. The format originated in PostGIS but is supported by many GIS tools. If the geometry is lacking SRID a WKT format is produced. See ST_SetSRID</p> <p>Format: <code>ST_AsEWKT (A:geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Spark SQL example: <pre><code>SELECT ST_AsEWKT(polygondf.countyshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/sql/Function/#st_asgeojson","title":"ST_AsGeoJSON","text":"<p>Introduction: Return the GeoJSON string representation of a geometry</p> <p>Format: <code>ST_AsGeoJSON (A:geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_AsGeoJSON(polygondf.countyshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/sql/Function/#st_asgml","title":"ST_AsGML","text":"<p>Introduction: Return the GML string representation of a geometry</p> <p>Format: <code>ST_AsGML (A:geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_AsGML(polygondf.countyshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/sql/Function/#st_askml","title":"ST_AsKML","text":"<p>Introduction: Return the KML string representation of a geometry</p> <p>Format: <code>ST_AsKML (A:geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_AsKML(polygondf.countyshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/sql/Function/#st_astext","title":"ST_AsText","text":"<p>Introduction: Return the Well-Known Text string representation of a geometry</p> <p>Format: <code>ST_AsText (A:geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_AsText(polygondf.countyshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/sql/Function/#st_azimuth","title":"ST_Azimuth","text":"<p>Introduction: Returns Azimuth for two given points in radians null otherwise.</p> <p>Format: <code>ST_Azimuth(pointA: Point, pointB: Point)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_Azimuth(ST_POINT(0.0, 25.0), ST_POINT(0.0, 0.0))\n</code></pre></p> <p>Output: <code>3.141592653589793</code></p>"},{"location":"api/sql/Function/#st_boundary","title":"ST_Boundary","text":"<p>Introduction: Returns the closure of the combinatorial boundary of this Geometry.</p> <p>Format: <code>ST_Boundary(geom: geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_Boundary(ST_GeomFromText('POLYGON((1 1,0 0, -1 1, 1 1))'))\n</code></pre></p> <p>Output: <code>LINESTRING (1 1, 0 0, -1 1, 1 1)</code></p>"},{"location":"api/sql/Function/#st_buffer","title":"ST_Buffer","text":"<p>Introduction: Returns a geometry/geography that represents all points whose distance from this Geometry/geography is less than or equal to distance.</p> <p>Format: <code>ST_Buffer (A:geometry, buffer: Double)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_Buffer(polygondf.countyshape, 1)\nFROM polygondf\n</code></pre></p>"},{"location":"api/sql/Function/#st_buildarea","title":"ST_BuildArea","text":"<p>Introduction: Returns the areal geometry formed by the constituent linework of the input geometry.</p> <p>Format: <code>ST_BuildArea (A:geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_BuildArea(\nST_GeomFromText('MULTILINESTRING((0 0, 20 0, 20 20, 0 20, 0 0),(2 2, 18 2, 18 18, 2 18, 2 2))')\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+----------------------------------------------------------------------------+\n|geom                                                                        |\n+----------------------------------------------------------------------------+\n|POLYGON((0 0,0 20,20 20,20 0,0 0),(2 2,18 2,18 18,2 18,2 2))                |\n+----------------------------------------------------------------------------+\n</code></pre>"},{"location":"api/sql/Function/#st_centroid","title":"ST_Centroid","text":"<p>Introduction: Return the centroid point of A</p> <p>Format: <code>ST_Centroid (A:geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_Centroid(polygondf.countyshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/sql/Function/#st_collect","title":"ST_Collect","text":"<p>Introduction: Returns MultiGeometry object based on geometry column/s or array with geometries</p> <p>Format</p> <p><code>ST_Collect(*geom: geometry)</code></p> <p><code>ST_Collect(geom: array&lt;geometry&gt;)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Collect(\nST_GeomFromText('POINT(21.427834 52.042576573)'),\nST_GeomFromText('POINT(45.342524 56.342354355)')\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+---------------------------------------------------------------+\n|geom                                                           |\n+---------------------------------------------------------------+\n|MULTIPOINT ((21.427834 52.042576573), (45.342524 56.342354355))|\n+---------------------------------------------------------------+\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_Collect(\nArray(\nST_GeomFromText('POINT(21.427834 52.042576573)'),\nST_GeomFromText('POINT(45.342524 56.342354355)')\n)\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+---------------------------------------------------------------+\n|geom                                                           |\n+---------------------------------------------------------------+\n|MULTIPOINT ((21.427834 52.042576573), (45.342524 56.342354355))|\n+---------------------------------------------------------------+\n</code></pre>"},{"location":"api/sql/Function/#st_collectionextract","title":"ST_CollectionExtract","text":"<p>Introduction: Returns a homogeneous multi-geometry from a given geometry collection.</p> <p>The type numbers are: 1. POINT 2. LINESTRING 3. POLYGON</p> <p>If the type parameter is omitted a multi-geometry of the highest dimension is returned.</p> <p>Format: <code>ST_CollectionExtract (A:geometry)</code></p> <p>Format: <code>ST_CollectionExtract (A:geometry, type:Int)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>WITH test_data as (\nST_GeomFromText(\n'GEOMETRYCOLLECTION(POINT(40 10), POLYGON((0 0, 0 5, 5 5, 5 0, 0 0)))'\n) as geom\n)\nSELECT ST_CollectionExtract(geom) as c1, ST_CollectionExtract(geom, 1) as c2\nFROM test_data\n</code></pre> <p>Result:</p> <pre><code>+----------------------------------------------------------------------------+\n|c1                                        |c2                               |\n+----------------------------------------------------------------------------+\n|MULTIPOLYGON(((0 0, 0 5, 5 5, 5 0, 0 0))) |MULTIPOINT(40 10)                |              |\n+----------------------------------------------------------------------------+\n</code></pre>"},{"location":"api/sql/Function/#st_concavehull","title":"ST_ConcaveHull","text":"<p>Introduction: Return the Concave Hull of polgyon A, with alpha set to pctConvex[0, 1] in the Delaunay Triangulation method, the concave hull will not contain a hole unless allowHoles is set to true</p> <p>Format: <code>ST_ConcaveHull (A:geometry, pctConvex:float)</code></p> <p>Format: <code>ST_ConcaveHull (A:geometry, pctConvex:float, allowHoles:Boolean)</code></p> <p>Since: <code>v1.4.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_ConcaveHull(polygondf.countyshape, pctConvex)`\nFROM polygondf\n</code></pre></p>"},{"location":"api/sql/Function/#st_convexhull","title":"ST_ConvexHull","text":"<p>Introduction: Return the Convex Hull of polgyon A</p> <p>Format: <code>ST_ConvexHull (A:geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_ConvexHull(polygondf.countyshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/sql/Function/#st_difference","title":"ST_Difference","text":"<p>Introduction: Return the difference between geometry A and B (return part of geometry A that does not intersect geometry B)</p> <p>Format: <code>ST_Difference (A:geometry, B:geometry)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Difference(ST_GeomFromWKT('POLYGON ((-3 -3, 3 -3, 3 3, -3 3, -3 -3))'), ST_GeomFromWKT('POLYGON ((0 -4, 4 -4, 4 4, 0 4, 0 -4))'))\n</code></pre> <p>Result:</p> <pre><code>POLYGON ((0 -3, -3 -3, -3 3, 0 3, 0 -3))\n</code></pre>"},{"location":"api/sql/Function/#st_distance","title":"ST_Distance","text":"<p>Introduction: Return the Euclidean distance between A and B</p> <p>Format: <code>ST_Distance (A:geometry, B:geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_Distance(polygondf.countyshape, polygondf.countyshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/sql/Function/#st_distancesphere","title":"ST_DistanceSphere","text":"<p>Introduction: Return the haversine / great-circle distance of A using a given earth radius (default radius: 6371008.0). Unit is meter. Compared to <code>ST_Distance</code> + <code>ST_Transform</code>, it works better for datasets that cover large regions such as continents or the entire planet. It is equivalent to PostGIS <code>ST_Distance(geography, use_spheroid=false)</code> and <code>ST_DistanceSphere</code> function and produces nearly identical results. It provides faster but less accurate result compared to <code>ST_DistanceSpheroid</code>.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lat/lon order. You can use ST_FlipCoordinates to swap lat and lon. For non-point data, we first take the centroids of both geometries and then compute the distance.</p> <p>Format: <code>ST_DistanceSphere (A:geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Spark SQL example 1: <pre><code>SELECT ST_DistanceSphere(ST_GeomFromWKT('POINT (51.3168 -0.56)'), ST_GeomFromWKT('POINT (55.9533 -3.1883)'))\n</code></pre></p> <p>Output: <code>543796.9506134904</code></p> <p>Spark SQL example 2: <pre><code>SELECT ST_DistanceSphere(ST_GeomFromWKT('POINT (51.3168 -0.56)'), ST_GeomFromWKT('POINT (55.9533 -3.1883)'), 6378137.0)\n</code></pre></p> <p>Output: <code>544405.4459192449</code></p>"},{"location":"api/sql/Function/#st_distancespheroid","title":"ST_DistanceSpheroid","text":"<p>Introduction: Return the geodesic distance of A using WGS84 spheroid. Unit is meter. Compared to <code>ST_Distance</code> + <code>ST_Transform</code>, it works better for datasets that cover large regions such as continents or the entire planet. It is equivalent to PostGIS <code>ST_Distance(geography, use_spheroid=true)</code> and <code>ST_DistanceSpheroid</code> function and produces nearly identical results. It provides slower but more accurate result compared to <code>ST_DistanceSphere</code>.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lat/lon order. You can use ST_FlipCoordinates to swap lat and lon. For non-point data, we first take the centroids of both geometries and then compute the distance.</p> <p>Format: <code>ST_DistanceSpheroid (A:geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Spark SQL example: <pre><code>SELECT ST_DistanceSpheroid(ST_GeomFromWKT('POINT (51.3168 -0.56)'), ST_GeomFromWKT('POINT (55.9533 -3.1883)'))\n</code></pre></p> <p>Output: <code>544430.9411996207</code></p>"},{"location":"api/sql/Function/#st_dump","title":"ST_Dump","text":"<p>Introduction: It expands the geometries. If the geometry is simple (Point, Polygon Linestring etc.) it returns the geometry itself, if the geometry is collection or multi it returns record for each of collection components.</p> <p>Format: <code>ST_Dump(geom: geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_Dump(ST_GeomFromText('MULTIPOINT ((10 40), (40 30), (20 20), (30 10))'))\n</code></pre></p> <p>Output: <code>[POINT (10 40), POINT (40 30), POINT (20 20), POINT (30 10)]</code></p>"},{"location":"api/sql/Function/#st_dumppoints","title":"ST_DumpPoints","text":"<p>Introduction: Returns list of Points which geometry consists of.</p> <p>Format: <code>ST_DumpPoints(geom: geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_DumpPoints(ST_GeomFromText('LINESTRING (0 0, 1 1, 1 0)'))\n</code></pre></p> <p>Output: <code>[POINT (0 0), POINT (0 1), POINT (1 1), POINT (1 0), POINT (0 0)]</code></p>"},{"location":"api/sql/Function/#st_endpoint","title":"ST_EndPoint","text":"<p>Introduction: Returns last point of given linestring.</p> <p>Format: <code>ST_EndPoint(geom: geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_EndPoint(ST_GeomFromText('LINESTRING(100 150,50 60, 70 80, 160 170)'))\n</code></pre></p> <p>Output: <code>POINT(160 170)</code></p>"},{"location":"api/sql/Function/#st_envelope","title":"ST_Envelope","text":"<p>Introduction: Return the envelop boundary of A</p> <p>Format: <code>ST_Envelope (A:geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example:</p> <pre><code>SELECT ST_Envelope(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/sql/Function/#st_exteriorring","title":"ST_ExteriorRing","text":"<p>Introduction: Returns a line string representing the exterior ring of the POLYGON geometry. Return NULL if the geometry is not a polygon.</p> <p>Format: <code>ST_ExteriorRing(geom: geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_ExteriorRing(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre></p> <p>Output: <code>LINESTRING (0 0, 1 1, 1 2, 1 1, 0 0)</code></p>"},{"location":"api/sql/Function/#st_flipcoordinates","title":"ST_FlipCoordinates","text":"<p>Introduction: Returns a version of the given geometry with X and Y axis flipped.</p> <p>Format: <code>ST_FlipCoordinates(A:geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_FlipCoordinates(df.geometry)\nFROM df\n</code></pre></p> <p>Input: <code>POINT (1 2)</code></p> <p>Output: <code>POINT (2 1)</code></p>"},{"location":"api/sql/Function/#st_force_2d","title":"ST_Force_2D","text":"<p>Introduction: Forces the geometries into a \"2-dimensional mode\" so that all output representations will only have the X and Y coordinates</p> <p>Format: <code>ST_Force_2D (A:geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(\nST_Force_2D(ST_GeomFromText('POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))'))\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+---------------------------------------------------------------+\n|geom                                                           |\n+---------------------------------------------------------------+\n|POLYGON((0 0,0 5,5 0,0 0),(1 1,3 1,1 3,1 1))                   |\n+---------------------------------------------------------------+\n</code></pre>"},{"location":"api/sql/Function/#st_force3d","title":"ST_Force3D","text":"<p>Introduction: Forces the geometry into a 3-dimensional model so that all output representations will have X, Y and Z coordinates. An optionally given zValue is tacked onto the geometry if the geometry is 2-dimensional. Default value of zValue is 0.0 If the given geometry is 3-dimensional, no change is performed on it. If the given geometry is empty, no change is performed on it.</p> <p>Note</p> <p>Example output is after calling ST_AsText() on returned geometry, which adds Z for in the WKT for 3D geometries</p> <p>Format: <code>ST_Force3D(geometry, zValue)</code></p> <p>Since: <code>1.4.1</code></p> <p>Spark SQL Example:</p> <pre><code>SELECT ST_Force3D(geometry) AS geom\n</code></pre> <p>Input: <code>LINESTRING(0 1, 1 2, 2 1)</code></p> <p>Output: <code>LINESTRING Z(0 1 0, 1 2 0, 2 1 0)</code></p> <p>Input: <code>POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))</code></p> <p>Output: <code>POLYGON Z((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))</code></p> <pre><code>SELECT ST_Force3D(geometry, 2.3) AS geom\n</code></pre> <p>Input: <code>LINESTRING(0 1, 1 2, 2 1)</code></p> <p>Output: <code>LINESTRING Z(0 1 2.3, 1 2 2.3, 2 1 2.3)</code></p> <p>Input: <code>POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))</code></p> <p>Output: <code>POLYGON Z((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))</code></p> <p>Input: <code>LINESTRING EMPTY</code></p> <p>Output: <code>LINESTRING EMPTY</code></p>"},{"location":"api/sql/Function/#st_geohash","title":"ST_GeoHash","text":"<p>Introduction: Returns GeoHash of the geometry with given precision</p> <p>Format: <code>ST_GeoHash(geom: geometry, precision: int)</code></p> <p>Since: <code>v1.1.1</code></p> <p>Example:</p> <p>Query:</p> <pre><code>SELECT ST_GeoHash(ST_GeomFromText('POINT(21.427834 52.042576573)'), 5) AS geohash\n</code></pre> <p>Result:</p> <pre><code>+-----------------------------+\n|geohash                      |\n+-----------------------------+\n|u3r0p                        |\n+-----------------------------+\n</code></pre>"},{"location":"api/sql/Function/#st_geometricmedian","title":"ST_GeometricMedian","text":"<p>Introduction: Computes the approximate geometric median of a MultiPoint geometry using the Weiszfeld algorithm. The geometric median provides a centrality measure that is less sensitive to outlier points than the centroid.</p> <p>The algorithm will iterate until the distance change between successive iterations is less than the supplied <code>tolerance</code> parameter. If this condition has not been met after <code>maxIter</code> iterations, the function will produce an error and exit, unless <code>failIfNotConverged</code> is set to <code>false</code>.</p> <p>If a <code>tolerance</code> value is not provided, a default <code>tolerance</code> value is <code>1e-6</code>.</p> <p>Format: <code>ST_GeometricMedian(geom: geometry, tolerance: float, maxIter: integer, failIfNotConverged: boolean)</code></p> <p>Format: <code>ST_GeometricMedian(geom: geometry, tolerance: float, maxIter: integer)</code></p> <p>Format: <code>ST_GeometricMedian(geom: geometry, tolerance: float)</code></p> <p>Format: <code>ST_GeometricMedian(geom: geometry)</code></p> <p>Default parameters: <code>tolerance: 1e-6, maxIter: 1000, failIfNotConverged: false</code></p> <p>Since: <code>1.4.1</code></p> <p>Example: <pre><code>SELECT ST_GeometricMedian(ST_GeomFromWKT('MULTIPOINT((0 0), (1 1), (2 2), (200 200))'))\n</code></pre></p> <p>Output: <pre><code>POINT (1.9761550281255005 1.9761550281255005)\n</code></pre></p>"},{"location":"api/sql/Function/#st_geometryn","title":"ST_GeometryN","text":"<p>Introduction: Return the 0-based Nth geometry if the geometry is a GEOMETRYCOLLECTION, (MULTI)POINT, (MULTI)LINESTRING, MULTICURVE or (MULTI)POLYGON. Otherwise, return null</p> <p>Format: <code>ST_GeometryN(geom: geometry, n: Int)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_GeometryN(ST_GeomFromText('MULTIPOINT((1 2), (3 4), (5 6), (8 9))'), 1)\n</code></pre></p> <p>Output: <code>POINT (3 4)</code></p>"},{"location":"api/sql/Function/#st_geometrytype","title":"ST_GeometryType","text":"<p>Introduction: Returns the type of the geometry as a string. EG: 'ST_Linestring', 'ST_Polygon' etc.</p> <p>Format: <code>ST_GeometryType (A:geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_GeometryType(polygondf.countyshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/sql/Function/#st_interiorringn","title":"ST_InteriorRingN","text":"<p>Introduction: Returns the Nth interior linestring ring of the polygon geometry. Returns NULL if the geometry is not a polygon or the given N is out of range</p> <p>Format: <code>ST_InteriorRingN(geom: geometry, n: Int)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_InteriorRingN(ST_GeomFromText('POLYGON((0 0, 0 5, 5 5, 5 0, 0 0), (1 1, 2 1, 2 2, 1 2, 1 1), (1 3, 2 3, 2 4, 1 4, 1 3), (3 3, 4 3, 4 4, 3 4, 3 3))'), 0)\n</code></pre></p> <p>Output: <code>LINESTRING (1 1, 2 1, 2 2, 1 2, 1 1)</code></p>"},{"location":"api/sql/Function/#st_intersection","title":"ST_Intersection","text":"<p>Introduction: Return the intersection geometry of A and B</p> <p>Format: <code>ST_Intersection (A:geometry, B:geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example:</p> <pre><code>SELECT ST_Intersection(polygondf.countyshape, polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/sql/Function/#st_isclosed","title":"ST_IsClosed","text":"<p>Introduction: RETURNS true if the LINESTRING start and end point are the same.</p> <p>Format: <code>ST_IsClosed(geom: geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_IsClosed(ST_GeomFromText('LINESTRING(0 0, 1 1, 1 0)'))\n</code></pre></p> <p>Output: <code>false</code></p>"},{"location":"api/sql/Function/#st_isempty","title":"ST_IsEmpty","text":"<p>Introduction: Test if a geometry is empty geometry</p> <p>Format: <code>ST_IsEmpty (A:geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Spark SQL example:</p> <pre><code>SELECT ST_IsEmpty(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/sql/Function/#st_isring","title":"ST_IsRing","text":"<p>Introduction: RETURN true if LINESTRING is ST_IsClosed and ST_IsSimple.</p> <p>Format: <code>ST_IsRing(geom: geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_IsRing(ST_GeomFromText(\"LINESTRING(0 0, 0 1, 1 1, 1 0, 0 0)\"))\n</code></pre></p> <p>Output: <code>true</code></p>"},{"location":"api/sql/Function/#st_issimple","title":"ST_IsSimple","text":"<p>Introduction: Test if geometry's only self-intersections are at boundary points.</p> <p>Format: <code>ST_IsSimple (A:geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example:</p> <pre><code>SELECT ST_IsSimple(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/sql/Function/#st_isvalid","title":"ST_IsValid","text":"<p>Introduction: Test if a geometry is well formed</p> <p>Format: <code>ST_IsValid (A:geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example:</p> <pre><code>SELECT ST_IsValid(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/sql/Function/#st_length","title":"ST_Length","text":"<p>Introduction: Return the perimeter of A</p> <p>Format: ST_Length (A:geometry)</p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_Length(polygondf.countyshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/sql/Function/#st_lengthspheroid","title":"ST_LengthSpheroid","text":"<p>Introduction: Return the geodesic perimeter of A using WGS84 spheroid. Unit is meter. Works better for large geometries (country level) compared to <code>ST_Length</code> + <code>ST_Transform</code>. It is equivalent to PostGIS <code>ST_Length(geography, use_spheroid=true)</code> and <code>ST_LengthSpheroid</code> function and produces nearly identical results.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lat/lon order. You can use ST_FlipCoordinates to swap lat and lon.</p> <p>Format: <code>ST_LengthSpheroid (A:geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Spark SQL example: <pre><code>SELECT ST_LengthSpheroid(ST_GeomFromWKT('Polygon ((0 0, 0 90, 0 0))'))\n</code></pre></p> <p>Output: <code>20037508.342789244</code></p>"},{"location":"api/sql/Function/#st_linefrommultipoint","title":"ST_LineFromMultiPoint","text":"<p>Introduction: Creates a LineString from a MultiPoint geometry.</p> <p>Format: <code>ST_LineFromMultiPoint (A:geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(\nST_LineFromMultiPoint(ST_GeomFromText('MULTIPOINT((10 40), (40 30), (20 20), (30 10))'))\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+---------------------------------------------------------------+\n|geom                                                           |\n+---------------------------------------------------------------+\n|LINESTRING (10 40, 40 30, 20 20, 30 10)                        |\n+---------------------------------------------------------------+\n</code></pre>"},{"location":"api/sql/Function/#st_lineinterpolatepoint","title":"ST_LineInterpolatePoint","text":"<p>Introduction: Returns a point interpolated along a line. First argument must be a LINESTRING. Second argument is a Double between 0 and 1 representing fraction of total linestring length the point has to be located.</p> <p>Format: <code>ST_LineInterpolatePoint (geom: geometry, fraction: Double)</code></p> <p>Since: <code>v1.0.1</code></p> <p>Spark SQL example: <pre><code>SELECT ST_LineInterpolatePoint(ST_GeomFromWKT('LINESTRING(25 50, 100 125, 150 190)'), 0.2) as Interpolated\n</code></pre></p> <p>Output: <pre><code>+-----------------------------------------+\n|Interpolated                             |\n+-----------------------------------------+\n|POINT (51.5974135047432 76.5974135047432)|\n+-----------------------------------------+\n</code></pre></p>"},{"location":"api/sql/Function/#st_linemerge","title":"ST_LineMerge","text":"<p>Introduction: Returns a LineString formed by sewing together the constituent line work of a MULTILINESTRING.</p> <p>Note</p> <p>Only works for MULTILINESTRING. Using other geometry will return a GEOMETRYCOLLECTION EMPTY. If the MultiLineString can't be merged, the original MULTILINESTRING is returned.</p> <p>Format: <code>ST_LineMerge (A:geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <pre><code>SELECT ST_LineMerge(geometry)\nFROM df\n</code></pre>"},{"location":"api/sql/Function/#st_linesubstring","title":"ST_LineSubstring","text":"<p>Introduction: Return a linestring being a substring of the input one starting and ending at the given fractions of total 2d length. Second and third arguments are Double values between 0 and 1. This only works with LINESTRINGs.</p> <p>Format: <code>ST_LineSubstring (geom: geometry, startfraction: Double, endfraction: Double)</code></p> <p>Since: <code>v1.0.1</code></p> <p>Spark SQL example: <pre><code>SELECT ST_LineSubstring(ST_GeomFromWKT('LINESTRING(25 50, 100 125, 150 190)'), 0.333, 0.666) as Substring\n</code></pre></p> <p>Output: <pre><code>+------------------------------------------------------------------------------------------------+\n|Substring                                                                                       |\n+------------------------------------------------------------------------------------------------+\n|LINESTRING (69.28469348539744 94.28469348539744, 100 125, 111.70035626068274 140.21046313888758)|\n+------------------------------------------------------------------------------------------------+\n</code></pre></p>"},{"location":"api/sql/Function/#st_makepolygon","title":"ST_MakePolygon","text":"<p>Introduction: Function to convert closed linestring to polygon including holes</p> <p>Format: <code>ST_MakePolygon(geom: geometry, holes: array&lt;geometry&gt;)</code></p> <p>Since: <code>v1.1.0</code></p> <p>Example:</p> <p>Query: <pre><code>SELECT\nST_MakePolygon(\nST_GeomFromText('LINESTRING(7 -1, 7 6, 9 6, 9 1, 7 -1)'),\nARRAY(ST_GeomFromText('LINESTRING(6 2, 8 2, 8 1, 6 1, 6 2)'))\n) AS polygon\n</code></pre></p> <p>Result:</p> <pre><code>+----------------------------------------------------------------+\n|polygon                                                         |\n+----------------------------------------------------------------+\n|POLYGON ((7 -1, 7 6, 9 6, 9 1, 7 -1), (6 2, 8 2, 8 1, 6 1, 6 2))|\n+----------------------------------------------------------------+\n</code></pre>"},{"location":"api/sql/Function/#st_makevalid","title":"ST_MakeValid","text":"<p>Introduction: Given an invalid geometry, create a valid representation of the geometry.</p> <p>Collapsed geometries are either converted to empty (keepCollaped=true) or a valid geometry of lower dimension (keepCollapsed=false). Default is keepCollapsed=false.</p> <p>Format: <code>ST_MakeValid (A:geometry)</code></p> <p>Format: <code>ST_MakeValid (A:geometry, keepCollapsed:Boolean)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example:</p> <pre><code>WITH linestring AS (\nSELECT ST_GeomFromWKT('LINESTRING(1 1, 1 1)') AS geom\n) SELECT ST_MakeValid(geom), ST_MakeValid(geom, true) FROM linestring\n</code></pre> <p>Result: <pre><code>+------------------+------------------------+\n|st_makevalid(geom)|st_makevalid(geom, true)|\n+------------------+------------------------+\n|  LINESTRING EMPTY|             POINT (1 1)|\n+------------------+------------------------+\n</code></pre></p> <p>Note</p> <p>In Sedona up to and including version 1.2 the behaviour of ST_MakeValid was different.</p> <p>Be sure to check you code when upgrading. The previous implementation only worked for (multi)polygons and had a different interpretation of the second, boolean, argument. It would also sometimes return multiple geometries for a single geometry input.</p>"},{"location":"api/sql/Function/#st_minimumboundingcircle","title":"ST_MinimumBoundingCircle","text":"<p>Introduction: Returns the smallest circle polygon that contains a geometry.</p> <p>Format: <code>ST_MinimumBoundingCircle(geom: geometry, [Optional] quadrantSegments:int)</code></p> <p>Since: <code>v1.0.1</code></p> <p>Spark SQL example: <pre><code>SELECT ST_MinimumBoundingCircle(ST_GeomFromText('POLYGON((1 1,0 0, -1 1, 1 1))'))\n</code></pre></p>"},{"location":"api/sql/Function/#st_minimumboundingradius","title":"ST_MinimumBoundingRadius","text":"<p>Introduction: Returns a struct containing the center point and radius of the smallest circle that contains a geometry.</p> <p>Format: <code>ST_MinimumBoundingRadius(geom: geometry)</code></p> <p>Since: <code>v1.0.1</code></p> <p>Spark SQL example: <pre><code>SELECT ST_MinimumBoundingRadius(ST_GeomFromText('POLYGON((1 1,0 0, -1 1, 1 1))'))\n</code></pre></p>"},{"location":"api/sql/Function/#st_multi","title":"ST_Multi","text":"<p>Introduction: Returns a MultiGeometry object based on the geometry input. ST_Multi is basically an alias for ST_Collect with one geometry.</p> <p>Format</p> <p><code>ST_Multi(geom: geometry)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Multi(\nST_GeomFromText('POINT(1 1)')\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+---------------------------------------------------------------+\n|geom                                                           |\n+---------------------------------------------------------------+\n|MULTIPOINT (1 1)                                               |\n+---------------------------------------------------------------+\n</code></pre>"},{"location":"api/sql/Function/#st_ndims","title":"ST_NDims","text":"<p>Introduction: Returns the coordinate dimension of the geometry.</p> <p>Format: <code>ST_NDims(geom: geometry)</code></p> <p>Since: <code>v1.3.1</code></p> <p>Spark SQL example with z co-rodinate:</p> <pre><code>SELECT ST_NDims(ST_GeomFromEWKT('POINT(1 1 2)'))\n</code></pre> <p>Output: <code>3</code></p> <p>Spark SQL example with x,y coordinate:</p> <pre><code>SELECT ST_NDims(ST_GeomFromText('POINT(1 1)'))\n</code></pre> <p>Output: <code>2</code></p>"},{"location":"api/sql/Function/#st_normalize","title":"ST_Normalize","text":"<p>Introduction: Returns the input geometry in its normalized form.</p> <p>Format</p> <p><code>ST_Normalize(geom: geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsEWKT(ST_Normalize(ST_GeomFromWKT('POLYGON((0 1, 1 1, 1 0, 0 0, 0 1))'))) AS geom\n</code></pre> <p>Result:</p> <pre><code>+-----------------------------------+\n|geom                               |\n+-----------------------------------+\n|POLYGON ((0 0, 0 1, 1 1, 1 0, 0 0))|\n+-----------------------------------+\n</code></pre>"},{"location":"api/sql/Function/#st_npoints","title":"ST_NPoints","text":"<p>Introduction: Return points of the geometry</p> <p>Since: <code>v1.0.0</code></p> <p>Format: <code>ST_NPoints (A:geometry)</code></p> <pre><code>SELECT ST_NPoints(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/sql/Function/#st_nrings","title":"ST_NRings","text":"<p>Introduction: Returns the number of rings in a Polygon or MultiPolygon. Contrary to ST_NumInteriorRings, this function also takes into account the number of  exterior rings.</p> <p>This function returns 0 for an empty Polygon or MultiPolygon. If the geometry is not a Polygon or MultiPolygon, an IllegalArgument Exception is thrown.</p> <p>Format: <code>ST_NRings(geom: geometry)</code></p> <p>Since: <code>1.4.1</code></p> <p>Examples:</p> <p>Input: <code>POLYGON ((1 0, 1 1, 2 1, 2 0, 1 0))</code></p> <p>Output: <code>1</code></p> <p>Input: <code>'MULTIPOLYGON (((1 0, 1 6, 6 6, 6 0, 1 0), (2 1, 2 2, 3 2, 3 1, 2 1)), ((10 0, 10 6, 16 6, 16 0, 10 0), (12 1, 12 2, 13 2, 13 1, 12 1)))'</code></p> <p>Output: <code>4</code></p> <p>Input: <code>'POLYGON EMPTY'</code></p> <p>Output: <code>0</code></p> <p>Input: <code>'LINESTRING (1 0, 1 1, 2 1)'</code></p> <p>Output: <code>Unsupported geometry type: LineString, only Polygon or MultiPolygon geometries are supported.</code></p>"},{"location":"api/sql/Function/#st_numgeometries","title":"ST_NumGeometries","text":"<p>Introduction: Returns the number of Geometries. If geometry is a GEOMETRYCOLLECTION (or MULTI*) return the number of geometries, for single geometries will return 1.</p> <p>Format: <code>ST_NumGeometries (A:geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <pre><code>SELECT ST_NumGeometries(df.geometry)\nFROM df\n</code></pre>"},{"location":"api/sql/Function/#st_numinteriorrings","title":"ST_NumInteriorRings","text":"<p>Introduction: RETURNS number of interior rings of polygon geometries.</p> <p>Format: <code>ST_NumInteriorRings(geom: geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_NumInteriorRings(ST_GeomFromText('POLYGON ((0 0, 0 5, 5 5, 5 0, 0 0), (1 1, 2 1, 2 2, 1 2, 1 1))'))\n</code></pre></p> <p>Output: <code>1</code></p>"},{"location":"api/sql/Function/#st_numpoints","title":"ST_NumPoints","text":"<p>Introduction: Returns number of points in a LineString</p> <p>Note</p> <p>If any other geometry is provided as an argument, an IllegalArgumentException is thrown. Example: <code>SELECT ST_NumPoints(ST_GeomFromWKT('MULTIPOINT ((0 0), (1 1), (0 1), (2 2))'))</code></p> <p>Output: <code>IllegalArgumentException: Unsupported geometry type: MultiPoint, only LineString geometry is supported.</code></p> <p>Format: <code>ST_NumPoints(geom: geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Spark SQL example: <pre><code>SELECT ST_NumPoints(ST_GeomFromText('LINESTRING(0 1, 1 0, 2 0)'))\n</code></pre></p> <p>Output: <code>3</code></p>"},{"location":"api/sql/Function/#st_pointn","title":"ST_PointN","text":"<p>Introduction: Return the Nth point in a single linestring or circular linestring in the geometry. Negative values are counted backwards from the end of the LineString, so that -1 is the last point. Returns NULL if there is no linestring in the geometry.</p> <p>Format: <code>ST_PointN(geom: geometry, n: integer)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Spark SQL example: <pre><code>SELECT ST_PointN(ST_GeomFromText(\"LINESTRING(0 0, 1 2, 2 4, 3 6)\"), 2) AS geom\n</code></pre></p> <p>Result:</p> <pre><code>+---------------------------------------------------------------+\n|geom                                                           |\n+---------------------------------------------------------------+\n|POINT (1 2)                                                    |\n+---------------------------------------------------------------+\n</code></pre>"},{"location":"api/sql/Function/#st_pointonsurface","title":"ST_PointOnSurface","text":"<p>Introduction: Returns a POINT guaranteed to lie on the surface.</p> <p>Format: <code>ST_PointOnSurface(A:geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Examples:</p> <pre><code>SELECT ST_AsText(ST_PointOnSurface(ST_GeomFromText('POINT(0 5)')));\n st_astext\n------------\n POINT(0 5)\n\nSELECT ST_AsText(ST_PointOnSurface(ST_GeomFromText('LINESTRING(0 5, 0 10)')));\n st_astext\n------------\n POINT(0 5)\n\nSELECT ST_AsText(ST_PointOnSurface(ST_GeomFromText('POLYGON((0 0, 0 5, 5 5, 5 0, 0 0))')));\n   st_astext\n----------------\n POINT(2.5 2.5)\n\nSELECT ST_AsText(ST_PointOnSurface(ST_GeomFromText('LINESTRING(0 5 1, 0 0 1, 0 10 2)')));\n   st_astext\n----------------\n POINT Z(0 0 1)\n</code></pre>"},{"location":"api/sql/Function/#st_precisionreduce","title":"ST_PrecisionReduce","text":"<p>Introduction: Reduce the decimals places in the coordinates of the geometry to the given number of decimal places. The last decimal place will be rounded.</p> <p>Format: <code>ST_PrecisionReduce (A:geometry, B:int)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example:</p> <p><pre><code>SELECT ST_PrecisionReduce(polygondf.countyshape, 9)\nFROM polygondf\n</code></pre> The new coordinates will only have 9 decimal places.</p>"},{"location":"api/sql/Function/#st_removepoint","title":"ST_RemovePoint","text":"<p>Introduction: RETURN Line with removed point at given index, position can be omitted and then last one will be removed.</p> <p>Format: <code>ST_RemovePoint(geom: geometry, position: integer)</code></p> <p>Format: <code>ST_RemovePoint(geom: geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_RemovePoint(ST_GeomFromText(\"LINESTRING(0 0, 1 1, 1 0)\"), 1)\n</code></pre></p> <p>Output: <code>LINESTRING(0 0, 1 0)</code></p>"},{"location":"api/sql/Function/#st_reverse","title":"ST_Reverse","text":"<p>Introduction: Return the geometry with vertex order reversed</p> <p>Format: <code>ST_Reverse (A:geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(\nST_Reverse(ST_GeomFromText('LINESTRING(0 0, 1 2, 2 4, 3 6)'))\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+---------------------------------------------------------------+\n|geom                                                           |\n+---------------------------------------------------------------+\n|LINESTRING (3 6, 2 4, 1 2, 0 0)                                |\n+---------------------------------------------------------------+\n</code></pre>"},{"location":"api/sql/Function/#st_s2cellids","title":"ST_S2CellIDs","text":"<p>Introduction: Cover the geometry with Google S2 Cells, return the corresponding cell IDs with the given level. The level indicates the size of cells. With a bigger level, the cells will be smaller, the coverage will be more accurate, but the result size will be exponentially increasing.</p> <p>Format: <code>ST_S2CellIDs(geom: geometry, level: Int)</code></p> <p>Since: <code>v1.4.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_S2CellIDs(ST_GeomFromText('LINESTRING(1 3 4, 5 6 7)'), 6)\n</code></pre></p> <p>Output: <pre><code>+------------------------------------------------------------------------------------------------------------------------------+\n|st_s2cellids(st_geomfromtext(LINESTRING(1 3 4, 5 6 7), 0), 6)                                                                 |\n+------------------------------------------------------------------------------------------------------------------------------+\n|[1159395429071192064, 1159958379024613376, 1160521328978034688, 1161084278931456000, 1170091478186196992, 1170654428139618304]|\n+------------------------------------------------------------------------------------------------------------------------------+\n</code></pre></p>"},{"location":"api/sql/Function/#st_setpoint","title":"ST_SetPoint","text":"<p>Introduction: Replace Nth point of linestring with given point. Index is 0-based. Negative index are counted backwards, e.g., -1 is last point.</p> <p>Format: <code>ST_SetPoint (linestring: geometry, index: integer, point: geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_SetPoint(ST_GeomFromText('LINESTRING (0 0, 0 1, 1 1)'), 2, ST_GeomFromText('POINT (1 0)')) AS geom\n</code></pre> <p>Result:</p> <pre><code>+--------------------------+\n|geom                      |\n+--------------------------+\n|LINESTRING (0 0, 0 1, 1 0)|\n+--------------------------+\n</code></pre>"},{"location":"api/sql/Function/#st_setsrid","title":"ST_SetSRID","text":"<p>Introduction: Sets the spatial reference system identifier (SRID) of the geometry.</p> <p>Format: <code>ST_SetSRID (A:geometry, srid: Integer)</code></p> <p>Since: <code>v1.1.1</code></p> <p>Spark SQL example: <pre><code>SELECT ST_SetSRID(polygondf.countyshape, 3021)\nFROM polygondf\n</code></pre></p>"},{"location":"api/sql/Function/#st_simplifypreservetopology","title":"ST_SimplifyPreserveTopology","text":"<p>Introduction: Simplifies a geometry and ensures that the result is a valid geometry having the same dimension and number of components as the input, and with the components having the same topological relationship.</p> <p>Since: <code>v1.0.0</code></p> <p>Format: <code>ST_SimplifyPreserveTopology (A:geometry, distanceTolerance: Double)</code></p> <pre><code>SELECT ST_SimplifyPreserveTopology(polygondf.countyshape, 10.0)\nFROM polygondf\n</code></pre>"},{"location":"api/sql/Function/#st_split","title":"ST_Split","text":"<p>Introduction: Split an input geometry by another geometry (called the blade). Linear (LineString or MultiLineString) geometry can be split by a Point, MultiPoint, LineString, MultiLineString, Polygon, or MultiPolygon. Polygonal (Polygon or MultiPolygon) geometry can be split by a LineString, MultiLineString, Polygon, or MultiPolygon. In either case, when a polygonal blade is used then the boundary of the blade is what is actually split by. ST_Split will always return either a MultiLineString or MultiPolygon even if they only contain a single geometry. Homogeneous GeometryCollections are treated as a multi-geometry of the type it contains. For example, if a GeometryCollection of only Point geometries is passed as a blade it is the same as passing a MultiPoint of the same geometries.</p> <p>Since: <code>v1.4.0</code></p> <p>Format: <code>ST_Split (input: geometry, blade: geometry)</code></p> <p>Spark SQL Example: <pre><code>SELECT ST_Split(\nST_GeomFromWKT('LINESTRING (0 0, 1.5 1.5, 2 2)'),\nST_GeomFromWKT('MULTIPOINT (0.5 0.5, 1 1)'))\n</code></pre></p> <p>Output: <code>MULTILINESTRING ((0 0, 0.5 0.5), (0.5 0.5, 1 1), (1 1, 1.5 1.5, 2 2))</code></p>"},{"location":"api/sql/Function/#st_srid","title":"ST_SRID","text":"<p>Introduction: Return the spatial reference system identifier (SRID) of the geometry.</p> <p>Format: <code>ST_SRID (A:geometry)</code></p> <p>Since: <code>v1.1.1</code></p> <p>Spark SQL example: <pre><code>SELECT ST_SRID(polygondf.countyshape)\nFROM polygondf\n</code></pre></p>"},{"location":"api/sql/Function/#st_startpoint","title":"ST_StartPoint","text":"<p>Introduction: Returns first point of given linestring.</p> <p>Format: <code>ST_StartPoint(geom: geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_StartPoint(ST_GeomFromText('LINESTRING(100 150,50 60, 70 80, 160 170)'))\n</code></pre></p> <p>Output: <code>POINT(100 150)</code></p>"},{"location":"api/sql/Function/#st_subdivide","title":"ST_SubDivide","text":"<p>Introduction: Returns list of geometries divided based of given maximum number of vertices.</p> <p>Format: <code>ST_SubDivide(geom: geometry, maxVertices: int)</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_SubDivide(ST_GeomFromText(\"POLYGON((35 10, 45 45, 15 40, 10 20, 35 10), (20 30, 35 35, 30 20, 20 30))\"), 5)\n</code></pre></p> <p>Output: <pre><code>[\n    POLYGON((37.857142857142854 20, 35 10, 10 20, 37.857142857142854 20)),\n    POLYGON((15 20, 10 20, 15 40, 15 20)),\n    POLYGON((20 20, 15 20, 15 30, 20 30, 20 20)),\n    POLYGON((26.428571428571427 20, 20 20, 20 30, 26.4285714 23.5714285, 26.4285714 20)),\n    POLYGON((15 30, 15 40, 20 40, 20 30, 15 30)),\n    POLYGON((20 40, 26.4285714 40, 26.4285714 32.1428571, 20 30, 20 40)),\n    POLYGON((37.8571428 20, 30 20, 34.0476190 32.1428571, 37.8571428 32.1428571, 37.8571428 20)),\n    POLYGON((34.0476190 34.6825396, 26.4285714 32.1428571, 26.4285714 40, 34.0476190 40, 34.0476190 34.6825396)),\n    POLYGON((34.0476190 32.1428571, 35 35, 37.8571428 35, 37.8571428 32.1428571, 34.0476190 32.1428571)),\n    POLYGON((35 35, 34.0476190 34.6825396, 34.0476190 35, 35 35)),\n    POLYGON((34.0476190 35, 34.0476190 40, 37.8571428 40, 37.8571428 35, 34.0476190 35)),\n    POLYGON((30 20, 26.4285714 20, 26.4285714 23.5714285, 30 20)),\n    POLYGON((15 40, 37.8571428 43.8095238, 37.8571428 40, 15 40)),\n    POLYGON((45 45, 37.8571428 20, 37.8571428 43.8095238, 45 45))\n]\n</code></pre></p> <p>Spark SQL example:</p> <pre><code>SELECT ST_SubDivide(ST_GeomFromText(\"LINESTRING(0 0, 85 85, 100 100, 120 120, 21 21, 10 10, 5 5)\"), 5)\n</code></pre> <p>Output: <pre><code>[\n    LINESTRING(0 0, 5 5)\n    LINESTRING(5 5, 10 10)\n    LINESTRING(10 10, 21 21)\n    LINESTRING(21 21, 60 60)\n    LINESTRING(60 60, 85 85)\n    LINESTRING(85 85, 100 100)\n    LINESTRING(100 100, 120 120)\n]\n</code></pre></p>"},{"location":"api/sql/Function/#st_subdivideexplode","title":"ST_SubDivideExplode","text":"<p>Introduction: It works the same as ST_SubDivide but returns new rows with geometries instead of list.</p> <p>Format: <code>ST_SubDivideExplode(geom: geometry, maxVertices: int)</code></p> <p>Since: <code>v1.1.0</code></p> <p>Example:</p> <p>Query: <pre><code>SELECT ST_SubDivideExplode(ST_GeomFromText(\"LINESTRING(0 0, 85 85, 100 100, 120 120, 21 21, 10 10, 5 5)\"), 5)\n</code></pre></p> <p>Result:</p> <pre><code>+-----------------------------+\n|geom                         |\n+-----------------------------+\n|LINESTRING(0 0, 5 5)         |\n|LINESTRING(5 5, 10 10)       |\n|LINESTRING(10 10, 21 21)     |\n|LINESTRING(21 21, 60 60)     |\n|LINESTRING(60 60, 85 85)     |\n|LINESTRING(85 85, 100 100)   |\n|LINESTRING(100 100, 120 120) |\n+-----------------------------+\n</code></pre> <p>Using Lateral View</p> <p>Table: <pre><code>+-------------------------------------------------------------+\n|geometry                                                     |\n+-------------------------------------------------------------+\n|LINESTRING(0 0, 85 85, 100 100, 120 120, 21 21, 10 10, 5 5)  |\n+-------------------------------------------------------------+\n</code></pre></p> <p>Query <pre><code>select geom from geometries LATERAL VIEW ST_SubdivideExplode(geometry, 5) AS geom\n</code></pre></p> <p>Result: <pre><code>+-----------------------------+\n|geom                         |\n+-----------------------------+\n|LINESTRING(0 0, 5 5)         |\n|LINESTRING(5 5, 10 10)       |\n|LINESTRING(10 10, 21 21)     |\n|LINESTRING(21 21, 60 60)     |\n|LINESTRING(60 60, 85 85)     |\n|LINESTRING(85 85, 100 100)   |\n|LINESTRING(100 100, 120 120) |\n+-----------------------------+\n</code></pre></p>"},{"location":"api/sql/Function/#st_symdifference","title":"ST_SymDifference","text":"<p>Introduction: Return the symmetrical difference between geometry A and B (return parts of geometries which are in either of the sets, but not in their intersection)</p> <p>Format: <code>ST_SymDifference (A:geometry, B:geometry)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_SymDifference(ST_GeomFromWKT('POLYGON ((-3 -3, 3 -3, 3 3, -3 3, -3 -3))'), ST_GeomFromWKT('POLYGON ((-2 -3, 4 -3, 4 3, -2 3, -2 -3))'))\n</code></pre> <p>Result:</p> <pre><code>MULTIPOLYGON (((-2 -3, -3 -3, -3 3, -2 3, -2 -3)), ((3 -3, 3 3, 4 3, 4 -3, 3 -3)))\n</code></pre>"},{"location":"api/sql/Function/#st_transform","title":"ST_Transform","text":"<p>Introduction:</p> <p>Transform the Spatial Reference System / Coordinate Reference System of A, from SourceCRS to TargetCRS. For SourceCRS and TargetCRS, WKT format is also available since v1.3.1.</p> <p>Note</p> <p>By default, this function uses lat/lon order. You can use ST_FlipCoordinates to swap X and Y.</p> <p>Note</p> <p>If ST_Transform throws an Exception called \"Bursa wolf parameters required\", you need to disable the error notification in ST_Transform. You can append a boolean value at the end.</p> <p>Format: <code>ST_Transform (A:geometry, SourceCRS:string, TargetCRS:string ,[Optional] DisableError)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example (simple): <pre><code>SELECT ST_Transform(polygondf.countyshape, 'epsg:4326','epsg:3857')\nFROM polygondf\n</code></pre></p> <p>Spark SQL example (with optional parameters): <pre><code>SELECT ST_Transform(polygondf.countyshape, 'epsg:4326','epsg:3857', false)\nFROM polygondf\n</code></pre></p> <p>Note</p> <p>The detailed EPSG information can be searched on EPSG.io.</p>"},{"location":"api/sql/Function/#st_translate","title":"ST_Translate","text":"<p>Introduction: Returns the input geometry with its X, Y and Z coordinates (if present in the geometry) translated by deltaX, deltaY and deltaZ (if specified)</p> <p>If the geometry is 2D, and a deltaZ parameter is specified, no change is done to the Z coordinate of the geometry and the resultant geometry is also 2D.</p> <p>If the geometry is empty, no change is done to it.  If the given geometry contains sub-geometries (GEOMETRY COLLECTION, MULTI POLYGON/LINE/POINT), all underlying geometries are individually translated.</p> <p>Format: <code>ST_Translate(geometry: geometry, deltaX: deltaX, deltaY: deltaY, deltaZ: deltaZ)</code></p> <p>Since: <code>1.4.1</code></p> <p>Example:</p> <p>Input: <code>ST_Translate(GEOMETRYCOLLECTION(MULTIPOLYGON (((1 0, 1 1, 2 1, 2 0, 1 0)), ((1 2, 3 4, 3 5, 1 2))), POINT(1, 1, 1), LINESTRING EMPTY), 2, 2, 3)</code></p> <p>Output: <code>GEOMETRYCOLLECTION(MULTIPOLYGON (((3 2, 3 3, 4 3, 4 2, 3 2)), ((3 4, 5 6, 5 7, 3 4))), POINT(3, 3, 4), LINESTRING EMPTY)</code></p> <p>Input: <code>ST_Translate(POINT(1, 3, 2), 1, 2)</code></p> <p>Output: <code>POINT(2, 5, 2)</code></p>"},{"location":"api/sql/Function/#st_union","title":"ST_Union","text":"<p>Introduction: Return the union of geometry A and B</p> <p>Format: <code>ST_Union (A:geometry, B:geometry)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Union(ST_GeomFromWKT('POLYGON ((-3 -3, 3 -3, 3 3, -3 3, -3 -3))'), ST_GeomFromWKT('POLYGON ((1 -2, 5 0, 1 2, 1 -2))'))\n</code></pre> <p>Result:</p> <pre><code>POLYGON ((3 -1, 3 -3, -3 -3, -3 3, 3 3, 3 1, 5 0, 3 -1))\n</code></pre>"},{"location":"api/sql/Function/#st_x","title":"ST_X","text":"<p>Introduction: Returns X Coordinate of given Point null otherwise.</p> <p>Format: <code>ST_X(pointA: Point)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_X(ST_POINT(0.0 25.0))\n</code></pre></p> <p>Output: <code>0.0</code></p>"},{"location":"api/sql/Function/#st_xmax","title":"ST_XMax","text":"<p>Introduction: Returns the maximum X coordinate of a geometry</p> <p>Format: <code>ST_XMax (A:geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_XMax(df.geometry) AS xmax\nFROM df\n</code></pre> <p>Input: <code>POLYGON ((-1 -11, 0 10, 1 11, 2 12, -1 -11))</code></p> <p>Output: <code>2</code></p>"},{"location":"api/sql/Function/#st_xmin","title":"ST_XMin","text":"<p>Introduction: Returns the minimum X coordinate of a geometry</p> <p>Format: <code>ST_XMin (A:geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_XMin(df.geometry) AS xmin\nFROM df\n</code></pre> <p>Input: <code>POLYGON ((-1 -11, 0 10, 1 11, 2 12, -1 -11))</code></p> <p>Output: <code>-1</code></p>"},{"location":"api/sql/Function/#st_y","title":"ST_Y","text":"<p>Introduction: Returns Y Coordinate of given Point, null otherwise.</p> <p>Format: <code>ST_Y(pointA: Point)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_Y(ST_POINT(0.0 25.0))\n</code></pre></p> <p>Output: <code>25.0</code></p>"},{"location":"api/sql/Function/#st_ymax","title":"ST_YMax","text":"<p>Introduction: Return the minimum Y coordinate of A</p> <p>Format: <code>ST_YMax (A:geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Spark SQL example: <pre><code>SELECT ST_YMax(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre></p> <p>Output: 2</p>"},{"location":"api/sql/Function/#st_ymin","title":"ST_YMin","text":"<p>Introduction: Return the minimum Y coordinate of A</p> <p>Format: <code>ST_Y_Min (A:geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Spark SQL example: <pre><code>SELECT ST_YMin(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre></p> <p>Output : 0</p>"},{"location":"api/sql/Function/#st_z","title":"ST_Z","text":"<p>Introduction: Returns Z Coordinate of given Point, null otherwise.</p> <p>Format: <code>ST_Z(pointA: Point)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_Z(ST_POINT(0.0 25.0 11.0))\n</code></pre></p> <p>Output: <code>11.0</code></p>"},{"location":"api/sql/Function/#st_zmax","title":"ST_ZMax","text":"<p>Introduction: Returns Z maxima of the given geometry or null if there is no Z coordinate.</p> <p>Format: <code>ST_ZMax(geom: geometry)</code></p> <p>Since: <code>v1.3.1</code></p> <p>Spark SQL example: <pre><code>SELECT ST_ZMax(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre></p> <p>Output: <code>1.0</code></p>"},{"location":"api/sql/Function/#st_zmin","title":"ST_ZMin","text":"<p>Introduction: Returns Z minima of the given geometry or null if there is no Z coordinate.</p> <p>Format: <code>ST_ZMin(geom: geometry)</code></p> <p>Since: <code>v1.3.1</code></p> <p>Spark SQL example: <pre><code>SELECT ST_ZMin(ST_GeomFromText('LINESTRING(1 3 4, 5 6 7)'))\n</code></pre></p> <p>Output: <code>4.0</code></p>"},{"location":"api/sql/Optimizer/","title":"Query optimization","text":"<p>Sedona Spatial operators fully supports Apache SparkSQL query optimizer. It has the following query optimization features:</p> <ul> <li>Automatically optimizes range join query and distance join query.</li> <li>Automatically performs predicate pushdown.</li> </ul> <p>Tip</p> <p>Sedona join performance is heavily affected by the number of partitions. If the join performance is not ideal, please increase the number of partitions by doing <code>df.repartition(XXX)</code> right after you create the original DataFrame.</p>"},{"location":"api/sql/Optimizer/#range-join","title":"Range join","text":"<p>Introduction: Find geometries from A and geometries from B such that each geometry pair satisfies a certain predicate. Most predicates supported by SedonaSQL can trigger a range join.</p> <p>Spark SQL Example:</p> <pre><code>SELECT *\nFROM polygondf, pointdf\nWHERE ST_Contains(polygondf.polygonshape,pointdf.pointshape)\n</code></pre> <pre><code>SELECT *\nFROM polygondf, pointdf\nWHERE ST_Intersects(polygondf.polygonshape,pointdf.pointshape)\n</code></pre> <pre><code>SELECT *\nFROM pointdf, polygondf\nWHERE ST_Within(pointdf.pointshape, polygondf.polygonshape)\n</code></pre> <p>Spark SQL Physical plan:</p> <pre><code>== Physical Plan ==\nRangeJoin polygonshape#20: geometry, pointshape#43: geometry, false\n:- Project [st_polygonfromenvelope(cast(_c0#0 as decimal(24,20)), cast(_c1#1 as decimal(24,20)), cast(_c2#2 as decimal(24,20)), cast(_c3#3 as decimal(24,20)), mypolygonid) AS polygonshape#20]\n:  +- *FileScan csv\n+- Project [st_point(cast(_c0#31 as decimal(24,20)), cast(_c1#32 as decimal(24,20)), myPointId) AS pointshape#43]\n   +- *FileScan csv\n</code></pre> <p>Note</p> <p>All join queries in SedonaSQL are inner joins</p>"},{"location":"api/sql/Optimizer/#distance-join","title":"Distance join","text":"<p>Introduction: Find geometries from A and geometries from B such that the distance of each geometry pair is less or equal than a certain distance. It supports the planar Euclidean distance calculator <code>ST_Distance</code> and the meter-based geodesic distance calculators <code>ST_DistanceSpheroid</code> and <code>ST_DistanceSphere</code>.</p> <p>Spark SQL Example for planar Euclidean distance:</p> <p>Only consider fully within a certain distance <pre><code>SELECT *\nFROM pointdf1, pointdf2\nWHERE ST_Distance(pointdf1.pointshape1,pointdf2.pointshape2) &lt; 2\n</code></pre></p> <p>Consider intersects within a certain distance <pre><code>SELECT *\nFROM pointdf1, pointdf2\nWHERE ST_Distance(pointdf1.pointshape1,pointdf2.pointshape2) &lt;= 2\n</code></pre></p> <p>Spark SQL Physical plan: <pre><code>== Physical Plan ==\nDistanceJoin pointshape1#12: geometry, pointshape2#33: geometry, 2.0, true\n:- Project [st_point(cast(_c0#0 as decimal(24,20)), cast(_c1#1 as decimal(24,20)), myPointId) AS pointshape1#12]\n:  +- *FileScan csv\n+- Project [st_point(cast(_c0#21 as decimal(24,20)), cast(_c1#22 as decimal(24,20)), myPointId) AS pointshape2#33]\n   +- *FileScan csv\n</code></pre></p> <p>Warning</p> <p>If you use <code>ST_Distance</code> as the predicate, Sedona doesn't control the distance's unit (degree or meter). It is same with the geometry. If your coordinates are in the longitude and latitude system, the unit of <code>distance</code> should be degree instead of meter or mile. To change the geometry's unit, please either transform the coordinate reference system to a meter-based system. See ST_Transform. If you don't want to transform your data, please consider using <code>ST_DistanceSpheroid</code> or <code>ST_DistanceSphere</code>.</p> <p>Spark SQL Example for meter-based geodesic distance <code>ST_DistanceSpheroid</code> (works for <code>ST_DistanceSphere</code> too):</p> <p>Less than a certain distance== <pre><code>SELECT *\nFROM pointdf1, pointdf2\nWHERE ST_DistanceSpheroid(pointdf1.pointshape1,pointdf2.pointshape2) &lt; 2\n</code></pre></p> <p>Less than or equal to a certain distance== <pre><code>SELECT *\nFROM pointdf1, pointdf2\nWHERE ST_DistanceSpheroid(pointdf1.pointshape1,pointdf2.pointshape2) &lt;= 2\n</code></pre></p> <p>Warning</p> <p>If you use <code>ST_DistanceSpheroid</code> or <code>ST_DistanceSphere</code> as the predicate, the unit of the distance is meter. Currently, distance join with geodesic distance calculators work best for point data. For non-point data, it only considers their centroids. The distance join algorithm internally uses an approximate distance buffer which might lead to inaccurate results if your data is close to the poles or antimeridian.</p>"},{"location":"api/sql/Optimizer/#broadcast-index-join","title":"Broadcast index join","text":"<p>Introduction: Perform a range join or distance join but broadcast one of the sides of the join. This maintains the partitioning of the non-broadcast side and doesn't require a shuffle.</p> <p>Sedona will create a spatial index on the broadcasted table.</p> <p>Sedona uses broadcast join only if the correct side has a broadcast hint. The supported join type - broadcast side combinations are:</p> <ul> <li>Inner - either side, preferring to broadcast left if both sides have the hint</li> <li>Left semi - broadcast right</li> <li>Left anti - broadcast right</li> <li>Left outer - broadcast right</li> <li>Right outer - broadcast left</li> </ul> <pre><code>pointDf.alias(\"pointDf\").join(broadcast(polygonDf).alias(\"polygonDf\"), expr(\"ST_Contains(polygonDf.polygonshape, pointDf.pointshape)\"))\n</code></pre> <p>Spark SQL Physical plan: <pre><code>== Physical Plan ==\nBroadcastIndexJoin pointshape#52: geometry, BuildRight, BuildRight, false ST_Contains(polygonshape#30, pointshape#52)\n:- Project [st_point(cast(_c0#48 as decimal(24,20)), cast(_c1#49 as decimal(24,20))) AS pointshape#52]\n:  +- FileScan csv\n+- SpatialIndex polygonshape#30: geometry, QUADTREE, [id=#62]\n   +- Project [st_polygonfromenvelope(cast(_c0#22 as decimal(24,20)), cast(_c1#23 as decimal(24,20)), cast(_c2#24 as decimal(24,20)), cast(_c3#25 as decimal(24,20))) AS polygonshape#30]\n      +- FileScan csv\n</code></pre></p> <p>This also works for distance joins with <code>ST_Distance</code>, <code>ST_DistanceSpheroid</code> or <code>ST_DistanceSphere</code>:</p> <pre><code>pointDf1.alias(\"pointDf1\").join(broadcast(pointDf2).alias(\"pointDf2\"), expr(\"ST_Distance(pointDf1.pointshape, pointDf2.pointshape) &lt;= 2\"))\n</code></pre> <p>Spark SQL Physical plan: <pre><code>== Physical Plan ==\nBroadcastIndexJoin pointshape#52: geometry, BuildRight, BuildLeft, true, 2.0 ST_Distance(pointshape#52, pointshape#415) &lt;= 2.0\n:- Project [st_point(cast(_c0#48 as decimal(24,20)), cast(_c1#49 as decimal(24,20))) AS pointshape#52]\n:  +- FileScan csv\n+- SpatialIndex pointshape#415: geometry, QUADTREE, [id=#1068]\n   +- Project [st_point(cast(_c0#48 as decimal(24,20)), cast(_c1#49 as decimal(24,20))) AS pointshape#415]\n      +- FileScan csv\n</code></pre></p> <p>Note: If the distance is an expression, it is only evaluated on the first argument to ST_Distance (<code>pointDf1</code> above).</p>"},{"location":"api/sql/Optimizer/#auotmatic-broadcast-index-join","title":"Auotmatic broadcast index join","text":"<p>When one table involved a spatial join query is smaller than a threadhold, Sedona will automatically choose broadcast index join instead of Sedona optimized join. The current threshold is controlled by sedona.join.autoBroadcastJoinThreshold and set to the same as <code>spark.sql.autoBroadcastJoinThreshold</code>.</p>"},{"location":"api/sql/Optimizer/#google-s2-based-approximate-equi-join","title":"Google S2 based approximate equi-join","text":"<p>If the performance of Sedona optimized join is not ideal, which is possibly caused by  complicated and overlapping geometries, you can resort to Sedona built-in Google S2-based approximate equi-join. This equi-join leverages Spark's internal equi-join algorithm and might be performant given that you can opt to skip the refinement step  by sacrificing query accuracy.</p> <p>Please use the following steps:</p>"},{"location":"api/sql/Optimizer/#1-generate-s2-ids-for-both-tables","title":"1. Generate S2 ids for both tables","text":"<p>Use ST_S2CellIds to generate cell IDs. Each geometry may produce one or more IDs.</p> <pre><code>SELECT id, geom, name, explode(ST_S2CellIDs(geom, 15)) as cellId\nFROM lefts\n</code></pre> <pre><code>SELECT id, geom, name, explode(ST_S2CellIDs(geom, 15)) as cellId\nFROM rights\n</code></pre>"},{"location":"api/sql/Optimizer/#2-perform-equi-join","title":"2. Perform equi-join","text":"<p>Join the two tables by their S2 cellId</p> <pre><code>SELECT lcs.id as lcs_id, lcs.geom as lcs_geom, lcs.name as lcs_name, rcs.id as rcs_id, rcs.geom as rcs_geom, rcs.name as rcs_name\nFROM lcs JOIN rcs ON lcs.cellId = rcs.cellId\n</code></pre>"},{"location":"api/sql/Optimizer/#3-optional-refine-the-result","title":"3. Optional: Refine the result","text":"<p>Due to the nature of S2 Cellid, the equi-join results might have a few false-positives depending on the S2 level you choose. A smaller level indicates bigger cells, less exploded rows, but more false positives.</p> <p>To ensure the correctness, you can use one of the Spatial Predicates to filter out them. Use this query instead of the query in Step 2.</p> <pre><code>SELECT lcs.id as lcs_id, lcs.geom as lcs_geom, lcs.name as lcs_name, rcs.id as rcs_id, rcs.geom as rcs_geom, rcs.name as rcs_name\nFROM lcs, rcs\nWHERE lcs.cellId = rcs.cellId AND ST_Contains(lcs.geom, rcs.geom)\n</code></pre> <p>As you see, compared to the query in Step 2, we added one more filter, which is <code>ST_Contains</code>, to remove false positives. You can also use <code>ST_Intersects</code> and so on.</p> <p>Tip</p> <p>You can skip this step if you don't need 100% accuracy and want faster query speed.</p>"},{"location":"api/sql/Optimizer/#4-optional-de-duplcate","title":"4. Optional: De-duplcate","text":"<p>Due to the explode function used when we generate S2 Cell Ids, the resulting DataFrame may have several duplicate  matches. You can remove them by performing a GroupBy query. <pre><code>SELECT lcs_id, rcs_id , first(lcs_geom), first(lcs_name), first(rcs_geom), first(rcs_name)\nFROM joinresult\nGROUP BY (lcs_id, rcs_id)\n</code></pre> <p>The <code>first</code> function is to take the first value from a number of duplicate values.</p> <p>If you don't have a unique id for each geometry, you can also group by geometry itself. See below:</p> <pre><code>SELECT lcs_geom, rcs_geom, first(lcs_name), first(rcs_name)\nFROM joinresult\nGROUP BY (lcs_geom, rcs_geom)\n</code></pre> <p>Note</p> <p>If you are doing point-in-polygon join, this is not a problem and you can safely discard this issue. This issue only happens when you do polygon-polygon, polygon-linestring, linestring-linestring join.</p>"},{"location":"api/sql/Optimizer/#s2-for-distance-join","title":"S2 for distance join","text":"<p>This also works for distance join. You first need to use <code>ST_Buffer(geometry, distance)</code> to wrap one of your original geometry column. If your original geometry column contains points, this <code>ST_Buffer</code> will make them become circles with a radius of <code>distance</code>.</p> <p>Since the coordinates are in the longitude and latitude system, so the unit of <code>distance</code> should be degree instead of meter or mile. You can get an approximation by performing <code>METER_DISTANCE/111000.0</code>, then filter out false-positives.  Note that this might lead to inaccurate results if your data is close to the poles or antimeridian.</p> <p>In a nutshell, run this query first on the left table before Step 1. Please replace <code>METER_DISTANCE</code> with a meter distance. In Step 1, generate S2 IDs based on the <code>buffered_geom</code> column. Then run Step 2, 3, 4 on the original <code>geom</code> column.</p> <pre><code>SELECT id, geom , ST_Buffer(geom, METER_DISTANCE/111000.0) as buffered_geom, name\nFROM lefts\n</code></pre>"},{"location":"api/sql/Optimizer/#regular-spatial-predicate-pushdown","title":"Regular spatial predicate pushdown","text":"<p>Introduction: Given a join query and a predicate in the same WHERE clause, first executes the Predicate as a filter, then executes the join query.</p> <p>Spark SQL Example:</p> <pre><code>SELECT *\nFROM polygondf, pointdf WHERE ST_Contains(polygondf.polygonshape,pointdf.pointshape)\nAND ST_Contains(ST_PolygonFromEnvelope(1.0,101.0,501.0,601.0), polygondf.polygonshape)\n</code></pre> <p>Spark SQL Physical plan:</p> <pre><code>== Physical Plan ==\nRangeJoin polygonshape#20: geometry, pointshape#43: geometry, false\n:- Project [st_polygonfromenvelope(cast(_c0#0 as decimal(24,20)), cast(_c1#1 as decimal(24,20)), cast(_c2#2 as decimal(24,20)), cast(_c3#3 as decimal(24,20)), mypolygonid) AS polygonshape#20]\n:  +- Filter  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains$**\n:     +- *FileScan csv\n+- Project [st_point(cast(_c0#31 as decimal(24,20)), cast(_c1#32 as decimal(24,20)), myPointId) AS pointshape#43]\n   +- *FileScan csv\n</code></pre>"},{"location":"api/sql/Optimizer/#push-spatial-predicates-to-geoparquet","title":"Push spatial predicates to GeoParquet","text":"<p>Sedona supports spatial predicate push-down for GeoParquet files. When spatial filters were applied to dataframes backed by GeoParquet files, Sedona will use the <code>bbox</code> properties in the metadata to determine if all data in the file will be discarded by the spatial predicate. This optimization could reduce the number of files scanned when the queried GeoParquet dataset was partitioned by spatial proximity.</p> <p>To maximize the performance of Sedona GeoParquet filter pushdown, we suggest that you sort the data by their geohash values (see ST_GeoHash) and then save as a GeoParquet file. An example is as follows:</p> <pre><code>SELECT col1, col2, geom, ST_GeoHash(geom, 5) as geohash\nFROM spatialDf\nORDER BY geohash\n</code></pre> <p>The following figure is the visualization of a GeoParquet dataset. <code>bbox</code>es of all GeoParquet files were plotted as blue rectangles and the query window was plotted as a red rectangle. Sedona will only scan 1 of the 6 files to answer queries such as <code>SELECT * FROM geoparquet_dataset WHERE ST_Intersects(geom, &lt;query window&gt;)</code>, thus only part of the data covered by the light green rectangle needs to be scanned.</p> <p></p> <p>We can compare the metrics of querying the GeoParquet dataset with or without the spatial predicate and observe that querying with spatial predicate results in fewer number of rows scanned.</p> Without spatial predicate With spatial predicate"},{"location":"api/sql/Overview/","title":"Introduction","text":""},{"location":"api/sql/Overview/#function-list","title":"Function list","text":"<p>SedonaSQL supports SQL/MM Part3 Spatial SQL Standard. It includes four kinds of SQL operators as follows. All these operators can be directly called through: <pre><code>var myDataFrame = sedona.sql(\"YOUR_SQL\")\n</code></pre></p> <p>Alternatively, <code>expr</code> and <code>selectExpr</code> can be used: <pre><code>myDataFrame.withColumn(\"geometry\", expr(\"ST_*\")).selectExpr(\"ST_*\")\n</code></pre></p> <ul> <li>Constructor: Construct a Geometry given an input string or coordinates<ul> <li>Example: ST_GeomFromWKT (string). Create a Geometry from a WKT String.</li> <li>Documentation: Here</li> </ul> </li> <li>Function: Execute a function on the given column or columns<ul> <li>Example: ST_Distance (A, B). Given two Geometry A and B, return the Euclidean distance of A and B.</li> <li>Documentation: Here</li> </ul> </li> <li>Aggregate function: Return the aggregated value on the given column<ul> <li>Example: ST_Envelope_Aggr (Geometry column). Given a Geometry column, calculate the entire envelope boundary of this column.</li> <li>Documentation: Here</li> </ul> </li> <li>Predicate: Execute a logic judgement on the given columns and return true or false<ul> <li>Example: ST_Contains (A, B). Check if A fully contains B. Return \"True\" if yes, else return \"False\".</li> <li>Documentation: Here</li> </ul> </li> </ul> <p>Sedona also provides an Adapter to convert SpatialRDD &lt;-&gt; DataFrame. Please read Adapter Scaladoc</p> <p>SedonaSQL supports SparkSQL query optimizer, documentation is Here</p>"},{"location":"api/sql/Overview/#quick-start","title":"Quick start","text":"<p>The detailed explanation is here Write a SQL/DataFrame application.</p> <ol> <li>Add Sedona-core and Sedona-SQL into your project POM.xml or build.sbt</li> <li>Create your Sedona config if you want to customize your SparkSession. <pre><code>import org.apache.sedona.spark.SedonaContext\nval config = SedonaContext.builder().\nmaster(\"local[*]\").appName(\"SedonaSQL\")\n.getOrCreate()\n</code></pre></li> <li>Add the following line after your Sedona context declaration: <pre><code>import org.apache.sedona.spark.SedonaContext\nval sedona = SedonaContext.create(config)\n</code></pre></li> </ol>"},{"location":"api/sql/Parameter/","title":"Parameter","text":""},{"location":"api/sql/Parameter/#usage","title":"Usage","text":"<p>SedonaSQL supports many parameters. To change their values,</p> <ol> <li>Set it through SparkConf: <pre><code>sparkSession = SparkSession.builder().\nconfig(\"spark.serializer\",\"org.apache.spark.serializer.KryoSerializer\").\nconfig(\"spark.kryo.registrator\", \"org.apache.sedona.core.serde.SedonaKryoRegistrator\").\nconfig(\"sedona.global.index\",\"true\")\nmaster(\"local[*]\").appName(\"mySedonaSQLdemo\").getOrCreate()\n</code></pre></li> <li>Check your current SedonaSQL configuration: <pre><code>val sedonaConf = new SedonaConf(sparkSession.conf)\nprintln(sedonaConf)\n</code></pre></li> <li>Sedona parameters can be changed at runtime: <pre><code>sparkSession.conf.set(\"sedona.global.index\",\"false\")\n</code></pre></li> </ol>"},{"location":"api/sql/Parameter/#explanation","title":"Explanation","text":"<ul> <li>sedona.global.index<ul> <li>Use spatial index (currently, only supports in SQL range join and SQL distance join)</li> <li>Default: true</li> <li>Possible values: true, false</li> </ul> </li> <li>sedona.global.indextype<ul> <li>Spatial index type, only valid when \"sedona.global.index\" is true</li> <li>Default: quadtree</li> <li>Possible values: rtree, quadtree</li> </ul> </li> <li>sedona.join.autoBroadcastJoinThreshold<ul> <li>Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join.   By setting this value to -1 automatic broadcasting can be disabled.</li> <li>Default: The default value is the same as spark.sql.autoBroadcastJoinThreshold</li> <li>Possible values: any integer with a byte suffix i.e. 10MB or 512KB</li> </ul> </li> <li>sedona.join.gridtype<ul> <li>Spatial partitioning grid type for join query</li> <li>Default: kdbtree</li> <li>Possible values: quadtree, kdbtree</li> </ul> </li> <li>sedona.join.indexbuildside (Advanced users only!)<ul> <li>The side which Sedona builds spatial indices on</li> <li>Default: left</li> <li>Possible values: left, right</li> </ul> </li> <li>sedona.join.numpartition (Advanced users only!)<ul> <li>Number of partitions for both sides in a join query</li> <li>Default: -1, which means use the existing partitions</li> <li>Possible values: any integers</li> </ul> </li> <li>sedona.join.spatitionside (Advanced users only!)<ul> <li>The dominant side in spatial partitioning stage</li> <li>Default: left</li> <li>Possible values: left, right</li> </ul> </li> <li>sedona.join.optimizationmode (Advanced users only!)<ul> <li>When should Sedona optimize spatial join SQL queries</li> <li>Default: nonequi</li> <li>Possible values:<ul> <li>all: Always optimize spatial join queries, even for equi-joins.</li> <li>none: Disable optimization for spatial joins.</li> <li>nonequi: Optimize spatial join queries that are not equi-joins.</li> </ul> </li> </ul> </li> </ul>"},{"location":"api/sql/Predicate/","title":"Predicate","text":""},{"location":"api/sql/Predicate/#st_contains","title":"ST_Contains","text":"<p>Introduction: Return true if A fully contains B</p> <p>Format: <code>ST_Contains (A:geometry, B:geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT * FROM pointdf WHERE ST_Contains(ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), pointdf.arealandmark)\n</code></pre></p>"},{"location":"api/sql/Predicate/#st_crosses","title":"ST_Crosses","text":"<p>Introduction: Return true if A crosses B</p> <p>Format: <code>ST_Crosses (A:geometry, B:geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT * FROM pointdf WHERE ST_Crosses(pointdf.arealandmark, ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0))\n</code></pre></p>"},{"location":"api/sql/Predicate/#st_disjoint","title":"ST_Disjoint","text":"<p>Introduction: Return true if A and B are disjoint</p> <p>Format: <code>ST_Disjoint (A:geometry, B:geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Spark SQL example: <pre><code>SELECT *\nFROM geom\nWHERE ST_Disjoinnt(geom.geom_a, geom.geom_b)\n</code></pre></p>"},{"location":"api/sql/Predicate/#st_equals","title":"ST_Equals","text":"<p>Introduction: Return true if A equals to B</p> <p>Format: <code>ST_Equals (A:geometry, B:geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT * FROM pointdf WHERE ST_Equals(pointdf.arealandmark, ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0))\n</code></pre></p>"},{"location":"api/sql/Predicate/#st_intersects","title":"ST_Intersects","text":"<p>Introduction: Return true if A intersects B</p> <p>Format: <code>ST_Intersects (A:geometry, B:geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT * FROM pointdf WHERE ST_Intersects(ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), pointdf.arealandmark)\n</code></pre></p>"},{"location":"api/sql/Predicate/#st_orderingequals","title":"ST_OrderingEquals","text":"<p>Introduction: Returns true if the geometries are equal and the coordinates are in the same order</p> <p>Format: <code>ST_OrderingEquals(A: geometry, B: geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Spark SQL example 1: <pre><code>SELECT ST_OrderingEquals(ST_GeomFromWKT('POLYGON((2 0, 0 2, -2 0, 2 0))'), ST_GeomFromWKT('POLYGON((2 0, 0 2, -2 0, 2 0))'))\n</code></pre></p> <p>Output: <code>true</code></p> <p>Spark SQL example 2: <pre><code>SELECT ST_OrderingEquals(ST_GeomFromWKT('POLYGON((2 0, 0 2, -2 0, 2 0))'), ST_GeomFromWKT('POLYGON((0 2, -2 0, 2 0, 0 2))'))\n</code></pre></p> <p>Output: <code>false</code></p>"},{"location":"api/sql/Predicate/#st_overlaps","title":"ST_Overlaps","text":"<p>Introduction: Return true if A overlaps B</p> <p>Format: <code>ST_Overlaps (A:geometry, B:geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT *\nFROM geom\nWHERE ST_Overlaps(geom.geom_a, geom.geom_b)\n</code></pre></p>"},{"location":"api/sql/Predicate/#st_touches","title":"ST_Touches","text":"<p>Introduction: Return true if A touches B</p> <p>Format: <code>ST_Touches (A:geometry, B:geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <pre><code>SELECT * FROM pointdf WHERE ST_Touches(pointdf.arealandmark, ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0))\n</code></pre>"},{"location":"api/sql/Predicate/#st_within","title":"ST_Within","text":"<p>Introduction: Return true if A is fully contained by B</p> <p>Format: <code>ST_Within (A:geometry, B:geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT * FROM pointdf WHERE ST_Within(pointdf.arealandmark, ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0))\n</code></pre></p>"},{"location":"api/sql/Predicate/#st_covers","title":"ST_Covers","text":"<p>Introduction: Return true if A covers B</p> <p>Format: <code>ST_Covers (A:geometry, B:geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Spark SQL example: <pre><code>SELECT * FROM pointdf WHERE ST_Covers(ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), pointdf.arealandmark)\n</code></pre></p>"},{"location":"api/sql/Predicate/#st_coveredby","title":"ST_CoveredBy","text":"<p>Introduction: Return true if A is covered by B</p> <p>Format: <code>ST_CoveredBy (A:geometry, B:geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Spark SQL example: <pre><code>SELECT * FROM pointdf WHERE ST_CoveredBy(pointdf.arealandmark, ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0))\n</code></pre></p>"},{"location":"api/sql/Raster-loader/","title":"Raster loader","text":"<p>Note</p> <p>Sedona loader are available in Scala, Java and Python and have the same APIs.</p> <p>Sedona provides two types of raster DataFrame loaders. They both use Sedona built-in data source but load raster images to different internal formats.</p>"},{"location":"api/sql/Raster-loader/#load-any-raster-to-rasterudt-format","title":"Load any raster to RasterUDT format","text":"<p>The raster loader of Sedona leverages Spark built-in binary data source and works with several RS RasterUDT constructors to produce RasterUDT type. Each raster is a row in the resulting DataFrame and stored in a <code>RasterUDT</code> format.</p>"},{"location":"api/sql/Raster-loader/#load-raster-to-a-binary-dataframe","title":"Load raster to a binary DataFrame","text":"<p>You can load any type of raster data using the code below. Then use the RS constructors below to create RasterUDT.</p> <pre><code>spark.read.format(\"binaryFile\").load(\"/some/path/*.asc\")\n</code></pre>"},{"location":"api/sql/Raster-loader/#rs_fromarcinfoasciigrid","title":"RS_FromArcInfoAsciiGrid","text":"<p>Introduction: Returns a raster geometry from an Arc Info Ascii Grid file.</p> <p>Format: <code>RS_FromArcInfoAsciiGrid(asc: Array[Byte])</code></p> <p>Since: <code>v1.4.0</code></p> <p>Spark SQL example:</p> <pre><code>var df = spark.read.format(\"binaryFile\").load(\"/some/path/*.asc\")\ndf = df.withColumn(\"raster\", f.expr(\"RS_FromArcInfoAsciiGrid(content)\"))\n</code></pre>"},{"location":"api/sql/Raster-loader/#rs_fromgeotiff","title":"RS_FromGeoTiff","text":"<p>Introduction: Returns a raster geometry from a GeoTiff file.</p> <p>Format: <code>RS_FromGeoTiff(asc: Array[Byte])</code></p> <p>Since: <code>v1.4.0</code></p> <p>Spark SQL example:</p> <pre><code>var df = spark.read.format(\"binaryFile\").load(\"/some/path/*.tiff\")\ndf = df.withColumn(\"raster\", f.expr(\"RS_FromGeoTiff(content)\"))\n</code></pre>"},{"location":"api/sql/Raster-loader/#rs_makeemptyraster","title":"RS_MakeEmptyRaster","text":"<p>Introduction: Returns an empty raster geometry. Every band in the raster is initialized to <code>0.0</code>.</p> <p>Since: <code>v1.4.1</code></p> <p>Format: <code>RS_MakeEmptyRaster(numBands:Int, width: Int, height: Int, upperleftX: Double, upperleftY: Double, cellSize:Double)</code></p> <ul> <li>NumBands: The number of bands in the raster. If not specified, the raster will have a single band.</li> <li>Width: The width of the raster in pixels.</li> <li>Height: The height of the raster in pixels.</li> <li>UpperleftX: The X coordinate of the upper left corner of the raster, in terms of the CRS units.</li> <li>UpperleftY: The Y coordinate of the upper left corner of the raster, in terms of the CRS units.</li> <li>Cell Size (pixel size): The size of the cells in the raster, in terms of the CRS units.</li> </ul> <p>It uses the default Cartesian coordinate system.</p> <p>Format: <code>RS_MakeEmptyRaster(numBands:Int, width: Int, height: Int, upperleftX: Double, upperleftY: Double, scaleX:Double, scaleY:Double, skewX:Double, skewY:Double, srid: Int)</code></p> <ul> <li>NumBands: The number of bands in the raster. If not specified, the raster will have a single band.</li> <li>Width: The width of the raster in pixels.</li> <li>Height: The height of the raster in pixels.</li> <li>UpperleftX: The X coordinate of the upper left corner of the raster, in terms of the CRS units.</li> <li>UpperleftY: The Y coordinate of the upper left corner of the raster, in terms of the CRS units.</li> <li>ScaleX (pixel size on X): The size of the cells on the X axis, in terms of the CRS units.</li> <li>ScaleY (pixel size on Y): The size of the cells on the Y axis, in terms of the CRS units.</li> <li>SkewX: The skew of the raster on the X axis, in terms of the CRS units.</li> <li>SkewY: The skew of the raster on the Y axis, in terms of the CRS units.</li> <li>SRID: The SRID of the raster. Use 0 if you want to use the default Cartesian coordinate system. Use 4326 if you want to use WGS84.</li> </ul> <p>SQL example 1 (with 2 bands):</p> <pre><code>SELECT RS_MakeEmptyRaster(2, 10, 10, 0.0, 0.0, 1.0) as raster\n</code></pre> <p>Output: <pre><code>+--------------------------------------------+\n|rs_makeemptyraster(2, 10, 10, 0.0, 0.0, 1.0)|\n+--------------------------------------------+\n|                        GridCoverage2D[\"g...|\n+--------------------------------------------+\n</code></pre></p> <p>SQL example 1 (with 2 bands, scale, skew, and SRID):</p> <pre><code>SELECT RS_MakeEmptyRaster(2, 10, 10, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 4326) as raster\n</code></pre> <p>Output: <pre><code>+--------------------------------------------------------------+\n|rs_makeemptyraster(2, 10, 10, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0)|\n+--------------------------------------------------------------+\n|                                          GridCoverage2D[\"g...|\n+--------------------------------------------------------------+\n</code></pre></p>"},{"location":"api/sql/Raster-loader/#load-geotiff-to-arraydouble-format","title":"Load GeoTiff to Array[Double] format","text":"<p>Warning</p> <p>This function has been deprecated since v1.4.1. Please use <code>RS_FromGeoTiff</code> instead and <code>binaryFile</code> data source to read GeoTiff files.</p> <p>The <code>geotiff</code> loader of Sedona is a Spark built-in data source. It can read a single geotiff image or a number of geotiff images into a DataFrame. Each geotiff is a row in the resulting DataFrame and stored in an array of Double type format.</p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL example:</p> <p>The input path could be a path to a single GeoTiff image or a directory of GeoTiff images.  You can optionally append an option to drop invalid images. The geometry bound of each image is automatically loaded as a Sedona geometry and is transformed to WGS84 (EPSG:4326) reference system.</p> <pre><code>var geotiffDF = sparkSession.read.format(\"geotiff\").option(\"dropInvalid\", true).load(\"YOUR_PATH\")\ngeotiffDF.printSchema()\n</code></pre> <p>Output:</p> <pre><code> |-- image: struct (nullable = true)\n |    |-- origin: string (nullable = true)\n |    |-- Geometry: string (nullable = true)\n |    |-- height: integer (nullable = true)\n |    |-- width: integer (nullable = true)\n |    |-- nBands: integer (nullable = true)\n |    |-- data: array (nullable = true)\n |    |    |-- element: double (containsNull = true)\n</code></pre> <p>There are three more optional parameters for reading GeoTiff:</p> <pre><code> |-- readfromCRS: Coordinate reference system of the geometry coordinates representing the location of the Geotiff. An example value of readfromCRS is EPSG:4326.\n |-- readToCRS: If you want to transform the Geotiff location geometry coordinates to a different coordinate reference system, you can define the target coordinate reference system with this option.\n |-- disableErrorInCRS: (Default value false) =&gt; Indicates whether to ignore errors in CRS transformation.\n</code></pre> <p>An example with all GeoTiff read options:</p> <pre><code>var geotiffDF = sparkSession.read.format(\"geotiff\").option(\"dropInvalid\", true).option(\"readFromCRS\", \"EPSG:4499\").option(\"readToCRS\", \"EPSG:4326\").option(\"disableErrorInCRS\", true).load(\"YOUR_PATH\")\ngeotiffDF.printSchema()\n</code></pre> <p>Output:</p> <pre><code> |-- image: struct (nullable = true)\n |    |-- origin: string (nullable = true)\n |    |-- Geometry: string (nullable = true)\n |    |-- height: integer (nullable = true)\n |    |-- width: integer (nullable = true)\n |    |-- nBands: integer (nullable = true)\n |    |-- data: array (nullable = true)\n |    |    |-- element: double (containsNull = true)\n</code></pre> <p>You can also select sub-attributes individually to construct a new DataFrame</p> <pre><code>geotiffDF = geotiffDF.selectExpr(\"image.origin as origin\",\"ST_GeomFromWkt(image.geometry) as Geom\", \"image.height as height\", \"image.width as width\", \"image.data as data\", \"image.nBands as bands\")\ngeotiffDF.createOrReplaceTempView(\"GeotiffDataframe\")\ngeotiffDF.show()\n</code></pre> <p>Output:</p> <pre><code>+--------------------+--------------------+------+-----+--------------------+-----+\n|              origin|                Geom|height|width|                data|bands|\n+--------------------+--------------------+------+-----+--------------------+-----+\n|file:///home/hp/D...|POLYGON ((-58.699...|    32|   32|[1058.0, 1039.0, ...|    4|\n|file:///home/hp/D...|POLYGON ((-58.297...|    32|   32|[1258.0, 1298.0, ...|    4|\n+--------------------+--------------------+------+-----+--------------------+-----+\n</code></pre>"},{"location":"api/sql/Raster-loader/#rs_array","title":"RS_Array","text":"<p>Introduction: Create an array that is filled by the given value</p> <p>Format: <code>RS_Array(length:Int, value: Decimal)</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL example:</p> <pre><code>SELECT RS_Array(height * width, 0.0)\n</code></pre>"},{"location":"api/sql/Raster-loader/#rs_getband","title":"RS_GetBand","text":"<p>Introduction: Return a particular band from Geotiff Dataframe</p> <p>The number of total bands can be obtained from the GeoTiff loader</p> <p>Format: <code>RS_GetBand (allBandValues: Array[Double], targetBand:Int, totalBands:Int)</code></p> <p>Since: <code>v1.1.0</code></p> <p>Note</p> <p>Index of targetBand starts from 1 (instead of 0). Index of the first band is 1.</p> <p>Spark SQL example:</p> <pre><code>val BandDF = spark.sql(\"select RS_GetBand(data, 2, Band) as targetBand from GeotiffDataframe\")\nBandDF.show()\n</code></pre> <p>Output:</p> <pre><code>+--------------------+\n|          targetBand|\n+--------------------+\n|[1058.0, 1039.0, ...|\n|[1258.0, 1298.0, ...|\n+--------------------+\n</code></pre>"},{"location":"api/sql/Raster-operators/","title":"Raster operators","text":""},{"location":"api/sql/Raster-operators/#raster-based-operators","title":"Raster based operators","text":""},{"location":"api/sql/Raster-operators/#rs_envelope","title":"RS_Envelope","text":"<p>Introduction: Returns the envelope of the raster as a Geometry.</p> <p>Format: <code>RS_Envelope (raster: Raster)</code></p> <p>Since: <code>v1.4.0</code></p> <p>Spark SQL example: <pre><code>SELECT RS_Envelope(raster) FROM raster_table\n</code></pre> Output: <pre><code>POLYGON((0 0,20 0,20 60,0 60,0 0))\n</code></pre></p>"},{"location":"api/sql/Raster-operators/#rs_metadata","title":"RS_MetaData","text":"<p>Introduction: Returns the metadata of the raster as an array of double. The array contains the following values:</p> <ul> <li>0: upper left x coordinate of the raster, in terms of CRS units (the minimum x coordinate)</li> <li>1: upper left y coordinate of the raster, in terms of CRS units (the maximum y coordinate)</li> <li>2: width of the raster, in terms of pixels</li> <li>3: height of the raster, in terms of pixels</li> <li>4: width of a pixel, in terms of CRS units (scaleX)</li> <li>5: height of a pixel, in terms of CRS units (scaleY)</li> <li>6: skew in x direction (rotation x)</li> <li>7: skew in y direction (rotation y)</li> <li>8: srid of the raster</li> <li>9: number of bands</li> </ul> <p>Format: <code>RS_MetaData (raster: Raster)</code></p> <p>Since: <code>v1.4.1</code></p> <p>SQL example: <pre><code>SELECT RS_MetaData(raster) FROM raster_table\n</code></pre></p> <p>Output: <pre><code>+-----------------------------------------------------------------------------------------------------------------------+\n|rs_metadata(raster)                                                                                                    |\n+-----------------------------------------------------------------------------------------------------------------------+\n|[-1.3095817809482181E7, 4021262.7487925636, 512.0, 517.0, 72.32861272132695, -72.32861272132695, 0.0, 0.0, 3857.0, 1.0]|\n+-----------------------------------------------------------------------------------------------------------------------+\n</code></pre></p>"},{"location":"api/sql/Raster-operators/#rs_numbands","title":"RS_NumBands","text":"<p>Introduction: Returns the number of the bands in the raster.</p> <p>Format: <code>RS_NumBands (raster: Raster)</code></p> <p>Since: <code>v1.4.0</code></p> <p>Spark SQL example: <pre><code>SELECT RS_NumBands(raster) FROM raster_table\n</code></pre></p> <p>Output: <pre><code>4\n</code></pre></p>"},{"location":"api/sql/Raster-operators/#rs_setsrid","title":"RS_SetSRID","text":"<p>Introduction: Sets the spatial reference system identifier (SRID) of the raster geometry.</p> <p>Format: <code>RS_SetSRID (raster: Raster, srid: Integer)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Spark SQL example: <pre><code>SELECT RS_SetSRID(raster, 4326)\nFROM raster_table\n</code></pre></p>"},{"location":"api/sql/Raster-operators/#rs_srid","title":"RS_SRID","text":"<p>Introduction: Returns the spatial reference system identifier (SRID) of the raster geometry.</p> <p>Format: <code>RS_SRID (raster: Raster)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Spark SQL example: <pre><code>SELECT RS_SRID(raster) FROM raster_table\n</code></pre></p> <p>Output: <pre><code>3857\n</code></pre></p>"},{"location":"api/sql/Raster-operators/#rs_value","title":"RS_Value","text":"<p>Introduction: Returns the value at the given point in the raster. If no band number is specified it defaults to 1. </p> <p>Format: <code>RS_Value (raster: Raster, point: Geometry)</code></p> <p>Format: <code>RS_Value (raster: Raster, point: Geometry, band: Int)</code></p> <p>Since: <code>v1.4.0</code></p> <p>Spark SQL example: <pre><code>SELECT RS_Value(raster, ST_Point(-13077301.685, 4002565.802)) FROM raster_table\n</code></pre></p> <p>Output: <pre><code>5.0\n</code></pre></p>"},{"location":"api/sql/Raster-operators/#rs_values","title":"RS_Values","text":"<p>Introduction: Returns the values at the given points in the raster. If no band number is specified it defaults to 1.</p> <p>RS_Values is similar to RS_Value but operates on an array of points. RS_Values can be significantly faster since a raster only has to be loaded once for several points.</p> <p>Format: <code>RS_Values (raster: Raster, points: Array[Geometry])</code></p> <p>Format: <code>RS_Values (raster: Raster, points: Array[Geometry], band: Int)</code></p> <p>Since: <code>v1.4.0</code></p> <p>Spark SQL example: <pre><code>SELECT RS_Values(raster, Array(ST_Point(-1307.5, 400.8), ST_Point(-1403.3, 399.1)))\nFROM raster_table\n</code></pre></p> <p>Output: <pre><code>Array(5.0, 3.0)\n</code></pre></p> <p>Spark SQL example for joining a point dataset with a raster dataset: <pre><code>val pointDf = spark.read...\nval rasterDf = spark.read.format(\"binaryFile\").load(\"/some/path/*.tiff\")\n.withColumn(\"raster\", expr(\"RS_FromGeoTiff(content)\"))\n.withColumn(\"envelope\", expr(\"RS_Envelope(raster)\"))\n\n// Join the points with the raster extent and aggregate points to arrays.\n// We only use the path and envelope of the raster to keep the shuffle as small as possible.\nval df = pointDf.join(rasterDf.select(\"path\", \"envelope\"), expr(\"ST_Within(point_geom, envelope)\"))\n.groupBy(\"path\")\n.agg(collect_list(\"point_geom\").alias(\"point\"), collect_list(\"point_id\").alias(\"id\"))\n\ndf.join(rasterDf, \"path\")\n.selectExpr(\"explode(arrays_zip(id, point, RS_Values(raster, point))) as result\")\n.selectExpr(\"result.*\")\n.show()\n</code></pre></p> <p>Output: <pre><code>+----+------------+-------+\n| id | point      | value |\n+----+------------+-------+\n|  4 | POINT(1 1) |   3.0 |\n|  5 | POINT(2 2) |   7.0 |\n+----+------------+-------+\n</code></pre></p>"},{"location":"api/sql/Raster-operators/#raster-to-map-algebra-operators","title":"Raster to Map Algebra operators","text":"<p>To bridge the gap between the raster and map algebra worlds, the following operators are provided. These operators convert a raster to a map algebra object. The map algebra object can then be used with the map algebra operators described in the next section.</p>"},{"location":"api/sql/Raster-operators/#rs_bandasarray","title":"RS_BandAsArray","text":"<p>Introduction: Extract a band from a raster as an array of doubles.</p> <p>Format: <code>RS_BandAsArray (raster: Raster, bandIndex: Int)</code>.</p> <p>Since: <code>v1.4.1</code></p> <p>BandIndex is 1-based and must be between 1 and RS_NumBands(raster). It returns null if the bandIndex is out of range or the raster is null.</p> <p>SQL example: <pre><code>SELECT RS_BandAsArray(raster, 1) FROM raster_table\n</code></pre></p> <p>Output:</p> <pre><code>+--------------------+\n|                band|\n+--------------------+\n|[0.0, 0.0, 0.0, 0...|\n+--------------------+\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_addbandfromarray","title":"RS_AddBandFromArray","text":"<p>Introduction: Add a band to a raster from an array of doubles.</p> <p>Format: <code>RS_AddBandFromArray (raster: Raster, band: Array[Double], bandIndex:Int)</code>.</p> <p>Since: <code>v1.4.1</code></p> <p>The bandIndex is 1-based and must be between 1 and RS_NumBands(raster) + 1. It throws an exception if the bandIndex is out of range or the raster is null.</p> <p>When the bandIndex is RS_NumBands(raster) + 1, it appends the band to the end of the raster. Otherwise, it replaces the existing band at the bandIndex.</p> <p>Note that: <code>bandIndex == RS_NumBands(raster) + 1</code> is an experimental feature and might not lead to the loss of raster metadata and properties such as color models.</p> <p>SQL example:</p> <pre><code>SELECT RS_AddBandFromArray(raster, RS_MultiplyFactor(RS_BandAsArray(RS_FromGeoTiff(content), 1), 2), 1) AS raster FROM raster_table\n</code></pre> <p>Output: <pre><code>+--------------------+\n|              raster|\n+--------------------+\n|GridCoverage2D[\"g...|\n+--------------------+\n</code></pre></p>"},{"location":"api/sql/Raster-operators/#map-algebra-operators","title":"Map Algebra operators","text":"<p>Map algebra operators work on a single band of a raster. Each band is represented as an array of doubles. The operators return an array of doubles.</p>"},{"location":"api/sql/Raster-operators/#rs_add","title":"RS_Add","text":"<p>Introduction: Add two spectral bands in a Geotiff image </p> <p>Format: <code>RS_Add (Band1: Array[Double], Band2: Array[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL example: <pre><code>val sumDF = spark.sql(\"select RS_Add(band1, band2) as sumOfBands from dataframe\")\n</code></pre></p>"},{"location":"api/sql/Raster-operators/#rs_append","title":"RS_Append","text":"<p>Introduction: Appends a new band to the end of Geotiff image data and returns the new data. The new band to be appended can be a normalized difference index between two bands (example: NBR, NDBI). Normalized difference index between two bands can be calculated with RS_NormalizedDifference operator described earlier in this page. Specific bands can be retrieved using RS_GetBand operator described here.</p> <p>Format: <code>RS_Append(data: Array[Double], newBand: Array[Double], nBands: Int)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Deprecated since: <code>v1.4.1</code></p> <p>Spark SQL example: <pre><code>val dfAppended = spark.sql(\"select RS_Append(data, normalizedDifference, nBands) as dataEdited from dataframe\")\n</code></pre></p>"},{"location":"api/sql/Raster-operators/#rs_bitwiseand","title":"RS_BitwiseAND","text":"<p>Introduction: Find Bitwise AND between two bands of Geotiff image</p> <p>Format: <code>RS_BitwiseAND (Band1: Array[Double], Band2: Array[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL example: <pre><code>val biwiseandDF = spark.sql(\"select RS_BitwiseAND(band1, band2) as andvalue from dataframe\")\n</code></pre></p>"},{"location":"api/sql/Raster-operators/#rs_bitwiseor","title":"RS_BitwiseOR","text":"<p>Introduction: Find Bitwise OR between two bands of Geotiff image</p> <p>Format: <code>RS_BitwiseOR (Band1: Array[Double], Band2: Array[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL example: <pre><code>val biwiseorDF = spark.sql(\"select RS_BitwiseOR(band1, band2) as or from dataframe\")\n</code></pre></p>"},{"location":"api/sql/Raster-operators/#rs_count","title":"RS_Count","text":"<p>Introduction: Returns count of a particular value from a spectral band in a raster image</p> <p>Format: <code>RS_Count (Band1: Array[Double], Target: Double)</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL example: <pre><code>val countDF = spark.sql(\"select RS_Count(band1, target) as count from dataframe\")\n</code></pre></p>"},{"location":"api/sql/Raster-operators/#rs_divide","title":"RS_Divide","text":"<p>Introduction: Divide band1 with band2 from a geotiff image</p> <p>Format: <code>RS_Divide (Band1: Array[Double], Band2: Array[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL example: <pre><code>val multiplyDF = spark.sql(\"select RS_Divide(band1, band2) as divideBands from dataframe\")\n</code></pre></p>"},{"location":"api/sql/Raster-operators/#rs_fetchregion","title":"RS_FetchRegion","text":"<p>Introduction: Fetch a subset of region from given Geotiff image based on minimumX, minimumY, maximumX and maximumY index as well original height and width of image</p> <p>Format: <code>RS_FetchRegion (Band: Array[Double], coordinates: Array[Int], dimensions: Array[Int])</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL example: <pre><code>val region = spark.sql(\"select RS_FetchRegion(Band,Array(0, 0, 1, 2),Array(3, 3)) as Region from dataframe\")\n</code></pre></p>"},{"location":"api/sql/Raster-operators/#rs_greaterthan","title":"RS_GreaterThan","text":"<p>Introduction: Mask all the values with 1 which are greater than a particular target value</p> <p>Format: <code>RS_GreaterThan (Band: Array[Double], Target: Double)</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL example: <pre><code>val greaterDF = spark.sql(\"select RS_GreaterThan(band, target) as maskedvalues from dataframe\")\n</code></pre></p>"},{"location":"api/sql/Raster-operators/#rs_greaterthanequal","title":"RS_GreaterThanEqual","text":"<p>Introduction: Mask all the values with 1 which are greater than equal to a particular target value</p> <p>Format: <code>RS_GreaterThanEqual (Band: Array[Double], Target: Double)</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL example: <pre><code>val greaterEqualDF = spark.sql(\"select RS_GreaterThanEqual(band, target) as maskedvalues from dataframe\")\n</code></pre></p>"},{"location":"api/sql/Raster-operators/#rs_lessthan","title":"RS_LessThan","text":"<p>Introduction: Mask all the values with 1 which are less than a particular target value</p> <p>Format: <code>RS_LessThan (Band: Array[Double], Target: Double)</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL example: <pre><code>val lessDF = spark.sql(\"select RS_LessThan(band, target) as maskedvalues from dataframe\")\n</code></pre></p>"},{"location":"api/sql/Raster-operators/#rs_lessthanequal","title":"RS_LessThanEqual","text":"<p>Introduction: Mask all the values with 1 which are less than equal to a particular target value</p> <p>Format: <code>RS_LessThanEqual (Band: Array[Double], Target: Double)</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL example: <pre><code>val lessEqualDF = spark.sql(\"select RS_LessThanEqual(band, target) as maskedvalues from dataframe\")\n</code></pre></p>"},{"location":"api/sql/Raster-operators/#rs_logicaldifference","title":"RS_LogicalDifference","text":"<p>Introduction: Return value from band 1 if a value in band1 and band2 are different, else return 0</p> <p>Format: <code>RS_LogicalDifference (Band1: Array[Double], Band2: Array[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL example: <pre><code>val logicalDifference = spark.sql(\"select RS_LogicalDifference(band1, band2) as logdifference from dataframe\")\n</code></pre></p>"},{"location":"api/sql/Raster-operators/#rs_logicalover","title":"RS_LogicalOver","text":"<p>Introduction: Return value from band1 if it's not equal to 0, else return band2 value</p> <p>Format: <code>RS_LogicalOver (Band1: Array[Double], Band2: Array[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL example: <pre><code>val logicalOver = spark.sql(\"select RS_LogicalOver(band1, band2) as logover from dataframe\")\n</code></pre></p>"},{"location":"api/sql/Raster-operators/#rs_mean","title":"RS_Mean","text":"<p>Introduction: Returns Mean value for a spectral band in a Geotiff image</p> <p>Format: <code>RS_Mean (Band: Array[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL example: <pre><code>val meanDF = spark.sql(\"select RS_Mean(band) as mean from dataframe\")\n</code></pre></p>"},{"location":"api/sql/Raster-operators/#rs_mode","title":"RS_Mode","text":"<p>Introduction: Returns Mode from a spectral band in a Geotiff image in form of an array</p> <p>Format: <code>RS_Mode (Band: Array[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL example: <pre><code>val modeDF = spark.sql(\"select RS_Mode(band) as mode from dataframe\")\n</code></pre></p>"},{"location":"api/sql/Raster-operators/#rs_modulo","title":"RS_Modulo","text":"<p>Introduction: Find modulo of pixels with respect to a particular value</p> <p>Format: <code>RS_Modulo (Band: Array[Double], Target: Double)</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL example: <pre><code>val moduloDF = spark.sql(\"select RS_Modulo(band, target) as modulo from dataframe\")\n</code></pre></p>"},{"location":"api/sql/Raster-operators/#rs_multiply","title":"RS_Multiply","text":"<p>Introduction: Multiply two spectral bands in a Geotiff image</p> <p>Format: <code>RS_Multiply (Band1: Array[Double], Band2: Array[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL example: <pre><code>val multiplyDF = spark.sql(\"select RS_Multiply(band1, band2) as multiplyBands from dataframe\")\n</code></pre></p>"},{"location":"api/sql/Raster-operators/#rs_multiplyfactor","title":"RS_MultiplyFactor","text":"<p>Introduction: Multiply a factor to a spectral band in a geotiff image</p> <p>Format: <code>RS_MultiplyFactor (Band1: Array[Double], Factor: Int)</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL example: <pre><code>val multiplyFactorDF = spark.sql(\"select RS_MultiplyFactor(band1, 2) as multiplyfactor from dataframe\")\n</code></pre></p>"},{"location":"api/sql/Raster-operators/#rs_normalize","title":"RS_Normalize","text":"<p>Introduction: Normalize the value in the array to [0, 255]</p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL example <pre><code>SELECT RS_Normalize(band)\n</code></pre></p>"},{"location":"api/sql/Raster-operators/#rs_normalizeddifference","title":"RS_NormalizedDifference","text":"<p>Introduction: Returns Normalized Difference between two bands(band2 and band1) in a Geotiff image(example: NDVI, NDBI)</p> <p>Format: <code>RS_NormalizedDifference (Band1: Array[Double], Band2: Array[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL example: <pre><code>val normalizedDF = spark.sql(\"select RS_NormalizedDifference(band1, band2) as normdifference from dataframe\")\n</code></pre></p>"},{"location":"api/sql/Raster-operators/#rs_squareroot","title":"RS_SquareRoot","text":"<p>Introduction: Find Square root of band values in a geotiff image </p> <p>Format: <code>RS_SquareRoot (Band: Array[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL example: <pre><code>val rootDF = spark.sql(\"select RS_SquareRoot(band) as squareroot from dataframe\")\n</code></pre></p>"},{"location":"api/sql/Raster-operators/#rs_subtract","title":"RS_Subtract","text":"<p>Introduction: Subtract two spectral bands in a Geotiff image(band2 - band1)</p> <p>Format: <code>RS_Subtract (Band1: Array[Double], Band2: Array[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL example: <pre><code>val subtractDF = spark.sql(\"select RS_Subtract(band1, band2) as differenceOfOfBands from dataframe\")\n</code></pre></p>"},{"location":"api/sql/Raster-writer/","title":"Raster writer","text":"<p>Note</p> <p>Sedona writers are available in Scala, Java and Python and have the same APIs.</p>"},{"location":"api/sql/Raster-writer/#write-raster-dataframe-to-raster-files","title":"Write Raster DataFrame to raster files","text":"<p>To write a Sedona Raster DataFrame to raster files, you need to (1) first convert the Raster DataFrame to a binary DataFrame using <code>RS_AsXXX</code> functions and (2) then write the binary DataFrame to raster files using Sedona's built-in <code>raster</code> data source.</p>"},{"location":"api/sql/Raster-writer/#write-raster-dataframe-to-a-binary-dataframe","title":"Write raster DataFrame to a binary DataFrame","text":"<p>You can use the following RS output functions (<code>RS_AsXXX</code>) to convert a Raster DataFrame to a binary DataFrame. Generally the output format of a raster can be different from the original input format. For example, you can use <code>RS_FromGeoTiff</code> to create rasters and save them using <code>RS_AsArcInfoAsciiGrid</code>.</p>"},{"location":"api/sql/Raster-writer/#rs_asgeotiff","title":"RS_AsGeoTiff","text":"<p>Introduction: Returns a binary DataFrame from a Raster DataFrame. Each raster object in the resulting DataFrame is a GeoTiff image in binary format.</p> <p>Since: <code>v1.4.1</code></p> <p>Format 1: <code>RS_AsGeoTiff(raster: Raster)</code></p> <p>Format 2: <code>RS_AsGeoTiff(raster: Raster, compressionType:String, imageQuality:Integer/Decimal)</code></p> <p>Possible values for <code>compressionType</code>: <code>None</code>, <code>PackBits</code>, <code>Deflate</code>, <code>Huffman</code>, <code>LZW</code> and <code>JPEG</code></p> <p>Possible values for <code>imageQuality</code>: any decimal number between 0 and 1. 0 means the lowest quality and 1 means the highest quality.</p> <p>SQL example 1:</p> <pre><code>SELECT RS_AsGeoTiff(raster) FROM my_raster_table\n</code></pre> <p>SQL example 2:</p> <pre><code>SELECT RS_AsGeoTiff(raster, 'LZW', '0.75') FROM my_raster_table\n</code></pre> <p>Output:</p> <pre><code>+--------------------+\n|             geotiff|\n+--------------------+\n|[4D 4D 00 2A 00 0...|\n+--------------------+\n</code></pre> <p>Output schema:</p> <pre><code>root\n|-- geotiff: binary (nullable = true)\n</code></pre>"},{"location":"api/sql/Raster-writer/#rs_asarcgrid","title":"RS_AsArcGrid","text":"<p>Introduction: Returns a binary DataFrame from a Raster DataFrame. Each raster object in the resulting DataFrame is an ArcGrid image in binary format. ArcGrid only takes 1 source band. If your raster has multiple bands, you need to specify which band you want to use as the source.</p> <p>Since: <code>v1.4.1</code></p> <p>Format 1: <code>RS_AsArcGrid(raster: Raster)</code></p> <p>Format 2: <code>RS_AsArcGrid(raster: Raster, sourceBand:Integer)</code></p> <p>Possible values for <code>sourceBand</code>: any non-negative value (&gt;=0). If not given, it will use Band 0.</p> <p>SQL example 1:</p> <pre><code>SELECT RS_AsArcGrid(raster) FROM my_raster_table\n</code></pre> <p>SQL example 2:</p> <pre><code>SELECT RS_AsArcGrid(raster, 1) FROM my_raster_table\n</code></pre> <p>Output:</p> <pre><code>+--------------------+\n|             arcgrid|\n+--------------------+\n|[4D 4D 00 2A 00 0...|\n+--------------------+\n</code></pre> <p>Output schema:</p> <pre><code>root\n|-- arcgrid: binary (nullable = true)\n</code></pre>"},{"location":"api/sql/Raster-writer/#write-a-binary-dataframe-to-raster-files","title":"Write a binary DataFrame to raster files","text":"<p>Introduction: You can write a Sedona binary DataFrame to external storage using Sedona's built-in <code>raster</code> data source. Note that: <code>raster</code> data source does not support reading rasters. Please use Spark built-in <code>binaryFile</code> and Sedona RS constructors together to read rasters.</p> <p>Since: <code>v1.4.1</code></p> <p>Available options:</p> <ul> <li>rasterField:<ul> <li>Default value: the <code>binary</code> type column in the DataFrame. If the input DataFrame has several binary columns, please specify which column you want to use.</li> <li>Allowed values: the name of the to-be-saved binary type column</li> </ul> </li> <li>fileExtension<ul> <li>Default value: <code>.tiff</code></li> <li>Allowed values: any string values such as <code>.png</code>, <code>.jpeg</code>, <code>.asc</code></li> </ul> </li> <li>pathField<ul> <li>No defaulut value. If you use this option, then the column specified in this option must exist in the DataFrame schema. If this option is not used, each produced raster image will have a random UUID file name.</li> <li>Allowed values: any column name that indicates the paths of each raster file</li> </ul> </li> </ul> <p>The schema of the Raster dataframe to be written can be one of the following two schemas:</p> <pre><code>root\n |-- rs_asgeotiff(raster): binary (nullable = true)\n</code></pre> <p>or</p> <pre><code>root\n |-- rs_asgeotiff(raster): binary (nullable = true)\n |-- path: string (nullable = true)\n</code></pre> <p>Spark SQL example 1:</p> <pre><code>sparkSession.write.format(\"raster\").mode(SaveMode.Overwrite).save(\"my_raster_file\")\n</code></pre> <p>Spark SQL example 2:</p> <pre><code>sparkSession.write.format(\"raster\").option(\"rasterField\", \"raster\").option(\"pathField\", \"path\").option(\"fileExtension\", \".tiff\").mode(SaveMode.Overwrite).save(\"my_raster_file\")\n</code></pre> <p>The produced file structure will look like this:</p> <pre><code>my_raster_file\n- part-00000-6c7af016-c371-4564-886d-1690f3b27ca8-c000\n    - test1.tiff\n    - .test1.tiff.crc\n- part-00001-6c7af016-c371-4564-886d-1690f3b27ca8-c000\n    - test2.tiff\n    - .test2.tiff.crc\n- part-00002-6c7af016-c371-4564-886d-1690f3b27ca8-c000\n    - test3.tiff\n    - .test3.tiff.crc\n- _SUCCESS\n</code></pre> <p>To read it back to Sedona Raster DataFrame, you can use the following command (note the <code>*</code> in the path):</p> <pre><code>sparkSession.read.format(\"binaryFile\").load(\"my_raster_file/*\")\n</code></pre> <p>Then you can create Raster type in Sedona like this <code>RS_FromGeoTiff(content)</code> (if the written data was in GeoTiff format).</p> <p>The newly created DataFrame can be written to disk again but must be under a different name such as <code>my_raster_file_modified</code></p>"},{"location":"api/sql/Raster-writer/#write-arraydouble-to-geotiff-files","title":"Write Array[Double] to GeoTiff files","text":"<p>Warning</p> <p>This function has been deprecated since v1.4.1. Please use <code>RS_AsGeoTiff</code> instead and <code>raster</code> data source to write GeoTiff files.</p> <p>Introduction: You can write a GeoTiff dataframe as GeoTiff images using the spark <code>write</code> feature with the format <code>geotiff</code>. The geotiff raster column needs to be an array of double type data.</p> <p>Since: <code>v1.2.1</code></p> <p>Spark SQL example:</p> <p>The schema of the GeoTiff dataframe to be written can be one of the following two schemas:</p> <pre><code> |-- image: struct (nullable = true)\n |    |-- origin: string (nullable = true)\n |    |-- Geometry: geometry (nullable = true)\n |    |-- height: integer (nullable = true)\n |    |-- width: integer (nullable = true)\n |    |-- nBands: integer (nullable = true)\n |    |-- data: array (nullable = true)\n |    |    |-- element: double (containsNull = true)\n</code></pre> <p>or</p> <pre><code> |-- origin: string (nullable = true)\n |-- Geometry: geometry (nullable = true)\n |-- height: integer (nullable = true)\n |-- width: integer (nullable = true)\n |-- nBands: integer (nullable = true)\n |-- data: array (nullable = true)\n |    |-- element: double (containsNull = true)\n</code></pre> <p>Field names can be renamed, but schema should exactly match with one of the above two schemas. The output path could be a path to a directory where GeoTiff images will be saved. If the directory already exists, <code>write</code> should be called in <code>overwrite</code> mode.</p> <pre><code>var dfToWrite = sparkSession.read.format(\"geotiff\").option(\"dropInvalid\", true).option(\"readToCRS\", \"EPSG:4326\").load(\"PATH_TO_INPUT_GEOTIFF_IMAGES\")\ndfToWrite.write.format(\"geotiff\").save(\"DESTINATION_PATH\")\n</code></pre> <p>You can override an existing path with the following approach:</p> <pre><code>dfToWrite.write.mode(\"overwrite\").format(\"geotiff\").save(\"DESTINATION_PATH\")\n</code></pre> <p>You can also extract the columns nested within <code>image</code> column and write the dataframe as GeoTiff image.</p> <pre><code>dfToWrite = dfToWrite.selectExpr(\"image.origin as origin\",\"image.geometry as geometry\", \"image.height as height\", \"image.width as width\", \"image.data as data\", \"image.nBands as nBands\")\ndfToWrite.write.mode(\"overwrite\").format(\"geotiff\").save(\"DESTINATION_PATH\")\n</code></pre> <p>If you want the saved GeoTiff images not to be distributed into multiple partitions, you can call coalesce to merge all files in a single partition.</p> <pre><code>dfToWrite.coalesce(1).write.mode(\"overwrite\").format(\"geotiff\").save(\"DESTINATION_PATH\")\n</code></pre> <p>In case, you rename the columns of GeoTiff dataframe, you can set the corresponding column names with the <code>option</code> parameter. All available optional parameters are listed below:</p> <pre><code> |-- writeToCRS: (Default value \"EPSG:4326\") =&gt; Coordinate reference system of the geometry coordinates representing the location of the Geotiff.\n |-- fieldImage: (Default value \"image\") =&gt; Indicates the image column of GeoTiff DataFrame.\n |-- fieldOrigin: (Default value \"origin\") =&gt; Indicates the origin column of GeoTiff DataFrame.\n |-- fieldNBands: (Default value \"nBands\") =&gt; Indicates the nBands column of GeoTiff DataFrame.\n |-- fieldWidth: (Default value \"width\") =&gt; Indicates the width column of GeoTiff DataFrame.\n |-- fieldHeight: (Default value \"height\") =&gt; Indicates the height column of GeoTiff DataFrame.\n |-- fieldGeometry: (Default value \"geometry\") =&gt; Indicates the geometry column of GeoTiff DataFrame.\n |-- fieldData: (Default value \"data\") =&gt; Indicates the data column of GeoTiff DataFrame.\n</code></pre> <p>An example:</p> <pre><code>dfToWrite = sparkSession.read.format(\"geotiff\").option(\"dropInvalid\", true).option(\"readToCRS\", \"EPSG:4326\").load(\"PATH_TO_INPUT_GEOTIFF_IMAGES\")\ndfToWrite = dfToWrite.selectExpr(\"image.origin as source\",\"ST_GeomFromWkt(image.geometry) as geom\", \"image.height as height\", \"image.width as width\", \"image.data as data\", \"image.nBands as bands\")\ndfToWrite.write.mode(\"overwrite\").format(\"geotiff\").option(\"writeToCRS\", \"EPSG:4326\").option(\"fieldOrigin\", \"source\").option(\"fieldGeometry\", \"geom\").option(\"fieldNBands\", \"bands\").save(\"DESTINATION_PATH\")\n</code></pre>"},{"location":"api/sql/Raster-writer/#write-arraydouble-to-other-formats","title":"Write Array[Double] to other formats","text":""},{"location":"api/sql/Raster-writer/#rs_base64","title":"RS_Base64","text":"<p>Introduction: Return a Base64 String from a geotiff image</p> <p>Format: <code>RS_Base64 (height:Int, width:Int, redBand: Array[Double], greenBand: Array[Double], blackBand: Array[Double], optional: alphaBand: Array[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>Spark SQL example: <pre><code>val BandDF = spark.sql(\"select RS_Base64(h, w, band1, band2, RS_Array(h*w, 0)) as baseString from dataframe\")\nBandDF.show()\n</code></pre></p> <p>Output:</p> <pre><code>+--------------------+\n|          baseString|\n+--------------------+\n|QJCIAAAAAABAkDwAA...|\n|QJOoAAAAAABAlEgAA...|\n+--------------------+\n</code></pre> <p>Note</p> <p>Although the 3 RGB bands are mandatory, you can use RS_Array(h*w, 0.0) to create an array (zeroed out, size = h * w) as input.</p>"},{"location":"api/sql/Raster-writer/#rs_html","title":"RS_HTML","text":"<p>Introduction: Return a html img tag with the base64 string embedded</p> <p>Format: <code>RS_HTML(base64:String, optional: width_in_px:String)</code></p> <p>Spark SQL example:</p> <pre><code>df.selectExpr(\"RS_HTML(encodedstring, '300') as htmlstring\" ).show()\n</code></pre> <p>Output:</p> <pre><code>+--------------------+\n|          htmlstring|\n+--------------------+\n|&lt;img src=\"data:im...|\n|&lt;img src=\"data:im...|\n+--------------------+\n</code></pre>"},{"location":"api/viz/java-api/","title":"RDD","text":"<p>Please read Javadoc</p> <p>Note: Scala can call Java APIs seamlessly. That means Scala users use the same APIs with Java users</p>"},{"location":"api/viz/sql/","title":"DataFrame/SQL","text":""},{"location":"api/viz/sql/#quick-start","title":"Quick start","text":"<p>The detailed explanation is here: Visualize Spatial DataFrame/RDD.</p> <ol> <li>Add Sedona-core, Sedona-SQL,Sedona-Viz into your project POM.xml or build.sbt</li> <li>Declare your Spark Session <pre><code>sparkSession = SparkSession.builder().\nconfig(\"spark.serializer\",\"org.apache.spark.serializer.KryoSerializer\").\nconfig(\"spark.kryo.registrator\", \"org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator\").\nmaster(\"local[*]\").appName(\"mySedonaVizDemo\").getOrCreate()\n</code></pre></li> <li>Add the following lines after your SparkSession declaration: <pre><code>SedonaSQLRegistrator.registerAll(sparkSession)\nSedonaVizRegistrator.registerAll(sparkSession)\n</code></pre></li> </ol>"},{"location":"api/viz/sql/#regular-functions","title":"Regular functions","text":""},{"location":"api/viz/sql/#st_colorize","title":"ST_Colorize","text":"<p>Introduction: Given the weight of a pixel, return the corresponding color. The weight can be the spatial aggregation of spatial objects or spatial observations such as temperature and humidity.</p> <p>Note</p> <p>The color is encoded to an Integer type value in DataFrame. When you print it, it will show some nonsense values. You can just treat them as colors in GeoSparkViz.</p> <p>Format: <code>ST_Colorize (weight:Double, maxWeight:Double, mandatory color: string (Optional))</code></p> <p>Since: <code>v1.0.0</code></p>"},{"location":"api/viz/sql/#produce-various-colors-heat-map","title":"Produce various colors - heat map","text":"<p>This function will normalize the weight according to the max weight among all pixels. Different pixel obtains different color.</p> <p>Spark SQL example: <pre><code>SELECT pixels.px, ST_Colorize(pixels.weight, 999) AS color\nFROM pixels\n</code></pre></p>"},{"location":"api/viz/sql/#produce-uniform-colors-scatter-plot","title":"Produce uniform colors - scatter plot","text":"<p>If a mandatory color name is put as the third input argument, this function will directly output this color, without considering the weights. In this case, every pixel will possess the same color.</p> <p>Spark SQL example: <pre><code>SELECT pixels.px, ST_Colorize(pixels.weight, 999, 'red') AS color\nFROM pixels\n</code></pre></p> <p>Here are some example color names can be entered: <pre><code>\"firebrick\"\n\"#aa38e0\"\n\"0x40A8CC\"\n\"rgba(112,36,228,0.9)\"\n</code></pre></p> <p>Please refer to AWT Colors for a list of pre-defined colors.</p>"},{"location":"api/viz/sql/#st_encodeimage","title":"ST_EncodeImage","text":"<p>Introduction: Return the base64 string representation of a Java PNG BufferedImage. This is specific for the server-client environment. For example, transfer the base64 string from GeoSparkViz to Apache Zeppelin.</p> <p>Format: <code>ST_EncodeImage (A:image)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_EncodeImage(images.img)\nFROM images\n</code></pre></p>"},{"location":"api/viz/sql/#st_pixelize","title":"ST_Pixelize","text":"<p>Introduction: Convert a geometry to an array of pixels given a resolution</p> <p>You should use it together with <code>Lateral View</code> and <code>Explode</code></p> <p>Format: <code>ST_Pixelize (A:geometry, ResolutionX:int, ResolutionY:int, Boundary:geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_Pixelize(shape, 256, 256, (ST_Envelope_Aggr(shape) FROM pointtable))\nFROM polygondf\n</code></pre></p>"},{"location":"api/viz/sql/#st_tilename","title":"ST_TileName","text":"<p>Introduction: Return the map tile name for a given zoom level. Please refer to OpenStreetMap ZoomLevel and OpenStreetMap tile name.</p> <p>Note</p> <p>Tile name is formatted as a \"Z-X-Y\" string. Z is zoom level. X is tile coordinate on X axis. Y is tile coordinate on Y axis.</p> <p>Format: <code>ST_TileName (A:pixel, ZoomLevel:int)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT ST_TileName(pixels.px, 3)\nFROM pixels\n</code></pre></p>"},{"location":"api/viz/sql/#aggregate-functions","title":"Aggregate functions","text":""},{"location":"api/viz/sql/#st_render","title":"ST_Render","text":"<p>Introduction: Given a group of pixels and their colors, return a single Java PNG BufferedImage. The 3rd parameter is optional and it is the zoom level. You should use zoom level when you want to render tiles, instead of a single image.</p> <p>Format: <code>ST_Render (A:pixel, B:color, C:Integer - optional zoom level)</code></p> <p>Since: <code>v1.0.0</code></p> <p>Spark SQL example: <pre><code>SELECT tilename, ST_Render(pixels.px, pixels.color) AS tileimg\nFROM pixels\nGROUP BY tilename\n</code></pre></p>"},{"location":"asf/asf/","title":"Copyright","text":"<p>Apache Sedona, Sedona, Apache, the Apache feather logo, and the Apache Sedona project logo are either registered trademarks or trademarks of The Apache Software Foundation in the United States and other countries. All other marks mentioned may be trademarks or registered trademarks of their respective owners. Please visit Apache Software Foundation for more details.</p>"},{"location":"community/contact/","title":"Community","text":"<p>Every volunteer project obtains its strength from the people involved in it. We invite you to participate as much or as little as you choose.</p> <p>You can participate in the community as follows:</p> <ul> <li>Use our project and provide a feedback.</li> <li>Provide us with the use-cases.</li> <li>Report bugs and submit patches.</li> <li>Contribute code and documentation.</li> </ul>"},{"location":"community/contact/#twitter","title":"Twitter","text":"<p>Apache Sedona@Twitter</p>"},{"location":"community/contact/#discord-server","title":"Discord Server","text":"<p>Join Apache Sedona community server!</p>"},{"location":"community/contact/#mailing-list","title":"Mailing list","text":"<p>Get help using Sedona or contribute to the project on our mailing lists</p> <p>Sedona Mailing Lists: dev@sedona.apache.org: project development and general questions</p> <ul> <li>Please first subscribe and then post emails. To subscribe, please send an email (leave the subject and content blank) to dev-subscribe@sedona.apache.org</li> </ul>"},{"location":"community/contact/#issue-tracker","title":"Issue tracker","text":""},{"location":"community/contact/#bug-reports","title":"Bug Reports","text":"<p>Found bug? Enter an issue in the Sedona JIRA</p> <p>Before submitting an issue, please:</p> <ul> <li>Verify that the bug does in fact exist.</li> <li>Search the issue tracker to verify there is no existing issue reporting the bug you\u2019ve found.</li> <li>Consider tracking down the bug yourself in the Sedona\u2019s source and submitting a patch along with your bug report. This is a great time saver for the Sedona developers and helps ensure the bug will be fixed quickly.</li> </ul>"},{"location":"community/contact/#feature-requests","title":"Feature Requests","text":"<p>Enhancement requests for new features are also welcome. The more concrete and rationale the request is, the greater the chance it will be incorporated into future releases.</p> <p>Enter an issue in the Sedona JIRA or send an email to dev@sedona.apache.org</p>"},{"location":"community/contributor/","title":"Project Management Committee","text":"<p>Sedona has received numerous help from the community. This page lists the contributors and committers of Apache Sedona. People on this page are ordered by their last name.</p>"},{"location":"community/contributor/#committers","title":"Committers","text":"<p>A contributor who contributes enough code to Sedona will be promoted to a committer. A committer has the write access to Sedona main repository</p>"},{"location":"community/contributor/#project-management-committee-pmc","title":"Project Management Committee (PMC)","text":"<p>A committer will be promoted to a PMC member when the community thinks he/she is able to be in charge at least a major component of this project.</p> <p>Current Sedona PMC members are as follows:</p> Name GitHub ID Apache ID Adam Binford Kimahriman kimahriman@apache.org Kanchan Chowdhury kanchanchy kanchanchy@apache.org Pawe\u0142 Koci\u0144ski Imbruced imbruced@apache.org Yitao Li yitao-li yitaoli@apache.org Netanel Malka netanel246 malka@apache.org Mohamed Sarwat Sarwat mosarwat@apache.org Kengo Seki sekikn sekikn@apache.org Sachio Wakai SW186000 swakai@apache.org Jinxuan Wu jinxuan jinxuanw@apache.org Jia Yu jiayuasu jiayu@apache.org Zongsi Zhang zongsizhang zongsizhang@apache.org Felix Cheung felixcheung@apache.org Von Gosling vongosling@apache.org Jean-Baptiste Onofr\u00e9 jbonofre@apache.org George Percivall percivall@apache.org Sunil Govindan sunilg@apache.org"},{"location":"community/contributor/#become-a-committer","title":"Become a committer","text":"<p>To get started contributing to Sedona, learn how to contribute \u2013 anyone can submit patches, documentation and examples to the project.</p> <p>The PMC regularly adds new committers from the active contributors, based on their contributions to Sedona. The qualifications for new committers include:</p> <ul> <li>Sustained contributions to Spark: Committers should have a history of major contributions to Sedona.</li> <li>Quality of contributions: Committers more than any other community member should submit simple, well-tested, and well-designed patches. In addition, they should show sufficient expertise to be able to review patches.</li> <li>Community involvement: Committers should have a constructive and friendly attitude in all community interactions. They should also be active on the dev mailing list &amp; Gitter, and help mentor newer contributors and users.</li> </ul> <p>The PMC also adds new PMC members. PMC members are expected to carry out PMC responsibilities as described in Apache Guidance, including helping vote on releases, enforce Apache project trademarks, take responsibility for legal and license issues, and ensure the project follows Apache project mechanics. The PMC periodically adds committers to the PMC who have shown they understand and can help with these activities.</p> <p>Currently, Sedona makes committers PMC members automatically.</p>"},{"location":"community/contributor/#nominate-a-committer-or-pmc-member","title":"Nominate a committer or PMC member","text":"<p>Steps are as follows: 1. Call a vote (templates/committerVote.txt) 2. Close the vote. If the result is positive, invite the new committer.</p>"},{"location":"community/contributor/#call-for-a-vote","title":"Call for a vote","text":"<p>We do the vote and discussion on the private@sedona.apache.org to enable a frank discussion.</p> <p>Let the Vote thread run for one week. A positive result is achieved by Consensus Approval: at least 3 +1 votes and no vetoes.</p>"},{"location":"community/contributor/#pmc-vote-template","title":"PMC vote template","text":"<p>This is the email to commence a vote for a new PMC candidate. New PMC members need to be voted for by the existing PMC members and subsequently approved by the Board.</p> <pre><code>To: private@sedona.apache.org\nSubject: [VOTE] New PMC candidate: [New PMC NAME]\n\n[ add the reasons behind your nomination here ]\n\nVoting ends one week from today, or until at least 3 +1 votes are cast.\n</code></pre>"},{"location":"community/contributor/#close-a-vote","title":"Close a vote","text":"<p>This email ends the vote and reports the result to the project.</p> <pre><code>To: private@sedona.apache.org\nSubject: [VOTE][RESULT] New PMC candidate: [New PMC NAME]\n\nThe vote has now closed: [paste the vote thread on https://lists.apache.org/list.html?private@sedona.apache.org]. The results are:\n\nBinding Votes:\n\n+1 [TOTAL BINDING +1 VOTES]\n 0 [TOTAL BINDING +0/-0 VOTES]\n-1 [TOTAL BINDING -1 VOTES]\n\nThe vote is ***successful/not successful***\n</code></pre>"},{"location":"community/contributor/#send-a-notice-to-asf-board","title":"Send a notice to ASF Board","text":"<p>The nominating PMC member should send a message to the ASF Board (board@apache.org) with a reference to the vote result in the following form:</p> <pre><code>To: board at apache.org\nCC: private at sedona.apache.org\nSubject: [NOTICE] New PMC NAME for Apache Sedona PMC\nBody:\n\nNew PMC NAME has been voted as a new member of the Apache Sedona PMC. the vote thread is at: *link to the vote result thread*\n</code></pre>"},{"location":"community/contributor/#send-the-invitation","title":"Send the invitation","text":"<pre><code>To: New PMC Email address\nCC: private@sedona.apache.org\n\nHello [New PMC NAME],\n\nThe Sedona Project Management Committee (PMC) \nhereby offers you committer privileges to the project \n[as well as membership in the PMC]. These privileges are\noffered on the understanding that you'll use them\nreasonably and with common sense. We like to work on trust\nrather than unnecessary constraints. \n\nBeing a committer enables you to more easily make \nchanges without needing to go through the patch \nsubmission process. Being a PMC member enables you \nto guide the direction of the project.\n\nBeing a committer does not require you to \nparticipate any more than you already do. It does \ntend to make one even more committed.  You will \nprobably find that you spend more time here.\n\nOf course, you can decline and instead remain as a \ncontributor, participating as you do now.\n\nA. This personal invitation is a chance for you to \naccept or decline in private.  Either way, please \nlet us know in reply to the private@sedona.apache.org \naddress only.\n\nB. If you accept, the next step is to register an iCLA:\n    1. Details of the iCLA and the forms are found \n    through this link: https://www.apache.org/licenses/#clas\n\n    2. Instructions for its completion and return to \n    the Secretary of the ASF are found at\n    https://www.apache.org/licenses/#submitting\n\n    3. When you transmit the completed iCLA, request \n    to notify the Apache Sedona project and choose a \n    unique Apache ID. Look to see if your preferred \n    ID is already taken at \n    https://people.apache.org/committer-index.html\n    This will allow the Secretary to notify the PMC \n    when your iCLA has been recorded.\n\nWhen recording of your iCLA is noted, you will \nreceive a follow-up message with the next steps for \nestablishing you as a committer.\n</code></pre>"},{"location":"community/contributor/#pmc-accept-and-icla-instruction","title":"PMC Accept and ICLA instruction","text":"<pre><code>To: New PMC Email address\nCc: private@sedona.apache.org\nSubject: Re: invitation to become Apache Sedona PMC\n\nWelcome. Here are the next steps in becoming a project committer. After that we will make an announcement to the dev@sedona.apache.org\n\n1. You need to send a Contributor License Agreement to the ASF.\nNormally you would send an Individual CLA. If you also make\ncontributions done in work time or using work resources,\nsee the Corporate CLA. Ask us if you have any issues.\nhttps://www.apache.org/licenses/#clas.\n\nYou need to choose a preferred ASF user name and alternatives.\nIn order to ensure it is available you can view a list of taken IDs at\nhttps://people.apache.org/committer-index.html\n\nPlease notify us when you have submitted the CLA and by what means \nyou did so. This will enable us to monitor its progress.\n\nWe will arrange for your Apache user account when the CLA has \nbeen recorded.\n\n2. After that is done, please use your ASF email to subscribe to the dev@sedona.apache.org\nand private@sedona.apache.org by sending an email to dev-subscribe@sedona.apache.org and \nprivate-subscribe@sedona.apache.org. We generally discuss everything on the dev list and \nkeep the private@sedona.apache.org list for occasional matters which must be private.\n\nThe developer section of the website describes roles within the ASF and provides other\nresources:\n  https://www.apache.org/foundation/how-it-works.html\n  https://www.apache.org/dev/\n\nJust as before you became a committer, participation in any ASF community\nrequires adherence to the ASF Code of Conduct:\n  https://www.apache.org/foundation/policies/conduct.html\n\nYours,\nThe Apache Sedona PMC\n</code></pre>"},{"location":"community/contributor/#create-asf-account","title":"Create ASF account","text":"<p>Once the ICLA has been filed, use the ASF New Account Request form to generate the request. Sedona mentors will request the account.</p> <p>Once Sedona graduates, the PMC chair will make the request.</p>"},{"location":"community/contributor/#add-to-the-system","title":"Add to the system","text":"<p>Once the new PMC subscribes to the Sedona mailing lists using his/her ASF account, one of the PMC needs to add the new PMC to the Whimsy system (https://whimsy.apache.org/roster/pmc/sedona).</p>"},{"location":"community/contributor/#pmc-announcement","title":"PMC announcement","text":"<p>This is the email to announce the new committer to sedona-dev once the account has been created.</p> <pre><code>To: dev@sedona.apache.org\nSubject: new committer: ###New PMC NAME\n\nThe Project Management Committee (PMC) for Apache Sedona\nhas invited New PMC NAME to become a committer and we are pleased \nto announce that they have accepted.\n\n### add specific details here ###\n\nBeing a committer enables easier contribution to the\nproject since there is no need to go via the patch\nsubmission process. This should enable better productivity.\nA PMC member helps manage and guide the direction of the project.\n</code></pre>"},{"location":"community/contributor/#committer-done-template","title":"Committer Done Template","text":"<p>After the committer account is established.</p> <pre><code>To: New PMC Email\nCC: private@sedona.apache.org\nSubject: account request: New PMC NAME\n\nNew PMC NAME, as you know, the ASF Infrastructure has set up your\ncommitter account with the username '####'.\n\nYou have commit access to specific sections of the\nASF repository, as follows:\nhttps://github.com/apache/sedona\n\nYou need to link your ASF Account with your GitHub account.\n\nHere are the steps\n\n1. Verify you have a Github ID enabled with 2FA\n    * https://help.github.com/articles/securing-your-account-with-two-factor-authentication-2fa/\n2. Enter your Github ID into your Apache ID profile https://id.apache.org/\n3. Merge your Apache and GitHub accounts using\n    * GitBox (Apache Account Linking utility) https://gitbox.apache.org/setup/\n    * You should see 3 green checks in GitBox.\n    * Wait at least 30  minutes for an email inviting you to Apache GitHub Organization and accept invitation\n4. After accepting the Github Invitation verify that you are a \nmember of the team https://github.com/orgs/apache/teams/sedona-committers\n\nOptionally, if you want, please follow the instructions to set up your GitHub, SSH, svn password, svn configuration, email forwarding, etc.\nhttps://www.apache.org/dev/#committers\n\nAdditionally, if you have been elected to the Sedona\n Project Mgmt. Committee (PMC): Verify you are part of the LDAP sedona\n  pmc https://whimsy.apache.org/roster/pmc/sedona\n</code></pre>"},{"location":"community/develop/","title":"Develop Sedona","text":""},{"location":"community/develop/#scalajava-developers","title":"Scala/Java developers","text":""},{"location":"community/develop/#ide","title":"IDE","text":"<p>We recommend Intellij IDEA with Scala plugin installed. Please make sure that the IDE has JDK 1.8 set as project default.</p>"},{"location":"community/develop/#import-the-project","title":"Import the project","text":""},{"location":"community/develop/#choose-open","title":"Choose <code>Open</code>","text":""},{"location":"community/develop/#go-to-the-sedona-root-folder-not-a-submodule-folder-and-choose-open","title":"Go to the Sedona root folder (not a submodule folder) and choose <code>open</code>","text":""},{"location":"community/develop/#the-ide-might-show-errors","title":"The IDE might show errors","text":"<p>The IDE usually has trouble understanding the complex project structure in Sedona. </p> <p></p>"},{"location":"community/develop/#fix-errors-by-changing-pomxml","title":"Fix errors by changing POM.xml","text":"<p>You need to comment out the following lines in <code>pom.xml</code> at the root folder, as follows. Remember that you should NOT submit this change to Sedona.</p> <pre><code>&lt;!--    &lt;parent&gt;--&gt;\n&lt;!--        &lt;groupId&gt;org.apache&lt;/groupId&gt;--&gt;\n&lt;!--        &lt;artifactId&gt;apache&lt;/artifactId&gt;--&gt;\n&lt;!--        &lt;version&gt;23&lt;/version&gt;--&gt;\n&lt;!--        &lt;relativePath /&gt;--&gt;\n&lt;!--    &lt;/parent&gt;--&gt;\n</code></pre>"},{"location":"community/develop/#reload-pomxml","title":"Reload POM.xml","text":"<p>Make sure you reload the POM.xml or reload the maven project. The IDE will ask you to remove some modules. Please select <code>yes</code>.</p> <p></p>"},{"location":"community/develop/#the-final-project-structure-should-be-like-this","title":"The final project structure should be like this:","text":""},{"location":"community/develop/#run-unit-tests","title":"Run unit tests","text":""},{"location":"community/develop/#run-all-unit-tests","title":"Run all unit tests","text":"<p>In a terminal, go to the Sedona root folder. Run <code>mvn clean install</code>. All tests will take more than 15 minutes. To only build the project jars, run <code>mvn clean install -DskipTests</code>.</p> <p>Note</p> <p><code>mvn clean install</code> will compile Sedona with Spark 3.0 and Scala 2.12. If you have a different version of Spark in $SPARK_HOME, make sure to specify that using -Dspark command line arg.  For example, to compile sedona with Spark 3.4 and Scala 2.12, use: <code>mvn clean install -Dspark=3.4 -Dscala=2.12</code></p> <p>More details can be found on Compile Sedona</p>"},{"location":"community/develop/#run-a-single-unit-test","title":"Run a single unit test","text":"<p>In the IDE, right-click a test case and run this test case.</p> <p></p> <p>The IDE might tell you that the PATH does not exist as follows:</p> <p></p> <p>Go to <code>Edit Configuration</code></p> <p></p> <p>Append the submodule folder to <code>Working Directory</code>. For example, <code>sedona/sql</code>.</p> <p></p> <p>Re-run the test case. Do NOT right click the test case to re-run. Instead, click the button as shown in the figure below.</p> <p></p>"},{"location":"community/develop/#python-developers","title":"Python developers","text":""},{"location":"community/develop/#run-all-python-tests","title":"Run all python tests","text":"<p>To run all Python test cases, follow steps mentioned here.</p>"},{"location":"community/develop/#run-all-python-tests-in-a-single-test-file","title":"Run all python tests in a single test file","text":"<p>To run a particular python test file, specify the path of the .py file to pipenv.</p> <p>For example, to run all tests in <code>test_function.py</code> located in <code>python/tests/sql/</code>, use: <code>pipenv run pytest tests/sql/test_function.py</code>.</p>"},{"location":"community/develop/#run-a-single-test","title":"Run a single test","text":"<p>To run a particular test in a particular .py test file, specify <code>file_name::class_name::test_name</code> to the pytest command.</p> <p>For example, to run the test on ST_Contains function located in sql/test_predicate.py, use: <code>pipenv run pytest tests/sql/test_predicate.py::TestPredicate::test_st_contains</code></p>"},{"location":"community/develop/#ide_1","title":"IDE","text":"<p>We recommend PyCharm</p>"},{"location":"community/develop/#import-the-project_1","title":"Import the project","text":""},{"location":"community/develop/#r-developers","title":"R developers","text":"<p>More details to come.</p>"},{"location":"community/develop/#ide_2","title":"IDE","text":"<p>We recommend RStudio</p>"},{"location":"community/develop/#import-the-project_2","title":"Import the project","text":""},{"location":"community/publication/","title":"Publication","text":"<p>Apache Sedona was formerly called GeoSpark, initiated by Arizona State University Data Systems Lab.</p>"},{"location":"community/publication/#key-publications","title":"Key publications","text":"<p>\"Spatial Data Management in Apache Spark: The GeoSpark Perspective and Beyond\" is the full research paper that talks about the entire GeoSpark ecosystem. Please cite this paper if your work mentions GeoSpark core system.</p> <p>\"GeoSparkViz: A Scalable Geospatial Data Visualization Framework in the Apache Spark Ecosystem\" is the full research paper that talks about map visualization system in GeoSpark. Please cite this paper if your work mentions GeoSpark visualization system.</p> <p>\"Building A Microscopic Road Network Traffic Simulator in Apache Spark\" is the full research paper that talks about the traffic simulator in GeoSpark. Please cite this paper if your work mentions GeoSparkSim traffic simulator.</p>"},{"location":"community/publication/#third-party-evaluation","title":"Third-party evaluation","text":"<p>GeoSpark were evaluated by papers published on database top venues. It is worth noting that we do not have any collaboration with the authors.</p> <ul> <li>SIGMOD 2020 paper \"Architecting a Query Compiler for Spatial Workloads\" Ruby Y. Tahboub, Tiark  Rompf (Purdue University). <p>In Figure 16a, GeoSpark distance join query runs around 7x - 9x faster than Simba, a spatial extension on Spark, on 1 - 24 core machines.</p> </li> <li>PVLDB 2018 paper \"How Good Are Modern Spatial Analytics Systems?\" Varun Pandey, Andreas Kipf, Thomas Neumann, Alfons Kemper (Technical University of Munich), quoted as follows:  <p>GeoSpark comes close to a complete spatial analytics system. It also exhibits the best performance in most cases.</p> </li> </ul>"},{"location":"community/publication/#full-publications","title":"Full publications","text":""},{"location":"community/publication/#geospark-ecosystem","title":"GeoSpark Ecosystem","text":"<p>\"Spatial Data Management in Apache Spark: The GeoSpark Perspective and Beyond\" (research paper). Jia Yu, Zongsi Zhang, Mohamed Sarwat. Geoinformatica Journal 2019.</p> <p>\"A Demonstration of GeoSpark: A Cluster Computing Framework for Processing Big Spatial Data\" (demo paper). Jia Yu, Jinxuan Wu, Mohamed Sarwat. In Proceeding of IEEE International Conference on Data Engineering ICDE 2016, Helsinki, FI, May 2016</p> <p>\"GeoSpark: A Cluster Computing Framework for Processing Large-Scale Spatial Data\" (short paper). Jia Yu, Jinxuan Wu, Mohamed Sarwat. In Proceeding of the ACM International Conference on Advances in Geographic Information Systems ACM SIGSPATIAL GIS 2015, Seattle, WA, USA November 2015</p>"},{"location":"community/publication/#geosparkviz-visualization-system","title":"GeoSparkViz Visualization System","text":"<p>\"GeoSparkViz in Action: A Data System with built-in support for Geospatial Visualization\" (demo paper) Jia Yu, Anique Tahir, and Mohamed Sarwat. In Proceedings of the International Conference on Data Engineering, ICDE, 2019</p> <p>\"GeoSparkViz: A Scalable Geospatial Data Visualization Framework in the Apache Spark Ecosystem\" (research paper). Jia Yu, Zongsi Zhang, Mohamed Sarwat. In Proceedings of the International Conference on Scientific and Statistical Database Management, SSDBM 2018, Bolzano-Bozen, Italy July 2018</p>"},{"location":"community/publication/#geosparksim-traffic-simulator","title":"GeoSparkSim Traffic Simulator","text":"<p>\"Dissecting GeoSparkSim: a scalable microscopic road network traffic simulator in Apache Spark\" (journal paper) Jia Yu, Zishan Fu, Mohamed Sarwat. Distributed Parallel Databases 38(4): 963-994 (2020)</p> <p>\"Demonstrating GeoSparkSim: A Scalable Microscopic Road Network Traffic Simulator Based on Apache Spark\". Zishan Fu, Jia Yu, Mohamed Sarwat. International Symposium on Spatial and Temporal Databases, SSTD, 2019</p> <p>\"Building A Microscopic Road Network Traffic Simulator in Apache Spark\" (research paper) Zishan Fu, Jia Yu, and Mohamed Sarwat. In Proceedings of the International Conference on Mobile Data Management, MDM, 2019</p>"},{"location":"community/publication/#a-tutorial-about-geospatial-data-management-in-spark","title":"A Tutorial about Geospatial Data Management in Spark","text":"<p>\"Geospatial Data Management in Apache Spark: A Tutorial\" (Tutorial) Jia Yu and Mohamed Sarwat.  In Proceedings of the International Conference on Data Engineering, ICDE, 2019</p>"},{"location":"community/publish/","title":"Make a Sedona release","text":"<p>This page is for Sedona PMC to publish Sedona releases.</p> <p>Warning</p> <p>All scripts on this page should be run in your local Sedona Git repo under master branch via a single script file.</p>"},{"location":"community/publish/#0-prepare-an-empty-script-file","title":"0. Prepare an empty script file","text":"<ol> <li>In your local Sedona Git repo under master branch, run <pre><code>echo '#!/bin/bash' &gt; create-release.sh\nchmod 777 create-release.sh\n</code></pre></li> <li>Use your favourite GUI text editor to open <code>create-release.sh</code>.</li> <li>Then keep copying the scripts on this web page to replace all content in this script file.</li> <li>Do NOT directly copy/paste the scripts to your terminal because a bug in <code>clipboard.js</code> will create link breaks in such case. </li> <li>Each time when you copy content to this script file, run <code>./create-release.sh</code> to execute it.</li> </ol>"},{"location":"community/publish/#1-check-asf-copyright-in-all-file-headers","title":"1. Check ASF copyright in all file headers","text":"<ol> <li>Run the following script: <pre><code>#!/bin/bash\nwget -q https://dlcdn.apache.org//creadur/apache-rat-0.15/apache-rat-0.15-bin.tar.gz\ntar -xvf  apache-rat-0.15-bin.tar.gz\ngit clone --shared --branch master https://github.com/apache/sedona.git sedona-src\njava -jar apache-rat-0.15/apache-rat-0.15.jar -d sedona-src &gt; report.txt\n</code></pre></li> <li>Read the generated report.txt file and make sure all source code files have ASF header.</li> <li>Delete the generated report and cloned files <pre><code>#!/bin/bash\nrm -rf apache-rat-0.15\nrm -rf sedona-src\nrm report.txt\n</code></pre></li> </ol>"},{"location":"community/publish/#2-update-sedona-python-r-and-zeppelin-versions","title":"2. Update Sedona Python, R and Zeppelin versions","text":"<p>Make sure the Sedona version in the following files are 1.4.1. </p> <ol> <li>https://github.com/apache/sedona/blob/master/python/sedona/version.py</li> <li>https://github.com/apache/sedona/blob/master/R/DESCRIPTION</li> <li>https://github.com/apache/sedona/blob/master/R/R/dependencies.R#L42</li> <li>https://github.com/apache/sedona/blob/master/zeppelin/package.json</li> </ol>"},{"location":"community/publish/#3-update-mkdocsyml","title":"3. Update mkdocs.yml","text":"<ul> <li>Please change the following variables in <code>mkdocs.yml</code> to the version you want to publish.<ul> <li><code>sedona_create_release.current_version</code></li> <li><code>sedona_create_release.current_rc</code></li> <li><code>sedona_create_release.current_git_tag</code></li> <li><code>sedona_create_release.current_snapshot</code></li> </ul> </li> <li>Then compile the website by <code>mkdocs serve</code>. This will generate the scripts listed on this page in your local browser.</li> <li>You can also publish this website if needed. See the instruction at bottom.</li> </ul>"},{"location":"community/publish/#4-stage-and-upload-release-candidates","title":"4. Stage and upload release candidates","text":"<pre><code>#!/bin/bash\n\ngit checkout master\ngit pull\n\nrm -f release.*\nrm -f pom.xml.*\n\necho \"*****Step 1. Stage the Release Candidate to GitHub.\"\n\nmvn -q -B clean release:prepare -Dtag=sedona-1.4.1-rc1 -DreleaseVersion=1.4.1 -DdevelopmentVersion=1.5.0-SNAPSHOT -Dresume=false -Penable-all-submodules -Darguments=\"-DskipTests\"\nmvn -q -B release:clean -Penable-all-submodules\n\necho \"Now the releases are staged. A tag and two commits have been created on Sedona GitHub repo\"\n\necho \"*****Step 2: Upload the Release Candidate to https://repository.apache.org.\"\n\n# For Spark 3.0 and Scala 2.12\nmvn -q org.apache.maven.plugins:maven-release-plugin:2.3.2:perform -DconnectionUrl=scm:git:https://github.com/apache/sedona.git -Dtag=sedona-1.4.1-rc1 -Dresume=false -Darguments=\"-DskipTests -Dspark=3.0 -Dscala=2.12\" -Dspark=3.0 -Dscala=2.12\n\n# For Spark 3.0 and Scala 2.13\n## Note that we use maven-release-plugin 2.3.2 instead of more recent version (e.g., 3.0.1) to get rid of a bug of maven-release-plugin,\n## which prevent us from cloning git repo with user specified -Dtag=&lt;tag&gt;.\n## Please refer to https://issues.apache.org/jira/browse/MRELEASE-933 and https://issues.apache.org/jira/browse/SCM-729 for details.\n##\n## Please also note that system properties `-Dspark` and `-Dscala` has to be specified both for release:perform and the actual build parameters\n## in `-Darguments`, because the build profiles activated for release:perform task will also affect the actual build task. It is safer to specify\n## these system properties for both tasks.\nmvn -q org.apache.maven.plugins:maven-release-plugin:2.3.2:perform -DconnectionUrl=scm:git:https://github.com/apache/sedona.git -Dtag=sedona-1.4.1-rc1 -Dresume=false -Darguments=\"-DskipTests -Dspark=3.0 -Dscala=2.13\" -Dspark=3.0 -Dscala=2.13\n\n# For Spark 3.4 and Scala 2.12\nmvn -q org.apache.maven.plugins:maven-release-plugin:2.3.2:perform -DconnectionUrl=scm:git:https://github.com/apache/sedona.git -Dtag=sedona-1.4.1-rc1 -Dresume=false -Darguments=\"-DskipTests -Dspark=3.4 -Dscala=2.12\" -Dspark=3.4 -Dscala=2.12\n\n# For Spark 3.4 and Scala 2.13\nmvn -q org.apache.maven.plugins:maven-release-plugin:2.3.2:perform -DconnectionUrl=scm:git:https://github.com/apache/sedona.git -Dtag=sedona-1.4.1-rc1 -Dresume=false -Darguments=\"-DskipTests -Dspark=3.4 -Dscala=2.13\" -Dspark=3.4 -Dscala=2.13\n\necho \"*****Step 3: Upload Release Candidate on ASF SVN: https://dist.apache.org/repos/dist/dev/sedona\"\n\necho \"Creating 1.4.1-rc1 folder on SVN...\"\n\nsvn mkdir -m \"Adding folder\" https://dist.apache.org/repos/dist/dev/sedona/1.4.1-rc1\n\necho \"Creating release files locally...\"\n\necho \"Downloading source code...\"\n\nwget https://github.com/apache/sedona/archive/refs/tags/sedona-1.4.1-rc1.tar.gz\ntar -xvf sedona-1.4.1-rc1.tar.gz\nmkdir apache-sedona-1.4.1-src\ncp -r sedona-sedona-1.4.1-rc1/* apache-sedona-1.4.1-src/\ntar czf apache-sedona-1.4.1-src.tar.gz apache-sedona-1.4.1-src\nrm sedona-1.4.1-rc1.tar.gz\nrm -rf sedona-sedona-1.4.1-rc1\n\necho \"Compiling the source code...\"\n\nmkdir apache-sedona-1.4.1-bin\n\ncd apache-sedona-1.4.1-src &amp;&amp; mvn -q clean install -DskipTests -Dspark=3.0 -Dscala=2.12 &amp;&amp; cd ..\ncp apache-sedona-1.4.1-src/common/target/sedona-*1.4.1.jar apache-sedona-1.4.1-bin/\ncp apache-sedona-1.4.1-src/core/target/sedona-*1.4.1.jar apache-sedona-1.4.1-bin/\ncp apache-sedona-1.4.1-src/sql/spark-3.0/target/sedona-*1.4.1.jar apache-sedona-1.4.1-bin/\ncp apache-sedona-1.4.1-src/viz/target/sedona-*1.4.1.jar apache-sedona-1.4.1-bin/\ncp apache-sedona-1.4.1-src/python-adapter/target/sedona-*1.4.1.jar apache-sedona-1.4.1-bin/\ncp apache-sedona-1.4.1-src/spark-shaded/target/sedona-*1.4.1.jar apache-sedona-1.4.1-bin/\ncp apache-sedona-1.4.1-src/flink/target/sedona-*1.4.1.jar apache-sedona-1.4.1-bin/\ncp apache-sedona-1.4.1-src/flink-shaded/target/sedona-*1.4.1.jar apache-sedona-1.4.1-bin/\n\ncd apache-sedona-1.4.1-src &amp;&amp; mvn -q clean install -DskipTests -Dspark=3.0 -Dscala=2.13 &amp;&amp; cd ..\ncp apache-sedona-1.4.1-src/core/target/sedona-*1.4.1.jar apache-sedona-1.4.1-bin/\ncp apache-sedona-1.4.1-src/sql/spark-3.0/target/sedona-*1.4.1.jar apache-sedona-1.4.1-bin/\ncp apache-sedona-1.4.1-src/viz/target/sedona-*1.4.1.jar apache-sedona-1.4.1-bin/\ncp apache-sedona-1.4.1-src/python-adapter/target/sedona-*1.4.1.jar apache-sedona-1.4.1-bin/\ncp apache-sedona-1.4.1-src/spark-shaded/target/sedona-*1.4.1.jar apache-sedona-1.4.1-bin/\n\ncd apache-sedona-1.4.1-src &amp;&amp; mvn -q clean install -DskipTests -Dspark=3.4 -Dscala=2.12 &amp;&amp; cd ..\ncp apache-sedona-1.4.1-src/core/target/sedona-*1.4.1.jar apache-sedona-1.4.1-bin/\ncp apache-sedona-1.4.1-src/sql/spark-3.4/target/sedona-*1.4.1.jar apache-sedona-1.4.1-bin/\ncp apache-sedona-1.4.1-src/viz/target/sedona-*1.4.1.jar apache-sedona-1.4.1-bin/\ncp apache-sedona-1.4.1-src/python-adapter/target/sedona-*1.4.1.jar apache-sedona-1.4.1-bin/\ncp apache-sedona-1.4.1-src/spark-shaded/target/sedona-*1.4.1.jar apache-sedona-1.4.1-bin/\n\ncd apache-sedona-1.4.1-src &amp;&amp; mvn -q clean install -DskipTests -Dspark=3.4 -Dscala=2.13 &amp;&amp; cd ..\ncp apache-sedona-1.4.1-src/core/target/sedona-*1.4.1.jar apache-sedona-1.4.1-bin/\ncp apache-sedona-1.4.1-src/sql/spark-3.4/target/sedona-*1.4.1.jar apache-sedona-1.4.1-bin/\ncp apache-sedona-1.4.1-src/viz/target/sedona-*1.4.1.jar apache-sedona-1.4.1-bin/\ncp apache-sedona-1.4.1-src/python-adapter/target/sedona-*1.4.1.jar apache-sedona-1.4.1-bin/\ncp apache-sedona-1.4.1-src/spark-shaded/target/sedona-*1.4.1.jar apache-sedona-1.4.1-bin/\n\ntar czf apache-sedona-1.4.1-bin.tar.gz apache-sedona-1.4.1-bin\nshasum -a 512 apache-sedona-1.4.1-src.tar.gz &gt; apache-sedona-1.4.1-src.tar.gz.sha512\nshasum -a 512 apache-sedona-1.4.1-bin.tar.gz &gt; apache-sedona-1.4.1-bin.tar.gz.sha512\ngpg -ab apache-sedona-1.4.1-src.tar.gz\ngpg -ab apache-sedona-1.4.1-bin.tar.gz\n\necho \"Uploading local release files...\"\n\nsvn import -m \"Adding file\" apache-sedona-1.4.1-src.tar.gz https://dist.apache.org/repos/dist/dev/sedona/1.4.1-rc1/apache-sedona-1.4.1-src.tar.gz\nsvn import -m \"Adding file\" apache-sedona-1.4.1-src.tar.gz.asc https://dist.apache.org/repos/dist/dev/sedona/1.4.1-rc1/apache-sedona-1.4.1-src.tar.gz.asc\nsvn import -m \"Adding file\" apache-sedona-1.4.1-src.tar.gz.sha512 https://dist.apache.org/repos/dist/dev/sedona/1.4.1-rc1/apache-sedona-1.4.1-src.tar.gz.sha512\nsvn import -m \"Adding file\" apache-sedona-1.4.1-bin.tar.gz https://dist.apache.org/repos/dist/dev/sedona/1.4.1-rc1/apache-sedona-1.4.1-bin.tar.gz\nsvn import -m \"Adding file\" apache-sedona-1.4.1-bin.tar.gz.asc https://dist.apache.org/repos/dist/dev/sedona/1.4.1-rc1/apache-sedona-1.4.1-bin.tar.gz.asc\nsvn import -m \"Adding file\" apache-sedona-1.4.1-bin.tar.gz.sha512 https://dist.apache.org/repos/dist/dev/sedona/1.4.1-rc1/apache-sedona-1.4.1-bin.tar.gz.sha512\n\necho \"Removing local release files...\"\n\nrm apache-sedona-1.4.1-src.tar.gz\nrm apache-sedona-1.4.1-src.tar.gz.asc\nrm apache-sedona-1.4.1-src.tar.gz.sha512\nrm apache-sedona-1.4.1-bin.tar.gz\nrm apache-sedona-1.4.1-bin.tar.gz.asc\nrm apache-sedona-1.4.1-bin.tar.gz.sha512\nrm -rf apache-sedona-1.4.1-src\nrm -rf apache-sedona-1.4.1-bin\n</code></pre>"},{"location":"community/publish/#5-vote-in-dev-sedonaapacheorg","title":"5. Vote in dev sedona.apache.org","text":""},{"location":"community/publish/#vote-email","title":"Vote email","text":"<p>Please add changes at the end if needed:</p> <pre><code>Subject: [VOTE] Release Apache Sedona 1.4.1-rc1\n\nHi all,\n\nThis is a call for vote on Apache Sedona 1.4.1-rc1. Please refer to the changes listed at the bottom of this email.\n\nRelease notes:\nhttps://github.com/apache/sedona/blob/sedona-1.4.1-rc1/docs/setup/release-notes.md\n\nBuild instructions:\nhttps://github.com/apache/sedona/blob/sedona-1.4.1-rc1/docs/setup/compile.md\n\nGitHub tag:\nhttps://github.com/apache/sedona/releases/tag/sedona-1.4.1-rc1\n\nGPG public key to verify the Release:\nhttps://downloads.apache.org/sedona/KEYS\n\nSource code and binaries:\nhttps://dist.apache.org/repos/dist/dev/sedona/1.4.1-rc1/\n\nThe vote will be open for at least 72 hours or until at least 3 \"+1\" PMC votes are cast\n\nInstruction for checking items on the checklist: https://sedona.apache.org/community/vote/\n\nWe recommend you use this Jupyter notebook on MyBinder to perform this task: https://mybinder.org/v2/gh/jiayuasu/sedona-tools/HEAD?labpath=binder%2Fverify-release.ipynb\n\n**Please vote accordingly and you must provide your checklist for your vote**.\n\n\n[ ] +1 approve\n\n[ ] +0 no opinion\n\n[ ] -1 disapprove with the reason\n\nChecklist:\n\n[ ] Download links are valid.\n\n[ ] Checksums and PGP signatures are valid.\n\n[ ] DISCLAIMER is included.\n\n[ ] Source code artifacts have correct names matching the current release.\n\nFor a detailed checklist  please refer to:\nhttps://cwiki.apache.org/confluence/display/INCUBATOR/Incubator+Release+Checklist\n\n------------\n\nChanges according to the comments on the previous release\nOriginal comment (Permalink from https://lists.apache.org/list.html): \n</code></pre>"},{"location":"community/publish/#pass-email","title":"Pass email","text":"<p>Please count the votes and add the Permalink of the vote thread at the end.</p> <pre><code>Subject: [RESULT][VOTE] Release Apache Sedona 1.4.1-rc1\n\nDear all,\n\nThe vote closes now as 72hr have passed. The vote PASSES with\n\n+? (binding): NAME1, NAME2, NAME3\n+? (non-binding): NAME4\nNo -1 votes\n\nThe vote thread (Permalink from https://lists.apache.org/list.html):\n\nI will make an announcement soon.\n</code></pre>"},{"location":"community/publish/#announce-email","title":"Announce email","text":"<ol> <li>This email should be sent to dev@sedona.apache.org</li> <li>Please add the permalink of the vote thread</li> <li>Please add the permalink of the vote result thread</li> </ol> <pre><code>Subject: [ANNOUNCE] Apache Sedona 1.4.1 released\n\nDear all,\n\nWe are happy to report that we have released Apache Sedona 1.4.1. Thank you again for your help.\n\nApache Sedona is a cluster computing system for processing large-scale spatial data. \n\n\nVote thread (Permalink from https://lists.apache.org/list.html):\n\n\nVote result thread (Permalink from https://lists.apache.org/list.html):\n\n\nWebsite:\nhttp://sedona.apache.org/\n\nRelease notes:\nhttps://github.com/apache/sedona/blob/sedona-1.4.1/docs/setup/release-notes.md\n\nDownload links:\nhttps://github.com/apache/sedona/releases/tag/sedona-1.4.1\n\nAdditional resources:\nMailing list: dev@sedona.apache.org\nTwitter: https://twitter.com/ApacheSedona\nGitter: https://gitter.im/apache/sedona\n\nRegards,\nApache Sedona Team\n</code></pre>"},{"location":"community/publish/#7-failed-vote","title":"7. Failed vote","text":"<p>If a vote failed, do the following:</p> <ol> <li>In the vote email, say that we will create another release candidate.</li> <li>Restart from Step 3 <code>Update mkdocs.yml</code>. Please increment the release candidate ID (e.g., <code>1.4.1-rc2</code>) and update <code>sedona_create_release.current_rc</code> and <code>sedona_create_release.current_git_tag</code> in <code>mkdocs.yml</code> to generate the script listed on this webpage.</li> </ol>"},{"location":"community/publish/#8-release-source-code-and-maven-package","title":"8. Release source code and Maven package","text":""},{"location":"community/publish/#upload-releases","title":"Upload releases","text":"<pre><code>#!/bin/bash\n\necho \"Move all files in https://dist.apache.org/repos/dist/dev/sedona to https://dist.apache.org/repos/dist/release/sedona, using svn\"\nsvn mkdir -m \"Adding folder\" https://dist.apache.org/repos/dist/release/sedona/1.4.1\nwget https://dist.apache.org/repos/dist/dev/sedona/1.4.1-rc1/apache-sedona-1.4.1-src.tar.gz\nwget https://dist.apache.org/repos/dist/dev/sedona/1.4.1-rc1/apache-sedona-1.4.1-src.tar.gz.asc\nwget https://dist.apache.org/repos/dist/dev/sedona/1.4.1-rc1/apache-sedona-1.4.1-src.tar.gz.sha512\nwget https://dist.apache.org/repos/dist/dev/sedona/1.4.1-rc1/apache-sedona-1.4.1-bin.tar.gz\nwget https://dist.apache.org/repos/dist/dev/sedona/1.4.1-rc1/apache-sedona-1.4.1-bin.tar.gz.asc\nwget https://dist.apache.org/repos/dist/dev/sedona/1.4.1-rc1/apache-sedona-1.4.1-bin.tar.gz.sha512\nsvn import -m \"Adding file\" apache-sedona-1.4.1-src.tar.gz https://dist.apache.org/repos/dist/release/sedona/1.4.1/apache-sedona-1.4.1-src.tar.gz\nsvn import -m \"Adding file\" apache-sedona-1.4.1-src.tar.gz.asc https://dist.apache.org/repos/dist/release/sedona/1.4.1/apache-sedona-1.4.1-src.tar.gz.asc\nsvn import -m \"Adding file\" apache-sedona-1.4.1-src.tar.gz.sha512 https://dist.apache.org/repos/dist/release/sedona/1.4.1/apache-sedona-1.4.1-src.tar.gz.sha512\nsvn import -m \"Adding file\" apache-sedona-1.4.1-bin.tar.gz https://dist.apache.org/repos/dist/release/sedona/1.4.1/apache-sedona-1.4.1-bin.tar.gz\nsvn import -m \"Adding file\" apache-sedona-1.4.1-bin.tar.gz.asc https://dist.apache.org/repos/dist/release/sedona/1.4.1/apache-sedona-1.4.1-bin.tar.gz.asc\nsvn import -m \"Adding file\" apache-sedona-1.4.1-bin.tar.gz.sha512 https://dist.apache.org/repos/dist/release/sedona/1.4.1/apache-sedona-1.4.1-bin.tar.gz.sha512\nrm apache-sedona-1.4.1-src.tar.gz\nrm apache-sedona-1.4.1-src.tar.gz.asc\nrm apache-sedona-1.4.1-src.tar.gz.sha512\nrm apache-sedona-1.4.1-bin.tar.gz\nrm apache-sedona-1.4.1-bin.tar.gz.asc\nrm apache-sedona-1.4.1-bin.tar.gz.sha512\n</code></pre>"},{"location":"community/publish/#manually-close-and-release-the-package","title":"Manually close and release the package","text":"<ol> <li>Click <code>Close</code> on the Sedona staging repo on https://repository.apache.org under <code>staging repository</code></li> <li>Once the staging repo is closed, click <code>Release</code> on this repo.</li> </ol> <p>NOTICE: The staging repo will be automatically dropped after 3 days without closing. If you find the staging repo being dropped, you can re-stage the release using the following script.</p> <pre><code>#!/bin/bash\n\necho \"Re-staging releases to https://repository.apache.org\"\n\ngit checkout master\ngit pull\n\nrm -f release.*\nrm -f pom.xml.*\n\n# For Spark 3.0 and Scala 2.12\nmvn -q org.apache.maven.plugins:maven-release-plugin:2.3.2:perform -DconnectionUrl=scm:git:https://github.com/apache/sedona.git -Dtag=sedona-1.4.1-rc1 -Dresume=false -Darguments=\"-DskipTests -Dspark=3.0 -Dscala=2.12\" -Dspark=3.0 -Dscala=2.12\n\n# For Spark 3.0 and Scala 2.13\nmvn -q org.apache.maven.plugins:maven-release-plugin:2.3.2:perform -DconnectionUrl=scm:git:https://github.com/apache/sedona.git -Dtag=sedona-1.4.1-rc1 -Dresume=false -Darguments=\"-DskipTests -Dspark=3.0 -Dscala=2.13\" -Dspark=3.0 -Dscala=2.13\n\n# For Spark 3.4 and Scala 2.12\nmvn -q org.apache.maven.plugins:maven-release-plugin:2.3.2:perform -DconnectionUrl=scm:git:https://github.com/apache/sedona.git -Dtag=sedona-1.4.1-rc1 -Dresume=false -Darguments=\"-DskipTests -Dspark=3.4 -Dscala=2.12\" -Dspark=3.4 -Dscala=2.12\n\n# For Spark 3.4 and Scala 2.13\nmvn -q org.apache.maven.plugins:maven-release-plugin:2.3.2:perform -DconnectionUrl=scm:git:https://github.com/apache/sedona.git -Dtag=sedona-1.4.1-rc1 -Dresume=false -Darguments=\"-DskipTests -Dspark=3.4 -Dscala=2.13\" -Dspark=3.4 -Dscala=2.13\n</code></pre>"},{"location":"community/publish/#9-release-sedona-python-and-zeppelin","title":"9. Release Sedona Python and Zeppelin","text":"<p>You must have the maintainer privilege of <code>https://pypi.org/project/apache-sedona/</code> and <code>https://www.npmjs.com/package/apache-sedona</code></p> <p>To publish Sedona pythons, you have to use GitHub actions since we release wheels for different platforms. Please use this repo: https://github.com/jiayuasu/sedona-publish-python</p> <pre><code>#!/bin/bash\n\nwget https://github.com/apache/sedona/archive/refs/tags/sedona-1.4.1-rc1.tar.gz\ntar -xvf sedona-1.4.1-rc1.tar.gz\nmkdir apache-sedona-1.4.1-src\ncp -r sedona-sedona-1.4.1-rc1/* apache-sedona-1.4.1-src/\n\nrm -rf sedona-sedona-1.4.1-rc1\n\ncd zeppelin &amp;&amp; npm publish &amp;&amp; cd ..\nrm -rf apache-sedona-1.4.1-src\n</code></pre>"},{"location":"community/publish/#10-release-sedona-r-to-cran","title":"10. Release Sedona R to CRAN.","text":"<pre><code>#!/bin/bash\nR CMD build .\nR CMD check --as-cran apache.sedona_*.tar.gz\n</code></pre> <p>Then submit to CRAN using this web form.</p>"},{"location":"community/publish/#11-publish-the-doc-website","title":"11. Publish the doc website","text":""},{"location":"community/publish/#prepare-the-environment-and-doc-folder","title":"Prepare the environment and doc folder","text":"<ol> <li>Check out the 1.4.1 Git tag on your local repo.</li> <li>Read Compile documentation website to set up your environment. But don't deploy anything yet.</li> <li>Add the download link to Download page.</li> <li>Add the news to <code>docs/index.md</code>.</li> </ol>"},{"location":"community/publish/#generate-javadoc-and-scaladoc","title":"Generate Javadoc and Scaladoc","text":"<p>Run the following script to build Javadoc and Scaladoc of sedona modules and move them to docs/api/javadoc directory.</p> <pre><code>#!/bin/bash\n\nmvn -q clean install -DskipTests\nrm -rf docs/api/javadoc &amp;&amp; mkdir docs/api/javadoc\nmv core/target/apidocs docs/api/javadoc/core\nmv viz/target/apidocs docs/api/javadoc/viz\nmv sql/common/target/site/scaladocs docs/api/javadoc/sql\n</code></pre> <p>Please do not commit these generated docs to Sedona GitHub.</p>"},{"location":"community/publish/#compile-r-html-docs","title":"Compile R html docs","text":"<p>From GitHub Action docs workflow, find generated-docs in the tagged commit. Download it and copy this folder <code>docs/api/rdocs</code> to the same location of the Sedona to-be-released source repo.</p>"},{"location":"community/publish/#deploy-the-website","title":"Deploy the website","text":"<ol> <li>Run <code>mike deploy --push --update-aliases 1.4.1 latest</code>. This will deploy this website to Sedona main repo's gh-page. But Sedona does not use gh-page for hosting website.</li> <li>Check out the master branch.</li> <li>Git commit and push your changes in <code>download.md</code> and <code>index.md</code> to master branch. Delete all generated docs.</li> <li>Check out the <code>gh-page</code> branch.</li> <li>In a separate folder, check out GitHub sedona-website asf-site branch</li> <li>Copy all content to in Sedona main repo <code>gh-page</code> branch to Sedona website repo <code>asf-site</code> branch.</li> <li>Commit and push the changes to the remote <code>asf-site</code> branch.</li> </ol>"},{"location":"community/release-manager/","title":"Become a release manager","text":"<p>You only need to perform these steps if this is your first time being a release manager.</p>"},{"location":"community/release-manager/#0-software-requirement","title":"0. Software requirement","text":"<ul> <li>JDK 8: <code>brew install openjdk@8</code></li> <li>Maven 3.X. Your Maven must point to JDK 8 (1.8). Check it by <code>mvn --version</code></li> <li>Git and SVN</li> </ul> <p>If your Maven (<code>mvn --version</code>) points to other JDK versions, you must change it to JDK 8. Steps are as follows:</p> <ol> <li>Find all Java installed on your machine: <code>/usr/libexec/java_home -V</code>. You should see multiple JDK versions including JDK 8.</li> <li>Run <code>whereis mvn</code> to get the installation location of your Maven. The result is a symlink to the actual location.</li> <li>Open it in the terminal (with <code>sudo</code> if needed). It will be like this <pre><code>#!/bin/bash\nJAVA_HOME=\"${JAVA_HOME:-$(/usr/libexec/java_home)}\" exec \"/usr/local/Cellar/maven/3.6.3/libexec/bin/mvn\" \"$@\"\n</code></pre></li> <li>Change <code>JAVA_HOME:-$(/usr/libexec/java_home)}</code> to <code>JAVA_HOME:-$(/usr/libexec/java_home -v 1.8)}</code>.  The resulting content will be like this: <pre><code>#!/bin/bash\nJAVA_HOME=\"${JAVA_HOME:-$(/usr/libexec/java_home -v 1.8)}\" exec \"/usr/local/Cellar/maven/3.6.3/libexec/bin/mvn\" \"$@\"\n</code></pre></li> <li>Run <code>mvn --version</code> again. It should now point to JDK 8.</li> </ol>"},{"location":"community/release-manager/#1-obtain-write-access-to-sedona-github-repo","title":"1. Obtain Write Access to Sedona GitHub repo","text":"<ol> <li>Verify you have a Github ID enabled with 2FA https://help.github.com/articles/securing-your-account-with-two-factor-authentication-2fa/</li> <li>Enter your Github ID into your Apache ID profile https://id.apache.org/</li> <li>Merge your Apache and GitHub accounts using GitBox (Apache Account Linking utility): https://gitbox.apache.org/setup/<ul> <li>You should see 5 green checks in GitBox</li> <li>Wait at least 30  minutes for an email inviting you to Apache GitHub Organization and accept invitation</li> </ul> </li> <li>After accepting the Github Invitation, verify that you are a member of the team https://github.com/orgs/apache/teams/sedona-committers</li> <li>Additionally, if you have been elected to the Sedona PMC, verify you are part of the LDAP Sedona PMC https://whimsy.apache.org/roster/pmc/sedona</li> </ol>"},{"location":"community/release-manager/#2-prepare-secret-gpg-key","title":"2. Prepare Secret GPG key","text":"<ol> <li>Install GNUGPG if it was not installed before. On Mac: <code>brew install gnupg gnupg2</code></li> <li>Generate a secret key. It must be RSA4096 (4096 bits long). <ul> <li>Run <code>gpg --full-generate-key</code>. If not work, run <code>gpg --default-new-key-algo rsa4096 --gen-key</code></li> <li>At the prompt, specify the kind of key you want: Select <code>RSA</code>, then press <code>enter</code></li> <li>At the prompt, specify the key size you want: Enter <code>4096</code></li> <li>At the prompt, enter the length of time the key should be valid: Press <code>enter</code> to make the key never expire.</li> <li>Verify that your selections are correct.</li> <li>Enter your user ID information: use your real name and Apache email address.</li> <li>Type a secure passphrase. Make sure you remember this because we will use it later.</li> <li>Use the <code>gpg --list-secret-keys --keyid-format=long</code> command to list the long form of the GPG keys.</li> <li>From the list of GPG keys, copy the long form of the GPG key ID you'd like to use (e.g., <code>3AA5C34371567BD2</code>)</li> <li>Run <code>gpg --export --armor 3AA5C34371567BD2</code>, substituting in the GPG key ID you'd like to use.</li> <li>Copy your GPG key, beginning with <code>-----BEGIN PGP PUBLIC KEY BLOCK-----</code> and ending with <code>-----END PGP PUBLIC KEY BLOCK-----</code>.</li> <li>There must be an empty line between <code>-----BEGIN PGP PUBLIC KEY BLOCK-----</code> and the actual key.</li> </ul> </li> <li>Publish your armored key in major key servers: https://keyserver.pgp.com/</li> </ol>"},{"location":"community/release-manager/#3-use-svn-to-update-keys","title":"3. Use SVN to update KEYS","text":"<p>Use SVN to append your armored PGP public key to the <code>KEYS</code> files      * https://dist.apache.org/repos/dist/dev/sedona/KEYS      * https://dist.apache.org/repos/dist/release/sedona/KEYS</p> <ol> <li>Check out both KEYS files <pre><code>svn checkout https://dist.apache.org/repos/dist/dev/sedona/ sedona-dev --depth files\nsvn checkout https://dist.apache.org/repos/dist/release/sedona/ sedona-release --depth files\n</code></pre></li> <li>Use your favorite text editor to open <code>sedona-dev/KEYS</code> and <code>sedona-release/KEYS</code>.</li> <li>Paste your armored key to the end of both files. Note: There must be an empty line between <code>-----BEGIN PGP PUBLIC KEY BLOCK-----</code> and the actual key.</li> <li>Commit both KEYS. SVN might ask you to enter your ASF ID and password. Make sure you do it so SVN can always store your ID and password locally. <pre><code>svn commit -m \"Update KEYS\" sedona-dev/KEYS\nsvn commit -m \"Update KEYS\" sedona-release/KEYS\n</code></pre></li> <li>Then remove both svn folders <pre><code>rm -rf sedona-dev\nrm -rf sedona-release\n</code></pre></li> </ol>"},{"location":"community/release-manager/#4-add-gpg_tty-environment-variable","title":"4. Add GPG_TTY environment variable","text":"<p>In your <code>~/.bashrc</code> file, add the following content. Then restart your terminal.</p> <pre><code>GPG_TTY=$(tty)\nexport GPG_TTY\n</code></pre>"},{"location":"community/release-manager/#5-get-github-personal-access-token-classic","title":"5. Get GitHub personal access token (classic)","text":"<p>You need to create a GitHub personal access token (classic). You can follow the instruction on GitHub.</p> <p>In short:</p> <ol> <li>On your GitHub interface -&gt; Settings</li> <li>In the left sidebar, click Developer settings.</li> <li>In the left sidebar, under  Personal access tokens, click Tokens (classic).</li> <li>Select Generate new token, then click Generate new token (classic).</li> <li>Give your token a descriptive name.</li> <li>To give your token an expiration, select the Expiration drop-down menu. Make sure you set the <code>Expiration</code> to <code>No expiration</code>.</li> <li>Select the scopes you'd like to grant this token. To use your token to access repositories from the command line, select <code>repo</code> and <code>admin:org</code>.</li> <li>Click <code>Generate token</code>.</li> <li>Please save your token somewhere because we will use it in the next step.</li> </ol>"},{"location":"community/release-manager/#6-set-up-credentials-for-maven","title":"6. Set up credentials for Maven","text":"<p>In your <code>~/.m2/settings.xml</code> file, add the following content. Please create this file or <code>.m2</code> folder if it does not exist.</p> <p>Please replace all capitalized text with your own ID and password.</p> <pre><code>&lt;settings&gt;\n  &lt;servers&gt;\n    &lt;server&gt;\n      &lt;id&gt;github&lt;/id&gt;\n      &lt;username&gt;YOUR_GITHUB_USERNAME&lt;/username&gt;\n      &lt;password&gt;YOUR_GITHUB_TOKEN&lt;/password&gt;\n    &lt;/server&gt;\n    &lt;server&gt;\n      &lt;id&gt;apache.snapshots.https&lt;/id&gt;\n      &lt;username&gt;YOUR_ASF_ID&lt;/username&gt;\n      &lt;password&gt;YOUR_ASF_PASSWORD&lt;/password&gt;\n    &lt;/server&gt;\n    &lt;server&gt;\n      &lt;id&gt;apache.releases.https&lt;/id&gt;\n      &lt;username&gt;YOUR_ASF_ID&lt;/username&gt;\n      &lt;password&gt;YOUR_ASF_PASSWORD&lt;/password&gt;\n    &lt;/server&gt;\n  &lt;/servers&gt;\n  &lt;profiles&gt;\n    &lt;profile&gt;\n      &lt;id&gt;gpg&lt;/id&gt;\n      &lt;properties&gt;\n        &lt;gpg.passphrase&gt;YOUR_GPG_PASSPHRASE&lt;/gpg.passphrase&gt;\n      &lt;/properties&gt;\n    &lt;/profile&gt;\n  &lt;/profiles&gt;\n  &lt;activeProfiles&gt;\n    &lt;activeProfile&gt;gpg&lt;/activeProfile&gt;\n  &lt;/activeProfiles&gt;\n&lt;/settings&gt;\n</code></pre>"},{"location":"community/rule/","title":"Contributing to Apache Sedona","text":"<p>The project welcomes contributions. You can contribute to Sedona code or documentation by making Pull Requests on Sedona GitHub Repo.</p> <p>The following sections brief the workflow of how to complete a contribution.</p>"},{"location":"community/rule/#pick-announce-a-task-using-jira","title":"Pick / Announce a task using JIRA","text":"<p>It is important to confirm that your contribution is acceptable. You should create a JIRA ticket or pick an existing ticket. A new JIRA ticket will be automatically sent to <code>dev@sedona.apache.org</code></p>"},{"location":"community/rule/#develop-a-code-contribution","title":"Develop a code contribution","text":"<p>Code contributions should include the following:</p> <ul> <li>Detailed documentations on classes and methods.</li> <li>Unit Tests to demonstrate code correctness and allow this to be maintained going forward.  In the case of bug fixes the unit test should demonstrate the bug in the absence of the fix (if any).  Unit Tests can be JUnit test or Scala test. Some Sedona functions need to be tested in both Scala and Java.</li> <li>Updates on corresponding Sedona documentation if necessary.</li> </ul> <p>Code contributions must include a Apache 2.0 license header at the top of each file.</p>"},{"location":"community/rule/#develop-a-document-contribution","title":"Develop a document contribution","text":"<p>Documentation contributions should satisfy the following requirements:</p> <ul> <li>Detailed explanation with examples.</li> <li>Place a newly added document in a proper folder</li> <li>Change the mkdocs.yml if necessary</li> </ul> <p>Note</p> <p>Please read Compile the source code to learn how to compile Sedona website.</p>"},{"location":"community/rule/#make-a-pull-request","title":"Make a Pull Request","text":"<p>After developing a contribution, the easiest and most visible way to submit a Pull Request (PR) to the GitHub repo. </p> <p>Please use the JIRA ticket ID in the PR name, such as \"[SEDONA-1] my subject\".</p> <p>When creating a PR, please answser the questions in the PR template.</p> <p>When a PR is submitted, GitHub Action will check the build correctness. Please check the PR status, and fix any reported problems.</p>"},{"location":"community/rule/#review-a-pull-request","title":"Review a Pull Request","text":"<ul> <li>Every PR requires (1) at least 1 approval from a committer and (2) no disapproval from a committer. Everyone is welcome to review a PR but only the committer can make the final decision.</li> <li>Other reviewers, including community members and committers, may comment on the changes and suggest modifications. Changes can be added by simply pushing more commits to the same branch.</li> <li>Lively, polite, rapid technical debate is encouraged from everyone in the community even if the outcome may be a rejection of the entire change.</li> <li>Keep in mind that changes to more critical parts of Sedona, like Sedona core and spatial join algorithms, will be subjected to more review, and may require more testing and proof of its correctness than other changes.</li> <li>Sometimes, other changes will be merged which conflict with your pull request\u2019s changes. The PR can\u2019t be merged until the conflict is resolved. This can be resolved by resolving the conflicts by hand, then pushing the result to your branch.</li> </ul>"},{"location":"community/rule/#code-of-conduct","title":"Code of Conduct","text":"<p>Please read Apache Software Foundation Code of Conduct.</p> <p>We expect everyone who participates in the Apache community formally or informally, or claims any affiliation with the Foundation, in any Foundation-related activities and especially when representing the ASF in any role to honor this code of conduct.</p>"},{"location":"community/snapshot/","title":"Publish a SNAPSHOT version","text":"<p>This step is to publish Maven SNAPSHOTs to https://repository.apache.org</p> <p>This is a good practice for a release manager to try out his/her credential setup.</p> <p>The detailed requirement is on ASF Infra website</p> <p>Warning</p> <p>All scripts on this page should be run in your local Sedona Git repo under master branch via a single script file.</p>"},{"location":"community/snapshot/#0-prepare-an-empty-script-file","title":"0. Prepare an empty script file","text":"<ol> <li>In your local Sedona Git repo under master branch, run <pre><code>echo '#!/bin/bash' &gt; create-release.sh\nchmod 777 create-release.sh\n</code></pre></li> <li>Use your favourite GUI text editor to open <code>create-release.sh</code>.</li> <li>Then keep copying the scripts on this web page to replace all content in this text file.</li> <li>Do NOT directly copy/paste the scripts to your terminal because a bug in <code>clipboard.js</code> will create link breaks in such case.</li> <li>Each time when you copy content to this script file, run <code>./create-release.sh</code> to execute it.</li> </ol>"},{"location":"community/snapshot/#1-upload-snapshot-versions","title":"1. Upload snapshot versions","text":"<p>In your Sedona GitHub repo, run this script:</p> <pre><code>#!/bin/bash\n\ngit checkout master\ngit pull\n\nrm -f release.*\nrm -f pom.xml.*\n\n# Validate the POMs and your credential setup\nmvn -q -B clean release:prepare -Dtag=sedona-1.4.1-rc1 -DreleaseVersion=1.4.1 -DdevelopmentVersion=1.5.0-SNAPSHOT -Dresume=false -DdryRun=true -Penable-all-submodules -Darguments=\"-DskipTests\"\nmvn -q -B release:clean -Penable-all-submodules\n\n# Spark 3.0 and Scala 2.12\nmvn -q deploy -DskipTests -Dspark=3.0 -Dscala=2.12\n\n# Spark 3.0 and Scala 2.13\nmvn -q deploy -DskipTests -Dspark=3.0 -Dscala=2.13\n\n# Spark 3.4 and Scala 2.12\nmvn -q deploy -DskipTests -Dspark=3.4 -Dscala=2.12\n\n# Spark 3.4 and Scala 2.13\nmvn -q deploy -DskipTests -Dspark=3.4 -Dscala=2.13\n</code></pre>"},{"location":"community/vote/","title":"Vote a Sedona release","text":"<p>This page is for Sedona community to vote a Sedona release. The script below is tested on MacOS.</p> <p>In order to vote a Sedona release, you must provide your checklist including the following minimum requirement:</p> <ul> <li>Download links are valid</li> <li>Checksums and PGP signatures are valid</li> <li>DISCLAIMER and NOTICE are included</li> <li>Source code artifacts have correct names matching the current release</li> <li>The project can compile from the source code</li> </ul> <p>To make your life easier, we have provided an online Jupyter notebook using MyBinder. Please click this button to open the notebook and verify the release: . Then you can vote <code>+1</code> in the vote email.</p> <p>If you prefer to run the steps on your local machine, please read the steps below. If you can successfully finish the steps, you will pass the items mentioned above. Then you can vote <code>+1</code> in the vote email and provide your checklist.</p>"},{"location":"community/vote/#install-necessary-software","title":"Install necessary software","text":"<ol> <li>GPG: On Mac <code>brew install gnupg gnupg2</code>. You can check in a terminal <code>gpg --version</code>.</li> <li>JDK 1.8 or 1.11. Your Mac might have many different Java versions installed. You can try to use it but not sure if it can pass. You can check in a terminal <code>java --version</code>.</li> <li>Apache Maven 3.3.1+. On Mac <code>brew install maven</code>. You can check it in a terminal <code>mvn -version</code>.</li> <li>Python3 installed on your machine. MacOS comes with Python3 by default. You can check in a terminal <code>python3 --version</code>.</li> </ol> <p>You can skip this step if you installed these software before.</p>"},{"location":"community/vote/#run-the-verify-script","title":"Run the verify script","text":"<p>Please replace SEDONA_CURRENT_RC and SEDONA_CURRENT_VERSION with the correct versions. Then paste the content in a script called <code>verify.sh</code> and re-direct the output to a file. To run a script, do the following:</p> <pre><code>#!/bin/bash\n\n## Change the permission of the script to executable\nchmod 777 verify.sh\n\n## Run and redirect the output to a file\n./verify.sh &amp;&gt; verify.out\n</code></pre> <p>The content of the <code>verify.sh</code> script is as follows. If you copy the following content, a line break is automatically added to a long line of code. Please remove it in your local script.</p> <pre><code>#!/bin/bash\n\nSEDONA_CURRENT_RC=1.4.1-rc1\nSEDONA_CURRENT_VERSION=1.4.1\n\n## Download a Sedona release\nwget -q https://downloads.apache.org/sedona/KEYS\nwget -q https://dist.apache.org/repos/dist/dev/sedona/$SEDONA_CURRENT_RC/apache-sedona-$SEDONA_CURRENT_VERSION-src.tar.gz\nwget -q https://dist.apache.org/repos/dist/dev/sedona/$SEDONA_CURRENT_RC/apache-sedona-$SEDONA_CURRENT_VERSION-src.tar.gz.asc\nwget -q https://dist.apache.org/repos/dist/dev/sedona/$SEDONA_CURRENT_RC/apache-sedona-$SEDONA_CURRENT_VERSION-src.tar.gz.sha512\nwget -q https://dist.apache.org/repos/dist/dev/sedona/$SEDONA_CURRENT_RC/apache-sedona-$SEDONA_CURRENT_VERSION-bin.tar.gz\nwget -q https://dist.apache.org/repos/dist/dev/sedona/$SEDONA_CURRENT_RC/apache-sedona-$SEDONA_CURRENT_VERSION-bin.tar.gz.asc\nwget -q https://dist.apache.org/repos/dist/dev/sedona/$SEDONA_CURRENT_RC/apache-sedona-$SEDONA_CURRENT_VERSION-bin.tar.gz.sha512\n\n## Verify the signature and checksum\ngpg --import KEYS\ngpg --verify apache-sedona-$SEDONA_CURRENT_VERSION-src.tar.gz.asc\ngpg --verify apache-sedona-$SEDONA_CURRENT_VERSION-bin.tar.gz.asc\nshasum -a 512 apache-sedona-$SEDONA_CURRENT_VERSION-src.tar.gz\ncat apache-sedona-$SEDONA_CURRENT_VERSION-src.tar.gz.sha512\nshasum -a 512 apache-sedona-$SEDONA_CURRENT_VERSION-bin.tar.gz\ncat apache-sedona-$SEDONA_CURRENT_VERSION-bin.tar.gz.sha512\n\n## Uncompress the source code folder\ntar -xvf apache-sedona-$SEDONA_CURRENT_VERSION-src.tar.gz\n\n## Compile the project from source\n(cd apache-sedona-$SEDONA_CURRENT_VERSION-src;mvn clean install -DskipTests)\n</code></pre> <ul> <li>If successful, in the output file, you should be able to see something similar to the following text. It should include <code>Good signature from</code> and the final 4 lines should be two pairs of checksum matching each other.</li> </ul> <pre><code>gpg: key 3A79A47AC26FF4CD: \"Jia Yu &lt;jiayu@apache.org&gt;\" not changed\ngpg: key 6C883CA80E7FD299: \"PawelKocinski &lt;imbruced@apache.org&gt;\" not changed\ngpg: Total number processed: 2\ngpg:              unchanged: 2\ngpg: assuming signed data in 'apache-sedona-1.2.0-incubating-src.tar.gz'\ngpg: Signature made Mon Apr  4 11:48:31 2022 PDT\ngpg:                using RSA key 949DD6275C69AB954B1872FC6C883CA80E7FD299\ngpg:                issuer \"imbruced@apache.org\"\ngpg: Good signature from \"PawelKocinski &lt;imbruced@apache.org&gt;\" [unknown]\ngpg: WARNING: The key's User ID is not certified with a trusted signature!\ngpg:          There is no indication that the signature belongs to the owner.\nPrimary key fingerprint: 949D D627 5C69 AB95 4B18  72FC 6C88 3CA8 0E7F D299\ngpg: assuming signed data in 'apache-sedona-1.2.0-incubating-bin.tar.gz'\ngpg: Signature made Mon Apr  4 11:48:42 2022 PDT\ngpg:                using RSA key 949DD6275C69AB954B1872FC6C883CA80E7FD299\ngpg:                issuer \"imbruced@apache.org\"\ngpg: Good signature from \"PawelKocinski &lt;imbruced@apache.org&gt;\" [unknown]\ngpg: WARNING: The key's User ID is not certified with a trusted signature!\ngpg:          There is no indication that the signature belongs to the owner.\nPrimary key fingerprint: 949D D627 5C69 AB95 4B18  72FC 6C88 3CA8 0E7F D299\nd3bdfd4d870838ebe63f21cb93634d2421ec1ac1b8184636206a5dc0d89a78a88257798b1f17371ad3cfcc3b1eb79c69e1410afdefeb4d9b52fc8bb5ea18dd2e  apache-sedona-1.2.0-incubating-src.tar.gz\nd3bdfd4d870838ebe63f21cb93634d2421ec1ac1b8184636206a5dc0d89a78a88257798b1f17371ad3cfcc3b1eb79c69e1410afdefeb4d9b52fc8bb5ea18dd2e  apache-sedona-1.2.0-incubating-src.tar.gz\n64cea38dd3ca171ee4e2a7365dbce999773862f2a11599bd0f27e9551d740659a519a9b976b3e7b0826088010967093e6acc9462f7073e9737c24b007a2df846  apache-sedona-1.2.0-incubating-bin.tar.gz\n64cea38dd3ca171ee4e2a7365dbce999773862f2a11599bd0f27e9551d740659a519a9b976b3e7b0826088010967093e6acc9462f7073e9737c24b007a2df846  apache-sedona-1.2.0-incubating-bin.tar.gz\n</code></pre> <ul> <li>At the end of the output, you should also see the <code>BUILD SUCCESS</code> if you can compile the source code. If this step fails, you can contact Sedona PMC and see if this is just because of your environment.</li> </ul>"},{"location":"community/vote/#check-files-manually","title":"Check files manually","text":"<ol> <li> <p>Check if the downloaded files have the correct version.</p> </li> <li> <p>In the unzipped source code folder, and check if DISCLAIMER and NOTICE files and included and up to date.</p> </li> </ol>"},{"location":"setup/cluster/","title":"Set up your Apache Spark cluster","text":"<p>Download a Spark distribution from Spark download page.</p>"},{"location":"setup/cluster/#preliminary","title":"Preliminary","text":"<ol> <li>Set up password-less SSH on your cluster. Each master-worker pair should have bi-directional password-less SSH.</li> <li>Make sure you have installed JRE 1.8 or later.</li> <li>Add the list of your workers' IP address in ./conf/slaves</li> <li>Besides the necessary Spark settings, you may need to add the following lines in Spark configuration files to avoid Sedona memory errors:</li> </ol> <p>In <code>./conf/spark-defaults.conf</code></p> <pre><code>spark.driver.memory 10g\nspark.network.timeout 1000s\nspark.driver.maxResultSize 5g\n</code></pre> <ul> <li><code>spark.driver.memory</code> tells Spark to allocate enough memory for the driver program because Sedona needs to build global grid files (global index) on the driver program. If you have a large amount of data (normally, over 100 GB), set this parameter to 2~5 GB will be good. Otherwise, you may observe \"out of memory\" error.</li> <li><code>spark.network.timeout</code> is the default timeout for all network interactions. Sometimes, spatial join query takes longer time to shuffle data. This will ensure Spark has enough patience to wait for the result.</li> <li><code>spark.driver.maxResultSize</code> is the limit of total size of serialized results of all partitions for each Spark action. Sometimes, the result size of spatial queries is large. The \"Collect\" operation may throw errors.</li> </ul> <p>For more details of Spark parameters, please visit Spark Website.</p>"},{"location":"setup/cluster/#start-your-cluster","title":"Start your cluster","text":"<p>Go the root folder of the uncompressed Apache Spark folder. Start your spark cluster via a terminal</p> <pre><code>./sbin/start-all.sh\n</code></pre>"},{"location":"setup/compile/","title":"Compile Sedona source code","text":""},{"location":"setup/compile/#compile-scala-java-source-code","title":"Compile Scala / Java source code","text":"<p>Sedona Scala/Java code is a project with multiple modules. Each module is a Scala/Java mixed project which is managed by Apache Maven 3.</p> <ul> <li>Make sure your Linux/Mac machine has Java 1.8, Apache Maven 3.3.1+, and Python3.7+. The compilation of Sedona is not tested on Windows machine.</li> </ul> <p>To compile all modules, please make sure you are in the root folder of all modules. Then enter the following command in the terminal:</p> Without unit testsWith unit testsWith Geotools jars packaged <p><pre><code>mvn clean install -DskipTests\n</code></pre> This command will first delete the old binary files and compile all modules. This compilation will skip the unit tests. To compile a single module, please make sure you are in the folder of that module. Then enter the same command.</p> <p><pre><code>mvn clean install\n</code></pre> The maven unit tests of all modules may take up to 30 minutes.  </p> <p><pre><code>mvn clean install -DskipTests -Dgeotools\n</code></pre> Geotools jars will be packaged into the produced fat jars.  </p> <p>Note</p> <p>By default, this command will compile Sedona with Spark 3.0 and Scala 2.12</p>"},{"location":"setup/compile/#compile-with-different-targets","title":"Compile with different targets","text":"<p>User can specify <code>-Dspark</code> and <code>-Dscala</code> command line options to compile with different targets. Available targets are:</p> <ul> <li><code>-Dspark</code>: <code>3.0</code> for Spark 3.0 to 3.3; <code>{major}.{minor}</code> for Spark 3.4 or later. For example, specify <code>-Dspark=3.4</code> to build for Spark 3.4.</li> <li><code>-Dscala</code>: <code>2.12</code> or <code>2.13</code></li> </ul> Spark 3.0 to 3.3 Scala 2.12Spark 3.4+ Scala 2.12Spark 3.0 to 3.3 Scala 2.13Spark 3.4+ Scala 2.13 <pre><code>mvn clean install -DskipTests -Dspark=3.0 -Dscala=2.12\n</code></pre> <p><pre><code>mvn clean install -DskipTests -Dspark=3.4 -Dscala=2.12\n</code></pre> Please replace <code>3.4</code> with Spark major.minor version when building for higher Spark versions.</p> <pre><code>mvn clean install -DskipTests -Dspark=3.0 -Dscala=2.13\n</code></pre> <p><pre><code>mvn clean install -DskipTests -Dspark=3.4 -Dscala=2.13\n</code></pre> Please replace <code>3.4</code> with Spark major.minor version when building for higher Spark versions.</p> <p>Tip</p> <p>To get the Sedona Spark Shaded jar with all GeoTools jars included, simply append <code>-Dgeotools</code> option. The command is like this:<code>mvn clean install -DskipTests -Dscala=2.12 -Dspark=3.0 -Dgeotools</code></p>"},{"location":"setup/compile/#download-staged-jars","title":"Download staged jars","text":"<p>Sedona uses GitHub action to automatically generate jars per commit. You can go here and download the jars by clicking the commit's Artifacts tag.</p>"},{"location":"setup/compile/#run-python-test","title":"Run Python test","text":"<ol> <li>Set up the environment variable SPARK_HOME and PYTHONPATH For example, <pre><code>export SPARK_HOME=$PWD/spark-3.0.1-bin-hadoop2.7\nexport PYTHONPATH=$SPARK_HOME/python\n</code></pre></li> <li>Compile the Sedona Scala and Java code with <code>-Dgeotools</code> and then copy the sedona-spark-shaded-1.4.0.jar to SPARK_HOME/jars/ folder. <pre><code>cp spark-shaded/target/sedona-spark-shaded-xxx.jar $SPARK_HOME/jars/\n</code></pre></li> <li>Install the following libraries <pre><code>sudo apt-get -y install python3-pip python-dev libgeos-dev\nsudo pip3 install -U setuptools\nsudo pip3 install -U wheel\nsudo pip3 install -U virtualenvwrapper\nsudo pip3 install -U pipenv\n</code></pre> Homebrew can be used to install libgeos-dev in macOS: <code>brew install geos</code></li> <li>Set up pipenv to the desired Python version: 3.7, 3.8, or 3.9 <pre><code>cd python\npipenv --python 3.7\n</code></pre></li> <li>Install the PySpark version and other dependency <pre><code>cd python\npipenv install pyspark\npipenv install --dev\n</code></pre> <code>pipenv install pyspark</code> install the latest version of pyspark. In order to remain consistent with installed spark version, use <code>pipenv install pyspark==&lt;spark_version&gt;</code></li> <li>Run the Python tests <pre><code>cd python\npipenv run python setup.py build_ext --inplace\npipenv run pytest tests\n</code></pre></li> </ol>"},{"location":"setup/compile/#compile-the-documentation","title":"Compile the documentation","text":"<p>The website is automatically built after each commit. The built website can be downloaded here: </p>"},{"location":"setup/compile/#mkdocs-website","title":"MkDocs website","text":"<p>The source code of the documentation website is written in Markdown and then compiled by MkDocs. The website is built upon Material for MkDocs template.</p> <p>In the Sedona repository, MkDocs configuration file mkdocs.yml is in the root folder and all documentation source code is in docs folder.</p> <p>To compile the source code and test the website on your local machine, please read MkDocs Tutorial and Materials for MkDocs Tutorial.</p> <p>In short, you need to run:</p> <pre><code>pip install mkdocs\npip install mkdocs-material\npip install mkdocs-macros-plugin\npip install mkdocs-git-revision-date-localized-plugin\npip install mike\n</code></pre> <p>After installing MkDocs and MkDocs-Material, run the command in Sedona root folder:</p> <pre><code>mike deploy --update-aliases latest-snapshot latest\nmike set-default latest\nmike serve\n</code></pre>"},{"location":"setup/databricks/","title":"Install on Databricks","text":""},{"location":"setup/databricks/#community-edition-free-tier","title":"Community edition (free-tier)","text":"<p>You just need to install the Sedona jars and Sedona Python on Databricks using Databricks default web UI. Then everything will work.</p>"},{"location":"setup/databricks/#advanced-editions","title":"Advanced editions","text":"<ul> <li>Sedona 1.0.1 &amp; 1.1.0 is compiled against Spark 3.1 (~ Databricks DBR 9 LTS, DBR 7 is Spark 3.0)</li> <li>Sedona 1.1.1, 1.2.0 are compiled against Spark 3.2 (~ DBR 10 &amp; 11)</li> <li>Sedona 1.2.1, 1.3.1, 1.4.0 are complied against Spark 3.3</li> </ul> <p>In Spark 3.2, <code>org.apache.spark.sql.catalyst.expressions.Generator</code> class added a field <code>nodePatterns</code>. Any SQL functions that rely on Generator class may have issues if compiled for a runtime with a differing spark version. For Sedona, those functions are:    * ST_MakeValid    * ST_SubDivideExplode</p> <p>Sedona <code>1.1.1-incubating</code> and above is overall the recommended version to use. It is generally backwards compatible with earlier Spark releases but you should be aware of what Spark version Sedona was compiled against versus which is being executed in case you hit issues.</p>"},{"location":"setup/databricks/#databricks-10x-recommended","title":"Databricks 10.x+ (Recommended)","text":"<ul> <li>You need to use Sedona version <code>1.1.1-incubating</code> or higher. </li> <li>In order to activate the Kryo serializer (this speeds up the serialization and deserialization of geometry types) you need to install the libraries via init script as described below.</li> </ul>"},{"location":"setup/databricks/#databricks-dbr-7x-9x","title":"Databricks DBR 7.x - 9.x","text":"<ul> <li>If you are using the commercial version of Databricks you can install the Sedona jars and Sedona Python using the Databricks default web UI. DBR 7 matches with Sedona <code>1.1.0-incubating</code> and DBR 9 matches better with Sedona <code>1.1.1-incubating</code> due to Databricks cherry-picking some Spark 3.2 private APIs.</li> </ul>"},{"location":"setup/databricks/#install-sedona-from-the-web-ui","title":"Install Sedona from the web UI","text":"<p>1) From the Libraries tab install from Maven Coordinates     <pre><code>org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.4.0\norg.datasyslab:geotools-wrapper:1.4.0-28.2\n</code></pre></p> <p>2) For enabling python support, from the Libraries tab install from PyPI     <pre><code>apache-sedona\n</code></pre></p> <p>3) (Only for DBR up to 7.3 LTS) You can speed up the serialization of geometry types by adding to your spark configurations (<code>Cluster</code> -&gt; <code>Edit</code> -&gt; <code>Configuration</code> -&gt; <code>Advanced options</code>) the following lines:     <pre><code>spark.serializer org.apache.spark.serializer.KryoSerializer\nspark.kryo.registrator org.apache.sedona.core.serde.SedonaKryoRegistrator\n</code></pre>     &gt; For DBRs after 7.3, use the Init Script method described further down.</p>"},{"location":"setup/databricks/#initialise","title":"Initialise","text":"<p>After you have installed the libraries and started the cluster, you can initialize the Sedona <code>ST_*</code> functions and types by running from your code: </p> <p>(scala) <pre><code>import org.apache.sedona.sql.utils.SedonaSQLRegistrator\nSedonaSQLRegistrator.registerAll(spark)\n</code></pre></p> <p>(or python) <pre><code>from sedona.register.geo_registrator import SedonaRegistrator\nSedonaRegistrator.registerAll(spark)\n</code></pre></p>"},{"location":"setup/databricks/#pure-sql-environment","title":"Pure SQL environment","text":"<p>In order to use the Sedona <code>ST_*</code> functions from SQL without having to register the Sedona functions from a python/scala cell, you need to install the Sedona libraries from the cluster init-scripts as follows.</p>"},{"location":"setup/databricks/#install-sedona-via-init-script-for-dbrs-73","title":"Install Sedona via init script (for DBRs &gt; 7.3)","text":"<p>Download the Sedona jars to a DBFS location. You can do that manually via UI or from a notebook by executing this code in a cell:</p> <pre><code>%sh # Create JAR directory for Sedona\nmkdir -p /dbfs/FileStore/jars/sedona/1.4.0\n\n# Download the dependencies from Maven into DBFS\ncurl -o /dbfs/FileStore/jars/sedona/1.4.0/geotools-wrapper-1.4.0-28.2.jar \"https://repo1.maven.org/maven2/org/datasyslab/geotools-wrapper/1.4.0-28.2/geotools-wrapper-1.4.0-28.2.jar\"\n\ncurl -o /dbfs/FileStore/jars/sedona/1.4.0/sedona-spark-shaded-3.0_2.12-1.4.0.jar \"https://repo1.maven.org/maven2/org/apache/sedona/sedona-spark-shaded-3.0_2.12/1.4.0/sedona-spark-shaded-3.0_2.12-1.4.0.jar\"\n\ncurl -o /dbfs/FileStore/jars/sedona/1.4.0/sedona-viz-3.0_2.12-1.4.0.jar \"https://repo1.maven.org/maven2/org/apache/sedona/sedona-viz-3.0_2.12/1.4.0/sedona-viz-3.0_2.12-1.4.0.jar\"\n</code></pre> <p>Create an init script in DBFS that loads the Sedona jars into the cluster's default jar directory. You can create that from any notebook by running: </p> <pre><code>%sh # Create init script directory for Sedona\nmkdir -p /dbfs/FileStore/sedona/\n\n# Create init script\ncat &gt; /dbfs/FileStore/sedona/sedona-init.sh &lt;&lt;'EOF'\n#!/bin/bash\n#\n# File: sedona-init.sh\n# Author: Erni Durdevic\n# Created: 2021-11-01\n# \n# On cluster startup, this script will copy the Sedona jars to the cluster's default jar directory.\n# In order to activate Sedona functions, remember to add to your spark configuration the Sedona extensions: \"spark.sql.extensions org.apache.sedona.viz.sql.SedonaVizExtensions,org.apache.sedona.sql.SedonaSqlExtensions\"\n\ncp /dbfs/FileStore/jars/sedona/1.4.0/*.jar /databricks/jars\n\nEOF\n</code></pre> <p>From your cluster configuration (<code>Cluster</code> -&gt; <code>Edit</code> -&gt; <code>Configuration</code> -&gt; <code>Advanced options</code> -&gt; <code>Spark</code>) activate the Sedona functions and the kryo serializer by adding to the Spark Config  <pre><code>spark.sql.extensions org.apache.sedona.viz.sql.SedonaVizExtensions,org.apache.sedona.sql.SedonaSqlExtensions\nspark.serializer org.apache.spark.serializer.KryoSerializer\nspark.kryo.registrator org.apache.sedona.core.serde.SedonaKryoRegistrator\n</code></pre></p> <p>From your cluster configuration (<code>Cluster</code> -&gt; <code>Edit</code> -&gt; <code>Configuration</code> -&gt; <code>Advanced options</code> -&gt; <code>Init Scripts</code>) add the newly created init script  <pre><code>dbfs:/FileStore/sedona/sedona-init.sh\n</code></pre></p> <p>For enabling python support, from the Libraries tab install from PyPI <pre><code>apache-sedona\n</code></pre></p> <p>Note: You need to install the Sedona libraries via init script because the libraries installed via UI are installed after the cluster has already started, and therefore the classes specified by the config <code>spark.sql.extensions</code>, <code>spark.serializer</code>, and <code>spark.kryo.registrator</code> are not available at startup time.</p>"},{"location":"setup/emr/","title":"Install on AWS EMR","text":"<p>We recommend Sedona-1.3.1-incuabting and above for EMR. In the tutorial, we use AWS Elastic MapReduce (EMR) 6.9.0. It has the following applications installed: Hadoop 3.3.3, JupyterEnterpriseGateway 2.6.0, Livy 0.7.1, Spark 3.3.0.</p> <p>This tutorial is tested on EMR on EC2 with EMR Studio (notebooks). EMR on EC2 uses YARN to manage resources.</p>"},{"location":"setup/emr/#prepare-initialization-script","title":"Prepare initialization script","text":"<p>In your S3 bucket, add a script that has the following content:</p> <pre><code>#!/bin/bash\n\n# EMR clusters only have ephemeral local storage. It does not really matter where we store the jars.\nsudo mkdir /jars\n\n# Download Sedona jar\nsudo curl -o /jars/sedona-spark-shaded-3.0_2.12-1.4.0.jar \"https://repo1.maven.org/maven2/org/apache/sedona/sedona-spark-shaded-3.0_2.12/1.4.0/sedona-spark-shaded-3.0_2.12-1.4.0.jar\"\n\n# Download GeoTools jar\nsudo curl -o /jars/geotools-wrapper-1.4.0-28.2.jar \"https://repo1.maven.org/maven2/org/datasyslab/geotools-wrapper/1.4.0-28.2/geotools-wrapper-1.4.0-28.2.jar\"\n\n# Install necessary python libraries\nsudo python3 -m pip install pandas shapely==1.8.5\nsudo python3 -m pip install pandas geopandas==0.10.2\nsudo python3 -m pip install attrs matplotlib descartes apache-sedona==1.4.0\n</code></pre> <p>When you create a EMR cluster, in the <code>bootstrap action</code>, specify the location of this script.</p>"},{"location":"setup/emr/#add-software-configuration","title":"Add software configuration","text":"<p>When you create a EMR cluster, in the software configuration, add the following content:</p> <pre><code>[\n{\n\"Classification\":\"spark-defaults\", \"Properties\":{\n\"spark.yarn.dist.jars\": \"/jars/sedona-spark-shaded-3.0_2.12-1.4.0.jar,/jars/geotools-wrapper-1.4.0-28.2.jar\",\n      \"spark.serializer\": \"org.apache.spark.serializer.KryoSerializer\",\n      \"spark.kryo.registrator\": \"org.apache.sedona.core.serde.SedonaKryoRegistrator\",\n      \"spark.sql.extensions\": \"org.apache.sedona.viz.sql.SedonaVizExtensions,org.apache.sedona.sql.SedonaSqlExtensions\"\n}\n}\n]\n</code></pre> <p>Note</p> <p>If you use Sedona 1.3.1-incubating, please use <code>sedona-python-adpater-3.0_2.12</code> jar in the content above, instead of <code>sedona-spark-shaded-3.0_2.12</code>.</p>"},{"location":"setup/install-python/","title":"Install Sedona Python","text":"<p>Click  and play the interactive Sedona Python Jupyter Notebook immediately!</p> <p>Apache Sedona extends pyspark functions which depends on libraries:</p> <ul> <li>pyspark</li> <li>shapely</li> <li>attrs</li> </ul> <p>You need to install necessary packages if your system does not have them installed. See \"packages\" in our Pipfile.</p>"},{"location":"setup/install-python/#install-sedona","title":"Install sedona","text":"<ul> <li>Installing from PyPI repositories. You can find the latest Sedona Python on PyPI. There is an known issue in Sedona v1.0.1 and earlier versions.</li> </ul> <pre><code>pip install apache-sedona\n</code></pre> <ul> <li>Since Sedona v1.1.0, pyspark is an optional dependency of Sedona Python because spark comes pre-installed on many spark platforms. To install pyspark along with Sedona Python in one go, use the <code>spark</code> extra:</li> </ul> <pre><code>pip install apache-sedona[spark]\n</code></pre> <ul> <li>Installing from Sedona Python source</li> </ul> <p>Clone Sedona GitHub source code and run the following command</p> <pre><code>cd python\npython3 setup.py install\n</code></pre>"},{"location":"setup/install-python/#prepare-sedona-spark-shaded-jar","title":"Prepare sedona-spark-shaded jar","text":"<p>Sedona Python needs one additional jar file called <code>sedona-spark-shaded</code> to work properly. Please make sure you use the correct version for Spark and Scala.</p> <ul> <li>For Spark 3.0 to 3.3 and Scala 2.12, it is called <code>sedona-spark-shaded-3.0_2.12-1.4.0.jar</code></li> <li>For Spark 3.4+ and Scala 2.12, it is called <code>sedona-spark-shaded-3.4_2.12-1.4.0.jar</code>. If you are using Spark versions higher than 3.4, please replace the <code>3.4</code> in artifact names with the corresponding major.minor version numbers.</li> </ul> <p>You can get it using one of the following methods:</p> <ol> <li> <p>Compile from the source within main project directory and copy it (in <code>spark-shaded/target</code> folder) to SPARK_HOME/jars/ folder (more details)</p> </li> <li> <p>Download from GitHub release and copy it to SPARK_HOME/jars/ folder</p> </li> <li>Call the Maven Central coordinate in your python program. For example, Sedona &gt;= 1.4.1</li> </ol> <pre><code>from sedona.spark import *\nconfig = SedonaContext.builder(). \\\n    config('spark.jars.packages',\n           'org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.4.0,'\n           'org.datasyslab:geotools-wrapper:1.4.0-28.2'). \\\n    getOrCreate()\nsedona = SedonaContext.create(config)\n</code></pre> <p>Sedona &lt; 1.4.1</p> <p>SedonaRegistrator is deprecated in Sedona 1.4.1 and later versions. Please use the above method instead.</p> <pre><code>from pyspark.sql import SparkSession\nfrom sedona.register import SedonaRegistrator\nfrom sedona.utils import SedonaKryoRegistrator, KryoSerializer\nspark = SparkSession. \\\n    builder. \\\n    appName('appName'). \\\n    config(\"spark.serializer\", KryoSerializer.getName). \\\n    config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName). \\\n    config('spark.jars.packages',\n           'org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.4.0,'\n           'org.datasyslab:geotools-wrapper:1.4.0-28.2'). \\\n    getOrCreate()\nSedonaRegistrator.registerAll(spark)\n</code></pre> <p>Warning</p> <p>If you are going to use Sedona CRS transformation and ShapefileReader functions, you have to use Method 1 or 3. Because these functions internally use GeoTools libraries which are under LGPL license, Apache Sedona binary release cannot include them.</p>"},{"location":"setup/install-python/#setup-environment-variables","title":"Setup environment variables","text":"<p>If you manually copy the sedona-spark-shaded jar to <code>SPARK_HOME/jars/</code> folder, you need to setup two environment variables</p> <ul> <li>SPARK_HOME. For example, run the command in your terminal</li> </ul> <pre><code>export SPARK_HOME=~/Downloads/spark-3.0.1-bin-hadoop2.7\n</code></pre> <ul> <li>PYTHONPATH. For example, run the command in your terminal</li> </ul> <pre><code>export PYTHONPATH=$SPARK_HOME/python\n</code></pre> <p>You can then play with Sedona Python Jupyter notebook.</p>"},{"location":"setup/install-scala/","title":"Install Sedona Scala/Java","text":"<p>Before starting the Sedona journey, you need to make sure your Apache Spark cluster is ready.</p> <p>There are two ways to use a Scala or Java library with Apache Spark. You can user either one to run Sedona.</p> <ul> <li>Spark interactive Scala or SQL shell: easy to start, good for new learners to try simple functions</li> <li>Self-contained Scala / Java project: a steep learning curve of package management, but good for large projects</li> </ul>"},{"location":"setup/install-scala/#spark-scala-shell","title":"Spark Scala shell","text":""},{"location":"setup/install-scala/#download-sedona-jar-automatically","title":"Download Sedona jar automatically","text":"<ol> <li> <p>Have your Spark cluster ready.</p> </li> <li> <p>Run Spark shell with <code>--packages</code> option. This command will automatically download Sedona jars from Maven Central. <pre><code>./bin/spark-shell --packages MavenCoordinates\n</code></pre> Please refer to Sedona Maven Central coordinates to select the corresponding Sedona packages for your Spark version.</p> <ul> <li> <p>Local mode: test Sedona without setting up a cluster <pre><code>./bin/spark-shell --packages org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.4.0,org.apache.sedona:sedona-viz-3.0_2.12:1.4.0,org.datasyslab:geotools-wrapper:1.4.0-28.2\n</code></pre></p> </li> <li> <p>Cluster mode: you need to specify Spark Master IP <pre><code>./bin/spark-shell --master spark://localhost:7077 --packages org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.4.0,org.apache.sedona:sedona-viz-3.0_2.12:1.4.0,org.datasyslab:geotools-wrapper:1.4.0-28.2\n</code></pre></p> </li> </ul> </li> </ol>"},{"location":"setup/install-scala/#download-sedona-jar-manually","title":"Download Sedona jar manually","text":"<ol> <li> <p>Have your Spark cluster ready.</p> </li> <li> <p>Download Sedona jars:</p> <ul> <li>Download the pre-compiled jars from Sedona Releases</li> <li>Download / Git clone Sedona source code and compile the code by yourself (see Compile Sedona)</li> </ul> </li> <li> <p>Run Spark shell with <code>--jars</code> option. <pre><code>./bin/spark-shell --jars /Path/To/SedonaJars.jar\n</code></pre> If you are using Spark 3.0 to 3.3, please use jars with filenames containing <code>3.0</code>, such as <code>sedona-spark-shaded-3.0_2.12-1.4.0</code>; If you are using Spark 3.4 or higher versions, please use jars with Spark major.minor versions in the filename, such as <code>sedona-spark-shaded-3.4_2.12-1.4.0</code>.</p> <ul> <li> <p>Local mode: test Sedona without setting up a cluster <pre><code>./bin/spark-shell --jars /path/to/sedona-spark-shaded-3.0_2.12-1.4.0.jar,/path/to/sedona-viz-3.0_2.12-1.4.0.jar,/path/to/geotools-wrapper-1.4.0-28.2.jar\n</code></pre></p> </li> <li> <p>Cluster mode: you need to specify Spark Master IP <pre><code>./bin/spark-shell --master spark://localhost:7077 --jars /path/to/sedona-spark-shaded-3.0_2.12-1.4.0.jar,/path/to/sedona-viz-3.0_2.12-1.4.0.jar,/path/to/geotools-wrapper-1.4.0-28.2.jar\n</code></pre></p> </li> </ul> </li> </ol>"},{"location":"setup/install-scala/#spark-sql-shell","title":"Spark SQL shell","text":"<p>Please see Use Sedona in a pure SQL environment</p>"},{"location":"setup/install-scala/#self-contained-spark-projects","title":"Self-contained Spark projects","text":"<p>A self-contained project allows you to create multiple Scala / Java files and write complex logics in one place. To use Sedona in your self-contained Spark project, you just need to add Sedona as a dependency in your POM.xml or build.sbt.</p> <ol> <li>To add Sedona as dependencies, please read Sedona Maven Central coordinates</li> <li>Use Sedona Template project to start: Sedona Template Project</li> <li>Compile your project using SBT. Make sure you obtain the fat jar which packages all dependencies.</li> <li>Submit your compiled fat jar to Spark cluster. Make sure you are in the root folder of Spark distribution. Then run the following command: <pre><code>./bin/spark-submit --master spark://YOUR-IP:7077 /Path/To/YourJar.jar\n</code></pre></li> </ol> <p>Note</p> <p>The detailed explanation of spark-submit is available on Spark website.</p>"},{"location":"setup/maven-coordinates/","title":"Maven Coordinates","text":""},{"location":"setup/maven-coordinates/#use-sedona-shaded-fat-jars","title":"Use Sedona shaded (fat) jars","text":"<p>Warning</p> <p>For Scala/Java/Python users, this is the most common way to use Sedona in your environment. Do not use separate Sedona jars unless you are sure that you do not need shaded jars.</p> <p>Warning</p> <p>For R users, this is the only way to use Sedona in your environment.</p> <p>Apache Sedona provides different packages for each supported version of Spark.</p> <ul> <li>For Spark 3.0 to 3.3, the artifacts to use should be <code>sedona-spark-shaded-3.0_2.12</code>, <code>sedona-vis-3.0_2.12</code>.</li> <li>For Spark 3.4 or higher versions, please use the artifacts with Spark major.minor version in the artifact name. For example, for Spark 3.4, the artifacts to use should be <code>sedona-spark-shaded-3.4_2.12</code>, <code>sedona-vis-3.4_2.12</code>.</li> </ul> <p>If you are using the Scala 2.13 builds of Spark, please use the corresponding packages for Scala 2.13, which are suffixed by <code>_2.13</code>.</p> <p>The optional GeoTools library is required if you want to use CRS transformation, ShapefileReader or GeoTiff reader. This wrapper library is a re-distribution of GeoTools official jars. The only purpose of this library is to bring GeoTools jars from OSGEO repository to Maven Central. This library is under GNU Lesser General Public License (LGPL) license so we cannot package it in Sedona official release.</p> <p>Sedona with Apache Spark</p> Spark 3.0 to 3.3 and Scala 2.12Spark 3.4+ and Scala 2.12Spark 3.0 to 3.3 and Scala 2.13Spark 3.4+ and Scala 2.13 <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-spark-shaded-3.0_2.12&lt;/artifactId&gt;\n&lt;version&gt;1.4.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-viz-3.0_2.12&lt;/artifactId&gt;\n&lt;version&gt;1.4.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- Optional: https://mvnrepository.com/artifact/org.datasyslab/geotools-wrapper --&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n&lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n&lt;version&gt;1.4.0-28.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p><pre><code>&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-spark-shaded-3.4_2.12&lt;/artifactId&gt;\n&lt;version&gt;1.4.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-viz-3.4_2.12&lt;/artifactId&gt;\n&lt;version&gt;1.4.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- Optional: https://mvnrepository.com/artifact/org.datasyslab/geotools-wrapper --&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n&lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n&lt;version&gt;1.4.0-28.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> If you are using Spark versions higher than 3.4, please replace the <code>3.4</code> in artifact names with the corresponding major.minor version numbers.</p> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-spark-shaded-3.0_2.13&lt;/artifactId&gt;\n&lt;version&gt;1.4.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-viz-3.0_2.13&lt;/artifactId&gt;\n&lt;version&gt;1.4.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- Optional: https://mvnrepository.com/artifact/org.datasyslab/geotools-wrapper --&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n&lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n&lt;version&gt;1.4.0-28.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p><pre><code>&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-spark-shaded-3.4_2.13&lt;/artifactId&gt;\n&lt;version&gt;1.4.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-viz-3.4_2.13&lt;/artifactId&gt;\n&lt;version&gt;1.4.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- Optional: https://mvnrepository.com/artifact/org.datasyslab/geotools-wrapper --&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n&lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n&lt;version&gt;1.4.0-28.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> If you are using Spark versions higher than 3.4, please replace the <code>3.4</code> in artifact names with the corresponding major.minor version numbers.</p> <p>Sedona with Apache Flink</p> Flink 1.12+ and Scala 2.12 <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-flink-shaded_2.12&lt;/artifactId&gt;\n&lt;version&gt;1.4.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- Optional: https://mvnrepository.com/artifact/org.datasyslab/geotools-wrapper --&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n&lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n&lt;version&gt;1.4.0-28.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"setup/maven-coordinates/#netcdf-java-542","title":"netCDF-Java 5.4.2","text":"<p>For Scala / Java API, it is required only if you want to read HDF/NetCDF files.</p> <p>HDF/NetCDF function is only supported in Spark RDD with Java/Scala API. The current function is deprecated and more mature support will be released soon.</p> <p>Under BSD 3-clause (compatible with Apache 2.0 license)</p> <p>Add HDF/NetCDF dependency</p> Sedona 1.3.1+Before Sedona 1.3.1 <p>Add unidata repo to your POM.xml</p> <pre><code>&lt;repositories&gt;\n    &lt;repository&gt;\n        &lt;id&gt;unidata-all&lt;/id&gt;\n        &lt;name&gt;Unidata All&lt;/name&gt;\n        &lt;url&gt;https://artifacts.unidata.ucar.edu/repository/unidata-all/&lt;/url&gt;\n    &lt;/repository&gt;\n&lt;/repositories&gt;\n</code></pre> <p>Then add cdm-core to your POM dependency.</p> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;edu.ucar&lt;/groupId&gt;\n&lt;artifactId&gt;cdm-core&lt;/artifactId&gt;\n&lt;version&gt;5.4.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>&lt;!-- https://mvnrepository.com/artifact/org.datasyslab/sernetcdf --&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n&lt;artifactId&gt;sernetcdf&lt;/artifactId&gt;\n&lt;version&gt;0.1.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"setup/maven-coordinates/#use-sedona-unshaded-jars","title":"Use Sedona unshaded jars","text":"<p>Warning</p> <p>For Scala, Java, Python users, please use the following jars only if you satisfy these conditions: (1) you know how to exclude transient dependencies in a complex application. (2) your environment has internet access (3) you are using some sort of Maven package resolver, or pom.xml, or build.sbt. It usually directly takes an input like this <code>GroupID:ArtifactID:Version</code>. If you don't understand what we are talking about, the following jars are not for you.</p> <p>Apache Sedona provides different packages for each supported version of Spark.</p> <ul> <li>For Spark 3.0 to 3.3, the artifacts to use should be <code>sedona-core-3.0_2.12</code>, <code>sedona-sql-3.0_2.12</code>, <code>sedona-vis-3.0_2.12</code>, <code>sedona-python-adapter-3.0_2.12</code>.</li> <li>For Spark 3.4 or higher versions, please use the artifacts with Spark major.minor version in the artifact name. For example, for Spark 3.4, the artifacts to use should be <code>sedona-core-3.4_2.12</code>, <code>sedona-sql-3.4_2.12</code>, <code>sedona-vis-3.4_2.12</code>, <code>sedona-python-adapter-3.4_2.12</code>.</li> </ul> <p>If you are using the Scala 2.13 builds of Spark, please use the corresponding packages for Scala 2.13, which are suffixed by <code>_2.13</code>.</p> <p>The optional GeoTools library is required if you want to use CRS transformation, ShapefileReader or GeoTiff reader. This wrapper library is a re-distribution of GeoTools official jars. The only purpose of this library is to bring GeoTools jars from OSGEO repository to Maven Central. This library is under GNU Lesser General Public License (LGPL) license so we cannot package it in Sedona official release.</p> <p>Sedona with Apache Spark</p> Spark 3.0 to 3.3 and Scala 2.12Spark 3.4+ and Scala 2.12Spark 3.0+ and Scala 2.13Spark 3.4+ and Scala 2.13 <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-core-3.0_2.12&lt;/artifactId&gt;\n&lt;version&gt;1.4.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-sql-3.0_2.12&lt;/artifactId&gt;\n&lt;version&gt;1.4.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-viz-3.0_2.12&lt;/artifactId&gt;\n&lt;version&gt;1.4.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- Required if you use Sedona Python --&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-python-adapter-3.0_2.12&lt;/artifactId&gt;\n&lt;version&gt;1.4.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n&lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n&lt;version&gt;1.4.0-28.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p><pre><code>&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-core-3.4_2.12&lt;/artifactId&gt;\n&lt;version&gt;1.4.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-sql-3.4_2.12&lt;/artifactId&gt;\n&lt;version&gt;1.4.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-viz-3.4_2.12&lt;/artifactId&gt;\n&lt;version&gt;1.4.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- Required if you use Sedona Python --&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-python-adapter-3.4_2.12&lt;/artifactId&gt;\n&lt;version&gt;1.4.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n&lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n&lt;version&gt;1.4.0-28.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> If you are using Spark versions higher than 3.4, please replace the <code>3.4</code> in artifact names with the corresponding major.minor version numbers.</p> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-core-3.0_2.13&lt;/artifactId&gt;\n&lt;version&gt;1.4.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-sql-3.0_2.13&lt;/artifactId&gt;\n&lt;version&gt;1.4.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-viz-3.0_2.13&lt;/artifactId&gt;\n&lt;version&gt;1.4.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- Required if you use Sedona Python --&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-python-adapter-3.0_2.12&lt;/artifactId&gt;\n&lt;version&gt;1.4.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n&lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n&lt;version&gt;1.4.0-28.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p><pre><code>&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-core-3.4_2.13&lt;/artifactId&gt;\n&lt;version&gt;1.4.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-sql-3.4_2.13&lt;/artifactId&gt;\n&lt;version&gt;1.4.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-viz-3.4_2.13&lt;/artifactId&gt;\n&lt;version&gt;1.4.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- Required if you use Sedona Python --&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-python-adapter-3.4_2.12&lt;/artifactId&gt;\n&lt;version&gt;1.4.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n&lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n&lt;version&gt;1.4.0-28.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> If you are using Spark versions higher than 3.4, please replace the <code>3.4</code> in artifact names with the corresponding major.minor version numbers.</p> <p>Sedona with Apache Flink</p> Flink 1.12+ and Scala 2.12 <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-core-3.0_2.12&lt;/artifactId&gt;\n&lt;version&gt;1.4.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-sql-3.0_2.12&lt;/artifactId&gt;\n&lt;version&gt;1.4.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n&lt;artifactId&gt;sedona-flink_2.12&lt;/artifactId&gt;\n&lt;version&gt;1.4.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n&lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n&lt;version&gt;1.4.0-28.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"setup/maven-coordinates/#netcdf-java-542_1","title":"netCDF-Java 5.4.2","text":"<p>For Scala / Java API, it is required only if you want to read HDF/NetCDF files.</p> <p>HDF/NetCDF function is only supported in Spark RDD with Java/Scala API. The current function is deprecated and more mature support will be released soon.</p> <p>Under BSD 3-clause (compatible with Apache 2.0 license)</p> <p>Add HDF/NetCDF dependency</p> Sedona 1.3.1+Before Sedona 1.3.1 <p>Add unidata repo to your POM.xml</p> <pre><code>&lt;repositories&gt;\n    &lt;repository&gt;\n        &lt;id&gt;unidata-all&lt;/id&gt;\n        &lt;name&gt;Unidata All&lt;/name&gt;\n        &lt;url&gt;https://artifacts.unidata.ucar.edu/repository/unidata-all/&lt;/url&gt;\n    &lt;/repository&gt;\n&lt;/repositories&gt;\n</code></pre> <p>Then add cdm-core to your POM dependency.</p> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;edu.ucar&lt;/groupId&gt;\n&lt;artifactId&gt;cdm-core&lt;/artifactId&gt;\n&lt;version&gt;5.4.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>&lt;!-- https://mvnrepository.com/artifact/org.datasyslab/sernetcdf --&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n&lt;artifactId&gt;sernetcdf&lt;/artifactId&gt;\n&lt;version&gt;0.1.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"setup/maven-coordinates/#snapshot-versions","title":"SNAPSHOT versions","text":"<p>Sometimes Sedona has a SNAPSHOT version for the upcoming release. It follows the same naming conversion but has \"SNAPSHOT\" as suffix in the version. For example, <code>1.5.0-SNAPSHOT</code></p> <p>In order to download SNAPSHOTs, you need to add the following repositories in your POM.XML or build.sbt</p>"},{"location":"setup/maven-coordinates/#buildsbt","title":"build.sbt","text":"<p>resolvers +=   \"Apache Software Foundation Snapshots\" at \"https://repository.apache.org/content/groups/snapshots\"</p>"},{"location":"setup/maven-coordinates/#pomxml","title":"POM.XML","text":"<pre><code>&lt;repositories&gt;\n&lt;repository&gt;\n&lt;id&gt;snapshots-repo&lt;/id&gt;\n&lt;url&gt;https://repository.apache.org/content/groups/snapshots&lt;/url&gt;\n&lt;releases&gt;&lt;enabled&gt;false&lt;/enabled&gt;&lt;/releases&gt;\n&lt;snapshots&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/snapshots&gt;\n&lt;/repository&gt;\n&lt;/repositories&gt;\n</code></pre>"},{"location":"setup/modules/","title":"Sedona modules for Apache Spark","text":"Name API Introduction Core RDD SpatialRDDs and Query Operators. SQL SQL/DataFrame SQL interfaces for Sedona core. Viz RDD, SQL/DataFrame Visualization for Spatial RDD and DataFrame Zeppelin Apache Zeppelin Plugin for Apache Zeppelin 0.8.1+"},{"location":"setup/modules/#api-availability","title":"API availability","text":"Core/RDD DataFrame/SQL Viz RDD/SQL Scala/Java \u2705 \u2705 \u2705 Python \u2705 \u2705 SQL only R \u2705 \u2705 \u2705"},{"location":"setup/overview/","title":"Download statistics","text":"Maven PyPI CRAN Apache Sedona 180k/month Archived GeoSpark releases 10k/month"},{"location":"setup/overview/#what-can-sedona-do","title":"What can Sedona do?","text":""},{"location":"setup/overview/#distributed-spatial-datasets","title":"Distributed spatial datasets","text":"<ul> <li> Spatial RDD on Spark</li> <li> Spatial DataFrame/SQL on Spark</li> <li> Spatial DataStream on Flink</li> <li> Spatial Table/SQL on Flink</li> </ul>"},{"location":"setup/overview/#complex-spatial-objects","title":"Complex spatial objects","text":"<ul> <li> Vector geometries / trajectories</li> <li> Raster images with Map Algebra</li> <li> Various input formats: CSV, TSV, WKT, WKB, GeoJSON, Shapefile, GeoTIFF, NetCDF/HDF</li> </ul>"},{"location":"setup/overview/#distributed-spatial-queries","title":"Distributed spatial queries","text":"<ul> <li> Spatial query: range query, range join query, distance join query, K Nearest Neighbor query</li> <li> Spatial index: R-Tree, Quad-Tree</li> </ul>"},{"location":"setup/overview/#rich-spatial-analytics-tools","title":"Rich spatial analytics tools","text":"<ul> <li> Coordinate Reference System / Spatial Reference System Transformation</li> <li> High resolution map generation: Visualize Spatial DataFrame/RDD</li> <li> Apache Zeppelin integration</li> <li> Support Scala, Java, Python, R</li> </ul>"},{"location":"setup/platform/","title":"Language wrappers","text":"<p>Sedona binary releases are compiled by Java 1.8 and Scala 2.11/2.12 and tested in the following environments:</p> <p>Warning</p> <p>Support of Spark 2.X and Scala 2.11 was removed in Sedona 1.3.0+ although some parts of the source code might still be compatible. Sedona 1.3.0+ release binary for both Scala 2.12 and 2.13.</p> Sedona Scala/JavaSedona PythonSedona R Spark 2.4 Spark 3.0 Spark 3.1 Spark 3.2 Spark 3.3 Spark 3.4 Scala 2.11 not tested not tested not tested not tested not tested not tested Scala 2.12 not tested \u2705 \u2705 \u2705 \u2705 \u2705 Scala 2.13 not tested not tested not tested not tested \u2705 \u2705 Spark 2.4 (Scala 2.11) Spark 3.0 (Scala 2.12) Spark 3.1 (Scala 2.12) Spark 3.2 (Scala 2.12) Spark 3.3 (Scala 2.12) Spark 3.4 (Scala 2.12) Python 3.7 not tested \u2705 \u2705 \u2705 \u2705 \u2705 Python 3.8 not tested not tested not tested not tested \u2705 \u2705 Python 3.9 not tested not tested not tested not tested \u2705 \u2705 Python 3.10 not tested not tested not tested not tested \u2705 \u2705 Spark 2.4 Spark 3.0 Spark 3.1 Spark 3.2 Spark 3.3 Spark 3.4 Scala 2.11 not tested not tested not tested not tested not tested not tested Scala 2.12 not tested \u2705 \u2705 \u2705 \u2705 \u2705"},{"location":"setup/release-notes/","title":"Release notes","text":"<p>Warning</p> <p>Support of Spark 2.X and Scala 2.11 was removed in Sedona 1.3.0+ although some parts of the source code might still be compatible. Sedona 1.3.0+ releases binary for both Scala 2.12 and 2.13.</p> <p>Danger</p> <p>Sedona Python currently only works with Shapely 1.x. If you use GeoPandas, please use &lt;= GeoPandas <code>0.11.1</code>. GeoPandas &gt; 0.11.1 will automatically install Shapely 2.0. If you use Shapely, please use &lt;= <code>1.8.4</code>.</p>"},{"location":"setup/release-notes/#sedona-141","title":"Sedona 1.4.1","text":"<p>Sedona 1.4.1 is compiled against, Spark 3.3 / Spark 3.4 / Flink 1.12, Java 8.</p>"},{"location":"setup/release-notes/#highlights","title":"Highlights","text":"<ul> <li> Sedona Spark More raster functions and bridge RasterUDT and Map Algebra operators. See Raster based operators and Raster to Map Algebra operators.</li> <li> Sedona Spark &amp; Flink Added geodesic / geography functions:<ul> <li>ST_DistanceSphere</li> <li>ST_DistanceSpheroid</li> <li>ST_AreaSpheroid</li> <li>ST_LengthSpheroid</li> </ul> </li> <li> Sedona Spark &amp; Flink Introduced <code>SedonaContext</code> to unify Sedona entry points.</li> <li> Sedona Spark Support Spark 3.4.</li> <li> Sedona Spark Added a number of new ST functions.</li> <li> Zeppelin Zeppelin helium plugin supports ploting geometries like linestring, polygon.</li> </ul>"},{"location":"setup/release-notes/#api-change","title":"API change","text":"<ul> <li>Sedona Spark &amp; Flink Introduced a new entry point called SedonaContext to unify all Sedona entry points in different compute engines and deprecate old Sedona register entry points. Users no longer have to register Sedona kryo serializer and import many tedious Python classes.<ul> <li>Sedona Spark:<ul> <li>Scala: <pre><code>import org.apache.sedona.spark.SedonaContext\nval sedona = SedonaContext.create(SedonaContext.builder().master(\"local[*]\").getOrCreate())\nsedona.sql(\"SELECT ST_GeomFromWKT(XXX) FROM\")\n</code></pre></li> <li>Python: <pre><code>from sedona.spark import *\n\nconfig = SedonaContext.builder().\\\n   config('spark.jars.packages',\n       'org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.4.1,'\n       'org.datasyslab:geotools-wrapper:1.4.0-28.2'). \\\n   getOrCreate()\nsedona = SedonaContext.create(config)\nsedona.sql(\"SELECT ST_GeomFromWKT(XXX) FROM\")\n</code></pre></li> </ul> </li> <li>Sedona Flink: <pre><code>import org.apache.sedona.flink.SedonaContext\nStreamTableEnvironment sedona = SedonaContext.create(env, tableEnv);\nsedona.sqlQuery(\"SELECT ST_GeomFromWKT(XXX) FROM\")\n</code></pre></li> </ul> </li> </ul>"},{"location":"setup/release-notes/#bug","title":"Bug","text":"<ul> <li>[SEDONA-266] -         RS_Values throws UnsupportedOperationException for shuffled point arrays </li> <li>[SEDONA-267] -         Cannot pip install apache-sedona 1.4.0 from source distribution </li> <li>[SEDONA-273] -         Set a upper bound for Shapely, Pandas and GeoPandas </li> <li>[SEDONA-277] -         Sedona spark artifacts for scala 2.13 do not have proper POMs </li> <li>[SEDONA-283] -         Artifacts were deployed twice when running mvn clean deploy </li> <li>[SEDONA-284] -         Property values in dependency deduced POMs for shaded modules were not substituted </li> </ul>"},{"location":"setup/release-notes/#new-feature","title":"New Feature","text":"<ul> <li>[SEDONA-196] -         Add ST_Force3D to Sedona </li> <li>[SEDONA-239] -         Implement ST_NumPoints </li> <li>[SEDONA-264] -         zeppelin helium plugin supports ploting geometry like linestring, polygon </li> <li>[SEDONA-280] -         Add ST_GeometricMedian </li> <li>[SEDONA-281] -         Support geodesic / geography functions </li> <li>[SEDONA-286] -         Support optimized distance join on ST_DistanceSpheroid and ST_DistanceSphere </li> <li>[SEDONA-287] -         Use SedonaContext to unify Sedona entry points </li> <li>[SEDONA-292] -         Bridge Sedona Raster and Map Algebra operators </li> <li>[SEDONA-297] -         Implement ST_NRings </li> <li>[SEDONA-302] -         Implement ST_Translate </li> </ul>"},{"location":"setup/release-notes/#improvement","title":"Improvement","text":"<ul> <li>[SEDONA-167] -         Add __pycache__ to Python .gitignore </li> <li>[SEDONA-265] -         Migrate all ST functions to Sedona Inferred Expressions </li> <li>[SEDONA-269] -         Add data source for writing binary files </li> <li>[SEDONA-270] -         Remove redundant serialization for rasters </li> <li>[SEDONA-271] -         Add raster function RS_SRID </li> <li>[SEDONA-274] -         Move all ST function logics to Sedona common </li> <li>[SEDONA-275] -         Add raster function RS_SetSRID </li> <li>[SEDONA-276] -         Add support for Spark 3.4 </li> <li>[SEDONA-279] -         Sedona-Flink should not depend on Sedona-Spark modules </li> <li>[SEDONA-282] -         R \u2013 Add raster write function </li> <li>[SEDONA-290] -         RDD Spatial Joins should follow the iterator model </li> </ul>"},{"location":"setup/release-notes/#sedona-140","title":"Sedona 1.4.0","text":"<p>Sedona 1.4.0 is compiled against, Spark 3.3 / Flink 1.12, Java 8.</p>"},{"location":"setup/release-notes/#highlights_1","title":"Highlights","text":"<ul> <li> Sedona Spark &amp; Flink Serialize and deserialize geometries 3 - 7X faster</li> <li> Sedona Spark &amp; Flink Google S2 based spatial join for fast approximate point-in-polygon join. See Join query in Spark and Join query in Flink</li> <li> Sedona Spark Pushdown spatial predicate on GeoParquet to reduce memory consumption by 10X: see explanation</li> <li> Sedona Spark Automatically use broadcast index spatial join for small datasets</li> <li> Sedona Spark New RasterUDT added to Sedona GeoTiff reader.</li> <li> Sedona Spark A number of bug fixes and improvement to the Sedona R module.</li> </ul>"},{"location":"setup/release-notes/#api-change_1","title":"API change","text":"<ul> <li>Sedona Spark &amp; Flink Packaging strategy changed. See Maven Coordinate. Please change your Sedona dependencies if needed. We recommend <code>sedona-spark-shaded-3.0_2.12-1.4.0</code> and <code>sedona-flink-shaded_2.12-1.4.0</code></li> <li>Sedona Spark &amp; Flink GeoTools-wrapper version upgraded. Please use <code>geotools-wrapper-1.4.0-28.2</code>.</li> </ul>"},{"location":"setup/release-notes/#behavior-change","title":"Behavior change","text":"<ul> <li>Sedona Flink Sedona Flink no longer outputs any LinearRing type geometry. All LinearRing are changed to LineString.</li> <li>Sedona Spark Join optimization strategy changed. Sedona no longer optimizes spatial join when use a spatial predicate together with a equijoin predicate. By default, it prefers equijoin whenever possible. SedonaConf adds a config option called <code>sedona.join.optimizationmode</code>, it can be configured as one of the following values:<ul> <li><code>all</code>: optimize all joins having spatial predicate in join conditions. This was the behavior of Apache Sedona prior to 1.4.0.</li> <li><code>none</code>: disable spatial join optimization.</li> <li><code>nonequi</code>: only enable spatial join optimization on non-equi joins. This is the default mode.</li> </ul> </li> </ul> <p>When <code>sedona.join.optimizationmode</code> is configured as <code>nonequi</code>, it won't optimize join queries such as <code>SELECT * FROM A, B WHERE A.x = B.x AND ST_Contains(A.geom, B.geom)</code>, since it is an equi-join with equi-condition <code>A.x = B.x</code>. Sedona will optimize for <code>SELECT * FROM A, B WHERE ST_Contains(A.geom, B.geom)</code></p>"},{"location":"setup/release-notes/#bug_1","title":"Bug","text":"<ul> <li>[SEDONA-218] -         Flaky test caused by improper handling of null struct values in Adapter.toDf </li> <li>[SEDONA-221] -         Outer join throws NPE for null geometries </li> <li>[SEDONA-222] -         GeoParquet reader does not work in non-local mode </li> <li>[SEDONA-224] -         java.lang.NoSuchMethodError when loading GeoParquet files using Spark 3.0.x ~ 3.2.x </li> <li>[SEDONA-225] -         Cannot count dataframes loaded from GeoParquet files </li> <li>[SEDONA-227] -         Python SerDe Performance Degradation </li> <li>[SEDONA-230] -         rdd.saveAsGeoJSON should generate feature properties with field names </li> <li>[SEDONA-233] -         Incorrect results for several joins in a single stage </li> <li>[SEDONA-236] -         Flakey python tests in tests.serialization.test_[de]serializers </li> <li>[SEDONA-242] -         Update jars dependencies in Sedona R to Sedona 1.4.0 version </li> <li>[SEDONA-250] -         R Deprecate use of Spark 2.4 </li> <li>[SEDONA-252] -         Fix disabled RS_Base64 test </li> <li>[SEDONA-255] -         R \u2013 Translation issue for ST_Point and ST_PolygonFromEnvelope </li> <li>[SEDONA-258] -         Cannot directly assign raw spatial RDD to CircleRDD using Python binding </li> <li>[SEDONA-259] -         Adapter.toSpatialRdd in Python binding does not have valid implementation for specifying custom field names for user data </li> <li>[SEDONA-261] -         Cannot run distance join using broadcast index join when the distance expression references to attributes from the right-side relation </li> </ul>"},{"location":"setup/release-notes/#new-feature_1","title":"New Feature","text":"<ul> <li>[SEDONA-156] -         predicate pushdown support for GeoParquet </li> <li>[SEDONA-215] -         Add ST_ConcaveHull </li> <li>[SEDONA-216] -         Upgrade jts version to 1.19.0 </li> <li>[SEDONA-235] -         Create ST_S2CellIds in Sedona </li> <li>[SEDONA-246] -         R GeoTiff read/write </li> <li>[SEDONA-254] -         R \u2013 Add raster type </li> <li>[SEDONA-262] -         Don't optimize equi-join by default, add an option to configure when to optimize spatial joins </li> </ul>         Improvement  <ul> <li>[SEDONA-205] -         Use BinaryType in GeometryUDT in Sedona Spark </li> <li>[SEDONA-207] -         Faster serialization/deserialization of geometry objects </li> <li>[SEDONA-212] -         Move shading to separate maven modules </li> <li>[SEDONA-217] -         Automatically broadcast small datasets </li> <li>[SEDONA-220] -         Upgrade Ubuntu build image from 18.04 to 20.04 </li> <li>[SEDONA-226] -         Support reading and writing GeoParquet file metadata </li> <li>[SEDONA-228] -         Standardize logging dependencies </li> <li>[SEDONA-231] -         Redundant Serde Removal </li> <li>[SEDONA-234] -         ST_Point inconsistencies </li> <li>[SEDONA-243] -         Improve Sedona R file readers: GeoParquet and Shapefile </li> <li>[SEDONA-244] -         Align R read/write functions with the Sparklyr framework </li> <li>[SEDONA-249] -         Add jvm flags for running tests on Java 17  </li> <li>[SEDONA-251] -         Add raster type to Sedona </li> <li>[SEDONA-253] -         Upgrade geotools to version 28.2 </li> <li>[SEDONA-260] -         More intuitive configuration of partition and index-build side of spatial joins in Sedona SQL </li> </ul>"},{"location":"setup/release-notes/#sedona-131","title":"Sedona 1.3.1","text":"<p>This version is a minor release on Sedoma 1.3.0 line. It fixes a few critical bugs in 1.3.0. We suggest all 1.3.0 users to migrate to this version.</p>"},{"location":"setup/release-notes/#bug-fixes","title":"Bug fixes","text":"<ul> <li>SEDONA-204 - Init value in X/Y/Z max should be -Double.MAX</li> <li>SEDONA-206 - Performance regression of ST_Transform in 1.3.0-incubating</li> <li>SEDONA-210 - 1.3.0-incubating doesn't work with Scala 2.12 sbt projects</li> <li>SEDONA-211 - Enforce release managers to use JDK 8</li> <li>SEDONA-201 - Implement ST_MLineFromText and ST_MPolyFromText methods</li> </ul>"},{"location":"setup/release-notes/#new-feature_2","title":"New Feature","text":"<ul> <li>SEDONA-196 - Add ST_Force3D to Sedona</li> <li>SEDONA-197 - Add ST_ZMin, ST_ZMax to Sedona</li> <li>SEDONA-199 - Add ST_NDims to Sedona</li> </ul>"},{"location":"setup/release-notes/#improvement_1","title":"Improvement","text":"<ul> <li>SEDONA-194 - Merge org.datasyslab.sernetcdf into Sedona</li> <li>SEDONA-208 - Use Spark RuntimeConfig in SedonaConf</li> </ul>"},{"location":"setup/release-notes/#sedona-130","title":"Sedona 1.3.0","text":"<p>This version is a major release on Sedona 1.3.0 line and consists of 50 PRs. It includes many new functions, optimization and bug fixes.</p>"},{"location":"setup/release-notes/#highlights_2","title":"Highlights","text":"<ul> <li> Sedona on Spark in this release is compiled against Spark 3.3.</li> <li> Sedona on Flink in this release is compiled against Flink 1.14.</li> <li> Scala 2.11 support is removed.</li> <li> Spark 2.X support is removed.</li> <li> Python 3.10 support is added.</li> <li> Aggregators in Flink are added</li> <li> Correctness fixes for corner cases in range join and distance join.</li> <li> Native GeoParquet read and write (../../tutorial/sql/#load-geoparquet).<ul> <li><code>df = spark.read.format(\"geoparquet\").option(\"fieldGeometry\", \"myGeometryColumn\").load(\"PATH/TO/MYFILE.parquet\")</code></li> <li><code>df.write.format(\"geoparquet\").save(\"PATH/TO/MYFILE.parquet\")</code></li> </ul> </li> <li> DataFrame style API (../../tutorial/sql/#dataframe-style-api)<ul> <li><code>df.select(ST_Point(min_value, max_value).as(\"point\"))</code></li> </ul> </li> <li> Allow WKT format CRS in ST_Transform<ul> <li><code>ST_Transform(geom, \"srcWktString\", \"tgtWktString\")</code></li> </ul> </li> </ul> <pre><code>GEOGCS[\"WGS 84\",\nDATUM[\"WGS_1984\",\nSPHEROID[\"WGS 84\",6378137,298.257223563,\nAUTHORITY[\"EPSG\",\"7030\"]],\nAUTHORITY[\"EPSG\",\"6326\"]],\nPRIMEM[\"Greenwich\",0,\nAUTHORITY[\"EPSG\",\"8901\"]],\nUNIT[\"degree\",0.0174532925199433,\nAUTHORITY[\"EPSG\",\"9122\"]],\nAUTHORITY[\"EPSG\",\"4326\"]]\n</code></pre>"},{"location":"setup/release-notes/#bug-fixes_1","title":"Bug fixes","text":"<ul> <li>SEDONA-119 - ST_Touches join query returns true for polygons whose interiors intersect</li> <li>SEDONA-136 - Enable testAsEWKT for Flink</li> <li>SEDONA-137 - Fix ST_Buffer for Flink to work</li> <li>SEDONA-138 - Fix ST_GeoHash for Flink to work</li> <li>SEDONA-153 - Python Serialization Fails with Nulls</li> <li>SEDONA-158 - Fix wrong description about ST_GeometryN in the API docs</li> <li>SEDONA-169 - Fix ST_RemovePoint in accordance with the API document</li> <li>SEDONA-178 - Correctness issue in distance join queries</li> <li>SEDONA-182 - ST_AsText should not return SRID</li> <li>SEDONA-186 - collecting result rows of a spatial join query with SELECT * fails with serde error</li> <li>SEDONA-188 - Python warns about missing <code>jars</code> even when some are found</li> <li>SEDONA-193 - ST_AsBinary produces EWKB by mistake</li> </ul>"},{"location":"setup/release-notes/#new-features","title":"New Features","text":"<ul> <li>SEDONA-94 - GeoParquet  Support For Sedona</li> <li>SEDONA-125 - Allows customized CRS in ST_Transform</li> <li>SEDONA-166 - Provide Type-safe DataFrame Style API</li> <li>SEDONA-168 - Add ST_Normalize to Apache Sedona</li> <li>SEDONA-171 - Add ST_SetPoint to Apache Sedona</li> </ul>"},{"location":"setup/release-notes/#improvement_2","title":"Improvement","text":"<ul> <li>SEDONA-121 - Add equivalent constructors left over from Spark to Flink</li> <li>SEDONA-132 - Create common module for SQL functions</li> <li>SEDONA-133 - Allow user-defined schemas in Adapter.toDf()</li> <li>SEDONA-139 - Fix wrong argument order in Flink unit tests</li> <li>SEDONA-140 - Update Sedona Dependencies in R Package</li> <li>SEDONA-143 - Add missing unit tests for the Flink predicates</li> <li>SEDONA-144 - Add ST_AsGeoJSON to the Flink API</li> <li>SEDONA-145 - Fix ST_AsEWKT to reserve the Z coordinate</li> <li>SEDONA-146 - Add missing output funtions to the Flink API</li> <li>SEDONA-147 - Add SRID functions to the Flink API</li> <li>SEDONA-148 - Add boolean functions to the Flink API</li> <li>SEDONA-149 - Add Python 3.10 support</li> <li>SEDONA-151 - Add ST aggregators to Sedona Flink</li> <li>SEDONA-152 - Add reader/writer functions for GML and KML</li> <li>SEDONA-154 - Add measurement functions to the Flink API</li> <li>SEDONA-157 - Add coordinate accessors to the Flink API</li> <li>SEDONA-159 - Add Nth accessor functions to the Flink API</li> <li>SEDONA-160 - Fix geoparquetIOTests.scala to cleanup after test</li> <li>SEDONA-161 - Add ST_Boundary to the Flink API</li> <li>SEDONA-162 - Add ST_Envelope to the Flink API</li> <li>SEDONA-163 - Better handle of unsupported types in shapefile reader</li> <li>SEDONA-164 - Add geometry count functions to the Flink API</li> <li>SEDONA-165 - Upgrade Apache Rat to 0.14</li> <li>SEDONA-170 - Add ST_AddPoint and ST_RemovePoint to the Flink API</li> <li>SEDONA-172 - Add ST_LineFromMultiPoint to Apache Sedona</li> <li>SEDONA-176 - Make ST_Contains conform with OGC standard, and add ST_Covers and ST_CoveredBy functions.</li> <li>SEDONA-177 - Support spatial predicates other than INTERSECTS and COVERS/COVERED_BY in RangeQuery.SpatialRangeQuery and JoinQuery.SpatialJoinQuery</li> <li>SEDONA-181 - Build fails with java.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$</li> <li>SEDONA-189 - Prepare geometries in broadcast join</li> <li>SEDONA-192 - Null handling in predicates</li> <li>SEDONA-195 - Add wkt validation and an optional srid to ST_GeomFromWKT/ST_GeomFromText</li> </ul>"},{"location":"setup/release-notes/#task","title":"Task","text":"<ul> <li>SEDONA-150 - Drop Spark 2.4 and Scala 2.11 support</li> </ul>"},{"location":"setup/release-notes/#sedona-121","title":"Sedona 1.2.1","text":"<p>This version is a maintenance release on Sedona 1.2.0 line. It includes bug fixes.</p> <p>Sedona on Spark is now compiled against Spark 3.3, instead of Spark 3.2.</p>"},{"location":"setup/release-notes/#sql-for-spark","title":"SQL (for Spark)","text":"<p>Bug fixes:</p> <ul> <li>SEDONA-104: Bug in reading band values of GeoTiff images</li> <li>SEDONA-118: Fix the wrong result in ST_Within</li> <li>SEDONA-123: Fix the check for invalid lat/lon in ST_GeoHash</li> </ul> <p>Improvement:</p> <ul> <li>SEDONA-96: Refactor ST_MakeValid to use GeometryFixer</li> <li>SEDONA-108: Write support for GeoTiff images</li> <li>SEDONA-122: Overload ST_GeomFromWKB for BYTES column</li> <li>SEDONA-127: Add null safety to ST_GeomFromWKT/WKB/Text</li> <li>SEDONA-129: Support Spark 3.3</li> <li>SEDONA-135: Consolidate and upgrade hadoop dependency</li> </ul> <p>New features:</p> <ul> <li>SEDONA-107: Add St_Reverse function</li> <li>SEDONA-105: Add ST_PointOnSurface function</li> <li>SEDONA-95: Add ST_Disjoint predicate</li> <li>SEDONA-112: Add ST_AsEWKT</li> <li>SEDONA-106: Add ST_LineFromText</li> <li>SEDONA-117: Add RS_AppendNormalizedDifference</li> <li>SEDONA-97: Add ST_Force_2D</li> <li>SEDONA-98: Add ST_IsEmpty</li> <li>SEDONA-116: Add ST_YMax and ST_Ymin</li> <li>SEDONA-115: Add ST_XMax and ST_Min</li> <li>SEDONA-120: Add ST_BuildArea</li> <li>SEDONA-113: Add ST_PointN</li> <li>SEDONA-124: Add ST_CollectionExtract</li> <li>SEDONA-109: Add ST_OrderingEquals</li> </ul>"},{"location":"setup/release-notes/#flink","title":"Flink","text":"<p>New features:</p> <ul> <li>SEDONA-107: Add St_Reverse function</li> <li>SEDONA-105: Add ST_PointOnSurface function</li> <li>SEDONA-95: Add ST_Disjoint predicate</li> <li>SEDONA-112: Add ST_AsEWKT</li> <li>SEDONA-97: Add ST_Force_2D</li> <li>SEDONA-98: Add ST_IsEmpty</li> <li>SEDONA-116: Add ST_YMax and ST_Ymin</li> <li>SEDONA-115: Add ST_XMax and ST_Min</li> <li>SEDONA-120: Add ST_BuildArea</li> <li>SEDONA-113: Add ST_PointN</li> <li>SEDONA-110: Add ST_GeomFromGeoHash</li> <li>SEDONA-121: More ST constructors to Flink</li> <li>SEDONA-122: Overload ST_GeomFromWKB for BYTES column</li> </ul>"},{"location":"setup/release-notes/#sedona-120","title":"Sedona 1.2.0","text":"<p>This version is a major release on Sedona 1.2.0 line. It includes bug fixes and new features: Sedona with Apache Flink.</p>"},{"location":"setup/release-notes/#rdd","title":"RDD","text":"<p>Bug fix:</p> <ul> <li>SEDONA-18: Fix an error reading Shapefile</li> <li>SEDONA-73: Exclude scala-library from scala-collection-compat</li> </ul> <p>Improvement:</p> <ul> <li>SEDONA-77: Refactor Format readers and spatial partitioning functions to be standalone libraries. So they can be used by Flink and others.</li> </ul>"},{"location":"setup/release-notes/#sql","title":"SQL","text":"<p>New features:</p> <ul> <li>SEDONA-4: Handle nulls in SQL functions</li> <li>SEDONA-65: Create ST_Difference function</li> <li>SEDONA-68 Add St_Collect function.</li> <li>SEDONA-82: Create ST_SymmDifference function</li> <li>SEDONA-75: Add support for \"3D\" geometries: Preserve Z coordinates on geometries when serializing, ST_AsText , ST_Z, ST_3DDistance</li> <li>SEDONA-86: Support empty geometries in ST_AsBinary and ST_AsEWKB</li> <li>SEDONA-90: Add ST_Union</li> <li>SEDONA-100: Add st_multi function</li> </ul> <p>Bug fix:</p> <ul> <li>SEDONA-89: GeometryUDT equals should test equivalence of the other object</li> </ul>"},{"location":"setup/release-notes/#flink_1","title":"Flink","text":"<p>Major update:</p> <ul> <li>SEDONA-80: Geospatial stream processing support in Flink Table API</li> <li>SEDONA-85: ST_Geohash function in Flink</li> <li>SEDONA-87: Support Flink Table and DataStream conversion</li> <li>SEDONA-93: Add ST_GeomFromGeoJSON</li> </ul>"},{"location":"setup/release-notes/#sedona-111","title":"Sedona 1.1.1","text":"<p>This version is a maintenance release on Sedona 1.1.X line. It includes bug fixes and a few new functions.</p>"},{"location":"setup/release-notes/#global","title":"Global","text":"<p>New feature:</p> <ul> <li>SEDONA-73: Scala source code supports Scala 2.13</li> </ul>"},{"location":"setup/release-notes/#sql_1","title":"SQL","text":"<p>Bug fix:</p> <ul> <li>SEDONA-67: Support Spark 3.2</li> </ul> <p>New features:</p> <ul> <li>SEDONA-43: Add ST_GeoHash and ST_GeomFromGeoHash</li> <li>SEDONA-45: Add ST_MakePolygon</li> <li>SEDONA-71: Add ST_AsBinary, ST_AsEWKB, ST_SRID, ST_SetSRID</li> </ul>"},{"location":"setup/release-notes/#sedona-110","title":"Sedona 1.1.0","text":"<p>This version is a major release on Sedona 1.1.0 line. It includes bug fixes and new features: R language API, Raster data and Map algebra support</p>"},{"location":"setup/release-notes/#global_1","title":"Global","text":"<p>Dependency upgrade: </p> <ul> <li>SEDONA-30: Use Geotools-wrapper 1.1.0-24.1 to include geotools GeoTiff libraries.</li> </ul> <p>Improvement on join queries in core and SQL:</p> <ul> <li>SEDONA-63: Skip empty partitions in NestedLoopJudgement</li> <li>SEDONA-64: Broadcast dedupParams to improve performance</li> </ul> <p>Behavior change:</p> <ul> <li>SEDONA-62: Ignore HDF test in order to avoid NASA copyright issue</li> </ul>"},{"location":"setup/release-notes/#core","title":"Core","text":"<p>Bug fix:</p> <ul> <li>SEDONA-41: Fix rangeFilter bug when the leftCoveredByRight para is false</li> <li>SEDONA-53: Fix SpatialKnnQuery NullPointerException</li> </ul>"},{"location":"setup/release-notes/#sql_2","title":"SQL","text":"<p>Major update:</p> <ul> <li>SEDONA-30: Add GeoTiff raster I/O and Map Algebra function</li> </ul> <p>New function:</p> <ul> <li>SEDONA-27: Add ST_Subdivide and ST_SubdivideExplode functions</li> </ul> <p>Bug fix:</p> <ul> <li>SEDONA-56: Fix broadcast join with Adapter Query Engine enabled</li> <li>SEDONA-22, SEDONA-60: Fix join queries in SparkSQL when one side has no rows or only one row</li> </ul>"},{"location":"setup/release-notes/#viz","title":"Viz","text":"<p>N/A</p>"},{"location":"setup/release-notes/#python","title":"Python","text":"<p>Improvement:</p> <ul> <li>SEDONA-59: Make pyspark dependency of Sedona Python optional</li> </ul> <p>Bug fix:</p> <ul> <li>SEDONA-50: Remove problematic logging conf that leads to errors on Databricks</li> <li>Fix the issue: Spark dependency in setup.py was configured to be &lt; v3.1.0 by mistake.</li> </ul>"},{"location":"setup/release-notes/#r","title":"R","text":"<p>Major update:</p> <ul> <li>SEDONA-31: Add R interface for Sedona</li> </ul>"},{"location":"setup/release-notes/#sedona-101","title":"Sedona 1.0.1","text":"<p>This version is a maintenance release on Sedona 1.0.0 line. It includes bug fixes, some new features, one API change</p>"},{"location":"setup/release-notes/#known-issue","title":"Known issue","text":"<p>In Sedona v1.0.1 and earlier versions, the Spark dependency in setup.py was configured to be &lt; v3.1.0 by mistake. When you install Sedona Python (apache-sedona v1.0.1) from PyPI, pip might uninstall PySpark 3.1.1 and install PySpark 3.0.2 on your machine.</p> <p>Three ways to fix this:</p> <ol> <li> <p>After install apache-sedona v1.0.1, uninstall PySpark 3.0.2 and reinstall PySpark 3.1.1</p> </li> <li> <p>Ask pip not to install Sedona dependencies: <code>pip install --no-deps apache-sedona</code></p> </li> <li> <p>Install Sedona from the latest setup.py (on GitHub) manually.</p> </li> </ol>"},{"location":"setup/release-notes/#global_2","title":"Global","text":"<p>Dependency upgrade:</p> <ul> <li>SEDONA-16: Use a GeoTools Maven Central wrapper to fix failed Jupyter notebook examples</li> <li>SEDONA-29: upgrade to Spark 3.1.1</li> <li>SEDONA-33: jts2geojson version from 0.14.3 to 0.16.1</li> </ul>"},{"location":"setup/release-notes/#core_1","title":"Core","text":"<p>Bug fix:</p> <ul> <li>SEDONA-35: Address user-data mutability issue with Adapter.toDF()</li> </ul>"},{"location":"setup/release-notes/#sql_3","title":"SQL","text":"<p>Bug fix:</p> <ul> <li>SEDONA-14: Saving dataframe to CSV or Parquet fails due to unknown type</li> <li>SEDONA-15: Add ST_MinimumBoundingRadius and ST_MinimumBoundingCircle functions</li> <li>SEDONA-19: Global indexing does not work with SQL joins</li> <li>SEDONA-20: Case object GeometryUDT and GeometryUDT instance not equal in Spark 3.0.2</li> </ul> <p>New function:</p> <ul> <li>SEDONA-21: allows Sedona to be used in pure SQL environment</li> <li>SEDONA-24: Add ST_LineSubString and ST_LineInterpolatePoint</li> <li>SEDONA-26: Add broadcast join support</li> </ul>"},{"location":"setup/release-notes/#viz_1","title":"Viz","text":"<p>Improvement:</p> <ul> <li>SEDONA-32: Speed up ST_Render</li> </ul> <p>API change:</p> <ul> <li>SEDONA-29: Upgrade to Spark 3.1.1 and fix ST_Pixelize</li> </ul>"},{"location":"setup/release-notes/#python_1","title":"Python","text":"<p>Bug fix:</p> <ul> <li>SEDONA-19: Global indexing does not work with SQL joins</li> </ul>"},{"location":"setup/release-notes/#sedona-100","title":"Sedona 1.0.0","text":"<p>This version is the first Sedona release since it joins the Apache Incubator. It includes new functions, bug fixes, and API changes.</p>"},{"location":"setup/release-notes/#global_3","title":"Global","text":"<p>Key dependency upgrade:</p> <ul> <li>SEDONA-1: upgrade to JTS 1.18</li> <li>upgrade to GeoTools 24.0</li> <li>upgrade to jts2geojson 0.14.3</li> </ul> <p>Key dependency packaging strategy change:</p> <ul> <li>JTS, GeoTools, jts2geojson are no longer packaged in Sedona jars. End users need to add them manually. See here.</li> </ul> <p>Key compilation target change:</p> <ul> <li>SEDONA-3: Paths and class names have been changed to Apache Sedona</li> <li>SEDONA-7: build the source code for Spark 2.4, 3.0, Scala 2.11, 2.12, Python 3.7, 3.8, 3.9. See here.</li> </ul>"},{"location":"setup/release-notes/#sedona-core","title":"Sedona-core","text":"<p>Bug fix:</p> <ul> <li>PR 443: read multiple Shape Files by multiPartitions</li> <li>PR 451 (API change): modify CRSTransform to ignore datum shift</li> </ul> <p>New function:</p> <ul> <li>SEDONA-8: spatialRDD.flipCoordinates()</li> </ul> <p>API / behavior change:</p> <ul> <li>PR 488: JoinQuery.SpatialJoinQuery/DistanceJoinQuery now returns <code>&lt;Geometry, List&gt;</code> instead of <code>&lt;Geometry, HashSet&gt;</code> because we can no longer use HashSet in Sedona for duplicates removal. All original duplicates in both input RDDs will be preserved in the output.</li> </ul>"},{"location":"setup/release-notes/#sedona-sql","title":"Sedona-sql","text":"<p>Bug fix:</p> <ul> <li>SEDONA-8 (API change): ST_Transform slow due to lock contention.</li> <li>PR 427: ST_Point and ST_PolygonFromEnvelope now allows Double type</li> </ul> <p>New function:</p> <ul> <li>PR 499: ST_Azimuth, ST_X, ST_Y, ST_StartPoint, ST_Boundary, ST_EndPoint, ST_ExteriorRing, ST_GeometryN, ST_InteriorRingN, ST_Dump, ST_DumpPoints, ST_IsClosed, ST_NumInteriorRings, ST_AddPoint, ST_RemovePoint, ST_IsRing</li> <li>PR 459: ST_LineMerge</li> <li>PR 460: ST_NumGeometries</li> <li>PR 469: ST_AsGeoJSON</li> <li>SEDONA-8: ST_FlipCoordinates</li> </ul> <p>Behavior change:</p> <ul> <li>PR 480: Aggregate Functions rewrite for new Aggregator API. The functions can be used as typed functions in code and enable compilation-time type check.</li> </ul> <p>API change:</p> <ul> <li>SEDONA-11: Adapter.toDf() will directly generate a geometry type column. ST_GeomFromWKT is no longer needed.</li> </ul>"},{"location":"setup/release-notes/#sedona-viz","title":"Sedona-viz","text":"<p>API change: Drop the function which can generate SVG vector images because the required library has an incompatible license and the SVG image is not good at plotting big data</p>"},{"location":"setup/release-notes/#sedona-python","title":"Sedona Python","text":"<p>API/Behavior change:</p> <ul> <li>Python-to-Sedona adapter is moved to a separate module. To use Sedona Python, see here</li> </ul> <p>New function:</p> <ul> <li>PR 448: Add support for partition number in spatialPartitioning function <code>spatial_rdd.spatialPartitioning(grid_type, NUM_PARTITION)</code></li> </ul>"},{"location":"setup/zeppelin/","title":"Install Sedona-Zeppelin","text":"<p>Warning</p> <p>Known issue: due to an issue in Leaflet JS, Sedona can only plot each geometry (point, line string and polygon) as a point on Zeppelin map. To enjoy the scalable and full-fleged visualization, please use SedonaViz to plot scatter plots and heat maps on Zeppelin map.</p>"},{"location":"setup/zeppelin/#compatibility","title":"Compatibility","text":"<p>Apache Spark 2.3+</p> <p>Apache Zeppelin 0.8.1+</p> <p>Sedona 1.0.0+: Sedona-core, Sedona-SQL, Sedona-Viz</p>"},{"location":"setup/zeppelin/#installation","title":"Installation","text":"<p>Note</p> <p>You only need to do Step 1 and 2 only if you cannot see Apache-sedona or GeoSpark Zeppelin in Zeppelin Helium package list.</p>"},{"location":"setup/zeppelin/#create-helium-folder-optional","title":"Create Helium folder (optional)","text":"<p>Create a folder called <code>helium</code> in Zeppelin root folder.</p>"},{"location":"setup/zeppelin/#add-sedona-zeppelin-description-optional","title":"Add Sedona-Zeppelin description (optional)","text":"<p>Create a file called <code>sedona-zeppelin.json</code> in this folder and put the following content in this file. You need to change the artifact path!</p> <pre><code>{\n  \"type\": \"VISUALIZATION\",\n  \"name\": \"sedona-zeppelin\",\n  \"description\": \"Zeppelin visualization support for Sedona\",\n  \"artifact\": \"/Absolute/Path/sedona/zeppelin\",\n  \"license\": \"BSD-2-Clause\",\n  \"icon\": \"&lt;i class='fa fa-globe'&gt;&lt;/i&gt;\"\n}\n</code></pre>"},{"location":"setup/zeppelin/#enable-sedona-zeppelin","title":"Enable Sedona-Zeppelin","text":"<p>Restart Zeppelin then open Zeppelin Helium interface and enable Sedona-Zeppelin.</p> <p></p>"},{"location":"setup/zeppelin/#add-sedona-dependencies-in-zeppelin-spark-interpreter","title":"Add Sedona dependencies in Zeppelin Spark Interpreter","text":""},{"location":"setup/zeppelin/#visualize-sedonasql-results","title":"Visualize SedonaSQL results","text":""},{"location":"setup/zeppelin/#display-sedonaviz-results","title":"Display SedonaViz results","text":"<p>Now, you are good to go! Please read Sedona-Zeppelin tutorial for a hands-on tutorial.</p>"},{"location":"setup/flink/install-scala/","title":"Install Sedona Scala/Java","text":"<p>Before starting the Sedona journey, you need to make sure your Apache Flink cluster is ready.</p> <p>Then you can create a self-contained Scala / Java project. A self-contained project allows you to create multiple Scala / Java files and write complex logics in one place.</p> <p>To use Sedona in your self-contained Flink project, you just need to add Sedona as a dependency in your POM.xml or build.sbt.</p> <ol> <li>To add Sedona as dependencies, please read Sedona Maven Central coordinates</li> <li>Read Sedona Flink guide and use Sedona Template project to start: Sedona Template Project</li> <li>Compile your project using Maven. Make sure you obtain the fat jar which packages all dependencies.</li> <li>Submit your compiled fat jar to Flink cluster. Make sure you are in the root folder of Flink distribution. Then run the following command: <pre><code>./bin/flink run /Path/To/YourJar.jar\n</code></pre></li> </ol>"},{"location":"setup/flink/modules/","title":"Sedona modules for Apache Flink","text":"Name Introduction Core Spatial query algorithms, data readers/writers SQL Spatial SQL function implementation Flink Spatial Table and DataStream implementation"},{"location":"setup/flink/modules/#api-availability","title":"API availability","text":"DataStream Table Scala/Java \u2705 \u2705 Python no no R no no"},{"location":"setup/flink/platform/","title":"Language wrappers","text":"<p>Sedona Flink binary releases are compiled by Java 1.8 and Scala 2.12, and tested in the following environments:</p> Sedona Scala/Java Flink 1.12 Flink 1.13 Flink 1.14 Scala 2.12 \u2705 \u2705 \u2705 Scala 2.11 not tested not tested not tested"},{"location":"tutorial/Advanced-Tutorial-Tune-your-Application/","title":"Advanced tutorial: Tune your Sedona RDD application","text":"<p>Before getting into this advanced tutorial, please make sure that you have tried several Sedona functions on your local machine.</p>"},{"location":"tutorial/Advanced-Tutorial-Tune-your-Application/#pick-a-proper-sedona-version","title":"Pick a proper Sedona version","text":"<p>The versions of Sedona have three levels: X.X.X (i.e., 0.8.1)</p> <p>The first level means that this version contains big structure redesign which may bring big changes in APIs and performance.</p> <p>The second level (i.e., 0.8) indicates that this version contains significant performance enhancement, big new features and API changes. An old Sedona user who wants to pick this version needs to be careful about the API changes. Before you move to this version, please read Sedona version release notes and make sure you are ready to accept the API changes.</p> <p>The third level (i.e., 0.8.1) tells that this version only contains bug fixes, some small new features and slight performance enhancement. This version will not contain any API changes. Moving to this version is safe. We highly suggest all Sedona users that stay at the same level move to the latest version in this level.</p>"},{"location":"tutorial/Advanced-Tutorial-Tune-your-Application/#choose-a-proper-spatial-rdd-constructor","title":"Choose a proper Spatial RDD constructor","text":"<p>Sedona provides a number of constructors for each SpatialRDD (PointRDD, PolygonRDD and LineStringRDD). In general, you have two options to start with.</p> <ol> <li>Initialize a SpatialRDD from your data source such as HDFS and S3. A typical example is as follows: <pre><code>public PointRDD(JavaSparkContext sparkContext, String InputLocation, Integer Offset, FileDataSplitter splitter, boolean carryInputData, Integer partitions, StorageLevel newLevel)\n</code></pre></li> <li>Initialize a SpatialRDD from an existing RDD. A typical example is as follows: <pre><code>public PointRDD(JavaRDD&lt;Point&gt; rawSpatialRDD, StorageLevel newLevel)\n</code></pre></li> </ol> <p>You may notice that these constructors all take as input a \"StorageLevel\" parameter. This is to tell Apache Spark cache the \"rawSpatialRDD\", one attribute of SpatialRDD. The reason why Sedona does this is that Sedona wants to calculate the dataset boundary and approximate total count using several Apache Spark \"Action\"s. These information are useful when doing Spatial Join Query and Distance Join Query.</p> <p>However, in some cases, you may know well about your datasets. If so, you can manually provide these information by calling this kind of Spatial RDD constructors:</p> <p><pre><code>public PointRDD(JavaSparkContext sparkContext, String InputLocation, Integer Offset, FileDataSplitter splitter, boolean carryInputData, Integer partitions, Envelope datasetBoundary, Integer approximateTotalCount) {\n</code></pre> Manually providing the dataset boundary and approximate total count helps Sedona avoiding several slow \"Action\"s during initialization.</p>"},{"location":"tutorial/Advanced-Tutorial-Tune-your-Application/#cache-the-spatial-rdd-that-is-repeatedly-used","title":"Cache the Spatial RDD that is repeatedly used","text":"<p>Each SpatialRDD (PointRDD, PolygonRDD and LineStringRDD) possesses four RDD attributes. They are:</p> <ol> <li>rawSpatialRDD: The RDD generated by SpatialRDD constructors.</li> <li>spatialPartitionedRDD: The RDD generated by spatial partition a rawSpatialRDD. Note that: this RDD has replicated spatial objects.</li> <li>indexedRawRDD: The RDD generated by indexing a rawSpatialRDD.</li> <li>indexedRDD: The RDD generated by indexing a spatialPartitionedRDD. Note that: this RDD has replicated spatial objects.</li> </ol> <p>These four RDDs don't co-exist so you don't need to worry about the memory issue. These four RDDs are invoked in different queries:</p> <ol> <li>Spatial Range Query / KNN Query, no index: rawSpatialRDD is used.</li> <li>Spatial Range Query / KNN Query, use index: indexedRawRDD is used.</li> <li>Spatial Join Query / Distance Join Query, no index: spatialPartitionedRDD is used.</li> <li>Spatial Join Query / Distance Join Query, use index: indexed RDD is used.</li> </ol> <p>Therefore, if you use one of the queries above many times, you'd better cache the associated RDD into memory. There are several possible use cases:</p> <ol> <li>In Spatial Data Mining such as Spatial Autocorrelation and Spatial Co-location Pattern Mining, you may need to use Spatial Join / Spatial Self-join iteratively in order to calculate the adjacency matrix. If so, please cache the spatialPartitionedRDD/indexedRDD which is queries many times.</li> <li>In Spark RDD sharing applications such as Livy and Spark Job Server, many users may do Spatial Range Query / KNN Query on the same Spatial RDD with different query predicates. You'd better cache the rawSpatialRDD/indexedRawRDD.</li> </ol>"},{"location":"tutorial/Advanced-Tutorial-Tune-your-Application/#be-aware-of-spatial-rdd-partitions","title":"Be aware of Spatial RDD partitions","text":"<p>Sometimes users complain that the execution time is slow in some cases. As the first step, you should always consider increasing the number of your SpatialRDD partitions (2 - 8 times more than the original number). You can do this when you initialize a SpatialRDD. This may significantly improve your performance.</p> <p>After that, you may consider tuning some other parameters in Apache Spark. For example, you may use Kyro serializer or change the RDD fraction that is cached into memory.</p>"},{"location":"tutorial/benchmark/","title":"Benchmark","text":""},{"location":"tutorial/benchmark/#benchmark","title":"Benchmark","text":"<p>We welcome people to use Sedona for benchmark purpose. To achieve the best performance or enjoy all features of Sedona,</p> <ul> <li>Please always use the latest version or state the version used in your benchmark so that we can trace back to the issues.</li> <li>Please consider using Sedona core instead of Sedona SQL. Due to the limitation of SparkSQL (for instance, not support clustered index), we are not able to expose all features to SparkSQL.</li> <li>Please open Sedona kryo serializer to reduce the memory footprint.</li> </ul>"},{"location":"tutorial/demo/","title":"Scala and Java Examples","text":"<p>Scala and Java Examples contains template projects for Sedona Spark (RDD, SQL and Viz) and Sedona Flink. The template projects have been configured properly.</p> <p>Note that, although the template projects are written in Scala, the same APIs can be  used in Java as well.</p>"},{"location":"tutorial/demo/#folder-structure","title":"Folder structure","text":"<p>The folder structure of this repository is as follows.</p> <ul> <li>spark-rdd-colocation-mining: a scala template shows how to use Sedona RDD API in Spatial Data Mining in Apache Spark</li> <li>spark-sql: a scala template shows how to use Sedona DataFrame and SQL API in Apache Spark</li> <li>spark-viz: a scala template shows how to use Sedona Viz RDD and SQL API in Apache Spark</li> <li>flink-sql: a Java template shows how to use Sedona SQL in Apache Flink</li> </ul>"},{"location":"tutorial/demo/#compile-and-package","title":"Compile and package","text":""},{"location":"tutorial/demo/#prerequisites","title":"Prerequisites","text":"<p>Please make sure you have the following software installed on your local machine:</p> <ul> <li>For Scala: Scala 2.12, SBT</li> <li>For Java: JDK 1.8, Apache Maven 3</li> </ul>"},{"location":"tutorial/demo/#compile","title":"Compile","text":"<p>Run a terminal command <code>sbt assembly</code> within the folder of each template</p>"},{"location":"tutorial/demo/#submit-your-fat-jar-to-spark-or-flink","title":"Submit your fat jar to Spark or Flink","text":"<p>After running the command mentioned above, you are able to see a fat jar in <code>./target</code> folder. Please take it and use <code>./bin/spark-submit</code> or <code>/bin/flink</code>to submit this jar.</p> <p>To run the jar in this way, you need to:</p> <ul> <li> <p>For Spark: either change Spark Master Address in template projects or simply delete it. Currently, they are hard coded to <code>local[*]</code> which means run locally with all cores.</p> </li> <li> <p>Change the dependency packaging scope of Apache Spark from \"compile\" to \"provided\". This is a common packaging strategy in Maven and SBT which means do not package Spark into your fat jar. Otherwise, this may lead to a huge jar and version conflicts!</p> </li> <li> <p>Make sure the dependency versions in build.sbt/pom.xml are consistent with your Spark/Flink version.</p> </li> </ul>"},{"location":"tutorial/demo/#run-template-projects-locally","title":"Run template projects locally","text":"<p>We highly suggest you use IDEs to run template projects on your local machine. For Scala, we recommend IntelliJ IDEA with Scala plug-in. For Java, we recommend IntelliJ IDEA and Eclipse. With the help of IDEs, you don't have to prepare anything (even don't need to download and set up Spark!). As long as you have Scala and Java, everything works properly!</p>"},{"location":"tutorial/demo/#scala","title":"Scala","text":"<p>Import the Scala template project as SBT project. Then run the Main file in this project.</p>"},{"location":"tutorial/demo/#java","title":"Java","text":"<p>Import the Java template project as Maven project. Then run the Main file in this project.</p>"},{"location":"tutorial/geopandas-shapely/","title":"Work with GeoPandas and Shapely","text":"<p>Danger</p> <p>Sedona Python currently only works with Shapely 1.x. If you use GeoPandas, please use &lt;= GeoPandas <code>0.11.1</code>. GeoPandas &gt; 0.11.1 will automatically install Shapely 2.0. If you use Shapely, please use &lt;= <code>1.8.4</code>.</p>"},{"location":"tutorial/geopandas-shapely/#interoperate-with-geopandas","title":"Interoperate with GeoPandas","text":"<p>Sedona Python has implemented serializers and deserializers which allows to convert Sedona Geometry objects into Shapely BaseGeometry objects. Based on that it is possible to load the data with geopandas from file (look at Fiona possible drivers) and create Spark DataFrame based on GeoDataFrame object.</p>"},{"location":"tutorial/geopandas-shapely/#from-geopandas-to-sedona-dataframe","title":"From GeoPandas to Sedona DataFrame","text":"<p>Loading the data from shapefile using geopandas read_file method and create Spark DataFrame based on GeoDataFrame:</p> <pre><code>import geopandas as gpd\nfrom sedona.spark import *\n\nconfig = SedonaContext.builder().\\\n      getOrCreate()\n\nsedona = SedonaContext.create(config)\n\ngdf = gpd.read_file(\"gis_osm_pois_free_1.shp\")\n\nsedona.createDataFrame(\n  gdf\n).show()\n</code></pre> <p>This query will show the following outputs:</p> <pre><code>+---------+----+-----------+--------------------+--------------------+\n|   osm_id|code|     fclass|                name|            geometry|\n+---------+----+-----------+--------------------+--------------------+\n| 26860257|2422|  camp_site|            de Kroon|POINT (15.3393145...|\n| 26860294|2406|     chalet|      Le\u015bne Ustronie|POINT (14.8709625...|\n| 29947493|2402|      motel|                null|POINT (15.0946636...|\n| 29947498|2602|        atm|                null|POINT (15.0732014...|\n| 29947499|2401|      hotel|                null|POINT (15.0696777...|\n| 29947505|2401|      hotel|                null|POINT (15.0155749...|\n+---------+----+-----------+--------------------+--------------------+\n</code></pre>"},{"location":"tutorial/geopandas-shapely/#from-sedona-dataframe-to-geopandas","title":"From Sedona DataFrame to GeoPandas","text":"<p>Reading data with Spark and converting to GeoPandas</p> <p><pre><code>import geopandas as gpd\nfrom sedona.spark import *\n\nconfig = SedonaContext.builder().\n    getOrCreate()\n\nsedona = SedonaContext.create(config)\n\ncounties = sedona.\\\n    read.\\\n    option(\"delimiter\", \"|\").\\\n    option(\"header\", \"true\").\\\n    csv(\"counties.csv\")\n\ncounties.createOrReplaceTempView(\"county\")\n\ncounties_geom = sedona.sql(\n    \"SELECT *, st_geomFromWKT(geom) as geometry from county\"\n)\n\ndf = counties_geom.toPandas()\ngdf = gpd.GeoDataFrame(df, geometry=\"geometry\")\n\ngdf.plot(\n    figsize=(10, 8),\n    column=\"value\",\n    legend=True,\n    cmap='YlOrBr',\n    scheme='quantiles',\n    edgecolor='lightgray'\n)\n</code></pre> </p> <p></p> <p> </p>"},{"location":"tutorial/geopandas-shapely/#interoperate-with-shapely-objects","title":"Interoperate with shapely objects","text":""},{"location":"tutorial/geopandas-shapely/#supported-shapely-objects","title":"Supported Shapely objects","text":"shapely object Available Point MultiPoint LineString MultiLinestring Polygon MultiPolygon <p>To create Spark DataFrame based on mentioned Geometry types, please use  GeometryType  from   sedona.sql.types  module. Converting works for list or tuple with shapely objects.</p> <p>Schema for target table with integer id and geometry type can be defined as follow:</p> <pre><code>from pyspark.sql.types import IntegerType, StructField, StructType\n\nfrom sedona.spark import *\n\nschema = StructType(\n    [\n        StructField(\"id\", IntegerType(), False),\n        StructField(\"geom\", GeometryType(), False)\n    ]\n)\n</code></pre> <p>Also Spark DataFrame with geometry type can be converted to list of shapely objects with  collect  method.</p>"},{"location":"tutorial/geopandas-shapely/#point-example","title":"Point example","text":"<pre><code>from shapely.geometry import Point\n\ndata = [\n    [1, Point(21.0, 52.0)],\n    [1, Point(23.0, 42.0)],\n    [1, Point(26.0, 32.0)]\n]\n\n\ngdf = sedona.createDataFrame(\n    data,\n    schema\n)\n\ngdf.show()\n</code></pre> <pre><code>+---+-------------+\n| id|         geom|\n+---+-------------+\n|  1|POINT (21 52)|\n|  1|POINT (23 42)|\n|  1|POINT (26 32)|\n+---+-------------+\n</code></pre> <pre><code>gdf.printSchema()\n</code></pre> <pre><code>root\n |-- id: integer (nullable = false)\n |-- geom: geometry (nullable = false)\n</code></pre>"},{"location":"tutorial/geopandas-shapely/#multipoint-example","title":"MultiPoint example","text":"<pre><code>from shapely.geometry import MultiPoint\n\ndata = [\n    [1, MultiPoint([[19.511463, 51.765158], [19.446408, 51.779752]])]\n]\n\ngdf = sedona.createDataFrame(\n    data,\n    schema\n).show(1, False)\n</code></pre> <pre><code>+---+---------------------------------------------------------+\n|id |geom                                                     |\n+---+---------------------------------------------------------+\n|1  |MULTIPOINT ((19.511463 51.765158), (19.446408 51.779752))|\n+---+---------------------------------------------------------+\n</code></pre>"},{"location":"tutorial/geopandas-shapely/#linestring-example","title":"LineString example","text":"<pre><code>from shapely.geometry import LineString\n\nline = [(40, 40), (30, 30), (40, 20), (30, 10)]\n\ndata = [\n    [1, LineString(line)]\n]\n\ngdf = sedona.createDataFrame(\n    data,\n    schema\n)\n\ngdf.show(1, False)\n</code></pre> <pre><code>+---+--------------------------------+\n|id |geom                            |\n+---+--------------------------------+\n|1  |LINESTRING (10 10, 20 20, 10 40)|\n+---+--------------------------------+\n</code></pre>"},{"location":"tutorial/geopandas-shapely/#multilinestring-example","title":"MultiLineString example","text":"<pre><code>from shapely.geometry import MultiLineString\n\nline1 = [(10, 10), (20, 20), (10, 40)]\nline2 = [(40, 40), (30, 30), (40, 20), (30, 10)]\n\ndata = [\n    [1, MultiLineString([line1, line2])]\n]\n\ngdf = sedona.createDataFrame(\n    data,\n    schema\n)\n\ngdf.show(1, False)\n</code></pre> <pre><code>+---+---------------------------------------------------------------------+\n|id |geom                                                                 |\n+---+---------------------------------------------------------------------+\n|1  |MULTILINESTRING ((10 10, 20 20, 10 40), (40 40, 30 30, 40 20, 30 10))|\n+---+---------------------------------------------------------------------+\n</code></pre>"},{"location":"tutorial/geopandas-shapely/#polygon-example","title":"Polygon example","text":"<pre><code>from shapely.geometry import Polygon\n\npolygon = Polygon(\n    [\n         [19.51121, 51.76426],\n         [19.51056, 51.76583],\n         [19.51216, 51.76599],\n         [19.51280, 51.76448],\n         [19.51121, 51.76426]\n    ]\n)\n\ndata = [\n    [1, polygon]\n]\n\ngdf = sedona.createDataFrame(\n    data,\n    schema\n)\n\ngdf.show(1, False)\n</code></pre> <pre><code>+---+--------------------------------------------------------------------------------------------------------+\n|id |geom                                                                                                    |\n+---+--------------------------------------------------------------------------------------------------------+\n|1  |POLYGON ((19.51121 51.76426, 19.51056 51.76583, 19.51216 51.76599, 19.5128 51.76448, 19.51121 51.76426))|\n+---+--------------------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"tutorial/geopandas-shapely/#multipolygon-example","title":"MultiPolygon example","text":"<pre><code>from shapely.geometry import MultiPolygon\n\nexterior_p1 = [(0, 0), (0, 2), (2, 2), (2, 0), (0, 0)]\ninterior_p1 = [(1, 1), (1, 1.5), (1.5, 1.5), (1.5, 1), (1, 1)]\n\nexterior_p2 = [(0, 0), (1, 0), (1, 1), (0, 1), (0, 0)]\n\npolygons = [\n    Polygon(exterior_p1, [interior_p1]),\n    Polygon(exterior_p2)\n]\n\ndata = [\n    [1, MultiPolygon(polygons)]\n]\n\ngdf = sedona.createDataFrame(\n    data,\n    schema\n)\n\ngdf.show(1, False)\n</code></pre> <pre><code>+---+----------------------------------------------------------------------------------------------------------+\n|id |geom                                                                                                      |\n+---+----------------------------------------------------------------------------------------------------------+\n|1  |MULTIPOLYGON (((0 0, 0 2, 2 2, 2 0, 0 0), (1 1, 1.5 1, 1.5 1.5, 1 1.5, 1 1)), ((0 0, 0 1, 1 1, 1 0, 0 0)))|\n+---+----------------------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"tutorial/jupyter-notebook/","title":"Python Jupyter Notebook Examples","text":"<p>Click  and play the interactive Sedona Python Jupyter Notebook immediately!</p> <p>Sedona Python provides a number of Jupyter Notebook examples.</p> <p>Please use the following steps to run Jupyter notebook with Pipenv on your machine</p> <ol> <li>Clone Sedona GitHub repo or download the source code</li> <li>Install Sedona Python from PyPI or GitHub source: Read Install Sedona Python to learn.</li> <li>Prepare spark-shaded jar: Read Install Sedona Python to learn.</li> <li>Setup pipenv python version. Please use your desired Python version. <pre><code>cd binder\npipenv --python 3.8\n</code></pre></li> <li>Install dependencies <pre><code>cd binder\npipenv install\n</code></pre></li> <li>Install jupyter notebook kernel for pipenv <pre><code>pipenv install ipykernel\npipenv shell\n</code></pre></li> <li>In the pipenv shell, do <pre><code>python -m ipykernel install --user --name=apache-sedona\n</code></pre></li> <li>Setup environment variables <code>SPARK_HOME</code> and <code>PYTHONPATH</code> if you didn't do it before. Read Install Sedona Python to learn.</li> <li>Launch jupyter notebook: <code>jupyter notebook</code></li> <li>Select Sedona notebook. In your notebook, Kernel -&gt; Change Kernel. Your kernel should now be an option.</li> </ol>"},{"location":"tutorial/python-vector-osm/","title":"Example of spark + sedona + hdfs with slave nodes and OSM vector data consults","text":"<pre><code>from IPython.display import display, HTML\nfrom pyspark.sql import SparkSession\nfrom pyspark import StorageLevel\nimport pandas as pd\nfrom pyspark.sql.types import StructType, StructField,StringType, LongType, IntegerType, DoubleType, ArrayType\nfrom pyspark.sql.functions import regexp_replace\nfrom sedona.register import SedonaRegistrator\nfrom sedona.utils import SedonaKryoRegistrator, KryoSerializer\nfrom pyspark.sql.functions import col, split, expr\nfrom pyspark.sql.functions import udf, lit\nfrom sedona.utils import SedonaKryoRegistrator, KryoSerializer\nfrom pyspark.sql.functions import col, split, expr\nfrom pyspark.sql.functions import udf, lit, flatten\nfrom pywebhdfs.webhdfs import PyWebHdfsClient\nfrom datetime import date\nfrom pyspark.sql.functions import monotonically_increasing_id \nimport json\n</code></pre>"},{"location":"tutorial/python-vector-osm/#registering-spark-session-adding-node-executor-configurations-and-sedona-registrator","title":"Registering spark session, adding node executor configurations and sedona registrator","text":"<pre><code>spark = SparkSession.\\\n    builder.\\\n    appName(\"Overpass-API\").\\\n    enableHiveSupport().\\\n    master(\"local[*]\").\\\n    master(\"spark://spark-master:7077\").\\\n    config(\"spark.executor.memory\", \"15G\").\\\n    config(\"spark.driver.maxResultSize\", \"135G\").\\\n    config(\"spark.sql.shuffle.partitions\", \"500\").\\\n    config(' spark.sql.adaptive.coalescePartitions.enabled', True).\\\n    config('spark.sql.adaptive.enabled', True).\\\n    config('spark.sql.adaptive.coalescePartitions.initialPartitionNum', 125).\\\n    config(\"spark.sql.execution.arrow.pyspark.enabled\", True).\\\n    config(\"spark.sql.execution.arrow.fallback.enabled\", True).\\\n    config('spark.kryoserializer.buffer.max', 2047).\\\n    config(\"spark.serializer\", KryoSerializer.getName).\\\n    config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName).\\\n    config(\"spark.jars.packages\", \"org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.4.0,org.datasyslab:geotools-wrapper:1.4.0-28.2\") .\\\n    enableHiveSupport().\\\n    getOrCreate()\n\nSedonaRegistrator.registerAll(spark)\nsc = spark.sparkContext\n</code></pre>"},{"location":"tutorial/python-vector-osm/#connecting-to-overpass-api-to-search-and-downloading-data-for-saving-into-hdfs","title":"Connecting to Overpass API to search and downloading data for saving into HDFS","text":"<pre><code>import requests\nimport json\n\noverpass_url = \"http://overpass-api.de/api/interpreter\"\noverpass_query = \"\"\"\n[out:json];\narea[name = \"Foz do Igua\u00e7u\"];\nway(area)[\"highway\"~\"\"];\nout geom;\n&gt;;\nout skel qt;\n\"\"\"\n\nresponse = requests.get(overpass_url, \n                         params={'data': overpass_query})\ndata = response.json()\nhdfs = PyWebHdfsClient(host='179.106.229.159',port='50070', user_name='root')\nfile_name = \"foz_roads_osm.json\"\nhdfs.delete_file_dir(file_name)\nhdfs.create_file(file_name, json.dumps(data))\n</code></pre>"},{"location":"tutorial/python-vector-osm/#connecting-spark-sedona-with-saved-hdfs-file","title":"Connecting spark sedona with saved hdfs file","text":"<pre><code>path = \"hdfs://776faf4d6a1e:8020/\"+file_name\ndf = spark.read.json(path, multiLine = \"true\")\n</code></pre>"},{"location":"tutorial/python-vector-osm/#consulting-and-organizing-data-for-analysis","title":"Consulting and organizing data for analysis","text":"<pre><code>from pyspark.sql.functions import explode, arrays_zip\n\ndf.createOrReplaceTempView(\"df\")\ntb = spark.sql(\"select *, size(elements) total_nodes from df\")\ntb.show(5)\n\nisolate_total_nodes = tb.select(\"total_nodes\").toPandas()\ntotal_nodes = isolate_total_nodes[\"total_nodes\"].iloc[0]\nprint(total_nodes)\n\nisolate_ids = tb.select(\"elements.id\").toPandas()\nids = pd.DataFrame(isolate_ids[\"id\"].iloc[0]).drop_duplicates()\nprint(ids[0].iloc[1])\n\nformatted_df = tb\\\n.withColumn(\"id\", explode(\"elements.id\"))\n\nformatted_df.show(5)\n\nformatted_df = tb\\\n.withColumn(\"new\", arrays_zip(\"elements.id\", \"elements.geometry\", \"elements.nodes\", \"elements.tags\"))\\\n.withColumn(\"new\", explode(\"new\"))\n\nformatted_df.show(5)\n\n# formatted_df.printSchema()\n\nformatted_df = formatted_df.select(\"new.0\",\"new.1\",\"new.2\",\"new.3.maxspeed\",\"new.3.incline\",\"new.3.surface\", \"new.3.name\", \"total_nodes\")\nformatted_df = formatted_df.withColumnRenamed(\"0\",\"id\").withColumnRenamed(\"1\",\"geom\").withColumnRenamed(\"2\",\"nodes\").withColumnRenamed(\"3\",\"tags\")\nformatted_df.createOrReplaceTempView(\"formatted_df\")\nformatted_df.show(5)\n# TODO atualizar daqui para baixo para considerar a linha inteira na l\u00f3gica\npoints_tb = spark.sql(\"select geom, id from formatted_df where geom IS NOT NULL\")\npoints_tb = points_tb\\\n.withColumn(\"new\", arrays_zip(\"geom.lat\", \"geom.lon\"))\\\n.withColumn(\"new\", explode(\"new\"))\n\npoints_tb = points_tb.select(\"new.0\",\"new.1\", \"id\")\n\npoints_tb = points_tb.withColumnRenamed(\"0\",\"lat\").withColumnRenamed(\"1\",\"lon\")\npoints_tb.printSchema()\n\npoints_tb.createOrReplaceTempView(\"points_tb\")\n\npoints_tb.show(5)\n\ncoordinates_tb = spark.sql(\"select (select collect_list(CONCAT(p1.lat,',',p1.lon)) from points_tb p1 where p1.id = p2.id group by p1.id) as coordinates, p2.id, p2.maxspeed, p2.incline, p2.surface, p2.name, p2.nodes, p2.total_nodes from formatted_df p2\")\ncoordinates_tb.createOrReplaceTempView(\"coordinates_tb\")\ncoordinates_tb.show(5)\n\nroads_tb = spark.sql(\"SELECT ST_LineStringFromText(REPLACE(REPLACE(CAST(coordinates as string),'[',''),']',''), ',') as geom, id, maxspeed, incline, surface, name, nodes, total_nodes FROM coordinates_tb WHERE coordinates IS NOT NULL\")\nroads_tb.createOrReplaceTempView(\"roads_tb\")\nroads_tb.show(5)\n</code></pre>"},{"location":"tutorial/raster/","title":"Raster SQL app","text":"<p>Starting from <code>v1.1.0</code>, Sedona SQL supports raster data sources and raster operators in DataFrame and SQL. Raster support is available in all Sedona language bindings including Scala, Java, Python and R.</p>"},{"location":"tutorial/raster/#initial-setup","title":"Initial setup","text":"<ol> <li>Set up dependencies</li> <li>Create Sedona config</li> <li>Initiate SedonaContext</li> </ol>"},{"location":"tutorial/raster/#api-docs","title":"API docs","text":"<p>Read raster data in DataFrame</p> <p>Write raster data in DataFrame</p> <p>Raster operators in DataFrame</p>"},{"location":"tutorial/raster/#tutorials","title":"Tutorials","text":"<p>Python Jupyter Notebook</p>"},{"location":"tutorial/raster/#performance","title":"Performance","text":"<p>Storing large raster geometries in Parquet files</p>"},{"location":"tutorial/rdd/","title":"Spatial RDD app","text":"<p>The page outlines the steps to create Spatial RDDs and run spatial queries using Sedona-core.</p>"},{"location":"tutorial/rdd/#set-up-dependencies","title":"Set up dependencies","text":"<p>Please refer to Set up dependencies to set up dependencies.</p>"},{"location":"tutorial/rdd/#create-sedona-config","title":"Create Sedona config","text":"<p>Please refer to Create Sedona config to create a Sedona config.</p>"},{"location":"tutorial/rdd/#initiate-sedonacontext","title":"Initiate SedonaContext","text":"<p>Please refer to Initiate SedonaContext to initiate a SedonaContext.</p>"},{"location":"tutorial/rdd/#create-a-spatialrdd","title":"Create a SpatialRDD","text":""},{"location":"tutorial/rdd/#create-a-typed-spatialrdd","title":"Create a typed SpatialRDD","text":"<p>Sedona-core provides three special SpatialRDDs: PointRDD, PolygonRDD, and LineStringRDD.</p> <p>Warning</p> <p>Typed SpatialRDD has been deprecated for a long time. We do NOT recommend it anymore.</p>"},{"location":"tutorial/rdd/#create-a-generic-spatialrdd","title":"Create a generic SpatialRDD","text":"<p>A generic SpatialRDD is not typed to a certain geometry type and open to more scenarios. It allows an input data file contains mixed types of geometries. For instance, a WKT file contains three types gemetries LineString, Polygon and MultiPolygon.</p>"},{"location":"tutorial/rdd/#from-wktwkb","title":"From WKT/WKB","text":"<p>Geometries in a WKT and WKB file always occupy a single column no matter how many coordinates they have. Sedona provides <code>WktReader</code> and <code>WkbReader</code> to create generic SpatialRDD.</p> <p>Suppose we have a <code>checkin.tsv</code> WKT TSV file at Path <code>/Download/checkin.tsv</code> as follows: <pre><code>POINT (-88.331492 32.324142)    hotel\nPOINT (-88.175933 32.360763)    gas\nPOINT (-88.388954 32.357073)    bar\nPOINT (-88.221102 32.35078) restaurant\n</code></pre> This file has two columns and corresponding offsets(Column IDs) are 0, 1. Column 0 is the WKT string and Column 1 is the checkin business type.</p> <p>Use the following code to create a SpatialRDD</p> ScalaJavaPython <pre><code>val inputLocation = \"/Download/checkin.tsv\"\nval wktColumn = 0 // The WKT string starts from Column 0\nval allowTopologyInvalidGeometries = true // Optional\nval skipSyntaxInvalidGeometries = false // Optional\nval spatialRDD = WktReader.readToGeometryRDD(sedona.sparkContext, inputLocation, wktColumn, allowTopologyInvalidGeometries, skipSyntaxInvalidGeometries)\n</code></pre> <pre><code>String inputLocation = \"/Download/checkin.tsv\"\nint wktColumn = 0 // The WKT string starts from Column 0\nboolean allowTopologyInvalidGeometries = true // Optional\nboolean skipSyntaxInvalidGeometries = false // Optional\nSpatialRDD spatialRDD = WktReader.readToGeometryRDD(sedona.sparkContext, inputLocation, wktColumn, allowTopologyInvalidGeometries, skipSyntaxInvalidGeometries)\n</code></pre> <pre><code>from sedona.core.formatMapper import WktReader\nfrom sedona.core.formatMapper import WkbReader\n\nWktReader.readToGeometryRDD(sc, wkt_geometries_location, 0, True, False)\n\nWkbReader.readToGeometryRDD(sc, wkb_geometries_location, 0, True, False)\n</code></pre>"},{"location":"tutorial/rdd/#from-geojson","title":"From GeoJSON","text":"<p>Geometries in GeoJSON is similar to WKT/WKB. However, a GeoJSON file must be beaked into multiple lines.</p> <p>Suppose we have a <code>polygon.json</code> GeoJSON file at Path <code>/Download/polygon.json</code> as follows:</p> <pre><code>{ \"type\": \"Feature\", \"properties\": { \"STATEFP\": \"01\", \"COUNTYFP\": \"077\", \"TRACTCE\": \"011501\", \"BLKGRPCE\": \"5\", \"AFFGEOID\": \"1500000US010770115015\", \"GEOID\": \"010770115015\", \"NAME\": \"5\", \"LSAD\": \"BG\", \"ALAND\": 6844991, \"AWATER\": 32636 }, \"geometry\": { \"type\": \"Polygon\", \"coordinates\": [ [ [ -87.621765, 34.873444 ], [ -87.617535, 34.873369 ], [ -87.6123, 34.873337 ], [ -87.604049, 34.873303 ], [ -87.604033, 34.872316 ], [ -87.60415, 34.867502 ], [ -87.604218, 34.865687 ], [ -87.604409, 34.858537 ], [ -87.604018, 34.851336 ], [ -87.603716, 34.844829 ], [ -87.603696, 34.844307 ], [ -87.603673, 34.841884 ], [ -87.60372, 34.841003 ], [ -87.603879, 34.838423 ], [ -87.603888, 34.837682 ], [ -87.603889, 34.83763 ], [ -87.613127, 34.833938 ], [ -87.616451, 34.832699 ], [ -87.621041, 34.831431 ], [ -87.621056, 34.831526 ], [ -87.62112, 34.831925 ], [ -87.621603, 34.8352 ], [ -87.62158, 34.836087 ], [ -87.621383, 34.84329 ], [ -87.621359, 34.844438 ], [ -87.62129, 34.846387 ], [ -87.62119, 34.85053 ], [ -87.62144, 34.865379 ], [ -87.621765, 34.873444 ] ] ] } },\n{ \"type\": \"Feature\", \"properties\": { \"STATEFP\": \"01\", \"COUNTYFP\": \"045\", \"TRACTCE\": \"021102\", \"BLKGRPCE\": \"4\", \"AFFGEOID\": \"1500000US010450211024\", \"GEOID\": \"010450211024\", \"NAME\": \"4\", \"LSAD\": \"BG\", \"ALAND\": 11360854, \"AWATER\": 0 }, \"geometry\": { \"type\": \"Polygon\", \"coordinates\": [ [ [ -85.719017, 31.297901 ], [ -85.715626, 31.305203 ], [ -85.714271, 31.307096 ], [ -85.69999, 31.307552 ], [ -85.697419, 31.307951 ], [ -85.675603, 31.31218 ], [ -85.672733, 31.312876 ], [ -85.672275, 31.311977 ], [ -85.67145, 31.310988 ], [ -85.670622, 31.309524 ], [ -85.670729, 31.307622 ], [ -85.669876, 31.30666 ], [ -85.669796, 31.306224 ], [ -85.670356, 31.306178 ], [ -85.671664, 31.305583 ], [ -85.67177, 31.305299 ], [ -85.671878, 31.302764 ], [ -85.671344, 31.302123 ], [ -85.668276, 31.302076 ], [ -85.66566, 31.30093 ], [ -85.665687, 31.30022 ], [ -85.669183, 31.297677 ], [ -85.668703, 31.295638 ], [ -85.671985, 31.29314 ], [ -85.677177, 31.288211 ], [ -85.678452, 31.286376 ], [ -85.679236, 31.28285 ], [ -85.679195, 31.281426 ], [ -85.676865, 31.281049 ], [ -85.674661, 31.28008 ], [ -85.674377, 31.27935 ], [ -85.675714, 31.276882 ], [ -85.677938, 31.275168 ], [ -85.680348, 31.276814 ], [ -85.684032, 31.278848 ], [ -85.684387, 31.279082 ], [ -85.692398, 31.283499 ], [ -85.705032, 31.289718 ], [ -85.706755, 31.290476 ], [ -85.718102, 31.295204 ], [ -85.719132, 31.29689 ], [ -85.719017, 31.297901 ] ] ] } },\n{ \"type\": \"Feature\", \"properties\": { \"STATEFP\": \"01\", \"COUNTYFP\": \"055\", \"TRACTCE\": \"001300\", \"BLKGRPCE\": \"3\", \"AFFGEOID\": \"1500000US010550013003\", \"GEOID\": \"010550013003\", \"NAME\": \"3\", \"LSAD\": \"BG\", \"ALAND\": 1378742, \"AWATER\": 247387 }, \"geometry\": { \"type\": \"Polygon\", \"coordinates\": [ [ [ -86.000685, 34.00537 ], [ -85.998837, 34.009768 ], [ -85.998012, 34.010398 ], [ -85.987865, 34.005426 ], [ -85.986656, 34.004552 ], [ -85.985, 34.002659 ], [ -85.98851, 34.001502 ], [ -85.987567, 33.999488 ], [ -85.988666, 33.99913 ], [ -85.992568, 33.999131 ], [ -85.993144, 33.999714 ], [ -85.994876, 33.995153 ], [ -85.998823, 33.989548 ], [ -85.999925, 33.994237 ], [ -86.000616, 34.000028 ], [ -86.000685, 34.00537 ] ] ] } },\n{ \"type\": \"Feature\", \"properties\": { \"STATEFP\": \"01\", \"COUNTYFP\": \"089\", \"TRACTCE\": \"001700\", \"BLKGRPCE\": \"2\", \"AFFGEOID\": \"1500000US010890017002\", \"GEOID\": \"010890017002\", \"NAME\": \"2\", \"LSAD\": \"BG\", \"ALAND\": 1040641, \"AWATER\": 0 }, \"geometry\": { \"type\": \"Polygon\", \"coordinates\": [ [ [ -86.574172, 34.727375 ], [ -86.562684, 34.727131 ], [ -86.562797, 34.723865 ], [ -86.562957, 34.723168 ], [ -86.562336, 34.719766 ], [ -86.557381, 34.719143 ], [ -86.557352, 34.718322 ], [ -86.559921, 34.717363 ], [ -86.564827, 34.718513 ], [ -86.567582, 34.718565 ], [ -86.570572, 34.718577 ], [ -86.573618, 34.719377 ], [ -86.574172, 34.727375 ] ] ] } },\n</code></pre> <p>Use the following code to create a generic SpatialRDD:</p> ScalaJavaPython <pre><code>val inputLocation = \"/Download/polygon.json\"\nval allowTopologyInvalidGeometries = true // Optional\nval skipSyntaxInvalidGeometries = false // Optional\nval spatialRDD = GeoJsonReader.readToGeometryRDD(sedona.sparkContext, inputLocation, allowTopologyInvalidGeometries, skipSyntaxInvalidGeometries)\n</code></pre> <pre><code>String inputLocation = \"/Download/polygon.json\"\nboolean allowTopologyInvalidGeometries = true // Optional\nboolean skipSyntaxInvalidGeometries = false // Optional\nSpatialRDD spatialRDD = GeoJsonReader.readToGeometryRDD(sedona.sparkContext, inputLocation, allowTopologyInvalidGeometries, skipSyntaxInvalidGeometries)\n</code></pre> <pre><code>from sedona.core.formatMapper import GeoJsonReader\n\nGeoJsonReader.readToGeometryRDD(sc, geo_json_file_location)\n</code></pre> <p>Warning</p> <p>The way that Sedona reads JSON file is different from SparkSQL</p>"},{"location":"tutorial/rdd/#from-shapefile","title":"From Shapefile","text":"ScalaJavaPython <pre><code>val shapefileInputLocation=\"/Download/myshapefile\"\nval spatialRDD = ShapefileReader.readToGeometryRDD(sedona.sparkContext, shapefileInputLocation)\n</code></pre> <pre><code>String shapefileInputLocation=\"/Download/myshapefile\"\nSpatialRDD spatialRDD = ShapefileReader.readToGeometryRDD(sedona.sparkContext, shapefileInputLocation)\n</code></pre> <pre><code>from sedona.core.formatMapper.shapefileParser import ShapefileReader\n\nShapefileReader.readToGeometryRDD(sc, shape_file_location)\n</code></pre> <p>Note</p> <p>The file extensions of .shp, .shx, .dbf must be in lowercase. Assume you have a shape file called myShapefile, the file structure should be like this:</p> <pre><code>- shapefile1\n- shapefile2\n- myshapefile\n    - myshapefile.shp\n    - myshapefile.shx\n    - myshapefile.dbf\n    - myshapefile...\n    - ...\n</code></pre> <p>If the file you are reading contains non-ASCII characters you'll need to explicitly set the encoding via <code>sedona.global.charset</code> system property before creating your Spark context.</p> <p>Example:</p> <pre><code>System.setProperty(\"sedona.global.charset\", \"utf8\")\n\nval sc = new SparkContext(...)\n</code></pre>"},{"location":"tutorial/rdd/#from-sedonasql-dataframe","title":"From SedonaSQL DataFrame","text":"<p>Note</p> <p>More details about SedonaSQL, please read the SedonaSQL tutorial.</p> <p>To create a generic SpatialRDD from CSV, TSV, WKT, WKB and GeoJSON input formats, you can use SedonaSQL.</p> <p>We use checkin.csv CSV file as the example. You can create a generic SpatialRDD using the following steps:</p> <ol> <li>Load data in SedonaSQL. <pre><code>var df = sedona.read.format(\"csv\").option(\"header\", \"false\").load(csvPointInputLocation)\ndf.createOrReplaceTempView(\"inputtable\")\n</code></pre></li> <li>Create a Geometry type column in SedonaSQL <pre><code>var spatialDf = sedona.sql(\n\"\"\"\n        |SELECT ST_Point(CAST(inputtable._c0 AS Decimal(24,20)),CAST(inputtable._c1 AS Decimal(24,20))) AS checkin\n        |FROM inputtable\n    \"\"\".stripMargin)\n</code></pre></li> <li>Use SedonaSQL DataFrame-RDD Adapter to convert a DataFrame to an SpatialRDD <pre><code>var spatialRDD = Adapter.toSpatialRdd(spatialDf, \"checkin\")\n</code></pre></li> </ol> <p>\"checkin\" is the name of the geometry column</p> <p>For WKT/WKB/GeoJSON data, please use ST_GeomFromWKT / ST_GeomFromWKB / ST_GeomFromGeoJSON instead.</p>"},{"location":"tutorial/rdd/#transform-the-coordinate-reference-system","title":"Transform the Coordinate Reference System","text":"<p>Sedona doesn't control the coordinate unit (degree-based or meter-based) of all geometries in an SpatialRDD. The unit of all related distances in Sedona is same as the unit of all geometries in an SpatialRDD.</p> <p>To convert Coordinate Reference System of an SpatialRDD, use the following code:</p> ScalaJavaPython <pre><code>val sourceCrsCode = \"epsg:4326\" // WGS84, the most common degree-based CRS\nval targetCrsCode = \"epsg:3857\" // The most common meter-based CRS\nobjectRDD.CRSTransform(sourceCrsCode, targetCrsCode, false)\n</code></pre> <pre><code>String sourceCrsCode = \"epsg:4326\" // WGS84, the most common degree-based CRS\nString targetCrsCode = \"epsg:3857\" // The most common meter-based CRS\nobjectRDD.CRSTransform(sourceCrsCode, targetCrsCode, false)\n</code></pre> <pre><code>sourceCrsCode = \"epsg:4326\" // WGS84, the most common degree-based CRS\ntargetCrsCode = \"epsg:3857\" // The most common meter-based CRS\nobjectRDD.CRSTransform(sourceCrsCode, targetCrsCode, False)\n</code></pre> <p><code>false</code> in CRSTransform(sourceCrsCode, targetCrsCode, false) means that it will not tolerate Datum shift. If you want it to be lenient, use <code>true</code> instead.</p> <p>Warning</p> <p>CRS transformation should be done right after creating each SpatialRDD, otherwise it will lead to wrong query results. For instance, use something like this:</p> ScalaJavaPython <pre><code>val objectRDD = WktReader.readToGeometryRDD(sedona.sparkContext, inputLocation, wktColumn, allowTopologyInvalidGeometries, skipSyntaxInvalidGeometries)\nobjectRDD.CRSTransform(\"epsg:4326\", \"epsg:3857\", false)\n</code></pre> <pre><code>SpatialRDD objectRDD = WktReader.readToGeometryRDD(sedona.sparkContext, inputLocation, wktColumn, allowTopologyInvalidGeometries, skipSyntaxInvalidGeometries)\nobjectRDD.CRSTransform(\"epsg:4326\", \"epsg:3857\", false)\n</code></pre> <pre><code>objectRDD = WktReader.readToGeometryRDD(sedona.sparkContext, inputLocation, wktColumn, allowTopologyInvalidGeometries, skipSyntaxInvalidGeometries)\nobjectRDD.CRSTransform(\"epsg:4326\", \"epsg:3857\", False)\n</code></pre> <p>The details CRS information can be found on EPSG.io</p>"},{"location":"tutorial/rdd/#read-other-attributes-in-an-spatialrdd","title":"Read other attributes in an SpatialRDD","text":"<p>Each SpatialRDD can carry non-spatial attributes such as price, age and name.</p> <p>The other attributes are combined together to a string and stored in UserData field of each geometry.</p> <p>To retrieve the UserData field, use the following code:</p> ScalaJavaPython <pre><code>val rddWithOtherAttributes = objectRDD.rawSpatialRDD.rdd.map[String](f=&gt;f.getUserData.asInstanceOf[String])\n</code></pre> <pre><code>SpatialRDD&lt;Geometry&gt; spatialRDD = Adapter.toSpatialRdd(spatialDf, \"arealandmark\");\nspatialRDD.rawSpatialRDD.map(obj -&gt; {return obj.getUserData();});\n</code></pre> <pre><code>rdd_with_other_attributes = object_rdd.rawSpatialRDD.map(lambda x: x.getUserData())\n</code></pre>"},{"location":"tutorial/rdd/#write-a-spatial-range-query","title":"Write a Spatial Range Query","text":"<p>A spatial range query takes as input a range query window and an SpatialRDD and returns all geometries that have specified relationship with the query window.</p> <p>Assume you now have an SpatialRDD (typed or generic). You can use the following code to issue an Spatial Range Query on it.</p> <p>spatialPredicate can be set to <code>SpatialPredicate.INTERSECTS</code> to return all geometries intersect with query window. Supported spatial predicates are:</p> <ul> <li><code>CONTAINS</code>: geometry is completely inside the query window</li> <li><code>INTERSECTS</code>: geometry have at least one point in common with the query window</li> <li><code>WITHIN</code>: geometry is completely within the query window (no touching edges)</li> <li><code>COVERS</code>: query window has no point outside of the geometry</li> <li><code>COVERED_BY</code>: geometry has no point outside of the query window</li> <li><code>OVERLAPS</code>: geometry and the query window spatially overlap</li> <li><code>CROSSES</code>: geometry and the query window spatially cross</li> <li><code>TOUCHES</code>: the only points shared between geometry and the query window are on the boundary of geometry and the query window</li> <li><code>EQUALS</code>: geometry and the query window are spatially equal</li> </ul> <p>Note</p> <p>Spatial range query is equivalent with a SELECT query with spatial predicate as search condition in Spatial SQL. An example query is as follows: <pre><code>SELECT *\nFROM checkin\nWHERE ST_Intersects(checkin.location, queryWindow)\n</code></pre></p> ScalaJavaPython <pre><code>val rangeQueryWindow = new Envelope(-90.01, -80.01, 30.01, 40.01)\nval spatialPredicate = SpatialPredicate.COVERED_BY // Only return gemeotries fully covered by the window\nval usingIndex = false\nvar queryResult = RangeQuery.SpatialRangeQuery(spatialRDD, rangeQueryWindow, spatialPredicate, usingIndex)\n</code></pre> <pre><code>Envelope rangeQueryWindow = new Envelope(-90.01, -80.01, 30.01, 40.01)\nSpatialPredicate spatialPredicate = SpatialPredicate.COVERED_BY // Only return gemeotries fully covered by the window\nboolean usingIndex = false\nJavaRDD queryResult = RangeQuery.SpatialRangeQuery(spatialRDD, rangeQueryWindow, spatialPredicate, usingIndex)\n</code></pre> <pre><code>from sedona.core.geom.envelope import Envelope\nfrom sedona.core.spatialOperator import RangeQuery\n\nrange_query_window = Envelope(-90.01, -80.01, 30.01, 40.01)\nconsider_boundary_intersection = False  ## Only return gemeotries fully covered by the window\nusing_index = False\nquery_result = RangeQuery.SpatialRangeQuery(spatial_rdd, range_query_window, consider_boundary_intersection, using_index)\n</code></pre> <p>Note</p> <p>Sedona Python users: Please use RangeQueryRaw from the same module if you want to avoid jvm python serde while converting to Spatial DataFrame. It takes the same parameters as RangeQuery but returns reference to jvm rdd which can be converted to dataframe without python - jvm serde using Adapter.</p> <p>Example: <pre><code>from sedona.core.geom.envelope import Envelope\nfrom sedona.core.spatialOperator import RangeQueryRaw\nfrom sedona.utils.adapter import Adapter\n\nrange_query_window = Envelope(-90.01, -80.01, 30.01, 40.01)\nconsider_boundary_intersection = False  ## Only return gemeotries fully covered by the window\nusing_index = False\nquery_result = RangeQueryRaw.SpatialRangeQuery(spatial_rdd, range_query_window, consider_boundary_intersection, using_index)\ngdf = Adapter.toDf(query_result, spark, [\"col1\", ..., \"coln\"])\n</code></pre></p>"},{"location":"tutorial/rdd/#range-query-window","title":"Range query window","text":"<p>Besides the rectangle (Envelope) type range query window, Sedona range query window can be Point/Polygon/LineString.</p> <p>The code to create a point, linestring (4 vertexes) and polygon (4 vertexes) is as follows:</p> ScalaJavaPython <pre><code>val geometryFactory = new GeometryFactory()\nval pointObject = geometryFactory.createPoint(new Coordinate(-84.01, 34.01))\n\nval geometryFactory = new GeometryFactory()\nval coordinates = new Array[Coordinate](5)\ncoordinates(0) = new Coordinate(0,0)\ncoordinates(1) = new Coordinate(0,4)\ncoordinates(2) = new Coordinate(4,4)\ncoordinates(3) = new Coordinate(4,0)\ncoordinates(4) = coordinates(0) // The last coordinate is the same as the first coordinate in order to compose a closed ring\nval polygonObject = geometryFactory.createPolygon(coordinates)\n\nval geometryFactory = new GeometryFactory()\nval coordinates = new Array[Coordinate](4)\ncoordinates(0) = new Coordinate(0,0)\ncoordinates(1) = new Coordinate(0,4)\ncoordinates(2) = new Coordinate(4,4)\ncoordinates(3) = new Coordinate(4,0)\nval linestringObject = geometryFactory.createLineString(coordinates)\n</code></pre> <pre><code>GeometryFactory geometryFactory = new GeometryFactory()\nPoint pointObject = geometryFactory.createPoint(new Coordinate(-84.01, 34.01))\n\nGeometryFactory geometryFactory = new GeometryFactory()\nCoordinate[] coordinates = new Array[Coordinate](5)\ncoordinates(0) = new Coordinate(0,0)\ncoordinates(1) = new Coordinate(0,4)\ncoordinates(2) = new Coordinate(4,4)\ncoordinates(3) = new Coordinate(4,0)\ncoordinates(4) = coordinates(0) // The last coordinate is the same as the first coordinate in order to compose a closed ring\nPolygon polygonObject = geometryFactory.createPolygon(coordinates)\n\nGeometryFactory geometryFactory = new GeometryFactory()\nval coordinates = new Array[Coordinate](4)\ncoordinates(0) = new Coordinate(0,0)\ncoordinates(1) = new Coordinate(0,4)\ncoordinates(2) = new Coordinate(4,4)\ncoordinates(3) = new Coordinate(4,0)\nLineString linestringObject = geometryFactory.createLineString(coordinates)\n</code></pre> <p>A Shapely geometry can be used as a query window. To create shapely geometries, please follow Shapely official docs</p>"},{"location":"tutorial/rdd/#use-spatial-indexes","title":"Use spatial indexes","text":"<p>Sedona provides two types of spatial indexes, Quad-Tree and R-Tree. Once you specify an index type, Sedona will build a local tree index on each of the SpatialRDD partition.</p> <p>To utilize a spatial index in a spatial range query, use the following code:</p> ScalaJavaPython <pre><code>val rangeQueryWindow = new Envelope(-90.01, -80.01, 30.01, 40.01)\nval spatialPredicate = SpatialPredicate.COVERED_BY // Only return gemeotries fully covered by the window\n\nval buildOnSpatialPartitionedRDD = false // Set to TRUE only if run join query\nspatialRDD.buildIndex(IndexType.QUADTREE, buildOnSpatialPartitionedRDD)\n\nval usingIndex = true\nvar queryResult = RangeQuery.SpatialRangeQuery(spatialRDD, rangeQueryWindow, spatialPredicate, usingIndex)\n</code></pre> <pre><code>Envelope rangeQueryWindow = new Envelope(-90.01, -80.01, 30.01, 40.01)\nSpatialPredicate spatialPredicate = SpatialPredicate.COVERED_BY // Only return gemeotries fully covered by the window\n\nboolean buildOnSpatialPartitionedRDD = false // Set to TRUE only if run join query\nspatialRDD.buildIndex(IndexType.QUADTREE, buildOnSpatialPartitionedRDD)\n\nboolean usingIndex = true\nJavaRDD queryResult = RangeQuery.SpatialRangeQuery(spatialRDD, rangeQueryWindow, spatialPredicate, usingIndex)\n</code></pre> <pre><code>from sedona.core.geom.envelope import Envelope\nfrom sedona.core.enums import IndexType\nfrom sedona.core.spatialOperator import RangeQuery\n\nrange_query_window = Envelope(-90.01, -80.01, 30.01, 40.01)\nconsider_boundary_intersection = False ## Only return gemeotries fully covered by the window\n\nbuild_on_spatial_partitioned_rdd = False ## Set to TRUE only if run join query\nspatial_rdd.buildIndex(IndexType.QUADTREE, build_on_spatial_partitioned_rdd)\n\nusing_index = True\n\nquery_result = RangeQuery.SpatialRangeQuery(\n    spatial_rdd,\n    range_query_window,\n    consider_boundary_intersection,\n    using_index\n)\n</code></pre> <p>Tip</p> <p>Using an index might not be the best choice all the time because building index also takes time. A spatial index is very useful when your data is complex polygons and line strings.</p>"},{"location":"tutorial/rdd/#output-format","title":"Output format","text":"Scala/JavaPython <p>The output format of the spatial range query is another SpatialRDD.</p> <p>The output format of the spatial range query is another RDD which consists of GeoData objects.</p> <p>SpatialRangeQuery result can be used as RDD with map or other spark RDD functions. Also it can be used as  Python objects when using collect method. Example:</p> <pre><code>query_result.map(lambda x: x.geom.length).collect()\n</code></pre> <pre><code>[\n 1.5900840000000045,\n 1.5906639999999896,\n 1.1110299999999995,\n 1.1096700000000084,\n 1.1415619999999933,\n 1.1386399999999952,\n 1.1415619999999933,\n 1.1418860000000137,\n 1.1392780000000045,\n ...\n]\n</code></pre> <p>Or transformed to GeoPandas GeoDataFrame</p> <pre><code>import geopandas as gpd\ngpd.GeoDataFrame(\n    query_result.map(lambda x: [x.geom, x.userData]).collect(),\n    columns=[\"geom\", \"user_data\"],\n    geometry=\"geom\"\n)\n</code></pre>"},{"location":"tutorial/rdd/#write-a-spatial-knn-query","title":"Write a Spatial KNN Query","text":"<p>A spatial K Nearnest Neighbor query takes as input a K, a query point and an SpatialRDD and finds the K geometries in the RDD which are the closest to he query point.</p> <p>Assume you now have an SpatialRDD (typed or generic). You can use the following code to issue an Spatial KNN Query on it.</p> ScalaJavaPython <pre><code>val geometryFactory = new GeometryFactory()\nval pointObject = geometryFactory.createPoint(new Coordinate(-84.01, 34.01))\nval K = 1000 // K Nearest Neighbors\nval usingIndex = false\nval result = KNNQuery.SpatialKnnQuery(objectRDD, pointObject, K, usingIndex)\n</code></pre> <pre><code>GeometryFactory geometryFactory = new GeometryFactory()\nPoint pointObject = geometryFactory.createPoint(new Coordinate(-84.01, 34.01))\nint K = 1000 // K Nearest Neighbors\nboolean usingIndex = false\nJavaRDD result = KNNQuery.SpatialKnnQuery(objectRDD, pointObject, K, usingIndex)\n</code></pre> <pre><code>from sedona.core.spatialOperator import KNNQuery\nfrom shapely.geometry import Point\n\npoint = Point(-84.01, 34.01)\nk = 1000 ## K Nearest Neighbors\nusing_index = False\nresult = KNNQuery.SpatialKnnQuery(object_rdd, point, k, using_index)\n</code></pre> <p>Note</p> <p>Spatial KNN query that returns 5 Nearest Neighbors is equal to the following statement in Spatial SQL <pre><code>SELECT ck.name, ck.rating, ST_Distance(ck.location, myLocation) AS distance\nFROM checkins ck\nORDER BY distance DESC\nLIMIT 5\n</code></pre></p>"},{"location":"tutorial/rdd/#query-center-geometry","title":"Query center geometry","text":"<p>Besides the Point type, Sedona KNN query center can be Polygon and LineString.</p> Scala/JavaPython <p>To learn how to create Polygon and LineString object, see Range query window.</p> <p>To create Polygon or Linestring object please follow Shapely official docs</p>"},{"location":"tutorial/rdd/#use-spatial-indexes_1","title":"Use spatial indexes","text":"<p>To utilize a spatial index in a spatial KNN query, use the following code:</p> ScalaJavaPython <pre><code>val geometryFactory = new GeometryFactory()\nval pointObject = geometryFactory.createPoint(new Coordinate(-84.01, 34.01))\nval K = 1000 // K Nearest Neighbors\n\n\nval buildOnSpatialPartitionedRDD = false // Set to TRUE only if run join query\nobjectRDD.buildIndex(IndexType.RTREE, buildOnSpatialPartitionedRDD)\n\nval usingIndex = true\nval result = KNNQuery.SpatialKnnQuery(objectRDD, pointObject, K, usingIndex)\n</code></pre> <pre><code>GeometryFactory geometryFactory = new GeometryFactory()\nPoint pointObject = geometryFactory.createPoint(new Coordinate(-84.01, 34.01))\nval K = 1000 // K Nearest Neighbors\n\n\nboolean buildOnSpatialPartitionedRDD = false // Set to TRUE only if run join query\nobjectRDD.buildIndex(IndexType.RTREE, buildOnSpatialPartitionedRDD)\n\nboolean usingIndex = true\nJavaRDD result = KNNQuery.SpatialKnnQuery(objectRDD, pointObject, K, usingIndex)\n</code></pre> <pre><code>from sedona.core.spatialOperator import KNNQuery\nfrom sedona.core.enums import IndexType\nfrom shapely.geometry import Point\n\npoint = Point(-84.01, 34.01)\nk = 5 ## K Nearest Neighbors\n\nbuild_on_spatial_partitioned_rdd = False ## Set to TRUE only if run join query\nspatial_rdd.buildIndex(IndexType.RTREE, build_on_spatial_partitioned_rdd)\n\nusing_index = True\nresult = KNNQuery.SpatialKnnQuery(spatial_rdd, point, k, using_index)\n</code></pre> <p>Warning</p> <p>Only R-Tree index supports Spatial KNN query</p>"},{"location":"tutorial/rdd/#output-format_1","title":"Output format","text":"Scala/JavaPython <p>The output format of the spatial KNN query is a list of geometries. The list has K geometry objects.</p> <p>The output format of the spatial KNN query is a list of GeoData objects.  The list has K GeoData objects.</p> <p>Example: <pre><code>&gt;&gt; result\n\n[GeoData, GeoData, GeoData, GeoData, GeoData]\n</code></pre></p>"},{"location":"tutorial/rdd/#write-a-spatial-join-query","title":"Write a Spatial Join Query","text":"<p>A spatial join query takes as input two Spatial RDD A and B. For each geometry in A, finds the geometries (from B) covered/intersected by it. A and B can be any geometry type and are not necessary to have the same geometry type.</p> <p>Assume you now have two SpatialRDDs (typed or generic). You can use the following code to issue an Spatial Join Query on them.</p> ScalaJavaPython <pre><code>val spatialPredicate = SpatialPredicate.COVERED_BY // Only return gemeotries fully covered by each query window in queryWindowRDD\nval usingIndex = false\n\nobjectRDD.analyze()\n\nobjectRDD.spatialPartitioning(GridType.KDBTREE)\nqueryWindowRDD.spatialPartitioning(objectRDD.getPartitioner)\n\nval result = JoinQuery.SpatialJoinQuery(objectRDD, queryWindowRDD, usingIndex, spatialPredicate)\n</code></pre> <pre><code>SpatialPredicate spatialPredicate = SpatialPredicate.COVERED_BY // Only return gemeotries fully covered by each query window in queryWindowRDD\nval usingIndex = false\n\nobjectRDD.analyze()\n\nobjectRDD.spatialPartitioning(GridType.KDBTREE)\nqueryWindowRDD.spatialPartitioning(objectRDD.getPartitioner)\n\nJavaPairRDD result = JoinQuery.SpatialJoinQuery(objectRDD, queryWindowRDD, usingIndex, spatialPredicate)\n</code></pre> <pre><code>from sedona.core.enums import GridType\nfrom sedona.core.spatialOperator import JoinQuery\n\nconsider_boundary_intersection = False ## Only return geometries fully covered by each query window in queryWindowRDD\nusing_index = False\n\nobject_rdd.analyze()\n\nobject_rdd.spatialPartitioning(GridType.KDBTREE)\nquery_window_rdd.spatialPartitioning(object_rdd.getPartitioner())\n\nresult = JoinQuery.SpatialJoinQuery(object_rdd, query_window_rdd, using_index, consider_boundary_intersection)\n</code></pre> <p>Note</p> <p>Spatial join query is equal to the following query in Spatial SQL: <pre><code>SELECT superhero.name\nFROM city, superhero\nWHERE ST_Contains(city.geom, superhero.geom);\n</code></pre> Find the superheroes in each city</p>"},{"location":"tutorial/rdd/#use-spatial-partitioning","title":"Use spatial partitioning","text":"<p>Sedona spatial partitioning method can significantly speed up the join query. Three spatial partitioning methods are available: KDB-Tree, Quad-Tree and R-Tree. Two SpatialRDD must be partitioned by the same way.</p> <p>If you first partition SpatialRDD A, then you must use the partitioner of A to partition B.</p> Scala/JavaPython <pre><code>objectRDD.spatialPartitioning(GridType.KDBTREE)\nqueryWindowRDD.spatialPartitioning(objectRDD.getPartitioner)\n</code></pre> <pre><code>object_rdd.spatialPartitioning(GridType.KDBTREE)\nquery_window_rdd.spatialPartitioning(object_rdd.getPartitioner())\n</code></pre> <p>Or </p> Scala/JavaPython <pre><code>queryWindowRDD.spatialPartitioning(GridType.KDBTREE)\nobjectRDD.spatialPartitioning(queryWindowRDD.getPartitioner)\n</code></pre> <pre><code>query_window_rdd.spatialPartitioning(GridType.KDBTREE)\nobject_rdd.spatialPartitioning(query_window_rdd.getPartitioner())\n</code></pre>"},{"location":"tutorial/rdd/#use-spatial-indexes_2","title":"Use spatial indexes","text":"<p>To utilize a spatial index in a spatial join query, use the following code:</p> ScalaJavaPython <pre><code>objectRDD.spatialPartitioning(joinQueryPartitioningType)\nqueryWindowRDD.spatialPartitioning(objectRDD.getPartitioner)\n\nval buildOnSpatialPartitionedRDD = true // Set to TRUE only if run join query\nval usingIndex = true\nqueryWindowRDD.buildIndex(IndexType.QUADTREE, buildOnSpatialPartitionedRDD)\n\nval result = JoinQuery.SpatialJoinQueryFlat(objectRDD, queryWindowRDD, usingIndex, spatialPredicate)\n</code></pre> <pre><code>objectRDD.spatialPartitioning(joinQueryPartitioningType)\nqueryWindowRDD.spatialPartitioning(objectRDD.getPartitioner)\n\nboolean buildOnSpatialPartitionedRDD = true // Set to TRUE only if run join query\nboolean usingIndex = true\nqueryWindowRDD.buildIndex(IndexType.QUADTREE, buildOnSpatialPartitionedRDD)\n\nJavaPairRDD result = JoinQuery.SpatialJoinQueryFlat(objectRDD, queryWindowRDD, usingIndex, spatialPredicate)\n</code></pre> <pre><code>from sedona.core.enums import GridType\nfrom sedona.core.enums import IndexType\nfrom sedona.core.spatialOperator import JoinQuery\n\nobject_rdd.spatialPartitioning(GridType.KDBTREE)\nquery_window_rdd.spatialPartitioning(object_rdd.getPartitioner())\n\nbuild_on_spatial_partitioned_rdd = True ## Set to TRUE only if run join query\nusing_index = True\nquery_window_rdd.buildIndex(IndexType.QUADTREE, build_on_spatial_partitioned_rdd)\n\nresult = JoinQuery.SpatialJoinQueryFlat(object_rdd, query_window_rdd, using_index, True)\n</code></pre> <p>The index should be built on either one of two SpatialRDDs. In general, you should build it on the larger SpatialRDD.</p>"},{"location":"tutorial/rdd/#output-format_2","title":"Output format","text":"Scala/JavaPython <p>The output format of the spatial join query is a PairRDD. In this PairRDD, each object is a pair of two geometries. The left one is the geometry from objectRDD and the right one is the geometry from the queryWindowRDD.</p> <pre><code>Point,Polygon\nPoint,Polygon\nPoint,Polygon\nPolygon,Polygon\nLineString,LineString\nPolygon,LineString\n...\n</code></pre> <p>Each object on the left is covered/intersected by the object on the right.</p> <p>Result for this query is RDD which holds two GeoData objects within list of lists. Example: <pre><code>result.collect()\n</code></pre></p> <pre><code>[[GeoData, GeoData], [GeoData, GeoData] ...]\n</code></pre> <p>It is possible to do some RDD operation on result data ex. Getting polygon centroid. <pre><code>result.map(lambda x: x[0].geom.centroid).collect()\n</code></pre></p> <p>Note</p> <p>Sedona Python users: Please use JoinQueryRaw from the same module for methods </p> <ul> <li> <p>spatialJoin</p> </li> <li> <p>DistanceJoinQueryFlat</p> </li> <li> <p>SpatialJoinQueryFlat</p> </li> </ul> <p>For better performance while converting to dataframe with adapter.  That approach allows to avoid costly serialization between Python  and jvm and in result operating on python object instead of native geometries.</p> <p>Example: <pre><code>from sedona.core.SpatialRDD import CircleRDD\nfrom sedona.core.enums import GridType\nfrom sedona.core.spatialOperator import JoinQueryRaw\n\nobject_rdd.analyze()\n\ncircle_rdd = CircleRDD(object_rdd, 0.1) ## Create a CircleRDD using the given distance\ncircle_rdd.analyze()\n\ncircle_rdd.spatialPartitioning(GridType.KDBTREE)\nspatial_rdd.spatialPartitioning(circle_rdd.getPartitioner())\n\nconsider_boundary_intersection = False ## Only return gemeotries fully covered by each query window in queryWindowRDD\nusing_index = False\n\nresult = JoinQueryRaw.DistanceJoinQueryFlat(spatial_rdd, circle_rdd, using_index, consider_boundary_intersection)\n\ngdf = Adapter.toDf(result, [\"left_col1\", ..., \"lefcoln\"], [\"rightcol1\", ..., \"rightcol2\"], spark)\n</code></pre></p>"},{"location":"tutorial/rdd/#write-a-distance-join-query","title":"Write a Distance Join Query","text":"<p>Warning</p> <p>RDD distance joins are only reliable for points. For other geometry types, please use Spatial SQL.</p> <p>A distance join query takes as input two Spatial RDD A and B and a distance. For each geometry in A, finds the geometries (from B) are within the given distance to it. A and B can be any geometry type and are not necessary to have the same geometry type. The unit of the distance is explained here.</p> <p>If you don't want to transform your data and are ok with sacrificing the query accuracy, you can use an approximate degree value for distance. Please use this calculator.</p> <p>Assume you now have two SpatialRDDs (typed or generic). You can use the following code to issue an Distance Join Query on them.</p> ScalaJavaPython <pre><code>objectRddA.analyze()\n\nval circleRDD = new CircleRDD(objectRddA, 0.1) // Create a CircleRDD using the given distance\n\ncircleRDD.spatialPartitioning(GridType.KDBTREE)\nobjectRddB.spatialPartitioning(circleRDD.getPartitioner)\n\nval spatialPredicate = SpatialPredicate.COVERED_BY // Only return gemeotries fully covered by each query window in queryWindowRDD\nval usingIndex = false\n\nval result = JoinQuery.DistanceJoinQueryFlat(objectRddB, circleRDD, usingIndex, spatialPredicate)\n</code></pre> <pre><code>objectRddA.analyze()\n\nCircleRDD circleRDD = new CircleRDD(objectRddA, 0.1) // Create a CircleRDD using the given distance\n\ncircleRDD.spatialPartitioning(GridType.KDBTREE)\nobjectRddB.spatialPartitioning(circleRDD.getPartitioner)\n\nSpatialPredicate spatialPredicate = SpatialPredicate.COVERED_BY // Only return gemeotries fully covered by each query window in queryWindowRDD\nboolean usingIndex = false\n\nJavaPairRDD result = JoinQuery.DistanceJoinQueryFlat(objectRddB, circleRDD, usingIndex, spatialPredicate)\n</code></pre> <pre><code>from sedona.core.SpatialRDD import CircleRDD\nfrom sedona.core.enums import GridType\nfrom sedona.core.spatialOperator import JoinQuery\n\nobject_rdd.analyze()\n\ncircle_rdd = CircleRDD(object_rdd, 0.1) ## Create a CircleRDD using the given distance\ncircle_rdd.analyze()\n\ncircle_rdd.spatialPartitioning(GridType.KDBTREE)\nspatial_rdd.spatialPartitioning(circle_rdd.getPartitioner())\n\nconsider_boundary_intersection = False ## Only return gemeotries fully covered by each query window in queryWindowRDD\nusing_index = False\n\nresult = JoinQuery.DistanceJoinQueryFlat(spatial_rdd, circle_rdd, using_index, consider_boundary_intersection)\n</code></pre> <p>Distance join can only accept <code>COVERED_BY</code> and <code>INTERSECTS</code> as spatial predicates. The rest part of the join query is same as the spatial join query.</p> <p>The details of spatial partitioning in join query is here.</p> <p>The details of using spatial indexes in join query is here.</p> <p>The output format of the distance join query is here.</p> <p>Note</p> <p>Distance join query is equal to the following query in Spatial SQL: <pre><code>SELECT superhero.name\nFROM city, superhero\nWHERE ST_Distance(city.geom, superhero.geom) &lt;= 10;\n</code></pre> Find the superheroes within 10 miles of each city</p>"},{"location":"tutorial/rdd/#save-to-permanent-storage","title":"Save to permanent storage","text":"<p>You can always save an SpatialRDD back to some permanent storage such as HDFS and Amazon S3. You can save distributed SpatialRDD to WKT, GeoJSON and object files.</p> <p>Note</p> <p>Non-spatial attributes such as price, age and name will also be stored to permanent storage.</p>"},{"location":"tutorial/rdd/#save-an-spatialrdd-not-indexed","title":"Save an SpatialRDD (not indexed)","text":"<p>Typed SpatialRDD and generic SpatialRDD can be saved to permanent storage.</p>"},{"location":"tutorial/rdd/#save-to-distributed-wkt-text-file","title":"Save to distributed WKT text file","text":"<p>Use the following code to save an SpatialRDD as a distributed WKT text file:</p> <pre><code>objectRDD.rawSpatialRDD.saveAsTextFile(\"hdfs://PATH\")\nobjectRDD.saveAsWKT(\"hdfs://PATH\")\n</code></pre>"},{"location":"tutorial/rdd/#save-to-distributed-wkb-text-file","title":"Save to distributed WKB text file","text":"<p>Use the following code to save an SpatialRDD as a distributed WKB text file:</p> <pre><code>objectRDD.saveAsWKB(\"hdfs://PATH\")\n</code></pre>"},{"location":"tutorial/rdd/#save-to-distributed-geojson-text-file","title":"Save to distributed GeoJSON text file","text":"<p>Use the following code to save an SpatialRDD as a distributed GeoJSON text file:</p> <pre><code>objectRDD.saveAsGeoJSON(\"hdfs://PATH\")\n</code></pre>"},{"location":"tutorial/rdd/#save-to-distributed-object-file","title":"Save to distributed object file","text":"<p>Use the following code to save an SpatialRDD as a distributed object file:</p> Scala/JavaPython <pre><code>objectRDD.rawSpatialRDD.saveAsObjectFile(\"hdfs://PATH\")\n</code></pre> <pre><code>object_rdd.rawJvmSpatialRDD.saveAsObjectFile(\"hdfs://PATH\")\n</code></pre> <p>Note</p> <p>Each object in a distributed object file is a byte array (not human-readable). This byte array is the serialized format of a Geometry or a SpatialIndex.</p>"},{"location":"tutorial/rdd/#save-an-spatialrdd-indexed","title":"Save an SpatialRDD (indexed)","text":"<p>Indexed typed SpatialRDD and generic SpatialRDD can be saved to permanent storage. However, the indexed SpatialRDD has to be stored as a distributed object file.</p>"},{"location":"tutorial/rdd/#save-to-distributed-object-file_1","title":"Save to distributed object file","text":"<p>Use the following code to save an SpatialRDD as a distributed object file:</p> <pre><code>objectRDD.indexedRawRDD.saveAsObjectFile(\"hdfs://PATH\")\n</code></pre>"},{"location":"tutorial/rdd/#save-an-spatialrdd-spatialpartitioned-wo-indexed","title":"Save an SpatialRDD (spatialPartitioned W/O indexed)","text":"<p>A spatial partitioned RDD can be saved to permanent storage but Spark is not able to maintain the same RDD partition Id of the original RDD. This will lead to wrong join query results. We are working on some solutions. Stay tuned!</p>"},{"location":"tutorial/rdd/#reload-a-saved-spatialrdd","title":"Reload a saved SpatialRDD","text":"<p>You can easily reload an SpatialRDD that has been saved to a distributed object file.</p>"},{"location":"tutorial/rdd/#load-to-a-typed-spatialrdd","title":"Load to a typed SpatialRDD","text":"<p>Warning</p> <p>Typed SpatialRDD has been deprecated for a long time. We do NOT recommend it anymore.</p>"},{"location":"tutorial/rdd/#load-to-a-generic-spatialrdd","title":"Load to a generic SpatialRDD","text":"<p>Use the following code to reload the SpatialRDD:</p> ScalaJavaPython <pre><code>var savedRDD = new SpatialRDD[Geometry]\nsavedRDD.rawSpatialRDD = sc.objectFile[Geometry](\"hdfs://PATH\")\n</code></pre> <pre><code>SpatialRDD savedRDD = new SpatialRDD&lt;Geometry&gt;\nsavedRDD.rawSpatialRDD = sc.objectFile&lt;Geometry&gt;(\"hdfs://PATH\")\n</code></pre> <pre><code>saved_rdd = load_spatial_rdd_from_disc(sc, \"hdfs://PATH\", GeoType.GEOMETRY)\n</code></pre> <p>Use the following code to reload the indexed SpatialRDD:</p> ScalaJavaPython <pre><code>var savedRDD = new SpatialRDD[Geometry]\nsavedRDD.indexedRawRDD = sc.objectFile[SpatialIndex](\"hdfs://PATH\")\n</code></pre> <pre><code>SpatialRDD savedRDD = new SpatialRDD&lt;Geometry&gt;\nsavedRDD.indexedRawRDD = sc.objectFile&lt;SpatialIndex&gt;(\"hdfs://PATH\")\n</code></pre> <pre><code>saved_rdd = SpatialRDD()\nsaved_rdd.indexedRawRDD = load_spatial_index_rdd_from_disc(sc, \"hdfs://PATH\")\n</code></pre>"},{"location":"tutorial/sql-pure-sql/","title":"Pure SQL environment","text":"<p>Starting from Sedona v1.0.1, you can use Sedona in a pure Spark SQL environment. The example code is written in SQL.</p> <p>SedonaSQL supports SQL/MM Part3 Spatial SQL Standard. Detailed SedonaSQL APIs are available here: SedonaSQL API</p>"},{"location":"tutorial/sql-pure-sql/#initiate-session","title":"Initiate Session","text":"<p>Start <code>spark-sql</code> as following (replace <code>&lt;VERSION&gt;</code> with actual version, like, <code>1.0.1-incubating</code> or <code>1.4.0</code>):</p> <p>Run spark-sql with Apache Sedona</p> Spark 3.0 to 3.3 and Scala 2.12Spark 3.4+ and Scala 2.12 <pre><code>spark-sql --packages org.apache.sedona:sedona-spark-shaded-3.0_2.12:&lt;VERSION&gt;,org.apache.sedona:sedona-viz-3.0_2.12:&lt;VERSION&gt;,org.datasyslab:geotools-wrapper:geotools-24.0 \\\n--conf spark.serializer=org.apache.spark.serializer.KryoSerializer \\\n--conf spark.kryo.registrator=org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator \\\n--conf spark.sql.extensions=org.apache.sedona.viz.sql.SedonaVizExtensions,org.apache.sedona.sql.SedonaSqlExtensions\n</code></pre> <p><pre><code>spark-sql --packages org.apache.sedona:sedona-spark-shaded-3.4_2.12:&lt;VERSION&gt;,org.apache.sedona:sedona-viz-3.4_2.12:&lt;VERSION&gt;,org.datasyslab:geotools-wrapper:geotools-24.0 \\\n--conf spark.serializer=org.apache.spark.serializer.KryoSerializer \\\n--conf spark.kryo.registrator=org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator \\\n--conf spark.sql.extensions=org.apache.sedona.viz.sql.SedonaVizExtensions,org.apache.sedona.sql.SedonaSqlExtensions\n</code></pre> If you are using Spark versions higher than 3.4, please replace the <code>3.4</code> in artifact names with the corresponding major.minor version of Spark.</p> <p>This will register all Sedona types, functions and optimizations in SedonaSQL and SedonaViz.</p>"},{"location":"tutorial/sql-pure-sql/#load-data","title":"Load data","text":"<p>Let use data from <code>examples/sql</code>.  To load data from CSV file we need to execute two commands:</p> <p>Use the following code to load the data and create a raw DataFrame:</p> <pre><code>CREATE TABLE IF NOT EXISTS pointraw (_c0 string, _c1 string) USING csv OPTIONS(header='false') LOCATION '&lt;some path&gt;/sedona/examples/sql/src/test/resources/testpoint.csv';\n\nCREATE TABLE IF NOT EXISTS polygonraw (_c0 string, _c1 string, _c2 string, _c3 string) USING csv OPTIONS(header='false') LOCATION '&lt;some path&gt;/sedona/examples/sql/src/test/resources/testenvelope.csv';\n</code></pre>"},{"location":"tutorial/sql-pure-sql/#transform-the-data","title":"Transform the data","text":"<p>We need to transform our point and polygon data into respective types:</p> <pre><code>CREATE OR REPLACE TEMP VIEW pointdata AS\nSELECT ST_Point(cast(pointraw._c0 as Decimal(24,20)), cast(pointraw._c1 as Decimal(24,20))) AS pointshape\nFROM pointraw;\n\nCREATE OR REPLACE TEMP VIEW polygondata AS\nselect ST_PolygonFromEnvelope(cast(polygonraw._c0 as Decimal(24,20)),\ncast(polygonraw._c1 as Decimal(24,20)), cast(polygonraw._c2 as Decimal(24,20)), cast(polygonraw._c3 as Decimal(24,20))) AS polygonshape FROM polygonraw;\n</code></pre>"},{"location":"tutorial/sql-pure-sql/#work-with-data","title":"Work with data","text":"<p>For example, let join polygon and test data:</p> <pre><code>SELECT * from polygondata, pointdata WHERE ST_Contains(polygondata.polygonshape, pointdata.pointshape) AND ST_Contains(ST_PolygonFromEnvelope(1.0,101.0,501.0,601.0), polygondata.polygonshape)\nLIMIT 5;\n</code></pre>"},{"location":"tutorial/sql/","title":"Spatial SQL app","text":"<p>The page outlines the steps to manage spatial data using SedonaSQL.</p> <p>SedonaSQL supports SQL/MM Part3 Spatial SQL Standard. It includes four kinds of SQL operators as follows. All these operators can be directly called through:</p> ScalaJavaPython <pre><code>var myDataFrame = sedona.sql(\"YOUR_SQL\")\nmyDataFrame.createOrReplaceTempView(\"spatialDf\")\n</code></pre> <pre><code>Dataset&lt;Row&gt; myDataFrame = sedona.sql(\"YOUR_SQL\")\nmyDataFrame.createOrReplaceTempView(\"spatialDf\")\n</code></pre> <pre><code>myDataFrame = sedona.sql(\"YOUR_SQL\")\nmyDataFrame.createOrReplaceTempView(\"spatialDf\")\n</code></pre> <p>Detailed SedonaSQL APIs are available here: SedonaSQL API. You can find example county data (i.e., <code>county_small.tsv</code>) in Sedona GitHub repo.</p>"},{"location":"tutorial/sql/#set-up-dependencies","title":"Set up dependencies","text":"Scala/JavaPython <ol> <li>Read Sedona Maven Central coordinates and add Sedona dependencies in build.sbt or pom.xml.</li> <li>Add Apache Spark core, Apache SparkSQL in build.sbt or pom.xml.</li> <li>Please see SQL example project</li> </ol> <ol> <li>Please read Quick start to install Sedona Python.</li> <li>This tutorial is based on Sedona SQL Jupyter Notebook example. You can interact with Sedona Python Jupyter notebook immediately on Binder. Click  to interact with Sedona Python Jupyter notebook immediately on Binder.</li> </ol>"},{"location":"tutorial/sql/#create-sedona-config","title":"Create Sedona config","text":"<p>Use the following code to create your Sedona config at the beginning. If you already have a SparkSession (usually named <code>spark</code>) created by Wherobots/AWS EMR/Databricks, please skip this step and can use <code>spark</code> directly.</p> <p>Sedona &gt;= 1.4.1</p> <p>You can add additional Spark runtime config to the config builder. For example, <code>SedonaContext.builder().config(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\")</code></p> ScalaJavaPython <p><pre><code>import org.apache.sedona.spark.SedonaContext\n\nval config = SedonaContext.builder()\n.master(\"local[*]\") // Delete this if run in cluster mode\n.appName(\"readTestScala\") // Change this to a proper name\n.getOrCreate()\n</code></pre> If you use SedonaViz together with SedonaSQL, please add the following line after <code>SedonaContext.builder()</code> to enable Sedona Kryo serializer: <pre><code>.config(\"spark.kryo.registrator\", classOf[SedonaVizKryoRegistrator].getName) // org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator\n</code></pre></p> <p><pre><code>import org.apache.sedona.spark.SedonaContext;\n\nSparkSession config = SedonaContext.builder()\n.master(\"local[*]\") // Delete this if run in cluster mode\n.appName(\"readTestScala\") // Change this to a proper name\n.getOrCreate()\n</code></pre> If you use SedonaViz together with SedonaSQL, please add the following line after <code>SedonaContext.builder()</code> to enable Sedona Kryo serializer: <pre><code>.config(\"spark.kryo.registrator\", SedonaVizKryoRegistrator.class.getName) // org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator\n</code></pre></p> <p><pre><code>from sedona.spark import *\n\nconfig = SedonaContext.builder() .\\\n    config('spark.jars.packages',\n           'org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.4.0,'\n           'org.datasyslab:geotools-wrapper:1.4.0-28.2'). \\\n    getOrCreate()\n</code></pre> If you are using Spark versions &gt;= 3.4, please replace the <code>3.0</code> in package name of sedona-spark-shaded with the corresponding major.minor version of Spark, such as <code>sedona-spark-shaded-3.4_2.12:1.4.0</code>.</p> <p>Sedona &lt; 1.4.1</p> <p>The following method has been deprecated since Sedona 1.4.1. Please use the method above to create your Sedona config.</p> ScalaJavaPython <p><pre><code>var sparkSession = SparkSession.builder()\n.master(\"local[*]\") // Delete this if run in cluster mode\n.appName(\"readTestScala\") // Change this to a proper name\n// Enable Sedona custom Kryo serializer\n.config(\"spark.serializer\", classOf[KryoSerializer].getName) // org.apache.spark.serializer.KryoSerializer\n.config(\"spark.kryo.registrator\", classOf[SedonaKryoRegistrator].getName)\n.getOrCreate() // org.apache.sedona.core.serde.SedonaKryoRegistrator\n</code></pre> If you use SedonaViz together with SedonaSQL, please use the following two lines to enable Sedona Kryo serializer instead: <pre><code>.config(\"spark.serializer\", classOf[KryoSerializer].getName) // org.apache.spark.serializer.KryoSerializer\n.config(\"spark.kryo.registrator\", classOf[SedonaVizKryoRegistrator].getName) // org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator\n</code></pre></p> <p><pre><code>SparkSession sparkSession = SparkSession.builder()\n.master(\"local[*]\") // Delete this if run in cluster mode\n.appName(\"readTestScala\") // Change this to a proper name\n// Enable Sedona custom Kryo serializer\n.config(\"spark.serializer\", KryoSerializer.class.getName) // org.apache.spark.serializer.KryoSerializer\n.config(\"spark.kryo.registrator\", SedonaKryoRegistrator.class.getName)\n.getOrCreate() // org.apache.sedona.core.serde.SedonaKryoRegistrator\n</code></pre> If you use SedonaViz together with SedonaSQL, please use the following two lines to enable Sedona Kryo serializer instead: <pre><code>.config(\"spark.serializer\", KryoSerializer.class.getName) // org.apache.spark.serializer.KryoSerializer\n.config(\"spark.kryo.registrator\", SedonaVizKryoRegistrator.class.getName) // org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator\n</code></pre></p> <p><pre><code>sparkSession = SparkSession. \\\n    builder. \\\n    appName('appName'). \\\n    config(\"spark.serializer\", KryoSerializer.getName). \\\n    config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName). \\\n    config('spark.jars.packages',\n           'org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.4.0,'\n           'org.datasyslab:geotools-wrapper:1.4.0-28.2'). \\\n    getOrCreate()\n</code></pre> If you are using Spark versions &gt;= 3.4, please replace the <code>3.0</code> in package name of sedona-spark-shaded with the corresponding major.minor version of Spark, such as <code>sedona-spark-shaded-3.4_2.12:1.4.0</code>.</p>"},{"location":"tutorial/sql/#initiate-sedonacontext","title":"Initiate SedonaContext","text":"<p>Add the following line after creating Sedona config. If you already have a SparkSession (usually named <code>spark</code>) created by Wherobots/AWS EMR/Databricks, please call <code>SedonaContext.create(spark)</code> instead.</p> <p>Sedona &gt;= 1.4.1</p> ScalaJavaPython <pre><code>import org.apache.sedona.spark.SedonaContext\n\nval sedona = SedonaContext.create(config)\n</code></pre> <pre><code>import org.apache.sedona.spark.SedonaContext;\n\nSparkSession sedona = SedonaContext.create(config)\n</code></pre> <pre><code>from sedona.spark import *\n\nsedona = SedonaContext.create(config)\n</code></pre> <p>Sedona &lt; 1.4.1</p> <p>The following method has been deprecated since Sedona 1.4.1. Please use the method above to create your SedonaContext.</p> ScalaJavaPython <pre><code>SedonaSQLRegistrator.registerAll(sparkSession)\n</code></pre> <pre><code>SedonaSQLRegistrator.registerAll(sparkSession)\n</code></pre> <pre><code>from sedona.register import SedonaRegistrator\n\nSedonaRegistrator.registerAll(spark)\n</code></pre> <p>You can also register everything by passing <code>--conf spark.sql.extensions=org.apache.sedona.sql.SedonaSqlExtensions</code> to <code>spark-submit</code> or <code>spark-shell</code>.</p>"},{"location":"tutorial/sql/#load-data-from-files","title":"Load data from files","text":"<p>Assume we have a WKT file, namely <code>usa-county.tsv</code>, at Path <code>/Download/usa-county.tsv</code> as follows:</p> <p><pre><code>POLYGON (..., ...)  Cuming County   \nPOLYGON (..., ...)  Wahkiakum County\nPOLYGON (..., ...)  De Baca County\nPOLYGON (..., ...)  Lancaster County\n</code></pre> The file may have many other columns.</p> <p>Use the following code to load the data and create a raw DataFrame:</p> ScalaJavaPython <pre><code>var rawDf = sedona.read.format(\"csv\").option(\"delimiter\", \"\\t\").option(\"header\", \"false\").load(\"/Download/usa-county.tsv\")\nrawDf.createOrReplaceTempView(\"rawdf\")\nrawDf.show()\n</code></pre> <pre><code>Dataset&lt;Row&gt; rawDf = sedona.read.format(\"csv\").option(\"delimiter\", \"\\t\").option(\"header\", \"false\").load(\"/Download/usa-county.tsv\")\nrawDf.createOrReplaceTempView(\"rawdf\")\nrawDf.show()\n</code></pre> <pre><code>rawDf = sedona.read.format(\"csv\").option(\"delimiter\", \"\\t\").option(\"header\", \"false\").load(\"/Download/usa-county.tsv\")\nrawDf.createOrReplaceTempView(\"rawdf\")\nrawDf.show()\n</code></pre> <p>The output will be like this:</p> <pre><code>|                 _c0|_c1|_c2|     _c3|  _c4|        _c5|                 _c6|_c7|_c8|  _c9|_c10| _c11|_c12|_c13|      _c14|    _c15|       _c16|        _c17|\n+--------------------+---+---+--------+-----+-----------+--------------------+---+---+-----+----+-----+----+----+----------+--------+-----------+------------+\n|POLYGON ((-97.019...| 31|039|00835841|31039|     Cuming|       Cuming County| 06| H1|G4020|null| null|null|   A|1477895811|10447360|+41.9158651|-096.7885168|\n|POLYGON ((-123.43...| 53|069|01513275|53069|  Wahkiakum|    Wahkiakum County| 06| H1|G4020|null| null|null|   A| 682138871|61658258|+46.2946377|-123.4244583|\n|POLYGON ((-104.56...| 35|011|00933054|35011|    De Baca|      De Baca County| 06| H1|G4020|null| null|null|   A|6015539696|29159492|+34.3592729|-104.3686961|\n|POLYGON ((-96.910...| 31|109|00835876|31109|  Lancaster|    Lancaster County| 06| H1|G4020| 339|30700|null|   A|2169240202|22877180|+40.7835474|-096.6886584|\n</code></pre>"},{"location":"tutorial/sql/#create-a-geometry-type-column","title":"Create a Geometry type column","text":"<p>All geometrical operations in SedonaSQL are on Geometry type objects. Therefore, before any kind of queries, you need to create a Geometry type column on a DataFrame.</p> <pre><code>SELECT ST_GeomFromWKT(_c0) AS countyshape, _c1, _c2\n</code></pre> <p>You can select many other attributes to compose this <code>spatialdDf</code>. The output will be something like this:</p> <pre><code>|                 countyshape|_c1|_c2|     _c3|  _c4|        _c5|                 _c6|_c7|_c8|  _c9|_c10| _c11|_c12|_c13|      _c14|    _c15|       _c16|        _c17|\n+--------------------+---+---+--------+-----+-----------+--------------------+---+---+-----+----+-----+----+----+----------+--------+-----------+------------+\n|POLYGON ((-97.019...| 31|039|00835841|31039|     Cuming|       Cuming County| 06| H1|G4020|null| null|null|   A|1477895811|10447360|+41.9158651|-096.7885168|\n|POLYGON ((-123.43...| 53|069|01513275|53069|  Wahkiakum|    Wahkiakum County| 06| H1|G4020|null| null|null|   A| 682138871|61658258|+46.2946377|-123.4244583|\n|POLYGON ((-104.56...| 35|011|00933054|35011|    De Baca|      De Baca County| 06| H1|G4020|null| null|null|   A|6015539696|29159492|+34.3592729|-104.3686961|\n|POLYGON ((-96.910...| 31|109|00835876|31109|  Lancaster|    Lancaster County| 06| H1|G4020| 339|30700|null|   A|2169240202|22877180|+40.7835474|-096.6886584|\n</code></pre> <p>Although it looks same with the input, but actually the type of column countyshape has been changed to Geometry type.</p> <p>To verify this, use the following code to print the schema of the DataFrame:</p> <pre><code>spatialDf.printSchema()\n</code></pre> <p>The output will be like this:</p> <pre><code>root\n |-- countyshape: geometry (nullable = false)\n |-- _c1: string (nullable = true)\n |-- _c2: string (nullable = true)\n |-- _c3: string (nullable = true)\n |-- _c4: string (nullable = true)\n |-- _c5: string (nullable = true)\n |-- _c6: string (nullable = true)\n |-- _c7: string (nullable = true)\n</code></pre> <p>Note</p> <p>SedonaSQL provides lots of functions to create a Geometry column, please read SedonaSQL constructor API.</p>"},{"location":"tutorial/sql/#load-geojson-using-spark-json-data-source","title":"Load GeoJSON using Spark JSON Data Source","text":"<p>Spark SQL's built-in JSON data source supports reading GeoJSON data. To ensure proper parsing of the geometry property, we can define a schema with the geometry property set to type 'string'. This prevents Spark from interpreting the property and allows us to use the ST_GeomFromGeoJSON function for accurate geometry parsing.</p> ScalaJavaPython <pre><code>val schema = \"type string, crs string, totalFeatures long, features array&lt;struct&lt;type string, geometry string, properties map&lt;string, string&gt;&gt;&gt;\"\nsedona.read.schema(schema).json(geojson_path)\n.selectExpr(\"explode(features) as features\") // Explode the envelope to get one feature per row.\n.select(\"features.*\") // Unpack the features struct.\n.withColumn(\"geometry\", expr(\"ST_GeomFromGeoJSON(geometry)\")) // Convert the geometry string.\n.printSchema()\n</code></pre> <pre><code>String schema = \"type string, crs string, totalFeatures long, features array&lt;struct&lt;type string, geometry string, properties map&lt;string, string&gt;&gt;&gt;\";\nsedona.read.schema(schema).json(geojson_path)\n.selectExpr(\"explode(features) as features\") // Explode the envelope to get one feature per row.\n.select(\"features.*\") // Unpack the features struct.\n.withColumn(\"geometry\", expr(\"ST_GeomFromGeoJSON(geometry)\")) // Convert the geometry string.\n.printSchema();\n</code></pre> <pre><code>schema = \"type string, crs string, totalFeatures long, features array&lt;struct&lt;type string, geometry string, properties map&lt;string, string&gt;&gt;&gt;\";\n(sedona.read.json(geojson_path, schema=schema) \n    .selectExpr(\"explode(features) as features\") # Explode the envelope to get one feature per row.\n    .select(\"features.*\") # Unpack the features struct.\n    .withColumn(\"geometry\", f.expr(\"ST_GeomFromGeoJSON(geometry)\")) # Convert the geometry string.\n    .printSchema())\n</code></pre>"},{"location":"tutorial/sql/#load-shapefile-and-geojson-using-spatialrdd","title":"Load Shapefile and GeoJSON using SpatialRDD","text":"<p>Shapefile and GeoJSON can be loaded by SpatialRDD and converted to DataFrame using Adapter. Please read Load SpatialRDD and DataFrame &lt;-&gt; RDD.</p>"},{"location":"tutorial/sql/#load-geoparquet","title":"Load GeoParquet","text":"<p>Since v<code>1.3.0</code>, Sedona natively supports loading GeoParquet file. Sedona will infer geometry fields using the \"geo\" metadata in GeoParquet files.</p> Scala/JavaJavaPython <pre><code>val df = sedona.read.format(\"geoparquet\").load(geoparquetdatalocation1)\ndf.printSchema()\n</code></pre> <pre><code>Dataset&lt;Row&gt; df = sedona.read.format(\"geoparquet\").load(geoparquetdatalocation1)\ndf.printSchema()\n</code></pre> <pre><code>df = sedona.read.format(\"geoparquet\").load(geoparquetdatalocation1)\ndf.printSchema()\n</code></pre> <p>The output will be as follows:</p> <pre><code>root\n |-- pop_est: long (nullable = true)\n |-- continent: string (nullable = true)\n |-- name: string (nullable = true)\n |-- iso_a3: string (nullable = true)\n |-- gdp_md_est: double (nullable = true)\n |-- geometry: geometry (nullable = true)\n</code></pre> <p>Sedona supports spatial predicate push-down for GeoParquet files, please refer to the SedonaSQL query optimizer documentation for details.</p>"},{"location":"tutorial/sql/#load-data-from-jdbc-data-sources","title":"Load data from JDBC data sources","text":"<p>The 'query' option in Spark SQL's JDBC data source can be used to convert geometry columns to a format that Sedona can interpret. This should work for most spatial JDBC data sources. For Postgis there is no need to add a query to convert geometry types since it's already using EWKB as it's wire format.</p> ScalaJavaPython <pre><code>// For any JDBC data source, including Postgis.\nval df = sedona.read.format(\"jdbc\")\n// Other options.\n.option(\"query\", \"SELECT id, ST_AsBinary(geom) as geom FROM my_table\")\n.load()\n.withColumn(\"geom\", expr(\"ST_GeomFromWKB(geom)\"))\n\n// This is a simplified version that works for Postgis.\nval df = sedona.read.format(\"jdbc\")\n// Other options.\n.option(\"dbtable\", \"my_table\")\n.load()\n.withColumn(\"geom\", expr(\"ST_GeomFromWKB(geom)\"))\n</code></pre> <pre><code>// For any JDBC data source, including Postgis.\nDataset&lt;Row&gt; df = sedona.read().format(\"jdbc\")\n// Other options.\n.option(\"query\", \"SELECT id, ST_AsBinary(geom) as geom FROM my_table\")\n.load()\n.withColumn(\"geom\", expr(\"ST_GeomFromWKB(geom)\"))\n\n// This is a simplified version that works for Postgis.\nDataset&lt;Row&gt; df = sedona.read().format(\"jdbc\")\n// Other options.\n.option(\"dbtable\", \"my_table\")\n.load()\n.withColumn(\"geom\", expr(\"ST_GeomFromWKB(geom)\"))\n</code></pre> <pre><code># For any JDBC data source, including Postgis.\ndf = (sedona.read.format(\"jdbc\")\n    # Other options.\n    .option(\"query\", \"SELECT id, ST_AsBinary(geom) as geom FROM my_table\")\n    .load()\n    .withColumn(\"geom\", f.expr(\"ST_GeomFromWKB(geom)\")))\n\n# This is a simplified version that works for Postgis.\ndf = (sedona.read.format(\"jdbc\")\n    # Other options.\n    .option(\"dbtable\", \"my_table\")\n    .load()\n    .withColumn(\"geom\", f.expr(\"ST_GeomFromWKB(geom)\")))\n</code></pre>"},{"location":"tutorial/sql/#transform-the-coordinate-reference-system","title":"Transform the Coordinate Reference System","text":"<p>Sedona doesn't control the coordinate unit (degree-based or meter-based) of all geometries in a Geometry column. The unit of all related distances in SedonaSQL is same as the unit of all geometries in a Geometry column.</p> <p>To convert Coordinate Reference System of the Geometry column created before, use the following code:</p> <pre><code>SELECT ST_Transform(countyshape, \"epsg:4326\", \"epsg:3857\") AS newcountyshape, _c1, _c2, _c3, _c4, _c5, _c6, _c7\nFROM spatialdf\n</code></pre> <p>The first EPSG code EPSG:4326 in <code>ST_Transform</code> is the source CRS of the geometries. It is WGS84, the most common degree-based CRS.</p> <p>The second EPSG code EPSG:3857 in <code>ST_Transform</code> is the target CRS of the geometries. It is the most common meter-based CRS.</p> <p>This <code>ST_Transform</code> transform the CRS of these geometries from EPSG:4326 to EPSG:3857. The details CRS information can be found on EPSG.io</p> <p>The coordinates of polygons have been changed. The output will be like this:</p> <pre><code>+--------------------+---+---+--------+-----+-----------+--------------------+---+\n|      newcountyshape|_c1|_c2|     _c3|  _c4|        _c5|                 _c6|_c7|\n+--------------------+---+---+--------+-----+-----------+--------------------+---+\n|POLYGON ((-108001...| 31|039|00835841|31039|     Cuming|       Cuming County| 06|\n|POLYGON ((-137408...| 53|069|01513275|53069|  Wahkiakum|    Wahkiakum County| 06|\n|POLYGON ((-116403...| 35|011|00933054|35011|    De Baca|      De Baca County| 06|\n|POLYGON ((-107880...| 31|109|00835876|31109|  Lancaster|    Lancaster County| 06|\n</code></pre>"},{"location":"tutorial/sql/#run-spatial-queries","title":"Run spatial queries","text":"<p>After creating a Geometry type column, you are able to run spatial queries.</p>"},{"location":"tutorial/sql/#range-query","title":"Range query","text":"<p>Use ST_Contains, ST_Intersects, ST_Within to run a range query over a single column.</p> <p>The following example finds all counties that are within the given polygon:</p> <pre><code>SELECT *\nFROM spatialdf\nWHERE ST_Contains (ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), newcountyshape)\n</code></pre> <p>Note</p> <p>Read SedonaSQL constructor API to learn how to create a Geometry type query window</p>"},{"location":"tutorial/sql/#knn-query","title":"KNN query","text":"<p>Use ST_Distance to calculate the distance and rank the distance.</p> <p>The following code returns the 5 nearest neighbor of the given polygon.</p> <pre><code>SELECT countyname, ST_Distance(ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), newcountyshape) AS distance\nFROM spatialdf\nORDER BY distance DESC\nLIMIT 5\n</code></pre>"},{"location":"tutorial/sql/#join-query","title":"Join query","text":"<p>The details of a join query is available here Join query.</p>"},{"location":"tutorial/sql/#other-queries","title":"Other queries","text":"<p>There are lots of other functions can be combined with these queries. Please read SedonaSQL functions and SedonaSQL aggregate functions.</p>"},{"location":"tutorial/sql/#save-to-permanent-storage","title":"Save to permanent storage","text":"<p>To save a Spatial DataFrame to some permanent storage such as Hive tables and HDFS, you can simply convert each geometry in the Geometry type column back to a plain String and save the plain DataFrame to wherever you want.</p> <p>Use the following code to convert the Geometry column in a DataFrame back to a WKT string column:</p> <pre><code>SELECT ST_AsText(countyshape)\nFROM polygondf\n</code></pre> <p>Note</p> <p>ST_AsGeoJSON is also available. We would like to invite you to contribute more functions</p>"},{"location":"tutorial/sql/#save-geoparquet","title":"Save GeoParquet","text":"<p>Since v<code>1.3.0</code>, Sedona natively supports writing GeoParquet file. GeoParquet can be saved as follows:</p> <pre><code>df.write.format(\"geoparquet\").save(geoparquetoutputlocation + \"/GeoParquet_File_Name.parquet\")\n</code></pre>"},{"location":"tutorial/sql/#sort-then-save-geoparquet","title":"Sort then Save GeoParquet","text":"<p>To maximize the performance of Sedona GeoParquet filter pushdown, we suggest that you sort the data by their geohash values (see ST_GeoHash) and then save as a GeoParquet file. An example is as follows:</p> <pre><code>SELECT col1, col2, geom, ST_GeoHash(geom, 5) as geohash\nFROM spatialDf\nORDER BY geohash\n</code></pre>"},{"location":"tutorial/sql/#save-to-postgis","title":"Save to Postgis","text":"<p>Unfortunately, the Spark SQL JDBC data source doesn't support creating geometry types in PostGIS using the 'createTableColumnTypes' option. Only the Spark built-in types are recognized. This means that you'll need to manage your PostGIS schema separately from Spark. One way to do this is to create the table with the correct geometry column before writing data to it with Spark. Alternatively, you can write your data to the table using Spark and then manually alter the column to be a geometry type afterward.</p> <p>Postgis uses EWKB to serialize geometries. If you convert your geometries to EWKB format in Sedona you don't have to do any additional conversion in Postgis.</p> <pre><code>my_postgis_db# create table my_table (id int8, geom geometry);\n\ndf.withColumn(\"geom\", expr(\"ST_AsEWKB(geom)\")\n    .write.format(\"jdbc\")\n    .option(\"truncate\",\"true\") // Don't let Spark recreate the table.\n    // Other options.\n    .save()\n\n// If you didn't create the table before writing you can change the type afterward.\nmy_postgis_db# alter table my_table alter column geom type geometry;\n</code></pre>"},{"location":"tutorial/sql/#convert-between-dataframe-and-spatialrdd","title":"Convert between DataFrame and SpatialRDD","text":""},{"location":"tutorial/sql/#dataframe-to-spatialrdd","title":"DataFrame to SpatialRDD","text":"<p>Use SedonaSQL DataFrame-RDD Adapter to convert a DataFrame to an SpatialRDD. Please read Adapter Scaladoc</p> ScalaJavaPython <pre><code>var spatialRDD = Adapter.toSpatialRdd(spatialDf, \"usacounty\")\n</code></pre> <pre><code>SpatialRDD spatialRDD = Adapter.toSpatialRdd(spatialDf, \"usacounty\")\n</code></pre> <pre><code>from sedona.utils.adapter import Adapter\n\nspatialRDD = Adapter.toSpatialRdd(spatialDf, \"usacounty\")\n</code></pre> <p>\"usacounty\" is the name of the geometry column</p> <p>Warning</p> <p>Only one Geometry type column is allowed per DataFrame.</p>"},{"location":"tutorial/sql/#spatialrdd-to-dataframe","title":"SpatialRDD to DataFrame","text":"<p>Use SedonaSQL DataFrame-RDD Adapter to convert a DataFrame to an SpatialRDD. Please read Adapter Scaladoc</p> ScalaJavaPython <pre><code>var spatialDf = Adapter.toDf(spatialRDD, sedona)\n</code></pre> <pre><code>Dataset&lt;Row&gt; spatialDf = Adapter.toDf(spatialRDD, sedona)\n</code></pre> <pre><code>from sedona.utils.adapter import Adapter\n\nspatialDf = Adapter.toDf(spatialRDD, sedona)\n</code></pre> <p>All other attributes such as price and age will be also brought to the DataFrame as long as you specify carryOtherAttributes (see Read other attributes in an SpatialRDD).</p> <p>You may also manually specify a schema for the resulting DataFrame in case you require different column names or data types. Note that string schemas and not all data types are supported\u2014please check the Adapter Scaladoc to confirm what is supported for your use case. At least one column for the user data must be provided.</p> Scala <pre><code>val schema = StructType(Array(\nStructField(\"county\", GeometryUDT, nullable = true),\nStructField(\"name\", StringType, nullable = true),\nStructField(\"price\", DoubleType, nullable = true),\nStructField(\"age\", IntegerType, nullable = true)\n))\nval spatialDf = Adapter.toDf(spatialRDD, schema, sedona)\n</code></pre>"},{"location":"tutorial/sql/#spatialpairrdd-to-dataframe","title":"SpatialPairRDD to DataFrame","text":"<p>PairRDD is the result of a spatial join query or distance join query. SedonaSQL DataFrame-RDD Adapter can convert the result to a DataFrame. But you need to provide the name of other attributes.</p> ScalaJavaPython <pre><code>var joinResultDf = Adapter.toDf(joinResultPairRDD, Seq(\"left_attribute1\", \"left_attribute2\"), Seq(\"right_attribute1\", \"right_attribute2\"), sedona)\n</code></pre> <pre><code>import scala.collection.JavaConverters; List leftFields = new ArrayList&lt;&gt;(Arrays.asList(\"c1\", \"c2\", \"c3\"));\nList rightFields = new ArrayList&lt;&gt;(Arrays.asList(\"c4\", \"c5\", \"c6\"));\nDataset joinResultDf = Adapter.toDf(joinResultPairRDD, JavaConverters.asScalaBuffer(leftFields).toSeq(), JavaConverters.asScalaBuffer(rightFields).toSeq(), sedona);\n</code></pre> <pre><code>from sedona.utils.adapter import Adapter\n\njoinResultDf = Adapter.toDf(jvm_sedona_rdd, [\"poi_from_id\", \"poi_from_name\"], [\"poi_to_id\", \"poi_to_name\"], spark))\n</code></pre> <p>or you can use the attribute names directly from the input RDD</p> ScalaJavaPython <pre><code>import scala.collection.JavaConversions._\nvar joinResultDf = Adapter.toDf(joinResultPairRDD, leftRdd.fieldNames, rightRdd.fieldNames, sedona)\n</code></pre> <pre><code>import scala.collection.JavaConverters; Dataset joinResultDf = Adapter.toDf(joinResultPairRDD, JavaConverters.asScalaBuffer(leftRdd.fieldNames).toSeq(), JavaConverters.asScalaBuffer(rightRdd.fieldNames).toSeq(), sedona);\n</code></pre> <pre><code>from sedona.utils.adapter import Adapter\n\njoinResultDf = Adapter.toDf(result_pair_rdd, leftRdd.fieldNames, rightRdd.fieldNames, spark)\n</code></pre> <p>All other attributes such as price and age will be also brought to the DataFrame as long as you specify carryOtherAttributes (see Read other attributes in an SpatialRDD).</p> <p>You may also manually specify a schema for the resulting DataFrame in case you require different column names or data types. Note that string schemas and not all data types are supported\u2014please check the Adapter Scaladoc to confirm what is supported for your use case. Columns for the left and right user data must be provided.</p> Scala <pre><code>val schema = StructType(Array(\nStructField(\"leftGeometry\", GeometryUDT, nullable = true),\nStructField(\"name\", StringType, nullable = true),\nStructField(\"price\", DoubleType, nullable = true),\nStructField(\"age\", IntegerType, nullable = true),\nStructField(\"rightGeometry\", GeometryUDT, nullable = true),\nStructField(\"category\", StringType, nullable = true)\n))\nval joinResultDf = Adapter.toDf(joinResultPairRDD, schema, sedona)\n</code></pre>"},{"location":"tutorial/storing-blobs-in-parquet/","title":"Storing large raster geometries in Parquet files","text":"<p>Warning</p> <p>Always convert the raster geometries to a well known format with the RS_AsXXX functions before saving them. It is possible to save the raw bytes of the raster geometries, but they will be stored in an internal Sedona format that is not guaranteed to be stable across versions.</p> <p>The default settings in Spark are not well suited for storing large binaries like raster geometries. It is very much worth the time to tune and benchmark your settings. Writing large binaries with the default settings will result in poorly structured Parquet files that are very expensive to read. Some basic tuning can increase the read performance by several magnitudes.</p>"},{"location":"tutorial/storing-blobs-in-parquet/#background","title":"Background","text":"<p>Parquet files are divided into one or several row groups. Each column in a row group is stored in a column chunk. Each column chunk is further divided into pages. A page is conceptually an indivisible unit in terms of compression and encoding. The default size for a page is 1 MB. Data is buffered until the page is full and then written to disk. The frequency of checks of the page size limit will be between <code>parquet.page.size.row.check.min</code> and <code>parquet.page.size.row.check.max</code> (default between 100 and 10000 rows).</p> <p>If you write 5 MB image files to Parquet with the default setting the first page size check will happen after 100 rows. You will end up with pages of 500 MB instead of 1 MB. Reading such a file will require a lot of memory and will be slow.</p>"},{"location":"tutorial/storing-blobs-in-parquet/#reading-poorly-structured-parquet-files","title":"Reading poorly structured Parquet files","text":"<p>Especially snappy compressed files are sensitive to oversized pages. More performant options are no compression or zstd compression. You can set <code>spark.buffer.size</code> to a value larger than the default of 64k to improve read performance. Increasing <code>spark.buffer.size</code> might add an io penalty for other columns in the Parquet file.</p>"},{"location":"tutorial/storing-blobs-in-parquet/#writing-better-structured-parquet-files-for-blobs","title":"Writing better structured Parquet files for blobs","text":"<p>Ideally you want to write Parquet files with a sane page size to get better and more consistent read performance across different clients. Since version 1.12.0 of parquet-hadoop, bundled with Spark 3.2, you can add Hadoop properties for controlling page size checks. Better values for writing blobs are:</p> <pre><code>spark.sql.parquet.compression.codec=zstd\nspark.hadoop.parquet.page.size.row.check.min=2\nspark.hadoop.parquet.page.size.row.check.max=10\n</code></pre> <p>Zstd performs better than snappy in general. Even more so for large pages. The first page size check will happen after 2 rows. If the page is not full after 2 rows the next check will happen after another 2-10 rows, depending on the size of the two rows already written.</p> <p>Spark will set Hadoop properties from Spark properties prefixed with \"spark.hadoop.\". For a full list of Parquet Hadoop properties see: https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/README.md</p>"},{"location":"tutorial/viz-gallery/","title":"Gallery","text":""},{"location":"tutorial/viz/","title":"Scala/Java","text":"<p>The page outlines the steps to visualize spatial data using SedonaViz. The example code is written in Scala but also works for Java.</p> <p>SedonaViz provides native support for general cartographic design by extending Sedona to process large-scale spatial data. It can visulize Spatial RDD and Spatial Queries and render super high resolution image in parallel.</p> <p>SedonaViz offers Map Visualization SQL. This gives users a more flexible way to design beautiful map visualization effects including scatter plots and heat maps. SedonaViz RDD API is also available.</p> <p>Note</p> <p>All SedonaViz SQL/DataFrame APIs are explained in SedonaViz API. Please see Viz example project</p>"},{"location":"tutorial/viz/#why-scalable-map-visualization","title":"Why scalable map visualization?","text":"<p>Data visualization allows users to summarize, analyze and reason about data. Guaranteeing detailed and accurate geospatial map visualization (e.g., at multiple zoom levels) requires extremely high-resolution maps. Classic visualization solutions such as Google Maps, MapBox and ArcGIS suffer from limited computation resources and hence take a tremendous amount of time to generate maps for large-scale geospatial data. In big spatial data scenarios, these tools just crash or run forever.</p> <p>SedonaViz encapsulates the main steps of map visualization process, e.g., pixelize, aggregate, and render, into a set of massively parallelized GeoViz operators and the user can assemble any customized styles.</p>"},{"location":"tutorial/viz/#visualize-spatialrdd","title":"Visualize SpatialRDD","text":"<p>This tutorial mainly focuses on explaining SQL/DataFrame API. SedonaViz RDD example can be found in Please see Viz example project</p>"},{"location":"tutorial/viz/#set-up-dependencies","title":"Set up dependencies","text":"<ol> <li>Read Sedona Maven Central coordinates</li> <li>Add Apache Spark core, Apache SparkSQL, Sedona-core, Sedona-SQL, Sedona-Viz</li> </ol>"},{"location":"tutorial/viz/#create-sedona-config","title":"Create Sedona config","text":"<p>Use the following code to create your Sedona config at the beginning. If you already have a SparkSession (usually named <code>spark</code>) created by Wherobots/AWS EMR/Databricks, please skip this step and can use <code>spark</code> directly.</p> <p>Sedona &gt;= 1.4.1=</p> <pre><code>val config = SedonaContext.builder()\n.config(\"spark.kryo.registrator\", classOf[SedonaVizKryoRegistrator].getName) // org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator\n.master(\"local[*]\") // Delete this if run in cluster mode\n.appName(\"Sedona Viz\") // Change this to a proper name\n.getOrCreate()\n</code></pre> <p>Sedona &lt;1.4.1</p> <p>The following method has been deprecated since Sedona 1.4.1. Please use the method above to create your Sedona config.</p> <pre><code>var sparkSession = SparkSession.builder()\n.master(\"local[*]\") // Delete this if run in cluster mode\n.appName(\"Sedona Viz\") // Change this to a proper name\n// Enable Sedona custom Kryo serializer\n.config(\"spark.serializer\", classOf[KryoSerializer].getName) // org.apache.spark.serializer.KryoSerializer\n.config(\"spark.kryo.registrator\", classOf[SedonaVizKryoRegistrator].getName) // org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator\n.getOrCreate()\n</code></pre>"},{"location":"tutorial/viz/#initiate-sedonacontext","title":"Initiate SedonaContext","text":"<p>Add the following line after creating Sedona config. If you already have a SparkSession (usually named <code>spark</code>) created by Wherobots/AWS EMR/Databricks, please call <code>SedonaContext.create(spark)</code> instead.</p> <p>Sedona &gt;= 1.4.1=</p> <pre><code>val sedona = SedonaContext.create(config)\nSedonaVizRegistrator.registerAll(sedona)\n</code></pre> <p>Sedona &lt;1.4.1</p> <p>The following method has been deprecated since Sedona 1.4.1. Please use the method above to create your SedonaContext.</p> <pre><code>SedonaSQLRegistrator.registerAll(sparkSession)\nSedonaVizRegistrator.registerAll(sparkSession)\n</code></pre> <p>You can also register everything by passing <code>--conf spark.sql.extensions=org.apache.sedona.viz.sql.SedonaVizExtensions,org.apache.sedona.sql.SedonaSqlExtensions</code> to <code>spark-submit</code> or <code>spark-shell</code>.</p>"},{"location":"tutorial/viz/#create-spatial-dataframe","title":"Create Spatial DataFrame","text":"<p>There is a DataFrame as follows:</p> <pre><code>+----------+---------+\n|       _c0|      _c1|\n+----------+---------+\n|-88.331492|32.324142|\n|-88.175933|32.360763|\n|-88.388954|32.357073|\n|-88.221102| 32.35078|\n</code></pre> <p>You first need to create a Geometry type column.</p> <pre><code>CREATE OR REPLACE TEMP VIEW pointtable AS\nSELECT ST_Point(cast(pointtable._c0 as Decimal(24,20)),cast(pointtable._c1 as Decimal(24,20))) as shape\nFROM pointtable\n</code></pre> <p>As you know, Sedona provides many different methods to load various spatial data formats. Please read Write an Spatial DataFrame application.</p>"},{"location":"tutorial/viz/#generate-a-single-image","title":"Generate a single image","text":"<p>In most cases, you just want to see a single image out of your spatial dataset.</p>"},{"location":"tutorial/viz/#pixelize-spatial-objects","title":"Pixelize spatial objects","text":"<p>To put spatial objects on a map image, you first need to convert them to pixels.</p> <p>First, compute the spatial boundary of this column.</p> <pre><code>CREATE OR REPLACE TEMP VIEW boundtable AS\nSELECT ST_Envelope_Aggr(shape) as bound FROM pointtable\n</code></pre> <p>Then use ST_Pixelize to convert them to pixels.</p> <p>This example is for Sedona before v1.0.1. ST_Pixelize extends Generator so it can directly flatten the array without the explode function.</p> <pre><code>CREATE OR REPLACE TEMP VIEW pixels AS\nSELECT pixel, shape FROM pointtable\nLATERAL VIEW ST_Pixelize(ST_Transform(shape, 'epsg:4326','epsg:3857'), 256, 256, (SELECT ST_Transform(bound, 'epsg:4326','epsg:3857') FROM boundtable)) AS pixel\n</code></pre> <p>This example is for Sedona on and after v1.0.1. ST_Pixelize returns an array of pixels. You need to use explode to flatten it.</p> <pre><code>CREATE OR REPLACE TEMP VIEW pixels AS\nSELECT pixel, shape FROM pointtable\nLATERAL VIEW explode(ST_Pixelize(ST_Transform(shape, 'epsg:4326','epsg:3857'), 256, 256, (SELECT ST_Transform(bound, 'epsg:4326','epsg:3857') FROM boundtable))) AS pixel\n</code></pre> <p>This will give you a 256*256 resolution image after you run ST_Render at the end of this tutorial.</p> <p>Warning</p> <p>We highly suggest that you should use ST_Transform to transform coordinates to a visualization-specific coordinate system such as epsg:3857. Otherwise you map may look distorted.</p>"},{"location":"tutorial/viz/#aggregate-pixels","title":"Aggregate pixels","text":"<p>Many objects may be pixelized to the same pixel locations. You now need to aggregate them based on either their spatial aggregation or spatial observations such as temperature or humidity.</p> <pre><code>CREATE OR REPLACE TEMP VIEW pixelaggregates AS\nSELECT pixel, count(*) as weight\nFROM pixels\nGROUP BY pixel\n</code></pre> <p>The weight indicates the degree of spatial aggregation or spatial observations. Later on, it will determine the color of this pixel.</p>"},{"location":"tutorial/viz/#colorize-pixels","title":"Colorize pixels","text":"<p>Run the following command to assign colors for pixels based on their weights.</p> <pre><code>CREATE OR REPLACE TEMP VIEW pixelaggregates AS\nSELECT pixel, ST_Colorize(weight, (SELECT max(weight) FROM pixelaggregates)) as color\nFROM pixelaggregates\n</code></pre> <p>Please read ST_Colorize for a detailed API description.</p>"},{"location":"tutorial/viz/#render-the-image","title":"Render the image","text":"<p>Use ST_Render to plot all pixels on a single image.</p> <pre><code>CREATE OR REPLACE TEMP VIEW images AS\nSELECT ST_Render(pixel, color) AS image, (SELECT ST_AsText(bound) FROM boundtable) AS boundary\nFROM pixelaggregates\n</code></pre> <p>This DataFrame will contain a Image type column which has only one image.</p>"},{"location":"tutorial/viz/#store-the-image-on-disk","title":"Store the image on disk","text":"<p>Fetch the image from the previous DataFrame</p> <pre><code>var image = sedona.table(\"images\").take(1)(0)(0).asInstanceOf[ImageSerializableWrapper].getImage\n</code></pre> <p>Use Sedona Viz ImageGenerator to store this image on disk.</p> <pre><code>var imageGenerator = new ImageGenerator\nimageGenerator.SaveRasterImageAsLocalFile(image, System.getProperty(\"user.dir\")+\"/target/points\", ImageType.PNG)\n</code></pre>"},{"location":"tutorial/viz/#generate-map-tiles","title":"Generate map tiles","text":"<p>If you are a map professional, you may need to generate map tiles for different zoom levels and eventually create the map tile layer.</p>"},{"location":"tutorial/viz/#pixelization-and-pixel-aggregation","title":"Pixelization and pixel aggregation","text":"<p>Please first do pixelization and pixel aggregation using the same commands in single image generation. In ST_Pixelize, you need specify a very high resolution, such as 1000*1000. Note that, each dimension should be divisible by 2^zoom-level </p>"},{"location":"tutorial/viz/#create-tile-name","title":"Create tile name","text":"<p>Run the following command to compute the tile name for every pixels</p> <pre><code>CREATE OR REPLACE TEMP VIEW pixelaggregates AS\nSELECT pixel, weight, ST_TileName(pixel, 3) AS pid\nFROM pixelaggregates\n</code></pre> <p>\"3\" is the zoom level for these map tiles.</p>"},{"location":"tutorial/viz/#colorize-pixels_1","title":"Colorize pixels","text":"<p>Use the same command explained in single image generation to assign colors.</p>"},{"location":"tutorial/viz/#render-map-tiles","title":"Render map tiles","text":"<p>You now need to group pixels by tiles and then render map tile images in parallel.</p> <pre><code>CREATE OR REPLACE TEMP VIEW images AS\nSELECT ST_Render(pixel, color, 3) AS image\nFROM pixelaggregates\nGROUP BY pid\n</code></pre> <p>\"3\" is the zoom level for these map tiles.</p>"},{"location":"tutorial/viz/#store-map-tiles-on-disk","title":"Store map tiles on disk","text":"<p>You can use the same commands in single image generation to fetch all map tiles and store them one by one.</p>"},{"location":"tutorial/zeppelin/","title":"Use Apache Zeppelin","text":"<p>Sedona provides a Helium visualization plugin tailored for Apache Zeppelin. This finally bridges the gap between Sedona and Zeppelin.  Please read Install Sedona-Zeppelin to learn how to install this plugin in Zeppelin.</p> <p>Sedona-Zeppelin equips two approaches to visualize spatial data in Zeppelin. The first approach uses Zeppelin to plot all spatial objects on the map. The second one leverages SedonaViz to generate map images and overlay them on maps.</p>"},{"location":"tutorial/zeppelin/#small-scale-without-sedonaviz","title":"Small-scale without SedonaViz","text":"<p>Danger</p> <p>Zeppelin is just a front-end visualization framework. This approach is not scalable and will fail at large-scale geospatial data. Please scroll down to read SedonaViz solution.</p> <p>You can use Apache Zeppelin to plot a small number of spatial objects, such as 1000 points. Assume you already have a Spatial DataFrame, you need to convert the geometry column to WKT string column use the following command in your Zeppelin Spark notebook Scala paragraph:</p> <pre><code>spark.sql(\n\"\"\"\n    |CREATE OR REPLACE TEMP VIEW wktpoint AS\n    |SELECT ST_AsText(shape) as geom\n    |FROM pointtable\n  \"\"\".stripMargin)\n</code></pre> <p>Then create an SQL paragraph to fetch the data <pre><code>%sql\nSELECT *\nFROM wktpoint\n</code></pre></p> <p>Select the geometry column to visualize:</p> <p></p>"},{"location":"tutorial/zeppelin/#large-scale-with-sedonaviz","title":"Large-scale with SedonaViz","text":"<p>SedonaViz is a distributed visualization system that allows you to visualize big spatial data at scale. Please read How to use SedonaViz.</p> <p>You can use Sedona-Zeppelin to ask Zeppelin to overlay SedonaViz images on a map background. This way, you can easily visualize 1 billion spatial objects or more (depends on your cluster size).</p> <p>First, encode images of SedonaViz DataFrame in Zeppelin Spark notebook Scala paragraph,</p> <pre><code>spark.sql(\n  \"\"\"\n    |CREATE OR REPLACE TEMP VIEW images AS\n    |SELECT ST_EncodeImage(image) AS image, (SELECT ST_AsText(bound) FROM boundtable) AS boundary\n    |FROM images\n  \"\"\".stripMargin)\n</code></pre> <p>Then create an SQL paragraph to fetch the data <pre><code>%sql\nSELECT *, 'I am the map center!'\nFROM images\n</code></pre></p> <p>Select the image and its geospatial boundary:</p> <p></p>"},{"location":"tutorial/zeppelin/#zeppelin-spark-notebook-demo","title":"Zeppelin Spark notebook demo","text":"<p>We provide a full Zeppelin Spark notebook which demonstrates al functions. Please download Sedona-Zeppelin notebook template and test data - arealm.csv.</p> <p>You need to use Zeppelin to import this notebook JSON file and modify the input data path in the notebook.</p>"},{"location":"tutorial/flink/sql/","title":"Spatial SQL app","text":"<p>The page outlines the steps to manage spatial data using SedonaSQL. The example code is written in Java but also works for Scala.</p> <p>SedonaSQL supports SQL/MM Part3 Spatial SQL Standard. It includes four kinds of SQL operators as follows. All these operators can be directly called through: <pre><code>Table myTable = tableEnv.sqlQuery(\"YOUR_SQL\")\n</code></pre></p> <p>Detailed SedonaSQL APIs are available here: SedonaSQL API</p>"},{"location":"tutorial/flink/sql/#set-up-dependencies","title":"Set up dependencies","text":"<ol> <li>Read Sedona Maven Central coordinates</li> <li>Add Sedona dependencies in build.sbt or pom.xml.</li> <li>Add Flink dependencies in build.sbt or pom.xml.</li> <li>Please see SQL example project</li> </ol>"},{"location":"tutorial/flink/sql/#initiate-stream-environment","title":"Initiate Stream Environment","text":"<p>Use the following code to initiate your <code>StreamExecutionEnvironment</code> at the beginning: <pre><code>StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\nEnvironmentSettings settings = EnvironmentSettings.newInstance().inStreamingMode().build();\nStreamTableEnvironment tableEnv = StreamTableEnvironment.create(env, settings);\n</code></pre></p>"},{"location":"tutorial/flink/sql/#initiate-sedonacontext","title":"Initiate SedonaContext","text":"<p>Add the following line after your <code>StreamExecutionEnvironment</code> and <code>StreamTableEnvironment</code> declaration</p> <p>Sedona &gt;= 1.4.1</p> <pre><code>StreamTableEnvironment sedona = SedonaContext.create(env, tableEnv);\n</code></pre> <p>Sedona &lt;1.4.1</p> <p>The following method has been deprecated since Sedona 1.4.1. Please use the method above to create your SedonaContext.</p> <pre><code>SedonaFlinkRegistrator.registerType(env);\nSedonaFlinkRegistrator.registerFunc(tableEnv);\n</code></pre> <p>Warning</p> <p>Sedona has a suite of well-written geometry and index serializers. Forgetting to enable these serializers will lead to high memory consumption.</p> <p>This function will register Sedona User Defined Type and User Defined Function</p>"},{"location":"tutorial/flink/sql/#create-a-geometry-type-column","title":"Create a Geometry type column","text":"<p>All geometrical operations in SedonaSQL are on Geometry type objects. Therefore, before any kind of queries, you need to create a Geometry type column on a DataFrame.</p> <p>Assume you have a Flink Table <code>tbl</code> like this:</p> <pre><code>+----+--------------------------------+--------------------------------+\n| op |                   geom_polygon |                   name_polygon |\n+----+--------------------------------+--------------------------------+\n| +I | POLYGON ((-0.5 -0.5, -0.5 0... |                       polygon0 |\n| +I | POLYGON ((0.5 0.5, 0.5 1.5,... |                       polygon1 |\n| +I | POLYGON ((1.5 1.5, 1.5 2.5,... |                       polygon2 |\n| +I | POLYGON ((2.5 2.5, 2.5 3.5,... |                       polygon3 |\n| +I | POLYGON ((3.5 3.5, 3.5 4.5,... |                       polygon4 |\n| +I | POLYGON ((4.5 4.5, 4.5 5.5,... |                       polygon5 |\n| +I | POLYGON ((5.5 5.5, 5.5 6.5,... |                       polygon6 |\n| +I | POLYGON ((6.5 6.5, 6.5 7.5,... |                       polygon7 |\n| +I | POLYGON ((7.5 7.5, 7.5 8.5,... |                       polygon8 |\n| +I | POLYGON ((8.5 8.5, 8.5 9.5,... |                       polygon9 |\n+----+--------------------------------+--------------------------------+\n10 rows in set\n</code></pre> <p>You can create a Table with a Geometry type column as follows:</p> <pre><code>sedona.createTemporaryView(\"myTable\", tbl)\nTable geomTbl = sedona.sqlQuery(\"SELECT ST_GeomFromWKT(geom_polygon) as geom_polygon, name_polygon FROM myTable\")\ngeomTbl.execute().print()\n</code></pre> <p>The output will be:</p> <pre><code>+----+--------------------------------+--------------------------------+\n| op |                   geom_polygon |                   name_polygon |\n+----+--------------------------------+--------------------------------+\n| +I | POLYGON ((-0.5 -0.5, -0.5 0... |                       polygon0 |\n| +I | POLYGON ((0.5 0.5, 0.5 1.5,... |                       polygon1 |\n| +I | POLYGON ((1.5 1.5, 1.5 2.5,... |                       polygon2 |\n| +I | POLYGON ((2.5 2.5, 2.5 3.5,... |                       polygon3 |\n| +I | POLYGON ((3.5 3.5, 3.5 4.5,... |                       polygon4 |\n| +I | POLYGON ((4.5 4.5, 4.5 5.5,... |                       polygon5 |\n| +I | POLYGON ((5.5 5.5, 5.5 6.5,... |                       polygon6 |\n| +I | POLYGON ((6.5 6.5, 6.5 7.5,... |                       polygon7 |\n| +I | POLYGON ((7.5 7.5, 7.5 8.5,... |                       polygon8 |\n| +I | POLYGON ((8.5 8.5, 8.5 9.5,... |                       polygon9 |\n+----+--------------------------------+--------------------------------+\n10 rows in set\n</code></pre> <p>Although it looks same with the input, actually the type of column geom_polygon has been changed to Geometry type.</p> <p>To verify this, use the following code to print the schema of the DataFrame:</p> <pre><code>geomTbl.printSchema()\n</code></pre> <p>The output will be like this:</p> <pre><code>(\n  `geom_polygon` RAW('org.locationtech.jts.geom.Geometry', '...'),\n  `name_polygon` STRING\n)\n</code></pre> <p>Note</p> <p>SedonaSQL provides lots of functions to create a Geometry column, please read SedonaSQL constructor API.</p>"},{"location":"tutorial/flink/sql/#transform-the-coordinate-reference-system","title":"Transform the Coordinate Reference System","text":"<p>Sedona doesn't control the coordinate unit (degree-based or meter-based) of all geometries in a Geometry column. The unit of all related distances in SedonaSQL is same as the unit of all geometries in a Geometry column.</p> <p>To convert Coordinate Reference System of the Geometry column created before, use the following code:</p> <pre><code>Table geomTbl3857 = sedona.sqlQuery(\"SELECT ST_Transform(countyshape, \"epsg:4326\", \"epsg:3857\") AS geom_polygon, name_polygon FROM myTable\")\ngeomTbl3857.execute().print()\n</code></pre> <p>The first EPSG code EPSG:4326 in <code>ST_Transform</code> is the source CRS of the geometries. It is WGS84, the most common degree-based CRS.</p> <p>The second EPSG code EPSG:3857 in <code>ST_Transform</code> is the target CRS of the geometries. It is the most common meter-based CRS.</p> <p>This <code>ST_Transform</code> transform the CRS of these geometries from EPSG:4326 to EPSG:3857. The details CRS information can be found on EPSG.io</p> <p>Note</p> <p>Read SedonaSQL ST_Transform API to learn different spatial query predicates.</p> <p>For example, a Table that has coordinates in the US will become like this.</p> <p>Before the transformation: <pre><code>+----+--------------------------------+--------------------------------+\n| op |                     geom_point |                     name_point |\n+----+--------------------------------+--------------------------------+\n| +I |                POINT (32 -118) |                          point |\n| +I |                POINT (33 -117) |                          point |\n| +I |                POINT (34 -116) |                          point |\n| +I |                POINT (35 -115) |                          point |\n| +I |                POINT (36 -114) |                          point |\n| +I |                POINT (37 -113) |                          point |\n| +I |                POINT (38 -112) |                          point |\n| +I |                POINT (39 -111) |                          point |\n| +I |                POINT (40 -110) |                          point |\n| +I |                POINT (41 -109) |                          point |\n+----+--------------------------------+--------------------------------+\n</code></pre></p> <p>After the transformation:</p> <pre><code>+----+--------------------------------+--------------------------------+\n| op |                            _c0 |                     name_point |\n+----+--------------------------------+--------------------------------+\n| +I | POINT (-13135699.91360628 3... |                          point |\n| +I | POINT (-13024380.422813008 ... |                          point |\n| +I | POINT (-12913060.932019735 ... |                          point |\n| +I | POINT (-12801741.44122646 4... |                          point |\n| +I | POINT (-12690421.950433187 ... |                          point |\n| +I | POINT (-12579102.459639912 ... |                          point |\n| +I | POINT (-12467782.96884664 4... |                          point |\n| +I | POINT (-12356463.478053367 ... |                          point |\n| +I | POINT (-12245143.987260092 ... |                          point |\n| +I | POINT (-12133824.496466817 ... |                          point |\n+----+--------------------------------+--------------------------------+\n</code></pre> <p>After creating a Geometry type column, you are able to run spatial queries.</p>"},{"location":"tutorial/flink/sql/#range-query","title":"Range query","text":"<p>Use ST_Contains, ST_Intersects and so on to run a range query over a single column.</p> <p>The following example finds all counties that are within the given polygon:</p> <pre><code>geomTable = sedona.sqlQuery(\n\"\n    SELECT *\n    FROM spatialdf\n    WHERE ST_Contains (ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), newcountyshape)\n  \")\ngeomTable.execute().print()\n</code></pre> <p>Note</p> <p>Read SedonaSQL Predicate API to learn different spatial query predicates.</p>"},{"location":"tutorial/flink/sql/#knn-query","title":"KNN query","text":"<p>Use ST_Distance to calculate the distance and rank the distance.</p> <p>The following code returns the 5 nearest neighbor of the given polygon.</p> <pre><code>geomTable = sedona.sqlQuery(\n\"\n    SELECT countyname, ST_Distance(ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), newcountyshape) AS distance\n    FROM geomTable\n    ORDER BY distance DESC\n    LIMIT 5\n  \")\ngeomTable.execute().print()\n</code></pre>"},{"location":"tutorial/flink/sql/#join-query","title":"Join query","text":"<p>This equi-join leverages Flink's internal equi-join algorithm. You can opt to skip the Sedona refinement step  by sacrificing query accuracy. A running example is in SQL example project.</p> <p>Please use the following steps:</p>"},{"location":"tutorial/flink/sql/#1-generate-s2-ids-for-both-tables","title":"1. Generate S2 ids for both tables","text":"<p>Use ST_S2CellIds to generate cell IDs. Each geometry may produce one or more IDs.</p> <pre><code>SELECT id, geom, name, ST_S2CellIDs(geom, 15) as idarray\nFROM lefts\n</code></pre> <pre><code>SELECT id, geom, name, ST_S2CellIDs(geom, 15) as idarray\nFROM rights\n</code></pre>"},{"location":"tutorial/flink/sql/#2-explode-id-array","title":"2. Explode id array","text":"<p>The produced S2 ids are arrays of integers. We need to explode these Ids to multiple rows so later we can join two tables by ids.</p> <pre><code>SELECT id, geom, name, cellId\nFROM lefts CROSS JOIN UNNEST(lefts.idarray) AS tmpTbl1(cellId)\n</code></pre> <pre><code>SELECT id, geom, name, cellId\nFROM rights CROSS JOIN UNNEST(rights.idarray) AS tmpTbl2(cellId)\n</code></pre>"},{"location":"tutorial/flink/sql/#3-perform-equi-join","title":"3. Perform equi-join","text":"<p>Join the two tables by their S2 cellId</p> <pre><code>SELECT lcs.id as lcs_id, lcs.geom as lcs_geom, lcs.name as lcs_name, rcs.id as rcs_id, rcs.geom as rcs_geom, rcs.name as rcs_name\nFROM lcs JOIN rcs ON lcs.cellId = rcs.cellId\n</code></pre>"},{"location":"tutorial/flink/sql/#4-optional-refine-the-result","title":"4. Optional: Refine the result","text":"<p>Due to the nature of S2 Cellid, the equi-join results might have a few false-positives depending on the S2 level you choose. A smaller level indicates bigger cells, less exploded rows, but more false positives.</p> <p>To ensure the correctness, you can use one of the Spatial Predicates to filter out them. Use this query as the query in Step 3.</p> <pre><code>SELECT lcs.id as lcs_id, lcs.geom as lcs_geom, lcs.name as lcs_name, rcs.id as rcs_id, rcs.geom as rcs_geom, rcs.name as rcs_name\nFROM lcs, rcs\nWHERE lcs.cellId = rcs.cellId AND ST_Contains(lcs.geom, rcs.geom)\n</code></pre> <p>As you see, compared to the query in Step 2, we added one more filter, which is <code>ST_Contains</code>, to remove false positives. You can also use <code>ST_Intersects</code> and so on.</p> <p>Tip</p> <p>You can skip this step if you don't need 100% accuracy and want faster query speed.</p>"},{"location":"tutorial/flink/sql/#5-optional-de-duplcate","title":"5. Optional: De-duplcate","text":"<p>Due to the explode function used when we generate S2 Cell Ids, the resulting DataFrame may have several duplicate  matches. You can remove them by performing a GroupBy query. <pre><code>SELECT lcs_id, rcs_id , FIRST_VALUE(lcs_geom), FIRST_VALUE(lcs_name), first(rcs_geom), first(rcs_name)\nFROM joinresult\nGROUP BY (lcs_id, rcs_id)\n</code></pre> <p>The <code>FIRST_VALUE</code> function is to take the first value from a number of duplicate values.</p> <p>If you don't have a unique id for each geometry, you can also group by geometry itself. See below:</p> <pre><code>SELECT lcs_geom, rcs_geom, first(lcs_name), first(rcs_name)\nFROM joinresult\nGROUP BY (lcs_geom, rcs_geom)\n</code></pre> <p>Note</p> <p>If you are doing point-in-polygon join, this is not a problem and you can safely discard this issue. This issue only happens when you do polygon-polygon, polygon-linestring, linestring-linestring join.</p>"},{"location":"tutorial/flink/sql/#s2-for-distance-join","title":"S2 for distance join","text":"<p>This also works for distance join. You first need to use <code>ST_Buffer(geometry, distance)</code> to wrap one of your original geometry column. If your original geometry column contains points, this <code>ST_Buffer</code> will make them become circles with a radius of <code>distance</code>.</p> <p>For example. run this query first on the left table before Step 1.</p> <pre><code>SELECT id, ST_Buffer(geom, DISTANCE), name\nFROM lefts\n</code></pre> <p>Since the coordinates are in the longitude and latitude system, so the unit of <code>distance</code> should be degree instead of meter or mile. You will have to estimate the corresponding degrees based on your meter values. Please use this calculator.</p>"},{"location":"tutorial/flink/sql/#convert-spatial-table-to-spatial-datastream","title":"Convert Spatial Table to Spatial DataStream","text":""},{"location":"tutorial/flink/sql/#get-datastream","title":"Get DataStream","text":"<p>Use TableEnv's toDataStream function</p> <pre><code>DataStream&lt;Row&gt; geomStream = sedona.toDataStream(geomTable)\n</code></pre>"},{"location":"tutorial/flink/sql/#retrieve-geometries","title":"Retrieve Geometries","text":"<p>Then get the Geometry from each Row object using Map</p> <pre><code>import org.locationtech.jts.geom.Geometry;\n\nDataStream&lt;Geometry&gt; geometries = geomStream.map(new MapFunction&lt;Row, Geometry&gt;() {\n@Override\npublic Geometry map(Row value) throws Exception {\nreturn (Geometry) value.getField(0);\n}\n});\ngeometries.print();\n</code></pre> <p>The output will be</p> <pre><code>14&gt; POLYGON ((1.5 1.5, 1.5 2.5, 2.5 2.5, 2.5 1.5, 1.5 1.5))\n2&gt; POLYGON ((5.5 5.5, 5.5 6.5, 6.5 6.5, 6.5 5.5, 5.5 5.5))\n5&gt; POLYGON ((8.5 8.5, 8.5 9.5, 9.5 9.5, 9.5 8.5, 8.5 8.5))\n16&gt; POLYGON ((3.5 3.5, 3.5 4.5, 4.5 4.5, 4.5 3.5, 3.5 3.5))\n12&gt; POLYGON ((-0.5 -0.5, -0.5 0.5, 0.5 0.5, 0.5 -0.5, -0.5 -0.5))\n13&gt; POLYGON ((0.5 0.5, 0.5 1.5, 1.5 1.5, 1.5 0.5, 0.5 0.5))\n15&gt; POLYGON ((2.5 2.5, 2.5 3.5, 3.5 3.5, 3.5 2.5, 2.5 2.5))\n3&gt; POLYGON ((6.5 6.5, 6.5 7.5, 7.5 7.5, 7.5 6.5, 6.5 6.5))\n1&gt; POLYGON ((4.5 4.5, 4.5 5.5, 5.5 5.5, 5.5 4.5, 4.5 4.5))\n4&gt; POLYGON ((7.5 7.5, 7.5 8.5, 8.5 8.5, 8.5 7.5, 7.5 7.5))\n</code></pre>"},{"location":"tutorial/flink/sql/#store-non-spatial-attributes-in-geometries","title":"Store non-spatial attributes in Geometries","text":"<p>You can concatenate other non-spatial attributes and store them in Geometry's <code>userData</code> field so you can recover them later on. <code>userData</code> field can be any object type.</p> <pre><code>import org.locationtech.jts.geom.Geometry;\n\nDataStream&lt;Geometry&gt; geometries = geomStream.map(new MapFunction&lt;Row, Geometry&gt;() {\n@Override\npublic Geometry map(Row value) throws Exception {\nGeometry geom = (Geometry) value.getField(0);\ngeom.setUserData(value.getField(1));\nreturn geom;\n}\n});\ngeometries.print();\n</code></pre> <p>The <code>print</code> command will not print out <code>userData</code> field. But you can get it this way:</p> <pre><code>import org.locationtech.jts.geom.Geometry;\n\ngeometries.map(new MapFunction&lt;Geometry, String&gt;() {\n@Override\npublic String map(Geometry value) throws Exception\n{\nreturn (String) value.getUserData();\n}\n}).print();\n</code></pre> <p>The output will be</p> <pre><code>13&gt; polygon9\n6&gt; polygon2\n10&gt; polygon6\n11&gt; polygon7\n5&gt; polygon1\n12&gt; polygon8\n8&gt; polygon4\n4&gt; polygon0\n7&gt; polygon3\n9&gt; polygon5\n</code></pre>"},{"location":"tutorial/flink/sql/#convert-spatial-datastream-to-spatial-table","title":"Convert Spatial DataStream to Spatial Table","text":""},{"location":"tutorial/flink/sql/#create-geometries-using-sedona-formatutils","title":"Create Geometries using Sedona FormatUtils","text":"<ul> <li>Create a Geometry from a WKT string</li> </ul> <pre><code>import org.apache.sedona.common.utils.FormatUtils;\nimport org.locationtech.jts.geom.Geometry;\n\nDataStream&lt;Geometry&gt; geometries = text.map(new MapFunction&lt;String, Geometry&gt;() {\n@Override\npublic Geometry map(String value) throws Exception\n{\nFormatUtils formatUtils = new FormatUtils(FileDataSplitter.WKT, false);\nreturn formatUtils.readGeometry(value);\n}\n})\n</code></pre> <ul> <li>Create a Point from a String <code>1.1, 2.2</code>. Use <code>,</code> as the delimiter.</li> </ul> <pre><code>import org.apache.sedona.common.utils.FormatUtils;\nimport org.locationtech.jts.geom.Geometry;\n\nDataStream&lt;Geometry&gt; geometries = text.map(new MapFunction&lt;String, Geometry&gt;() {\n@Override\npublic Geometry map(String value) throws Exception\n{\nFormatUtils&lt;Geometry&gt; formatUtils = new FormatUtils(\",\", false, GeometryType.POINT);\nreturn formatUtils.readGeometry(value);\n}\n})\n</code></pre> <ul> <li>Create a Polygon from a String <code>1.1, 1.1, 10.1, 10.1</code>. This is a rectangle with (1.1, 1.1) and (10.1, 10.1) as their min/max corners.</li> </ul> <pre><code>import org.apache.sedona.common.utils.FormatUtils;\nimport org.locationtech.jts.geom.GeometryFactory;\nimport org.locationtech.jts.geom.Geometry;\n\nDataStream&lt;Geometry&gt; geometries = text.map(new MapFunction&lt;String, Geometry&gt;() {\n@Override\npublic Geometry map(String value) throws Exception\n{\n// Write some code to get four double type values: minX, minY, maxX, maxY\n...\nCoordinate[] coordinates = new Coordinate[5];\ncoordinates[0] = new Coordinate(minX, minY);\ncoordinates[1] = new Coordinate(minX, maxY);\ncoordinates[2] = new Coordinate(maxX, maxY);\ncoordinates[3] = new Coordinate(maxX, minY);\ncoordinates[4] = coordinates[0];\nGeometryFactory geometryFactory = new GeometryFactory();\nreturn geometryFactory.createPolygon(coordinates);\n}\n})\n</code></pre>"},{"location":"tutorial/flink/sql/#create-row-objects","title":"Create Row objects","text":"<p>Put a geometry in a Flink Row to a <code>geomStream</code>. Note that you can put other attributes in Row as well. This example uses a constant value <code>myName</code> for all geometries.</p> <pre><code>import org.apache.sedona.common.utils.FormatUtils;\nimport org.locationtech.jts.geom.Geometry;\nimport org.apache.flink.types.Row;\n\nDataStream&lt;Row&gt; geomStream = text.map(new MapFunction&lt;String, Row&gt;() {\n@Override\npublic Row map(String value) throws Exception\n{\nFormatUtils formatUtils = new FormatUtils(FileDataSplitter.WKT, false);\nreturn Row.of(formatUtils.readGeometry(value), \"myName\");\n}\n})\n</code></pre>"},{"location":"tutorial/flink/sql/#get-spatial-table","title":"Get Spatial Table","text":"<p>Use TableEnv's fromDataStream function, with two column names <code>geom</code> and <code>geom_name</code>. <pre><code>Table geomTable = sedona.fromDataStream(geomStream, \"geom\", \"geom_name\")\n</code></pre></p>"},{"location":"usecases/airport-country/","title":"Spatially aggregate airports per country","text":""},{"location":"usecases/foot-traffic/","title":"Match foot traffic to Seattle coffee shops","text":""},{"location":"usecases/raster/","title":"Raster image manipulation","text":""}]}